"""
HopRAGåœ–æ§‹å»ºå™¨æ¨¡çµ„
åŒ…å«PassageGraphBuilderã€PseudoQueryGeneratorã€EdgeConnector
"""

import json
import numpy as np
import networkx as nx
import time
import re
import jieba
from typing import Dict, List, Any, Optional, Tuple, Set
from dataclasses import dataclass, asdict
import asyncio

from .hoprag_config import HopRAGConfig, NodeType, EdgeType, QueryType, DEFAULT_CONFIG

@dataclass
class PseudoQuery:
    """å½æŸ¥è©¢æ•¸æ“šçµæ§‹"""
    query_id: str
    content: str
    query_type: QueryType
    embedding: Optional[np.ndarray] = None
    keywords: Optional[set] = None  # k: é—œéµè©é›†åˆ
    similarity_threshold: float = 0.7
    
    def __post_init__(self):
        if self.keywords is None:
            self.keywords = set()

@dataclass
class EdgeFeature:
    """é‚Šç‰¹å¾µæ•¸æ“šçµæ§‹ - æŒ‰ç…§è«–æ–‡è¦æ±‚"""
    query: str  # q_t*,j*^-
    combined_keywords: Set[str]  # k_t*,j*^- âˆª k_s,i^+
    embedding: np.ndarray  # v_t*,j*^-
    jaccard_score: float
    cosine_score: float
    mixed_score: float

@dataclass
class LegalNode:
    """æ³•å¾‹ç¯€é»æ•¸æ“šçµæ§‹"""
    node_id: str
    node_type: NodeType
    content: str
    contextualized_text: str
    law_name: str
    article_number: str
    item_number: Optional[str] = None
    parent_article_id: Optional[str] = None
    metadata: Dict[str, Any] = None
    
    # å½æŸ¥è©¢
    incoming_questions: List[str] = None
    outgoing_questions: List[str] = None
    pseudo_queries: Dict[str, List[PseudoQuery]] = None
    
    # åœ–çµæ§‹ç›¸é—œ
    outgoing_edges: List[str] = None
    incoming_edges: List[str] = None

    def __post_init__(self):
        if self.metadata is None:
            self.metadata = {}
        if self.incoming_questions is None:
            self.incoming_questions = []
        if self.outgoing_questions is None:
            self.outgoing_questions = []
        if self.pseudo_queries is None:
            self.pseudo_queries = {"incoming": [], "outgoing": []}
        if self.outgoing_edges is None:
            self.outgoing_edges = []
        if self.incoming_edges is None:
            self.incoming_edges = []

class PseudoQueryGenerator:
    """å½æŸ¥è©¢ç”Ÿæˆå™¨"""
    
    def __init__(self, llm_client, config: HopRAGConfig = DEFAULT_CONFIG):
        self.llm_client = llm_client
        self.config = config
        
    async def generate_pseudo_queries_for_node(self, node: LegalNode) -> LegalNode:
        """ç‚ºå–®å€‹ç¯€é»ç”Ÿæˆå…§å‘å’Œå¤–å‘å•é¡Œ"""
        print(f"ğŸ” ç‚ºç¯€é» {node.node_id} ç”Ÿæˆå½æŸ¥è©¢...")
        
        # ç”Ÿæˆå…§å‘å•é¡Œ
        incoming_questions = await self._generate_incoming_questions(node)
        
        # ç”Ÿæˆå¤–å‘å•é¡Œ
        outgoing_questions = await self._generate_outgoing_questions(node)
        
        # æ›´æ–°ç¯€é»
        node.incoming_questions = incoming_questions
        node.outgoing_questions = outgoing_questions
        
        # å‰µå»ºPseudoQueryå°è±¡
        node.pseudo_queries = {
            "incoming": [
                PseudoQuery(
                    query_id=f"{node.node_id}_in_{i}",
                    content=question,
                    query_type=QueryType.INCOMING,
                    keywords=self._extract_keywords_from_query(question)
                ) for i, question in enumerate(incoming_questions)
            ],
            "outgoing": [
                PseudoQuery(
                    query_id=f"{node.node_id}_out_{i}",
                    content=question,
                    query_type=QueryType.OUTGOING,
                    keywords=self._extract_keywords_from_query(question)
                ) for i, question in enumerate(outgoing_questions)
            ]
        }
        
        print(f"âœ… ç¯€é» {node.node_id} å½æŸ¥è©¢ç”Ÿæˆå®Œæˆï¼š{len(incoming_questions)}å€‹å…§å‘ï¼Œ{len(outgoing_questions)}å€‹å¤–å‘")
        return node
    
    async def _generate_incoming_questions(self, node: LegalNode) -> List[str]:
        """ç”Ÿæˆå…§å‘å•é¡Œ - å‹•æ…‹æ•¸é‡ç‰ˆæœ¬"""
        
        if self.config.use_dynamic_question_count:
            return await self._generate_dynamic_incoming_questions(node)
        else:
            return await self._generate_fixed_incoming_questions(node)
    
    async def _generate_dynamic_incoming_questions(self, node: LegalNode) -> List[str]:
        """å‹•æ…‹ç”Ÿæˆå…§å‘å•é¡Œ - è®“LLMæ±ºå®šé©ç•¶æ•¸é‡"""
        
        prompt = f"""
æ‚¨æ˜¯ä¸€ä½æ³•å¾‹å°ˆå®¶ï¼Œæ“…é•·æå‡ºå•é¡Œä¸¦ç²¾é€šä¸­æ–‡ã€‚æ‚¨éœ€è¦æ ¹æ“šæ³•å¾‹æ¢æ–‡ä¸­çš„å¹¾å¥é€£çºŒå¥å­ç”Ÿæˆå•é¡Œã€‚

ä»»å‹™ï¼šç‚ºä»¥ä¸‹æ³•å¾‹æ¢æ–‡ç”Ÿæˆå•é¡Œï¼Œé€™äº›å•é¡Œçš„ç­”æ¡ˆå¿…é ˆç›´æ¥ä¾†è‡ªè©²æ–‡æœ¬æœ¬èº«ã€‚

æ³•å¾‹æ¢æ–‡å…§å®¹ï¼š
{node.contextualized_text}

è¦æ±‚ï¼š
1. æ³•å¾‹è¦ç´ ï¼šæ¯å€‹å•é¡Œå¿…é ˆåŒ…å«ç‰¹å®šçš„æ³•å¾‹è¦ç´ ï¼ˆæ¢æ–‡è™Ÿã€é©ç”¨ç¯„åœã€æ³•å¾‹å¾Œæœã€æ¬Šåˆ©ç¾©å‹™ã€æ§‹æˆè¦ä»¶ç­‰ï¼‰æˆ–å…¶ä»–é—œéµç‰¹å¾µï¼Œä»¥æ¸›å°‘æ­§ç¾©ã€æ¾„æ¸…ä¸Šä¸‹æ–‡ä¸¦ç¢ºä¿è‡ªåŒ…å«æ€§ã€‚
2. çœç•¥/ç©ºç™½ï¼šæ‚¨å¯ä»¥çœç•¥æˆ–ç•™ç©ºå¥å­çš„é‡è¦éƒ¨åˆ†ä¾†å½¢æˆå•é¡Œï¼Œä½†ä¸æ‡‰å°åŒä¸€éƒ¨åˆ†æå‡ºå¤šå€‹å•é¡Œã€‚ä¸å¿…ç‚ºå¥å­çš„æ¯å€‹éƒ¨åˆ†éƒ½æå‡ºå•é¡Œã€‚
3. èˆ‡çœç•¥éƒ¨åˆ†çš„é€£è²«æ€§ï¼šç•¶è©¢å•çœç•¥çš„éƒ¨åˆ†æ™‚ï¼Œéçœç•¥çš„ä¿¡æ¯æ‡‰åŒ…å«åœ¨å•é¡Œä¸­ï¼Œåªè¦ä¿æŒé€£è²«æ€§ã€‚
4. å¤šæ¨£æ€§ï¼šä¸åŒçš„å•é¡Œæ‡‰é—œæ³¨å¥å­ä¸­ä¿¡æ¯çš„ä¸åŒæ–¹é¢ï¼Œç¢ºä¿å¤šæ¨£æ€§å’Œä»£è¡¨æ€§ã€‚
5. è¦†è“‹å’Œæ¨™æº–åŒ–ï¼šæ‰€æœ‰å•é¡Œçµåˆèµ·ä¾†æ‡‰æ¶µè“‹æ‰€æä¾›å¥å­çš„æ‰€æœ‰é—œéµé»ï¼Œæªè¾­æ‡‰æ¨™æº–åŒ–ã€‚
6. å®¢è§€å’Œè©³ç´°ï¼šå•é¡Œæ‡‰å®¢è§€ã€åŸºæ–¼äº‹å¯¦ä¸”æ³¨é‡ç´°ç¯€ï¼ˆä¾‹å¦‚ï¼Œè©¢å•æ³•å¾‹è¦ä»¶ã€é©ç”¨æ¢ä»¶ã€æ³•å¾‹æ•ˆæœç­‰ï¼‰ã€‚ç­”æ¡ˆå¿…é ˆåƒ…ä¾†è‡ªæ‰€æä¾›çš„å¥å­ã€‚

æ•¸é‡è¦æ±‚ï¼š
- æœ€å°‘ç”Ÿæˆ {self.config.min_incoming_questions} å€‹å•é¡Œ
- æœ€å¤šç”Ÿæˆ {self.config.max_incoming_questions} å€‹å•é¡Œ
- è«‹æ ¹æ“šæ³•å¾‹æ¢æ–‡çš„è¤‡é›œåº¦å’Œä¿¡æ¯è±å¯Œç¨‹åº¦ï¼Œç”Ÿæˆé©ç•¶æ•¸é‡çš„å•é¡Œä¾†å……åˆ†è¦†è“‹æ‰€æœ‰é‡è¦ä¿¡æ¯
- å¦‚æœæ¢æ–‡å…§å®¹ç°¡å–®ï¼Œç”Ÿæˆè¼ƒå°‘å•é¡Œï¼›å¦‚æœæ¢æ–‡å…§å®¹è¤‡é›œï¼Œç”Ÿæˆè¼ƒå¤šå•é¡Œ

ç¯„ä¾‹ï¼š
å¥å­åˆ—è¡¨ï¼š["ç¬¬å…«æ¢ï¼šè‘—ä½œæ¬Šäººäº«æœ‰ä¸‹åˆ—æ¬Šåˆ©ï¼šä¸€ã€é‡è£½æ¬Šï¼›äºŒã€å…¬é–‹æ’­é€æ¬Šï¼›ä¸‰ã€å…¬é–‹å‚³è¼¸æ¬Šã€‚"]
ç­”æ¡ˆç¯„ä¾‹ï¼š
{{
    "Question List": [
        "ç¬¬å…«æ¢è¦å®šäº†è‘—ä½œæ¬Šäººçš„å“ªäº›æ¬Šåˆ©ï¼Ÿ",
        "è‘—ä½œæ¬Šäººçš„é‡è£½æ¬Šæ˜¯ä»€éº¼ï¼Ÿ",
        "è‘—ä½œæ¬Šäººçš„å…¬é–‹æ’­é€æ¬Šæ˜¯ä»€éº¼ï¼Ÿ",
        "è‘—ä½œæ¬Šäººçš„å…¬é–‹å‚³è¼¸æ¬Šæ˜¯ä»€éº¼ï¼Ÿ",
        "ç¬¬å…«æ¢ç¸½å…±è¦å®šäº†å¹¾é …è‘—ä½œæ¬Šï¼Ÿ"
    ]
}}

è«‹ä»¥JSONæ ¼å¼è¿”å›ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
{{
    "Question List": [
        "å•é¡Œ1",
        "å•é¡Œ2",
        "å•é¡Œ3"
    ]
}}

è«‹ç¢ºä¿JSONæ ¼å¼æ­£ç¢ºï¼Œé¿å…ä¸å¿…è¦çš„è½‰ç¾©ã€æ›è¡Œæˆ–ç©ºæ ¼ã€‚æ‚¨é‚„æ‡‰è©²ç‰¹åˆ¥æ³¨æ„ç¢ºä¿ï¼Œé™¤äº†JSONå’Œåˆ—è¡¨æ ¼å¼æœ¬èº«ä½¿ç”¨é›™å¼•è™Ÿ(")å¤–ï¼Œå…¶ä»–æ‰€æœ‰é›™å¼•è™Ÿçš„å¯¦ä¾‹éƒ½æ‡‰æ›¿æ›ç‚ºå–®å¼•è™Ÿã€‚ä¾‹å¦‚ï¼Œä½¿ç”¨'è‘—ä½œæ¬Šæ³•'è€Œä¸æ˜¯"è‘—ä½œæ¬Šæ³•"ã€‚
è«‹ç¢ºä¿JSONæ ¼å¼æ­£ç¢ºï¼Œä¸è¦åŒ…å«ä»»ä½•å…¶ä»–æ–‡å­—ã€‚
"""
        
        try:
            response = await self.llm_client.generate_async(prompt)
            
            # è§£æJSONéŸ¿æ‡‰
            if response.strip().startswith('{'):
                result = json.loads(response.strip())
                questions = result.get('Question List', [])
                
                # é©—è­‰å•é¡Œæ•¸é‡
                questions = self._validate_question_count(
                    questions, 
                    self.config.min_incoming_questions, 
                    self.config.max_incoming_questions,
                    "incoming"
                )
                
                # é©—è­‰å•é¡Œè³ªé‡
                validated_questions = self._validate_questions(questions, "incoming")
                return validated_questions
            else:
                # å¦‚æœéŸ¿æ‡‰ä¸æ˜¯JSONæ ¼å¼ï¼Œå˜—è©¦æå–å•é¡Œ
                questions = self._extract_questions_from_text(response)
                questions = self._validate_question_count(
                    questions, 
                    self.config.min_incoming_questions, 
                    self.config.max_incoming_questions,
                    "incoming"
                )
                return questions
                
        except Exception as e:
            print(f"âŒ ç”Ÿæˆå‹•æ…‹å…§å‘å•é¡Œå¤±æ•—: {e}")
            # è¿”å›é»˜èªå•é¡Œ
            return self._generate_default_incoming_questions(node)
    
    async def _generate_fixed_incoming_questions(self, node: LegalNode) -> List[str]:
        """å›ºå®šæ•¸é‡ç”Ÿæˆå…§å‘å•é¡Œ - å‘å¾Œå…¼å®¹"""
        
        prompt = f"""
æ‚¨æ˜¯ä¸€ä½æ³•å¾‹å°ˆå®¶ï¼Œæ“…é•·æå‡ºå•é¡Œä¸¦ç²¾é€šä¸­æ–‡ã€‚æ‚¨éœ€è¦æ ¹æ“šæ³•å¾‹æ¢æ–‡ä¸­çš„å¹¾å¥é€£çºŒå¥å­ç”Ÿæˆå•é¡Œã€‚

ä»»å‹™ï¼šç‚ºä»¥ä¸‹æ³•å¾‹æ¢æ–‡ç”Ÿæˆå•é¡Œï¼Œé€™äº›å•é¡Œçš„ç­”æ¡ˆå¿…é ˆç›´æ¥ä¾†è‡ªè©²æ–‡æœ¬æœ¬èº«ã€‚

æ³•å¾‹æ¢æ–‡å…§å®¹ï¼š
{node.contextualized_text}

è¦æ±‚ï¼š
1. æ³•å¾‹è¦ç´ ï¼šæ¯å€‹å•é¡Œå¿…é ˆåŒ…å«ç‰¹å®šçš„æ³•å¾‹è¦ç´ ï¼ˆæ¢æ–‡è™Ÿã€é©ç”¨ç¯„åœã€æ³•å¾‹å¾Œæœã€æ¬Šåˆ©ç¾©å‹™ã€æ§‹æˆè¦ä»¶ç­‰ï¼‰æˆ–å…¶ä»–é—œéµç‰¹å¾µï¼Œä»¥æ¸›å°‘æ­§ç¾©ã€æ¾„æ¸…ä¸Šä¸‹æ–‡ä¸¦ç¢ºä¿è‡ªåŒ…å«æ€§ã€‚
2. çœç•¥/ç©ºç™½ï¼šæ‚¨å¯ä»¥çœç•¥æˆ–ç•™ç©ºå¥å­çš„é‡è¦éƒ¨åˆ†ä¾†å½¢æˆå•é¡Œï¼Œä½†ä¸æ‡‰å°åŒä¸€éƒ¨åˆ†æå‡ºå¤šå€‹å•é¡Œã€‚ä¸å¿…ç‚ºå¥å­çš„æ¯å€‹éƒ¨åˆ†éƒ½æå‡ºå•é¡Œã€‚
3. èˆ‡çœç•¥éƒ¨åˆ†çš„é€£è²«æ€§ï¼šç•¶è©¢å•çœç•¥çš„éƒ¨åˆ†æ™‚ï¼Œéçœç•¥çš„ä¿¡æ¯æ‡‰åŒ…å«åœ¨å•é¡Œä¸­ï¼Œåªè¦ä¿æŒé€£è²«æ€§ã€‚
4. å¤šæ¨£æ€§ï¼šä¸åŒçš„å•é¡Œæ‡‰é—œæ³¨å¥å­ä¸­ä¿¡æ¯çš„ä¸åŒæ–¹é¢ï¼Œç¢ºä¿å¤šæ¨£æ€§å’Œä»£è¡¨æ€§ã€‚
5. è¦†è“‹å’Œæ¨™æº–åŒ–ï¼šæ‰€æœ‰å•é¡Œçµåˆèµ·ä¾†æ‡‰æ¶µè“‹æ‰€æä¾›å¥å­çš„æ‰€æœ‰é—œéµé»ï¼Œæªè¾­æ‡‰æ¨™æº–åŒ–ã€‚
6. å®¢è§€å’Œè©³ç´°ï¼šå•é¡Œæ‡‰å®¢è§€ã€åŸºæ–¼äº‹å¯¦ä¸”æ³¨é‡ç´°ç¯€ï¼ˆä¾‹å¦‚ï¼Œè©¢å•æ³•å¾‹è¦ä»¶ã€é©ç”¨æ¢ä»¶ã€æ³•å¾‹æ•ˆæœç­‰ï¼‰ã€‚ç­”æ¡ˆå¿…é ˆåƒ…ä¾†è‡ªæ‰€æä¾›çš„å¥å­ã€‚

ç¯„ä¾‹ï¼š
å¥å­åˆ—è¡¨ï¼š["ç¬¬å…«æ¢ï¼šè‘—ä½œæ¬Šäººäº«æœ‰ä¸‹åˆ—æ¬Šåˆ©ï¼šä¸€ã€é‡è£½æ¬Šï¼›äºŒã€å…¬é–‹æ’­é€æ¬Šï¼›ä¸‰ã€å…¬é–‹å‚³è¼¸æ¬Šã€‚"]

ç­”æ¡ˆç¯„ä¾‹ï¼š
{{
    "Question List": [
        "ç¬¬å…«æ¢è¦å®šäº†è‘—ä½œæ¬Šäººçš„å“ªäº›æ¬Šåˆ©ï¼Ÿ",
        "è‘—ä½œæ¬Šäººçš„é‡è£½æ¬Šæ˜¯ä»€éº¼ï¼Ÿ",
        "è‘—ä½œæ¬Šäººçš„å…¬é–‹æ’­é€æ¬Šæ˜¯ä»€éº¼ï¼Ÿ",
        "è‘—ä½œæ¬Šäººçš„å…¬é–‹å‚³è¼¸æ¬Šæ˜¯ä»€éº¼ï¼Ÿ",
        "ç¬¬å…«æ¢ç¸½å…±è¦å®šäº†å¹¾é …è‘—ä½œæ¬Šï¼Ÿ"
    ]
}}

è«‹ç”Ÿæˆ{self.config.max_pseudo_queries_per_node}å€‹å•é¡Œï¼Œåš´æ ¼éµå¾ªJSONæ ¼å¼ï¼Œé¿å…ä¸å¿…è¦çš„è½‰ç¾©ã€æ›è¡Œæˆ–ç©ºæ ¼ã€‚æ‚¨é‚„æ‡‰è©²ç‰¹åˆ¥æ³¨æ„ç¢ºä¿ï¼Œé™¤äº†JSONå’Œåˆ—è¡¨æ ¼å¼æœ¬èº«ä½¿ç”¨é›™å¼•è™Ÿ(")å¤–ï¼Œå…¶ä»–æ‰€æœ‰é›™å¼•è™Ÿçš„å¯¦ä¾‹éƒ½æ‡‰æ›¿æ›ç‚ºå–®å¼•è™Ÿã€‚ä¾‹å¦‚ï¼Œä½¿ç”¨'è‘—ä½œæ¬Šæ³•'è€Œä¸æ˜¯"è‘—ä½œæ¬Šæ³•"ã€‚

è«‹ä»¥JSONæ ¼å¼è¿”å›ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
{{
    "Question List": [
        "å•é¡Œ1",
        "å•é¡Œ2",
        "å•é¡Œ3",
        "å•é¡Œ4",
        "å•é¡Œ5"
    ]
}}

è«‹ç¢ºä¿JSONæ ¼å¼æ­£ç¢ºï¼Œä¸è¦åŒ…å«ä»»ä½•å…¶ä»–æ–‡å­—ã€‚
"""
        
        try:
            response = await self.llm_client.generate_async(prompt)
            
            # è§£æJSONéŸ¿æ‡‰
            if response.strip().startswith('{'):
                result = json.loads(response.strip())
                questions = result.get('Question List', [])
                
                # é©—è­‰å•é¡Œè³ªé‡
                validated_questions = self._validate_questions(questions, "incoming")
                # æ ¹æ“šé…ç½®æ±ºå®šæ˜¯å¦é™åˆ¶æ•¸é‡
                if self.config.use_dynamic_question_count:
                    # å‹•æ…‹æ¨¡å¼ï¼šä¸é™åˆ¶æ•¸é‡ï¼Œè®“LLMæ±ºå®š
                    return validated_questions
                else:
                    # å›ºå®šæ¨¡å¼ï¼šä½¿ç”¨é…ç½®çš„æ•¸é‡é™åˆ¶
                    return validated_questions[:self.config.max_pseudo_queries_per_node]
            else:
                # å¦‚æœéŸ¿æ‡‰ä¸æ˜¯JSONæ ¼å¼ï¼Œå˜—è©¦æå–å•é¡Œ
                questions = self._extract_questions_from_text(response)
                # æ ¹æ“šé…ç½®æ±ºå®šæ˜¯å¦é™åˆ¶æ•¸é‡
                if self.config.use_dynamic_question_count:
                    # å‹•æ…‹æ¨¡å¼ï¼šä¸é™åˆ¶æ•¸é‡ï¼Œè®“LLMæ±ºå®š
                    return questions
                else:
                    # å›ºå®šæ¨¡å¼ï¼šä½¿ç”¨é…ç½®çš„æ•¸é‡é™åˆ¶
                    return questions[:self.config.max_pseudo_queries_per_node]
                
        except Exception as e:
            print(f"âŒ ç”Ÿæˆå…§å‘å•é¡Œå¤±æ•—: {e}")
            # è¿”å›é»˜èªå•é¡Œ
            return self._generate_default_incoming_questions(node)
    
    async def _generate_outgoing_questions(self, node: LegalNode) -> List[str]:
        """ç”Ÿæˆå¤–å‘å•é¡Œ - å‹•æ…‹æ•¸é‡ç‰ˆæœ¬"""
        
        if self.config.use_dynamic_question_count:
            return await self._generate_dynamic_outgoing_questions(node)
        else:
            return await self._generate_fixed_outgoing_questions(node)
    
    async def _generate_dynamic_outgoing_questions(self, node: LegalNode) -> List[str]:
        """å‹•æ…‹ç”Ÿæˆå¤–å‘å•é¡Œ - è®“LLMæ±ºå®šé©ç•¶æ•¸é‡"""
        
        prompt = f"""
æ‚¨æ˜¯ä¸€ä½æ³•å¾‹å°ˆå®¶ï¼Œæ“…é•·æå‡ºæ·±åˆ»å•é¡Œä¸¦ç²¾é€šä¸­æ–‡ã€‚æ‚¨éœ€è¦æ ¹æ“šæ³•å¾‹æ¢æ–‡ä¸­çš„å¹¾å¥é€£çºŒå¥å­ç”Ÿæˆå¾ŒçºŒå•é¡Œã€‚

ä»»å‹™ï¼šç‚ºä»¥ä¸‹æ³•å¾‹æ¢æ–‡ç”Ÿæˆå¾ŒçºŒå•é¡Œï¼Œé€™äº›å•é¡Œçš„ç­”æ¡ˆä¸åœ¨çµ¦å®šçš„å¥å­ä¸­æ‰¾åˆ°ã€‚

å¾ŒçºŒå•é¡Œå®šç¾©ï¼šç­”æ¡ˆä¸åœ¨çµ¦å®šçš„å¥å­ä¸­æ‰¾åˆ°ã€‚ç­”æ¡ˆå¯ä»¥å¾çµ¦å®šå¥å­ä¹‹å‰æˆ–ä¹‹å¾Œçš„ä¸Šä¸‹æ–‡ã€æ¶µè“‹ç›¸åŒæ³•å¾‹äº‹ä»¶çš„ç›¸é—œæ³•æ¢ã€æˆ–å¾çµ¦å®šå¥å­ä¸­é—œéµè©çš„é‚è¼¯ã€å› æœæˆ–æ™‚é–“å»¶ä¼¸ä¸­æ¨æ–·å‡ºä¾†ã€‚

æ³•å¾‹æ¢æ–‡å…§å®¹ï¼š
{node.contextualized_text}

è¦æ±‚ï¼š
1. æ³•å¾‹è¦ç´ ï¼šæ¯å€‹å•é¡Œå¿…é ˆåŒ…å«ç‰¹å®šçš„æ³•å¾‹è¦ç´ ï¼ˆæ¢æ–‡è™Ÿã€é©ç”¨ç¯„åœã€æ³•å¾‹å¾Œæœã€æ¬Šåˆ©ç¾©å‹™ã€æ§‹æˆè¦ä»¶ç­‰ï¼‰æˆ–å…¶ä»–é—œéµç‰¹å¾µï¼Œä»¥æ¸›å°‘æ­§ç¾©ä¸¦ç¢ºä¿å•é¡Œçš„ç¨ç«‹æ€§ã€‚
2. å¤šæ¨£æ€§å’Œå®¢è§€æ€§ï¼šä¸åŒçš„å¾ŒçºŒå•é¡Œæ‡‰é—œæ³¨é€™äº›å¥å­æ‰€ä»£è¡¨çš„æ•´é«”æ³•å¾‹äº‹ä»¶çš„å¤šæ¨£åŒ–ã€å®¢è§€æ–¹é¢ï¼Œç¢ºä¿å¤šæ¨£æ€§å’Œä»£è¡¨æ€§ã€‚å„ªå…ˆè€ƒæ…®å®¢è§€å•é¡Œã€‚
3. å› æœå’Œé‚è¼¯é—œä¿‚ï¼šæ ¹æ“šçµ¦å®šçš„å¥å­ï¼Œç”Ÿæˆæ¶‰åŠå› æœé—œä¿‚ã€ä¸¦è¡Œé—œä¿‚ã€åºåˆ—ã€é€²ç¨‹ã€é€£æ¥å’Œå…¶ä»–é‚è¼¯æ–¹é¢çš„å•é¡Œã€‚å¯æ¢ç´¢çš„é ˜åŸŸåŒ…æ‹¬ä½†ä¸é™æ–¼ï¼šæ³•å¾‹äº‹ä»¶çš„èƒŒæ™¯ã€ä¿¡æ¯ã€åŸå› ã€å½±éŸ¿ã€æ„ç¾©ã€ç™¼å±•è¶¨å‹¢æˆ–ç›¸é—œå€‹äººçš„è§€é»ã€‚

æ•¸é‡è¦æ±‚ï¼š
- æœ€å°‘ç”Ÿæˆ {self.config.min_outgoing_questions} å€‹å•é¡Œ
- æœ€å¤šç”Ÿæˆ {self.config.max_outgoing_questions} å€‹å•é¡Œ
- è«‹æ ¹æ“šæ³•å¾‹æ¢æ–‡çš„è¤‡é›œåº¦å’Œå»¶ä¼¸æ€§ï¼Œç”Ÿæˆé©ç•¶æ•¸é‡çš„å•é¡Œä¾†å……åˆ†æ¢ç´¢é‚è¼¯é€£æ¥
- å¦‚æœæ¢æ–‡å…§å®¹ç°¡å–®ï¼Œç”Ÿæˆè¼ƒå°‘å•é¡Œï¼›å¦‚æœæ¢æ–‡å…§å®¹è¤‡é›œä¸”æœ‰å¤šå€‹å»¶ä¼¸é»ï¼Œç”Ÿæˆè¼ƒå¤šå•é¡Œ

ç¯„ä¾‹ï¼š
å¥å­åˆ—è¡¨ï¼š["ç¬¬å…«æ¢ï¼šè‘—ä½œæ¬Šäººäº«æœ‰ä¸‹åˆ—æ¬Šåˆ©ï¼šä¸€ã€é‡è£½æ¬Šï¼›äºŒã€å…¬é–‹æ’­é€æ¬Šï¼›ä¸‰ã€å…¬é–‹å‚³è¼¸æ¬Šã€‚"]
ç­”æ¡ˆç¯„ä¾‹ï¼š
{{
    "Question List": [
        "è‘—ä½œæ¬Šäººå¦‚ä½•è¡Œä½¿é‡è£½æ¬Šï¼Ÿ",
        "é‡è£½æ¬Šçš„ä¿è­·æœŸé™æ˜¯å¤šä¹…ï¼Ÿ",
        "é•åé‡è£½æ¬Šæœƒæœ‰ä»€éº¼æ³•å¾‹å¾Œæœï¼Ÿ",
        "å…¬é–‹æ’­é€æ¬Šèˆ‡å…¬é–‹å‚³è¼¸æ¬Šæœ‰ä»€éº¼å€åˆ¥ï¼Ÿ",
        "è‘—ä½œæ¬Šäººå¦‚ä½•è­‰æ˜å…¶æ¬Šåˆ©å—åˆ°ä¾µå®³ï¼Ÿ"
    ]
}}

è«‹ä»¥JSONæ ¼å¼è¿”å›ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
{{
    "Question List": [
        "å•é¡Œ1",
        "å•é¡Œ2",
        "å•é¡Œ3"
    ]
}}

è«‹ç¢ºä¿JSONæ ¼å¼æ­£ç¢ºï¼Œé¿å…ä¸å¿…è¦çš„è½‰ç¾©ã€æ›è¡Œæˆ–ç©ºæ ¼ã€‚æ‚¨é‚„æ‡‰è©²ç‰¹åˆ¥æ³¨æ„ç¢ºä¿ï¼Œé™¤äº†JSONå’Œåˆ—è¡¨æ ¼å¼æœ¬èº«ä½¿ç”¨é›™å¼•è™Ÿ(")å¤–ï¼Œå…¶ä»–æ‰€æœ‰é›™å¼•è™Ÿçš„å¯¦ä¾‹éƒ½æ‡‰æ›¿æ›ç‚ºå–®å¼•è™Ÿã€‚ä¾‹å¦‚ï¼Œä½¿ç”¨'è‘—ä½œæ¬Šæ³•'è€Œä¸æ˜¯"è‘—ä½œæ¬Šæ³•"ã€‚
è«‹ç¢ºä¿JSONæ ¼å¼æ­£ç¢ºï¼Œä¸è¦åŒ…å«ä»»ä½•å…¶ä»–æ–‡å­—ã€‚
"""
        
        try:
            response = await self.llm_client.generate_async(prompt)
            
            # è§£æJSONéŸ¿æ‡‰
            if response.strip().startswith('{'):
                result = json.loads(response.strip())
                questions = result.get('Question List', [])
                
                # é©—è­‰å•é¡Œæ•¸é‡
                questions = self._validate_question_count(
                    questions, 
                    self.config.min_outgoing_questions, 
                    self.config.max_outgoing_questions,
                    "outgoing"
                )
                
                # é©—è­‰å•é¡Œè³ªé‡
                validated_questions = self._validate_questions(questions, "outgoing")
                return validated_questions
            else:
                # å¦‚æœéŸ¿æ‡‰ä¸æ˜¯JSONæ ¼å¼ï¼Œå˜—è©¦æå–å•é¡Œ
                questions = self._extract_questions_from_text(response)
                questions = self._validate_question_count(
                    questions, 
                    self.config.min_outgoing_questions, 
                    self.config.max_outgoing_questions,
                    "outgoing"
                )
                return questions
                
        except Exception as e:
            print(f"âŒ ç”Ÿæˆå‹•æ…‹å¤–å‘å•é¡Œå¤±æ•—: {e}")
            # è¿”å›é»˜èªå•é¡Œ
            return self._generate_default_outgoing_questions(node)
    
    async def _generate_fixed_outgoing_questions(self, node: LegalNode) -> List[str]:
        """å›ºå®šæ•¸é‡ç”Ÿæˆå¤–å‘å•é¡Œ - å‘å¾Œå…¼å®¹"""
        
        prompt = f"""
æ‚¨æ˜¯ä¸€ä½æ³•å¾‹å°ˆå®¶ï¼Œæ“…é•·æå‡ºæ·±åˆ»å•é¡Œä¸¦ç²¾é€šä¸­æ–‡ã€‚æ‚¨éœ€è¦æ ¹æ“šæ³•å¾‹æ¢æ–‡ä¸­çš„å¹¾å¥é€£çºŒå¥å­ç”Ÿæˆå¾ŒçºŒå•é¡Œã€‚

ä»»å‹™ï¼šç‚ºä»¥ä¸‹æ³•å¾‹æ¢æ–‡ç”Ÿæˆå¾ŒçºŒå•é¡Œï¼Œé€™äº›å•é¡Œçš„ç­”æ¡ˆä¸åœ¨çµ¦å®šçš„å¥å­ä¸­æ‰¾åˆ°ã€‚

å¾ŒçºŒå•é¡Œå®šç¾©ï¼šç­”æ¡ˆä¸åœ¨çµ¦å®šçš„å¥å­ä¸­æ‰¾åˆ°ã€‚ç­”æ¡ˆå¯ä»¥å¾çµ¦å®šå¥å­ä¹‹å‰æˆ–ä¹‹å¾Œçš„ä¸Šä¸‹æ–‡ã€æ¶µè“‹ç›¸åŒæ³•å¾‹äº‹ä»¶çš„ç›¸é—œæ³•æ¢ã€æˆ–å¾çµ¦å®šå¥å­ä¸­é—œéµè©çš„é‚è¼¯ã€å› æœæˆ–æ™‚é–“å»¶ä¼¸ä¸­æ¨æ–·å‡ºä¾†ã€‚

æ³•å¾‹æ¢æ–‡å…§å®¹ï¼š
{node.contextualized_text}

è¦æ±‚ï¼š
1. æ³•å¾‹è¦ç´ ï¼šæ¯å€‹å•é¡Œå¿…é ˆåŒ…å«ç‰¹å®šçš„æ³•å¾‹è¦ç´ ï¼ˆæ¢æ–‡è™Ÿã€é©ç”¨ç¯„åœã€æ³•å¾‹å¾Œæœã€æ¬Šåˆ©ç¾©å‹™ã€æ§‹æˆè¦ä»¶ç­‰ï¼‰æˆ–å…¶ä»–é—œéµç‰¹å¾µï¼Œä»¥æ¸›å°‘æ­§ç¾©ä¸¦ç¢ºä¿å•é¡Œçš„ç¨ç«‹æ€§ã€‚
2. å¤šæ¨£æ€§å’Œå®¢è§€æ€§ï¼šä¸åŒçš„å¾ŒçºŒå•é¡Œæ‡‰é—œæ³¨é€™äº›å¥å­æ‰€ä»£è¡¨çš„æ•´é«”æ³•å¾‹äº‹ä»¶çš„å¤šæ¨£åŒ–ã€å®¢è§€æ–¹é¢ï¼Œç¢ºä¿å¤šæ¨£æ€§å’Œä»£è¡¨æ€§ã€‚å„ªå…ˆè€ƒæ…®å®¢è§€å•é¡Œã€‚
3. å› æœå’Œé‚è¼¯é—œä¿‚ï¼šæ ¹æ“šçµ¦å®šçš„å¥å­ï¼Œç”Ÿæˆæ¶‰åŠå› æœé—œä¿‚ã€ä¸¦è¡Œé—œä¿‚ã€åºåˆ—ã€é€²ç¨‹ã€é€£æ¥å’Œå…¶ä»–é‚è¼¯æ–¹é¢çš„å•é¡Œã€‚å¯æ¢ç´¢çš„é ˜åŸŸåŒ…æ‹¬ä½†ä¸é™æ–¼ï¼šæ³•å¾‹äº‹ä»¶çš„èƒŒæ™¯ã€ä¿¡æ¯ã€åŸå› ã€å½±éŸ¿ã€æ„ç¾©ã€ç™¼å±•è¶¨å‹¢æˆ–ç›¸é—œå€‹äººçš„è§€é»ã€‚

ç¯„ä¾‹ï¼š
å¥å­åˆ—è¡¨ï¼š["ç¬¬å…«æ¢ï¼šè‘—ä½œæ¬Šäººäº«æœ‰ä¸‹åˆ—æ¬Šåˆ©ï¼šä¸€ã€é‡è£½æ¬Šï¼›äºŒã€å…¬é–‹æ’­é€æ¬Šï¼›ä¸‰ã€å…¬é–‹å‚³è¼¸æ¬Šã€‚"]

ç­”æ¡ˆç¯„ä¾‹ï¼š
{{
    "Question List": [
        "è‘—ä½œæ¬Šäººå¦‚ä½•è¡Œä½¿é‡è£½æ¬Šï¼Ÿ",
        "é‡è£½æ¬Šçš„ä¿è­·æœŸé™æ˜¯å¤šä¹…ï¼Ÿ",
        "é•åé‡è£½æ¬Šæœƒæœ‰ä»€éº¼æ³•å¾‹å¾Œæœï¼Ÿ",
        "å…¬é–‹æ’­é€æ¬Šèˆ‡å…¬é–‹å‚³è¼¸æ¬Šæœ‰ä»€éº¼å€åˆ¥ï¼Ÿ",
        "è‘—ä½œæ¬Šäººå¦‚ä½•è­‰æ˜å…¶æ¬Šåˆ©å—åˆ°ä¾µå®³ï¼Ÿ"
    ]
}}

è«‹ç”Ÿæˆ{self.config.max_pseudo_queries_per_node}å€‹å•é¡Œï¼Œåš´æ ¼éµå¾ªJSONæ ¼å¼ï¼Œé¿å…ä¸å¿…è¦çš„è½‰ç¾©ã€æ›è¡Œæˆ–ç©ºæ ¼ã€‚æ‚¨é‚„æ‡‰è©²ç‰¹åˆ¥æ³¨æ„ç¢ºä¿ï¼Œé™¤äº†JSONå’Œåˆ—è¡¨æ ¼å¼æœ¬èº«ä½¿ç”¨é›™å¼•è™Ÿ(")å¤–ï¼Œå…¶ä»–æ‰€æœ‰é›™å¼•è™Ÿçš„å¯¦ä¾‹éƒ½æ‡‰æ›¿æ›ç‚ºå–®å¼•è™Ÿã€‚ä¾‹å¦‚ï¼Œä½¿ç”¨'è‘—ä½œæ¬Šæ³•'è€Œä¸æ˜¯"è‘—ä½œæ¬Šæ³•"ã€‚

è«‹ä»¥JSONæ ¼å¼è¿”å›ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
{{
    "Question List": [
        "å•é¡Œ1",
        "å•é¡Œ2",
        "å•é¡Œ3",
        "å•é¡Œ4",
        "å•é¡Œ5"
    ]
}}

è«‹ç¢ºä¿JSONæ ¼å¼æ­£ç¢ºï¼Œä¸è¦åŒ…å«ä»»ä½•å…¶ä»–æ–‡å­—ã€‚
"""
        
        try:
            response = await self.llm_client.generate_async(prompt)
            
            # è§£æJSONéŸ¿æ‡‰
            if response.strip().startswith('{'):
                result = json.loads(response.strip())
                questions = result.get('Question List', [])
                
                # é©—è­‰å•é¡Œè³ªé‡
                validated_questions = self._validate_questions(questions, "outgoing")
                # æ ¹æ“šé…ç½®æ±ºå®šæ˜¯å¦é™åˆ¶æ•¸é‡
                if self.config.use_dynamic_question_count:
                    # å‹•æ…‹æ¨¡å¼ï¼šä¸é™åˆ¶æ•¸é‡ï¼Œè®“LLMæ±ºå®š
                    return validated_questions
                else:
                    # å›ºå®šæ¨¡å¼ï¼šä½¿ç”¨é…ç½®çš„æ•¸é‡é™åˆ¶
                    return validated_questions[:self.config.max_pseudo_queries_per_node]
            else:
                # å¦‚æœéŸ¿æ‡‰ä¸æ˜¯JSONæ ¼å¼ï¼Œå˜—è©¦æå–å•é¡Œ
                questions = self._extract_questions_from_text(response)
                # æ ¹æ“šé…ç½®æ±ºå®šæ˜¯å¦é™åˆ¶æ•¸é‡
                if self.config.use_dynamic_question_count:
                    # å‹•æ…‹æ¨¡å¼ï¼šä¸é™åˆ¶æ•¸é‡ï¼Œè®“LLMæ±ºå®š
                    return questions
                else:
                    # å›ºå®šæ¨¡å¼ï¼šä½¿ç”¨é…ç½®çš„æ•¸é‡é™åˆ¶
                    return questions[:self.config.max_pseudo_queries_per_node]
                
        except Exception as e:
            print(f"âŒ ç”Ÿæˆå¤–å‘å•é¡Œå¤±æ•—: {e}")
            # è¿”å›é»˜èªå•é¡Œ
            return self._generate_default_outgoing_questions(node)
    
    def _validate_questions(self, questions: List[str], question_type: str) -> List[str]:
        """é©—è­‰å•é¡Œè³ªé‡"""
        validated = []
        
        for question in questions:
            if isinstance(question, str) and len(question.strip()) > 10:
                # åŸºæœ¬é©—è­‰ï¼šå•é¡Œé•·åº¦ã€åŒ…å«å•è™Ÿç­‰
                question = question.strip()
                if question.endswith('ï¼Ÿ') or question.endswith('?'):
                    validated.append(question)
                else:
                    # å¦‚æœæ²’æœ‰å•è™Ÿï¼Œæ·»åŠ ä¸€å€‹
                    validated.append(question + 'ï¼Ÿ')
        
        return validated
    
    def _validate_question_count(self, questions: List[str], min_count: int, max_count: int, question_type: str) -> List[str]:
        """é©—è­‰å•é¡Œæ•¸é‡æ˜¯å¦ç¬¦åˆè¦æ±‚"""
        if not questions:
            print(f"âš ï¸ {question_type}å•é¡Œç‚ºç©ºï¼Œä½¿ç”¨é»˜èªå•é¡Œ")
            return self._get_default_questions(question_type, min_count)
        
        question_count = len(questions)
        
        if question_count < min_count:
            print(f"âš ï¸ {question_type}å•é¡Œæ•¸é‡ä¸è¶³ï¼ˆ{question_count} < {min_count}ï¼‰ï¼Œè£œå……é»˜èªå•é¡Œ")
            # è£œå……é»˜èªå•é¡Œåˆ°æœ€å°‘æ•¸é‡
            default_questions = self._get_default_questions(question_type, min_count - question_count)
            questions.extend(default_questions)
        elif question_count > max_count:
            print(f"âš ï¸ {question_type}å•é¡Œæ•¸é‡éå¤šï¼ˆ{question_count} > {max_count}ï¼‰ï¼Œæˆªå–å‰{max_count}å€‹")
            questions = questions[:max_count]
        
        print(f"âœ… {question_type}å•é¡Œæ•¸é‡é©—è­‰å®Œæˆï¼š{len(questions)}å€‹å•é¡Œ")
        return questions
    
    def _get_default_questions(self, question_type: str, count: int) -> List[str]:
        """ç²å–é»˜èªå•é¡Œ"""
        if question_type == "incoming":
            return [
                f"æ­¤æ¢æ–‡çš„ä¸»è¦å…§å®¹æ˜¯ä»€éº¼ï¼Ÿ",
                f"æ­¤æ¢æ–‡è¦å®šäº†å“ªäº›æ³•å¾‹è¦ä»¶ï¼Ÿ",
                f"æ­¤æ¢æ–‡çš„é©ç”¨ç¯„åœæ˜¯ä»€éº¼ï¼Ÿ"
            ][:count]
        else:  # outgoing
            return [
                f"é•åæ­¤æ¢æ–‡æœƒæœ‰ä»€éº¼æ³•å¾‹å¾Œæœï¼Ÿ",
                f"æ­¤æ¢æ–‡èˆ‡å…¶ä»–ç›¸é—œæ³•æ¢æœ‰ä»€éº¼é—œä¿‚ï¼Ÿ",
                f"æ­¤æ¢æ–‡åœ¨å¯¦å‹™ä¸­å¦‚ä½•æ‡‰ç”¨ï¼Ÿ",
                f"æ­¤æ¢æ–‡çš„ç«‹æ³•ç›®çš„æ˜¯ä»€éº¼ï¼Ÿ"
            ][:count]
    
    def _extract_questions_from_text(self, text: str) -> List[str]:
        """å¾æ–‡æœ¬ä¸­æå–å•é¡Œ"""
        questions = []
        lines = text.split('\n')
        
        for line in lines:
            line = line.strip()
            if ('ï¼Ÿ' in line or '?' in line) and len(line) > 10:
                # æ¸…ç†è¡Œå…§å®¹
                line = line.replace('- ', '').replace('* ', '').replace('1. ', '').replace('2. ', '').replace('3. ', '')
                questions.append(line)
        
        # æ ¹æ“šé…ç½®æ±ºå®šæ˜¯å¦é™åˆ¶æ•¸é‡
        if self.config.use_dynamic_question_count:
            # å‹•æ…‹æ¨¡å¼ï¼šä¸é™åˆ¶æ•¸é‡ï¼Œè®“LLMæ±ºå®š
            return questions
        else:
            # å›ºå®šæ¨¡å¼ï¼šä½¿ç”¨é…ç½®çš„æ•¸é‡é™åˆ¶
            return questions[:self.config.max_pseudo_queries_per_node]
    
    def _generate_default_incoming_questions(self, node: LegalNode) -> List[str]:
        """ç”Ÿæˆé»˜èªå…§å‘å•é¡Œ"""
        article_num = node.article_number
        
        return [
            f"{article_num}çš„ä¸»è¦å…§å®¹æ˜¯ä»€éº¼ï¼Ÿ",
            f"{article_num}è¦å®šäº†å“ªäº›æ³•å¾‹è¦ä»¶ï¼Ÿ",
            f"æ ¹æ“š{article_num}ï¼Œç›¸é—œçš„å®šç¾©æ˜¯ä»€éº¼ï¼Ÿ",
            f"{article_num}çš„é©ç”¨ç¯„åœæ˜¯ä»€éº¼ï¼Ÿ",
            f"{article_num}è¦å®šäº†ä»€éº¼æ³•å¾‹å¾Œæœï¼Ÿ"
        ][:self.config.max_pseudo_queries_per_node]
    
    def _generate_default_outgoing_questions(self, node: LegalNode) -> List[str]:
        """ç”Ÿæˆé»˜èªå¤–å‘å•é¡Œ"""
        article_num = node.article_number
        
        return [
            f"é•å{article_num}æœƒæœ‰ä»€éº¼æ³•å¾‹å¾Œæœï¼Ÿ",
            f"å¦‚ä½•ç”³è«‹{article_num}è¦å®šçš„æ¬Šåˆ©ï¼Ÿ",
            f"{article_num}èˆ‡å…¶ä»–æ³•æ¢æœ‰ä»€éº¼é—œè¯ï¼Ÿ",
            f"åœ¨ä»€éº¼æƒ…æ³ä¸‹é©ç”¨{article_num}ï¼Ÿ",
            f"{article_num}çš„å¯¦å‹™æ“ä½œç¨‹åºæ˜¯ä»€éº¼ï¼Ÿ"
        ][:self.config.max_pseudo_queries_per_node]
    
    def _extract_keywords_from_query(self, query_content: str) -> Set[str]:
        """å¾æŸ¥è©¢å…§å®¹ä¸­æå–é—œéµè©"""
        # æ³•å¾‹é ˜åŸŸåœç”¨è©
        stop_words = {
            'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº', 'éƒ½', 'ä¸€', 'ä¸€å€‹', 'ä¸Š', 'ä¹Ÿ', 'å¾ˆ', 'åˆ°', 'èªª', 'è¦', 'å»', 'ä½ ', 'æœƒ', 'è‘—', 'æ²’æœ‰', 'çœ‹', 'å¥½', 'è‡ªå·±', 'é€™',
            'ä»€éº¼', 'å¦‚ä½•', 'ç‚ºä»€éº¼', 'å“ªäº›', 'ä»€éº¼æ™‚å€™', 'å“ªè£¡', 'å¤šå°‘', 'æ€éº¼', 'æ˜¯å¦', 'èƒ½å¦', 'å¯ä»¥', 'æ‡‰è©²', 'å¿…é ˆ', 'éœ€è¦', 'è¦æ±‚', 'è¦å®š', 'æ¢æ–‡', 'æ³•å¾‹', 'æ³•æ¢', 'æ¢æ¬¾', 'é …ç›®', 'å…§å®¹', 'å®šç¾©', 'é©ç”¨', 'ç¯„åœ', 'å¾Œæœ', 'ç¨‹åº', 'æ¬Šåˆ©', 'ç¾©å‹™', 'è²¬ä»»', 'è™•ç½°', 'ç½°å‰‡', 'é•å', 'ç”³è«‹', 'å¯¦å‹™', 'æ“ä½œ', 'é—œè¯', 'æƒ…æ³', 'æ¢ä»¶', 'æ¨™æº–', 'åŸå‰‡', 'æ–¹æ³•', 'æ–¹å¼', 'æµç¨‹', 'æ­¥é©Ÿ', 'éç¨‹', 'çµæœ', 'æ•ˆæœ', 'å½±éŸ¿', 'æ„ç¾©', 'ä½œç”¨', 'åŠŸèƒ½', 'ç‰¹é»', 'æ€§è³ª', 'é¡å‹', 'ç¨®é¡', 'åˆ†é¡', 'å€åˆ¥', 'å·®ç•°', 'ç›¸åŒ', 'ä¸åŒ', 'é¡ä¼¼', 'ç›¸é—œ', 'ç„¡é—œ', 'é‡è¦', 'ä¸»è¦', 'åŸºæœ¬', 'æ ¸å¿ƒ', 'é—œéµ', 'å¿…è¦', 'å……åˆ†', 'æœ‰æ•ˆ', 'ç„¡æ•ˆ', 'åˆæ³•', 'éæ³•', 'æ­£ç•¶', 'ä¸æ­£ç•¶', 'åˆç†', 'ä¸åˆç†', 'é©ç•¶', 'ä¸é©ç•¶', 'æ­£ç¢º', 'éŒ¯èª¤', 'æº–ç¢º', 'ä¸æº–ç¢º', 'å®Œæ•´', 'ä¸å®Œæ•´', 'æ¸…æ¥š', 'ä¸æ¸…æ¥š', 'æ˜ç¢º', 'ä¸æ˜ç¢º', 'å…·é«”', 'æŠ½è±¡', 'è©³ç´°', 'ç°¡ç•¥', 'å…¨é¢', 'ç‰‡é¢', 'å®¢è§€', 'ä¸»è§€', 'å…¬æ­£', 'ä¸å…¬æ­£', 'å…¬å¹³', 'ä¸å…¬å¹³', 'å¹³ç­‰', 'ä¸å¹³ç­‰', 'è‡ªç”±', 'é™åˆ¶', 'ä¿è­·', 'ä¿éšœ', 'ç¶­è­·', 'ä¿ƒé€²', 'ç™¼å±•', 'æ”¹å–„', 'æé«˜', 'é™ä½', 'å¢åŠ ', 'æ¸›å°‘', 'æ“´å¤§', 'ç¸®å°', 'åŠ å¼·', 'å‰Šå¼±', 'å®Œå–„', 'æ”¹é€²', 'å„ªåŒ–', 'èª¿æ•´', 'ä¿®æ”¹', 'è®Šæ›´', 'æ›´æ–°', 'å‡ç´š', 'é™ç´š', 'æå‡', 'ä¸‹é™', 'ä¸Šå‡', 'ä¸‹é™', 'å¢é•·', 'ä¸‹é™', 'ä¸Šå‡', 'ä¸‹é™', 'æé«˜', 'é™ä½', 'å¢åŠ ', 'æ¸›å°‘', 'æ“´å¤§', 'ç¸®å°', 'åŠ å¼·', 'å‰Šå¼±', 'å®Œå–„', 'æ”¹é€²', 'å„ªåŒ–', 'èª¿æ•´', 'ä¿®æ”¹', 'è®Šæ›´', 'æ›´æ–°', 'å‡ç´š', 'é™ç´š', 'æå‡', 'ä¸‹é™', 'ä¸Šå‡', 'ä¸‹é™', 'å¢é•·', 'ä¸‹é™', 'ä¸Šå‡', 'ä¸‹é™'
        }
        
        # 1. åˆ†è©
        words = jieba.lcut(query_content)
        
        # 2. éæ¿¾åœç”¨è©å’ŒçŸ­è©
        keywords = set()
        for word in words:
            word = word.strip()
            if (len(word) >= 2 and 
                word not in stop_words and 
                not re.match(r'^[0-9]+$', word) and
                not re.match(r'^[a-zA-Z]+$', word) and
                not re.match(r'^[^\u4e00-\u9fff]+$', word)):  # åªä¿ç•™ä¸­æ–‡å­—ç¬¦
                keywords.add(word)
        
        return keywords

class EdgeConnector:
    """é‚Šé€£æ¥å™¨"""
    
    def __init__(self, config: HopRAGConfig = DEFAULT_CONFIG):
        self.config = config
    
    async def connect_edges(self, nodes: Dict[str, LegalNode], embedding_model) -> Dict[str, List[Dict[str, Any]]]:
        """é€£æ¥ç¯€é»é‚Š - æ”¯æŒå‹•æ…‹é‚Šæ•¸é™åˆ¶"""
        print("ğŸ”— é–‹å§‹é‚ŠåŒ¹é…å’Œé€£æ¥...")
        
        # è¨ˆç®—å‹•æ…‹é‚Šæ•¸é™åˆ¶ï¼ˆåŸºæ–¼è«–æ–‡O(n log n)è¦æ±‚ï¼‰
        n = len(nodes)
        if self.config.use_dynamic_edge_limit:
            dynamic_limit = int(self.config.edge_limit_factor * n * np.log2(n + 1))
            print(f"ğŸ“Š å‹•æ…‹é‚Šæ•¸é™åˆ¶: O(n log n) = {dynamic_limit} (n={n})")
        else:
            dynamic_limit = self.config.max_edges_per_node * n
            print(f"ğŸ“Š å›ºå®šé‚Šæ•¸é™åˆ¶: {dynamic_limit}")
        
        # Step 1: ç‚ºæ‰€æœ‰å½æŸ¥è©¢ç”Ÿæˆembedding
        await self._generate_pseudo_query_embeddings(nodes, embedding_model)
        
        # Step 2: åŸ·è¡Œé‚ŠåŒ¹é…ç®—æ³•
        edges = await self._perform_edge_matching(nodes)
        
        # Step 3: æ‡‰ç”¨é‚Šæ•¸é™åˆ¶
        edges = self._apply_edge_limit(edges, dynamic_limit)
        
        # çµ±è¨ˆæ··åˆæª¢ç´¢ä¿¡æ¯
        total_edges = sum(len(edge_list) for edge_list in edges.values())
        print(f"âœ… é‚Šé€£æ¥å®Œæˆï¼Œå…±å»ºç«‹ {total_edges} æ¢é‚Š")
        
        if self.config.use_hybrid_retrieval:
            print(f"ğŸ”— æ··åˆæª¢ç´¢çµ±è¨ˆ:")
            print(f"   - Jaccardæ¬Šé‡: {self.config.jaccard_weight}")
            print(f"   - é¤˜å¼¦æ¬Šé‡: {self.config.cosine_weight}")
            print(f"   - è©å½™é–¾å€¼: {self.config.lexical_threshold}")
            print(f"   - èªç¾©é–¾å€¼: {self.config.semantic_threshold}")
        
        return edges
    
    def _apply_edge_limit(self, edges: Dict[str, List[Dict[str, Any]]], total_limit: int) -> Dict[str, List[Dict[str, Any]]]:
        """æ‡‰ç”¨é‚Šæ•¸é™åˆ¶ - åŸºæ–¼è«–æ–‡O(n log n)è¦æ±‚"""
        current_total = sum(len(edge_list) for edge_list in edges.values())
        
        if current_total <= total_limit:
            print(f"âœ… é‚Šæ•¸æœªè¶…éé™åˆ¶: {current_total} <= {total_limit}")
            return edges
        
        print(f"âš ï¸ é‚Šæ•¸è¶…éé™åˆ¶: {current_total} > {total_limit}ï¼Œé–‹å§‹ä¿®å‰ª...")
        
        # æ”¶é›†æ‰€æœ‰é‚Šä¸¦æŒ‰ç›¸ä¼¼åº¦æ’åº
        all_edges = []
        for node_id, edge_list in edges.items():
            for edge in edge_list:
                edge['source_node'] = node_id
                all_edges.append(edge)
        
        # æŒ‰ç›¸ä¼¼åº¦åˆ†æ•¸æ’åºï¼ˆé™åºï¼‰
        all_edges.sort(key=lambda x: x.get('similarity_score', 0), reverse=True)
        
        # ä¿ç•™å‰total_limitæ¢é‚Š
        selected_edges = all_edges[:total_limit]
        
        # é‡æ–°çµ„ç¹”é‚Šå­—å…¸
        limited_edges = {}
        for edge in selected_edges:
            source_node = edge.pop('source_node')
            if source_node not in limited_edges:
                limited_edges[source_node] = []
            limited_edges[source_node].append(edge)
        
        # ç¢ºä¿æ‰€æœ‰ç¯€é»éƒ½æœ‰é‚Šåˆ—è¡¨ï¼ˆå³ä½¿ç‚ºç©ºï¼‰
        for node_id in edges.keys():
            if node_id not in limited_edges:
                limited_edges[node_id] = []
        
        print(f"âœ… é‚Šæ•¸ä¿®å‰ªå®Œæˆ: {current_total} -> {sum(len(edge_list) for edge_list in limited_edges.values())}")
        
        return limited_edges
    
    async def _generate_pseudo_query_embeddings(self, nodes: Dict[str, LegalNode], embedding_model):
        """ç‚ºæ‰€æœ‰å½æŸ¥è©¢ç”Ÿæˆembedding"""
        print("ğŸ“Š é–‹å§‹ç”Ÿæˆå½æŸ¥è©¢embeddingå‘é‡...")
        
        all_queries = []
        query_mapping = {}
        
        # æ”¶é›†æ‰€æœ‰å½æŸ¥è©¢
        for node_id, node in nodes.items():
            for pseudo_query in node.pseudo_queries.get("outgoing", []):
                all_queries.append(pseudo_query.content)
                query_mapping[pseudo_query.content] = (node_id, "outgoing", pseudo_query)
                
            for pseudo_query in node.pseudo_queries.get("incoming", []):
                all_queries.append(pseudo_query.content)
                query_mapping[pseudo_query.content] = (node_id, "incoming", pseudo_query)
        
        if not all_queries:
            print("âš ï¸ æ²’æœ‰å½æŸ¥è©¢éœ€è¦ç”Ÿæˆembedding")
            return
        
        print(f"ğŸ“ˆ ç¸½å…±éœ€è¦ç”Ÿæˆ {len(all_queries)} å€‹embeddingå‘é‡")
        print(f"â±ï¸ é è¨ˆéœ€è¦ 1-2 åˆ†é˜")
        
        # æ‰¹é‡ç”Ÿæˆembedding
        try:
            start_time = time.time()
            if hasattr(embedding_model, 'encode_async'):
                embeddings = await embedding_model.encode_async(all_queries)
            else:
                embeddings = embedding_model.encode(all_queries)
            
            embedding_time = time.time() - start_time
            print(f"âœ… Embeddingç”Ÿæˆå®Œæˆï¼è€—æ™‚: {embedding_time:.1f} ç§’")
            
            # å°‡embeddingåˆ†é…å›å½æŸ¥è©¢
            for i, query_content in enumerate(all_queries):
                node_id, query_type, pseudo_query = query_mapping[query_content]
                pseudo_query.embedding = embeddings[i]
                
        except Exception as e:
            print(f"âŒ ç”Ÿæˆå½æŸ¥è©¢embeddingå¤±æ•—: {e}")
    
    async def _perform_edge_matching(self, nodes: Dict[str, LegalNode]) -> Dict[str, List[Dict[str, Any]]]:
        """åŸ·è¡Œé‚ŠåŒ¹é…ç®—æ³•"""
        print("ğŸ”— åŸ·è¡Œé‚ŠåŒ¹é…ç®—æ³•...")
        
        node_ids = list(nodes.keys())
        edges = {}
        
        for i, node_a in enumerate(node_ids):
            outgoing_queries_a = nodes[node_a].pseudo_queries.get("outgoing", [])
            edges[node_a] = []
            
            for j, node_b in enumerate(node_ids):
                if i == j:
                    continue
                    
                incoming_queries_b = nodes[node_b].pseudo_queries.get("incoming", [])
                
                # è¨ˆç®—æœ€ä½³ç›¸ä¼¼åº¦
                best_similarity = 0
                best_outgoing_query = None
                best_incoming_query = None
                
                for out_query in outgoing_queries_a:
                    for in_query in incoming_queries_b:
                        if (out_query.embedding is not None and in_query.embedding is not None and
                            out_query.keywords and in_query.keywords):
                            
                            # è¨ˆç®—å„ç¨®ç›¸ä¼¼åº¦
                            jaccard_sim = self._calculate_jaccard_similarity(out_query.keywords, in_query.keywords)
                            cosine_sim = self._calculate_cosine_similarity(out_query.embedding, in_query.embedding)
                            
                            # æ··åˆæª¢ç´¢éæ¿¾ï¼šæª¢æŸ¥è©å½™å’Œèªç¾©é–¾å€¼
                            if self.config.use_hybrid_retrieval:
                                if (jaccard_sim < self.config.lexical_threshold or 
                                    cosine_sim < self.config.semantic_threshold):
                                    continue  # è·³éä¸æ»¿è¶³é–¾å€¼çš„çµ„åˆ
                            
                            similarity = self._calculate_similarity(out_query, in_query)
                            
                            if similarity > best_similarity:
                                best_similarity = similarity
                                best_outgoing_query = out_query
                                best_incoming_query = in_query
                
                # å¦‚æœç›¸ä¼¼åº¦è¶…éé–¾å€¼ï¼Œå»ºç«‹é‚Š
                if best_similarity >= self.config.similarity_threshold:
                    # å‰µå»ºå®Œæ•´çš„é‚Šç‰¹å¾µ
                    edge_feature = self._create_edge_feature(best_outgoing_query, best_incoming_query)
                    
                    edge_attr = {
                        'to_node': node_b,
                        'pseudo_query': f"{best_outgoing_query.content} -> {best_incoming_query.content}",
                        'similarity_score': best_similarity,
                        'jaccard_score': edge_feature.jaccard_score,
                        'cosine_score': edge_feature.cosine_score,
                        'mixed_score': edge_feature.mixed_score,
                        'combined_keywords': list(edge_feature.combined_keywords),
                        'edge_query': edge_feature.query,
                        'edge_embedding': edge_feature.embedding.tolist() if edge_feature.embedding is not None else None,
                        'outgoing_query_id': best_outgoing_query.query_id,
                        'incoming_query_id': best_incoming_query.query_id,
                        'edge_type': self._determine_edge_type(node_a, node_b)
                    }
                    
                    edges[node_a].append(edge_attr)
                    
                    # é™åˆ¶æ¯å€‹ç¯€é»çš„æœ€å¤§å‡ºé‚Šæ•¸é‡
                    if len(edges[node_a]) >= self.config.max_edges_per_node:
                        break
            
            if i % 10 == 0:
                print(f"  è™•ç†é€²åº¦: {i+1}/{len(node_ids)} ç¯€é»")
        
        return edges
    
    def _calculate_similarity(self, query_a: PseudoQuery, query_b: PseudoQuery) -> float:
        """è¨ˆç®—å…©å€‹å½æŸ¥è©¢çš„æ··åˆç›¸ä¼¼åº¦ï¼ˆJaccard + é¤˜å¼¦ç›¸ä¼¼åº¦ï¼‰"""
        # 1. è¨ˆç®—Jaccardç›¸ä¼¼åº¦ï¼ˆè©å½™å±¤é¢ï¼‰
        jaccard_sim = self._calculate_jaccard_similarity(query_a.keywords, query_b.keywords)
        
        # 2. è¨ˆç®—é¤˜å¼¦ç›¸ä¼¼åº¦ï¼ˆèªç¾©å±¤é¢ï¼‰
        cosine_sim = self._calculate_cosine_similarity(query_a.embedding, query_b.embedding)
        
        # 3. æ··åˆæª¢ç´¢ï¼šæ ¹æ“šé…ç½®æ¬Šé‡è¨ˆç®—
        if self.config.use_hybrid_retrieval:
            # ä½¿ç”¨é…ç½®çš„æ¬Šé‡
            mixed_similarity = (jaccard_sim * self.config.jaccard_weight + 
                              cosine_sim * self.config.cosine_weight)
        else:
            # ä½¿ç”¨è«–æ–‡å…¬å¼ï¼šå…©è€…å¹³å‡å€¼
            mixed_similarity = (jaccard_sim + cosine_sim) / 2
        
        return float(mixed_similarity)
    
    def _calculate_jaccard_similarity(self, keywords_a: Set[str], keywords_b: Set[str]) -> float:
        """è¨ˆç®—Jaccardç›¸ä¼¼åº¦"""
        if not keywords_a or not keywords_b:
            return 0.0
        
        intersection = len(keywords_a.intersection(keywords_b))
        union = len(keywords_a.union(keywords_b))
        
        if union == 0:
            return 0.0
        
        return intersection / union
    
    def _calculate_cosine_similarity(self, embedding_a: np.ndarray, embedding_b: np.ndarray) -> float:
        """è¨ˆç®—é¤˜å¼¦ç›¸ä¼¼åº¦"""
        if embedding_a is None or embedding_b is None:
            return 0.0
        
        dot_product = np.dot(embedding_a, embedding_b)
        norm_a = np.linalg.norm(embedding_a)
        norm_b = np.linalg.norm(embedding_b)
        
        if norm_a == 0 or norm_b == 0:
            return 0.0
        
        return dot_product / (norm_a * norm_b)
    
    def _create_edge_feature(self, outgoing_query: PseudoQuery, incoming_query: PseudoQuery) -> EdgeFeature:
        """å‰µå»ºé‚Šç‰¹å¾µ - æŒ‰ç…§è«–æ–‡è¦æ±‚ e_s,t* = (q_t*,j*^-, k_t*,j*^- âˆª k_s,i^+, v_t*,j*^-)"""
        
        # åˆä½µé—œéµè©é›†åˆï¼šk_t*,j*^- âˆª k_s,i^+
        combined_keywords = incoming_query.keywords.union(outgoing_query.keywords)
        
        # è¨ˆç®—å„ç¨®ç›¸ä¼¼åº¦åˆ†æ•¸
        jaccard_score = self._calculate_jaccard_similarity(
            outgoing_query.keywords, incoming_query.keywords
        )
        cosine_score = self._calculate_cosine_similarity(
            outgoing_query.embedding, incoming_query.embedding
        )
        mixed_score = (jaccard_score + cosine_score) / 2
        
        return EdgeFeature(
            query=incoming_query.content,  # q_t*,j*^-
            combined_keywords=combined_keywords,  # k_t*,j*^- âˆª k_s,i^+
            embedding=incoming_query.embedding,  # v_t*,j*^-
            jaccard_score=jaccard_score,
            cosine_score=cosine_score,
            mixed_score=mixed_score
        )
    
    def _determine_edge_type(self, from_node: str, to_node: str) -> EdgeType:
        """ç¢ºå®šé‚Šçš„é¡å‹"""
        # é€™è£¡éœ€è¦æ ¹æ“šå¯¦éš›çš„ç¯€é»é¡å‹ä¾†åˆ¤æ–·
        # æš«æ™‚ä½¿ç”¨å­—ç¬¦ä¸²é¡å‹
        if from_node.startswith('article_') and to_node.startswith('article_'):
            return EdgeType.ARTICLE_TO_ARTICLE
        elif from_node.startswith('article_') and to_node.startswith('item_'):
            return EdgeType.ARTICLE_TO_ITEM
        elif from_node.startswith('item_') and to_node.startswith('article_'):
            return EdgeType.ITEM_TO_ARTICLE
        else:
            return EdgeType.ITEM_TO_ITEM

class PassageGraphBuilder:
    """æ®µè½åœ–æ§‹å»ºå™¨"""
    
    def __init__(self, config: HopRAGConfig = DEFAULT_CONFIG):
        self.config = config
        self.pseudo_query_generator = None
        self.edge_connector = None
        self.embedding_model = None
        
    def set_components(self, pseudo_query_generator: PseudoQueryGenerator, 
                      edge_connector: EdgeConnector, embedding_model):
        """è¨­ç½®çµ„ä»¶"""
        self.pseudo_query_generator = pseudo_query_generator
        self.edge_connector = edge_connector
        self.embedding_model = embedding_model
        
    async def build_graph(self, multi_level_chunks: Dict[str, Dict[str, List[Dict]]]) -> Tuple[Dict[str, LegalNode], Dict[str, List[Dict[str, Any]]]]:
        """æ§‹å»ºå®Œæ•´çš„çŸ¥è­˜åœ–è­œ"""
        print("ğŸ—ï¸ é–‹å§‹æ§‹å»ºHopRAGçŸ¥è­˜åœ–è­œ...")
        
        # Step 1: å‰µå»ºç¯€é»
        nodes = await self._create_nodes_from_chunks(multi_level_chunks)
        
        # Step 2: ç”Ÿæˆå½æŸ¥è©¢
        await self._generate_pseudo_queries(nodes)
        
        # Step 3: æ§‹å»ºåœ–é‚Š
        edges = await self.edge_connector.connect_edges(nodes, self.embedding_model)
        
        print("âœ… HopRAGçŸ¥è­˜åœ–è­œæ§‹å»ºå®Œæˆï¼")
        return nodes, edges
    
    async def _create_nodes_from_chunks(self, multi_level_chunks: Dict[str, Dict[str, List[Dict]]]) -> Dict[str, LegalNode]:
        """å¾chunkså‰µå»ºç¯€é»"""
        print("ğŸ“ å‰µå»ºç¯€é»...")
        
        nodes = {}
        
        for doc_id, levels in multi_level_chunks.items():
            # è™•ç†æ¢ç´šç¯€é» (basic_unitå±¤æ¬¡)
            if 'basic_unit' in levels:
                for chunk_idx, chunk in enumerate(levels['basic_unit']):
                    try:
                        # ç¢ºä¿chunkçµæ§‹å®Œæ•´
                        if not isinstance(chunk, dict):
                            print(f"âŒ Chunk {chunk_idx} ä¸æ˜¯å­—å…¸é¡å‹: {type(chunk)}")
                            continue
                        
                        # ç¢ºä¿metadataå­˜åœ¨
                        if 'metadata' not in chunk:
                            chunk['metadata'] = {}
                        
                        # ç¢ºä¿contentå­˜åœ¨
                        if 'content' not in chunk:
                            print(f"âŒ Chunk {chunk_idx} æ²’æœ‰contentæ¬„ä½")
                            continue
                        
                        # ç”Ÿæˆnode_idï¼Œå¦‚æœmetadataä¸­æ²’æœ‰idæ¬„ä½
                        node_id = chunk['metadata'].get('id', f"{doc_id}_basic_unit_{chunk_idx}")
                        
                        article_node = LegalNode(
                            node_id=node_id,
                            node_type=NodeType.ARTICLE,
                            content=chunk['content'],
                            contextualized_text=chunk['content'],
                            law_name=chunk['metadata'].get('law_name', ''),
                            article_number=chunk['metadata'].get('article_label', ''),
                            metadata=chunk['metadata']
                        )
                        nodes[article_node.node_id] = article_node
                        
                    except Exception as e:
                        print(f"âŒ å‰µå»ºbasic_unitç¯€é»å¤±æ•— (chunk {chunk_idx}): {e}")
                        continue
            
            # è™•ç†é …ç´šç¯€é» (basic_unit_componentå±¤æ¬¡)
            if 'basic_unit_component' in levels:
                for chunk_idx, chunk in enumerate(levels['basic_unit_component']):
                    try:
                        # ç¢ºä¿chunkçµæ§‹å®Œæ•´
                        if not isinstance(chunk, dict):
                            print(f"âŒ Component Chunk {chunk_idx} ä¸æ˜¯å­—å…¸é¡å‹: {type(chunk)}")
                            continue
                        
                        # ç¢ºä¿metadataå­˜åœ¨
                        if 'metadata' not in chunk:
                            chunk['metadata'] = {}
                        
                        # ç¢ºä¿contentå­˜åœ¨
                        if 'content' not in chunk:
                            print(f"âŒ Component Chunk {chunk_idx} æ²’æœ‰contentæ¬„ä½")
                            continue
                        
                        # ç”Ÿæˆnode_idï¼Œå¦‚æœmetadataä¸­æ²’æœ‰idæ¬„ä½
                        node_id = chunk['metadata'].get('id', f"{doc_id}_basic_unit_component_{chunk_idx}")
                        
                        item_node = LegalNode(
                            node_id=node_id,
                            node_type=NodeType.ITEM,
                            content=chunk['content'],
                            contextualized_text=chunk['content'],
                            law_name=chunk['metadata'].get('law_name', ''),
                            article_number=chunk['metadata'].get('article_label', ''),
                            item_number=chunk['metadata'].get('item_label', ''),
                            parent_article_id=chunk['metadata'].get('parent_article_id'),
                            metadata=chunk['metadata']
                        )
                        nodes[item_node.node_id] = item_node
                        
                    except Exception as e:
                        print(f"âŒ å‰µå»ºbasic_unit_componentç¯€é»å¤±æ•— (chunk {chunk_idx}): {e}")
                        continue
        
        print(f"âœ… ç¯€é»å‰µå»ºå®Œæˆï¼Œå…± {len(nodes)} å€‹ç¯€é»")
        return nodes
    
    async def _generate_pseudo_queries(self, nodes: Dict[str, LegalNode]):
        """ç‚ºæ‰€æœ‰ç¯€é»ç”Ÿæˆå½æŸ¥è©¢"""
        print("ğŸ¤– é–‹å§‹ç”Ÿæˆå½æŸ¥è©¢...")
        
        node_list = list(nodes.values())
        total_nodes = len(node_list)
        start_time = time.time()
        
        print(f"ğŸ“Š ç¸½å…±éœ€è¦è™•ç† {total_nodes} å€‹ç¯€é»")
        print(f"â±ï¸ é è¨ˆæ¯å€‹ç¯€é»éœ€è¦ 2-3 ç§’ï¼ˆåŒ…å«LLMèª¿ç”¨ï¼‰")
        print(f"ğŸ• é è¨ˆç¸½æ™‚é–“: {total_nodes * 2.5 / 60:.1f} åˆ†é˜")
        print("=" * 60)
        
        for i, node in enumerate(node_list):
            try:
                node_start_time = time.time()
                await self.pseudo_query_generator.generate_pseudo_queries_for_node(node)
                node_time = time.time() - node_start_time
                
                # è¨ˆç®—é€²åº¦å’Œå‰©é¤˜æ™‚é–“
                progress = (i + 1) / total_nodes * 100
                elapsed_time = time.time() - start_time
                avg_time_per_node = elapsed_time / (i + 1)
                remaining_nodes = total_nodes - (i + 1)
                estimated_remaining_time = remaining_nodes * avg_time_per_node
                
                # æ¯5å€‹ç¯€é»é¡¯ç¤ºä¸€æ¬¡é€²åº¦
                if (i + 1) % 5 == 0 or i == 0:
                    print(f"ğŸ“ˆ é€²åº¦: {i+1}/{total_nodes} ({progress:.1f}%) | "
                          f"ç¯€é»: {node.node_id[:20]}... | "
                          f"è€—æ™‚: {node_time:.1f}s | "
                          f"å‰©é¤˜: {estimated_remaining_time/60:.1f}åˆ†é˜")
                
                # æ¯20å€‹ç¯€é»é¡¯ç¤ºè©³ç´°çµ±è¨ˆ
                if (i + 1) % 20 == 0:
                    print(f"ğŸ“Š çµ±è¨ˆ: å¹³å‡ {avg_time_per_node:.1f}s/ç¯€é» | "
                          f"å·²ç”¨æ™‚: {elapsed_time/60:.1f}åˆ†é˜ | "
                          f"é è¨ˆå®Œæˆ: {estimated_remaining_time/60:.1f}åˆ†é˜")
                    
            except Exception as e:
                print(f"âŒ ç¯€é» {node.node_id} å½æŸ¥è©¢ç”Ÿæˆå¤±æ•—: {e}")
                continue
        
        total_time = time.time() - start_time
        print("=" * 60)
        print(f"âœ… å½æŸ¥è©¢ç”Ÿæˆå®Œæˆï¼ç¸½è€—æ™‚: {total_time/60:.1f} åˆ†é˜")
        print(f"ğŸ“Š å¹³å‡æ¯å€‹ç¯€é»: {total_time/total_nodes:.1f} ç§’")
