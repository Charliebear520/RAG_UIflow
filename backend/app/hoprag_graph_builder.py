"""
HopRAGåœ–æ§‹å»ºå™¨æ¨¡çµ„
åŒ…å«PassageGraphBuilderã€PseudoQueryGeneratorã€EdgeConnector
"""

import json
import numpy as np
import networkx as nx
import time
import re
import jieba
from typing import Dict, List, Any, Optional, Tuple, Set
from dataclasses import dataclass, asdict
import asyncio

from .hoprag_config import HopRAGConfig, NodeType, EdgeType, QueryType, DEFAULT_CONFIG

@dataclass
class PseudoQuery:
    """å½æŸ¥è©¢æ•¸æ“šçµæ§‹"""
    query_id: str
    content: str
    query_type: QueryType
    embedding: Optional[np.ndarray] = None
    keywords: Optional[set] = None  # k: é—œéµè©é›†åˆ
    similarity_threshold: float = 0.7
    
    def __post_init__(self):
        if self.keywords is None:
            self.keywords = set()

@dataclass
class EdgeFeature:
    """é‚Šç‰¹å¾µæ•¸æ“šçµæ§‹ - æŒ‰ç…§è«–æ–‡è¦æ±‚"""
    query: str  # q_t*,j*^-
    combined_keywords: Set[str]  # k_t*,j*^- âˆª k_s,i^+
    embedding: np.ndarray  # v_t*,j*^-
    jaccard_score: float
    cosine_score: float
    mixed_score: float

@dataclass
class LegalNode:
    """æ³•å¾‹ç¯€é»æ•¸æ“šçµæ§‹"""
    node_id: str
    node_type: NodeType
    content: str
    contextualized_text: str
    law_name: str
    article_number: str
    item_number: Optional[str] = None
    parent_article_id: Optional[str] = None
    metadata: Dict[str, Any] = None
    
    # å½æŸ¥è©¢
    incoming_questions: List[str] = None
    outgoing_questions: List[str] = None
    pseudo_queries: Dict[str, List[PseudoQuery]] = None
    
    # åœ–çµæ§‹ç›¸é—œ
    outgoing_edges: List[str] = None
    incoming_edges: List[str] = None

    def __post_init__(self):
        if self.metadata is None:
            self.metadata = {}
        if self.incoming_questions is None:
            self.incoming_questions = []
        if self.outgoing_questions is None:
            self.outgoing_questions = []
        if self.pseudo_queries is None:
            self.pseudo_queries = {"incoming": [], "outgoing": []}
        if self.outgoing_edges is None:
            self.outgoing_edges = []
        if self.incoming_edges is None:
            self.incoming_edges = []

class PseudoQueryGenerator:
    """å½æŸ¥è©¢ç”Ÿæˆå™¨"""
    
    def __init__(self, llm_client, config: HopRAGConfig = DEFAULT_CONFIG):
        self.llm_client = llm_client
        self.config = config
        
    async def generate_pseudo_queries_for_node(self, node: LegalNode) -> LegalNode:
        """ç‚ºå–®å€‹ç¯€é»ç”Ÿæˆå…§å‘å’Œå¤–å‘å•é¡Œ"""
        print(f"ğŸ” ç‚ºç¯€é» {node.node_id} ç”Ÿæˆå½æŸ¥è©¢...")
        
        # ç”Ÿæˆå…§å‘å•é¡Œ
        incoming_questions = await self._generate_incoming_questions(node)
        
        # ç”Ÿæˆå¤–å‘å•é¡Œ
        outgoing_questions = await self._generate_outgoing_questions(node)
        
        # æ›´æ–°ç¯€é»
        node.incoming_questions = incoming_questions
        node.outgoing_questions = outgoing_questions
        
        # å‰µå»ºPseudoQueryå°è±¡
        node.pseudo_queries = {
            "incoming": [
                PseudoQuery(
                    query_id=f"{node.node_id}_in_{i}",
                    content=question,
                    query_type=QueryType.INCOMING,
                    keywords=self._extract_keywords_from_query(question)
                ) for i, question in enumerate(incoming_questions)
            ],
            "outgoing": [
                PseudoQuery(
                    query_id=f"{node.node_id}_out_{i}",
                    content=question,
                    query_type=QueryType.OUTGOING,
                    keywords=self._extract_keywords_from_query(question)
                ) for i, question in enumerate(outgoing_questions)
            ]
        }
        
        print(f"âœ… ç¯€é» {node.node_id} å½æŸ¥è©¢ç”Ÿæˆå®Œæˆï¼š{len(incoming_questions)}å€‹å…§å‘ï¼Œ{len(outgoing_questions)}å€‹å¤–å‘")
        return node
    
    async def _generate_incoming_questions(self, node: LegalNode) -> List[str]:
        """ç”Ÿæˆå…§å‘å•é¡Œ - å‹•æ…‹æ•¸é‡ç‰ˆæœ¬"""
        
        if self.config.use_dynamic_question_count:
            return await self._generate_dynamic_incoming_questions(node)
        else:
            return await self._generate_fixed_incoming_questions(node, target_count=1)  # å›ºå®š1ä¸ªå†…å‘é—®é¢˜
    
    async def _generate_dynamic_incoming_questions(self, node: LegalNode) -> List[str]:
        """å‹•æ…‹ç”Ÿæˆå…§å‘å•é¡Œ - è®“LLMæ±ºå®šé©ç•¶æ•¸é‡"""
        
        prompt = f"""
æ‚¨æ˜¯ä¸€ä½æ³•å¾‹å°ˆå®¶ï¼Œæ“…é•·æå‡ºå•é¡Œä¸¦ç²¾é€šä¸­æ–‡ã€‚æ‚¨éœ€è¦æ ¹æ“šæ³•å¾‹æ¢æ–‡ä¸­çš„å¹¾å¥é€£çºŒå¥å­ç”Ÿæˆå•é¡Œã€‚

ä»»å‹™ï¼šç‚ºä»¥ä¸‹æ³•å¾‹æ¢æ–‡ç”Ÿæˆå•é¡Œï¼Œé€™äº›å•é¡Œçš„ç­”æ¡ˆå¿…é ˆç›´æ¥ä¾†è‡ªè©²æ–‡æœ¬æœ¬èº«ã€‚

æ³•å¾‹æ¢æ–‡å…§å®¹ï¼š
{node.contextualized_text}

è¦æ±‚ï¼š
1. æ³•å¾‹è¦ç´ ï¼šæ¯å€‹å•é¡Œå¿…é ˆåŒ…å«ç‰¹å®šçš„æ³•å¾‹è¦ç´ ï¼ˆæ¢æ–‡è™Ÿã€é©ç”¨ç¯„åœã€æ³•å¾‹å¾Œæœã€æ¬Šåˆ©ç¾©å‹™ã€æ§‹æˆè¦ä»¶ç­‰ï¼‰æˆ–å…¶ä»–é—œéµç‰¹å¾µï¼Œä»¥æ¸›å°‘æ­§ç¾©ã€æ¾„æ¸…ä¸Šä¸‹æ–‡ä¸¦ç¢ºä¿è‡ªåŒ…å«æ€§ã€‚
2. çœç•¥/ç©ºç™½ï¼šæ‚¨å¯ä»¥çœç•¥æˆ–ç•™ç©ºå¥å­çš„é‡è¦éƒ¨åˆ†ä¾†å½¢æˆå•é¡Œï¼Œä½†ä¸æ‡‰å°åŒä¸€éƒ¨åˆ†æå‡ºå¤šå€‹å•é¡Œã€‚ä¸å¿…ç‚ºå¥å­çš„æ¯å€‹éƒ¨åˆ†éƒ½æå‡ºå•é¡Œã€‚
3. èˆ‡çœç•¥éƒ¨åˆ†çš„é€£è²«æ€§ï¼šç•¶è©¢å•çœç•¥çš„éƒ¨åˆ†æ™‚ï¼Œéçœç•¥çš„ä¿¡æ¯æ‡‰åŒ…å«åœ¨å•é¡Œä¸­ï¼Œåªè¦ä¿æŒé€£è²«æ€§ã€‚
4. å¤šæ¨£æ€§ï¼šä¸åŒçš„å•é¡Œæ‡‰é—œæ³¨å¥å­ä¸­ä¿¡æ¯çš„ä¸åŒæ–¹é¢ï¼Œç¢ºä¿å¤šæ¨£æ€§å’Œä»£è¡¨æ€§ã€‚
5. è¦†è“‹å’Œæ¨™æº–åŒ–ï¼šæ‰€æœ‰å•é¡Œçµåˆèµ·ä¾†æ‡‰æ¶µè“‹æ‰€æä¾›å¥å­çš„æ‰€æœ‰é—œéµé»ï¼Œæªè¾­æ‡‰æ¨™æº–åŒ–ã€‚
6. å®¢è§€å’Œè©³ç´°ï¼šå•é¡Œæ‡‰å®¢è§€ã€åŸºæ–¼äº‹å¯¦ä¸”æ³¨é‡ç´°ç¯€ï¼ˆä¾‹å¦‚ï¼Œè©¢å•æ³•å¾‹è¦ä»¶ã€é©ç”¨æ¢ä»¶ã€æ³•å¾‹æ•ˆæœç­‰ï¼‰ã€‚ç­”æ¡ˆå¿…é ˆåƒ…ä¾†è‡ªæ‰€æä¾›çš„å¥å­ã€‚

æ•¸é‡è¦æ±‚ï¼š
- æœ€å°‘ç”Ÿæˆ {self.config.min_incoming_questions} å€‹å•é¡Œ
- æœ€å¤šç”Ÿæˆ {self.config.max_incoming_questions} å€‹å•é¡Œ
- è«‹æ ¹æ“šæ³•å¾‹æ¢æ–‡çš„è¤‡é›œåº¦å’Œä¿¡æ¯è±å¯Œç¨‹åº¦ï¼Œç”Ÿæˆé©ç•¶æ•¸é‡çš„å•é¡Œä¾†å……åˆ†è¦†è“‹æ‰€æœ‰é‡è¦ä¿¡æ¯
- å¦‚æœæ¢æ–‡å…§å®¹ç°¡å–®ï¼Œç”Ÿæˆè¼ƒå°‘å•é¡Œï¼›å¦‚æœæ¢æ–‡å…§å®¹è¤‡é›œï¼Œç”Ÿæˆè¼ƒå¤šå•é¡Œ

ç¯„ä¾‹ï¼š
å¥å­åˆ—è¡¨ï¼š["ç¬¬å…«æ¢ï¼šè‘—ä½œæ¬Šäººäº«æœ‰ä¸‹åˆ—æ¬Šåˆ©ï¼šä¸€ã€é‡è£½æ¬Šï¼›äºŒã€å…¬é–‹æ’­é€æ¬Šï¼›ä¸‰ã€å…¬é–‹å‚³è¼¸æ¬Šã€‚"]
ç­”æ¡ˆç¯„ä¾‹ï¼š
{{
    "Question List": [
        "ç¬¬å…«æ¢è¦å®šäº†è‘—ä½œæ¬Šäººçš„å“ªäº›æ¬Šåˆ©ï¼Ÿ",
        "è‘—ä½œæ¬Šäººçš„é‡è£½æ¬Šæ˜¯ä»€éº¼ï¼Ÿ",
        "è‘—ä½œæ¬Šäººçš„å…¬é–‹æ’­é€æ¬Šæ˜¯ä»€éº¼ï¼Ÿ",
        "è‘—ä½œæ¬Šäººçš„å…¬é–‹å‚³è¼¸æ¬Šæ˜¯ä»€éº¼ï¼Ÿ",
        "ç¬¬å…«æ¢ç¸½å…±è¦å®šäº†å¹¾é …è‘—ä½œæ¬Šï¼Ÿ"
    ]
}}

è«‹ä»¥JSONæ ¼å¼è¿”å›ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
{{
    "Question List": [
        "å•é¡Œ1",
        "å•é¡Œ2",
        "å•é¡Œ3"
    ]
}}

è«‹ç¢ºä¿JSONæ ¼å¼æ­£ç¢ºï¼Œé¿å…ä¸å¿…è¦çš„è½‰ç¾©ã€æ›è¡Œæˆ–ç©ºæ ¼ã€‚æ‚¨é‚„æ‡‰è©²ç‰¹åˆ¥æ³¨æ„ç¢ºä¿ï¼Œé™¤äº†JSONå’Œåˆ—è¡¨æ ¼å¼æœ¬èº«ä½¿ç”¨é›™å¼•è™Ÿ(")å¤–ï¼Œå…¶ä»–æ‰€æœ‰é›™å¼•è™Ÿçš„å¯¦ä¾‹éƒ½æ‡‰æ›¿æ›ç‚ºå–®å¼•è™Ÿã€‚ä¾‹å¦‚ï¼Œä½¿ç”¨'è‘—ä½œæ¬Šæ³•'è€Œä¸æ˜¯"è‘—ä½œæ¬Šæ³•"ã€‚
è«‹ç¢ºä¿JSONæ ¼å¼æ­£ç¢ºï¼Œä¸è¦åŒ…å«ä»»ä½•å…¶ä»–æ–‡å­—ã€‚
"""
        
        try:
            response = await self.llm_client.generate_async(prompt)
            
            # è§£æJSONéŸ¿æ‡‰
            if response.strip().startswith('{'):
                result = json.loads(response.strip())
                questions = result.get('Question List', [])
                
                # æš«æ™‚ç§»é™¤å•é¡Œæ•¸é‡é©—è­‰
                # questions = self._validate_question_count(
                #     questions, 
                #     self.config.min_incoming_questions, 
                #     self.config.max_incoming_questions,
                #     "incoming"
                # )
                
                # é©—è­‰å•é¡Œè³ªé‡
                validated_questions = self._validate_questions(questions, "incoming")
                return validated_questions
            else:
                # å¦‚æœéŸ¿æ‡‰ä¸æ˜¯JSONæ ¼å¼ï¼Œå˜—è©¦æå–å•é¡Œ
                questions = self._extract_questions_from_text(response)
                # æš«æ™‚ç§»é™¤å•é¡Œæ•¸é‡é©—è­‰
                # questions = self._validate_question_count(
                #     questions, 
                #     self.config.min_incoming_questions, 
                #     self.config.max_incoming_questions,
                #     "incoming"
                # )
                return questions
                
        except Exception as e:
            print(f"âŒ ç”Ÿæˆå‹•æ…‹å…§å‘å•é¡Œå¤±æ•—: {e}")
            # è¿”å›é»˜èªå•é¡Œ
            return self._generate_default_incoming_questions(node)
    
    async def _generate_fixed_incoming_questions(self, node: LegalNode, target_count: int = None) -> List[str]:
        """å›ºå®šæ•¸é‡ç”Ÿæˆå…§å‘å•é¡Œ - å‘å¾Œå…¼å®¹"""
        if target_count is None:
            target_count = self.config.max_pseudo_queries_per_node
        
        prompt = f"""
æ‚¨æ˜¯ä¸€ä½æ³•å¾‹å°ˆå®¶ï¼Œæ“…é•·æå‡ºå•é¡Œä¸¦ç²¾é€šä¸­æ–‡ã€‚æ‚¨éœ€è¦æ ¹æ“šæ³•å¾‹æ¢æ–‡ä¸­çš„å¹¾å¥é€£çºŒå¥å­ç”Ÿæˆå•é¡Œã€‚

ä»»å‹™ï¼šç‚ºä»¥ä¸‹æ³•å¾‹æ¢æ–‡ç”Ÿæˆå•é¡Œï¼Œé€™äº›å•é¡Œçš„ç­”æ¡ˆå¿…é ˆç›´æ¥ä¾†è‡ªè©²æ–‡æœ¬æœ¬èº«ã€‚

æ³•å¾‹æ¢æ–‡å…§å®¹ï¼š
{node.contextualized_text}

è¦æ±‚ï¼š
1. æ³•å¾‹è¦ç´ ï¼šæ¯å€‹å•é¡Œå¿…é ˆåŒ…å«ç‰¹å®šçš„æ³•å¾‹è¦ç´ ï¼ˆæ¢æ–‡è™Ÿã€é©ç”¨ç¯„åœã€æ³•å¾‹å¾Œæœã€æ¬Šåˆ©ç¾©å‹™ã€æ§‹æˆè¦ä»¶ç­‰ï¼‰æˆ–å…¶ä»–é—œéµç‰¹å¾µï¼Œä»¥æ¸›å°‘æ­§ç¾©ã€æ¾„æ¸…ä¸Šä¸‹æ–‡ä¸¦ç¢ºä¿è‡ªåŒ…å«æ€§ã€‚
2. çœç•¥/ç©ºç™½ï¼šæ‚¨å¯ä»¥çœç•¥æˆ–ç•™ç©ºå¥å­çš„é‡è¦éƒ¨åˆ†ä¾†å½¢æˆå•é¡Œï¼Œä½†ä¸æ‡‰å°åŒä¸€éƒ¨åˆ†æå‡ºå¤šå€‹å•é¡Œã€‚ä¸å¿…ç‚ºå¥å­çš„æ¯å€‹éƒ¨åˆ†éƒ½æå‡ºå•é¡Œã€‚
3. èˆ‡çœç•¥éƒ¨åˆ†çš„é€£è²«æ€§ï¼šç•¶è©¢å•çœç•¥çš„éƒ¨åˆ†æ™‚ï¼Œéçœç•¥çš„ä¿¡æ¯æ‡‰åŒ…å«åœ¨å•é¡Œä¸­ï¼Œåªè¦ä¿æŒé€£è²«æ€§ã€‚
4. å¤šæ¨£æ€§ï¼šä¸åŒçš„å•é¡Œæ‡‰é—œæ³¨å¥å­ä¸­ä¿¡æ¯çš„ä¸åŒæ–¹é¢ï¼Œç¢ºä¿å¤šæ¨£æ€§å’Œä»£è¡¨æ€§ã€‚
5. è¦†è“‹å’Œæ¨™æº–åŒ–ï¼šæ‰€æœ‰å•é¡Œçµåˆèµ·ä¾†æ‡‰æ¶µè“‹æ‰€æä¾›å¥å­çš„æ‰€æœ‰é—œéµé»ï¼Œæªè¾­æ‡‰æ¨™æº–åŒ–ã€‚
6. å®¢è§€å’Œè©³ç´°ï¼šå•é¡Œæ‡‰å®¢è§€ã€åŸºæ–¼äº‹å¯¦ä¸”æ³¨é‡ç´°ç¯€ï¼ˆä¾‹å¦‚ï¼Œè©¢å•æ³•å¾‹è¦ä»¶ã€é©ç”¨æ¢ä»¶ã€æ³•å¾‹æ•ˆæœç­‰ï¼‰ã€‚ç­”æ¡ˆå¿…é ˆåƒ…ä¾†è‡ªæ‰€æä¾›çš„å¥å­ã€‚

ç¯„ä¾‹ï¼š
å¥å­åˆ—è¡¨ï¼š["ç¬¬å…«æ¢ï¼šè‘—ä½œæ¬Šäººäº«æœ‰ä¸‹åˆ—æ¬Šåˆ©ï¼šä¸€ã€é‡è£½æ¬Šï¼›äºŒã€å…¬é–‹æ’­é€æ¬Šï¼›ä¸‰ã€å…¬é–‹å‚³è¼¸æ¬Šã€‚"]

ç­”æ¡ˆç¯„ä¾‹ï¼š
{{
    "Question List": [
        "ç¬¬å…«æ¢è¦å®šäº†è‘—ä½œæ¬Šäººçš„å“ªäº›æ¬Šåˆ©ï¼Ÿ"
    ]
}}

è«‹ç”Ÿæˆ{target_count}å€‹å•é¡Œï¼Œåš´æ ¼éµå¾ªJSONæ ¼å¼ï¼Œé¿å…ä¸å¿…è¦çš„è½‰ç¾©ã€æ›è¡Œæˆ–ç©ºæ ¼ã€‚æ‚¨é‚„æ‡‰è©²ç‰¹åˆ¥æ³¨æ„ç¢ºä¿ï¼Œé™¤äº†JSONå’Œåˆ—è¡¨æ ¼å¼æœ¬èº«ä½¿ç”¨é›™å¼•è™Ÿ(")å¤–ï¼Œå…¶ä»–æ‰€æœ‰é›™å¼•è™Ÿçš„å¯¦ä¾‹éƒ½æ‡‰æ›¿æ›ç‚ºå–®å¼•è™Ÿã€‚ä¾‹å¦‚ï¼Œä½¿ç”¨'è‘—ä½œæ¬Šæ³•'è€Œä¸æ˜¯"è‘—ä½œæ¬Šæ³•"ã€‚

è«‹ä»¥JSONæ ¼å¼è¿”å›ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
{{
    "Question List": [
        "å•é¡Œ1"
    ]
}}

è«‹ç¢ºä¿JSONæ ¼å¼æ­£ç¢ºï¼Œä¸è¦åŒ…å«ä»»ä½•å…¶ä»–æ–‡å­—ã€‚
"""
        
        try:
            response = await self.llm_client.generate_async(prompt)
            
            # è§£æJSONéŸ¿æ‡‰
            if response.strip().startswith('{'):
                result = json.loads(response.strip())
                questions = result.get('Question List', [])
                
                # é©—è­‰å•é¡Œè³ªé‡
                validated_questions = self._validate_questions(questions, "incoming")
                # æ ¹æ“šé…ç½®æ±ºå®šæ˜¯å¦é™åˆ¶æ•¸é‡
                if self.config.use_dynamic_question_count:
                    # å‹•æ…‹æ¨¡å¼ï¼šä¸é™åˆ¶æ•¸é‡ï¼Œè®“LLMæ±ºå®š
                    return validated_questions
                else:
                    # å›ºå®šæ¨¡å¼ï¼šä½¿ç”¨é…ç½®çš„æ•¸é‡é™åˆ¶
                    return validated_questions[:self.config.max_pseudo_queries_per_node]
            else:
                # å¦‚æœéŸ¿æ‡‰ä¸æ˜¯JSONæ ¼å¼ï¼Œå˜—è©¦æå–å•é¡Œ
                questions = self._extract_questions_from_text(response)
                # æ ¹æ“šé…ç½®æ±ºå®šæ˜¯å¦é™åˆ¶æ•¸é‡
                if self.config.use_dynamic_question_count:
                    # å‹•æ…‹æ¨¡å¼ï¼šä¸é™åˆ¶æ•¸é‡ï¼Œè®“LLMæ±ºå®š
                    return questions
                else:
                    # å›ºå®šæ¨¡å¼ï¼šä½¿ç”¨é…ç½®çš„æ•¸é‡é™åˆ¶
                    return questions[:self.config.max_pseudo_queries_per_node]
                
        except Exception as e:
            print(f"âŒ ç”Ÿæˆå…§å‘å•é¡Œå¤±æ•—: {e}")
            # è¿”å›é»˜èªå•é¡Œ
            return self._generate_default_incoming_questions(node)
    
    async def _generate_outgoing_questions(self, node: LegalNode) -> List[str]:
        """ç”Ÿæˆå¤–å‘å•é¡Œ - å‹•æ…‹æ•¸é‡ç‰ˆæœ¬"""
        
        if self.config.use_dynamic_question_count:
            return await self._generate_dynamic_outgoing_questions(node)
        else:
            return await self._generate_fixed_outgoing_questions(node, target_count=2)  # å›ºå®š2ä¸ªå¤–å‘é—®é¢˜
    
    async def _generate_dynamic_outgoing_questions(self, node: LegalNode) -> List[str]:
        """å‹•æ…‹ç”Ÿæˆå¤–å‘å•é¡Œ - è®“LLMæ±ºå®šé©ç•¶æ•¸é‡"""
        
        prompt = f"""
æ‚¨æ˜¯ä¸€ä½æ³•å¾‹å°ˆå®¶ï¼Œæ“…é•·æå‡ºæ·±åˆ»å•é¡Œä¸¦ç²¾é€šä¸­æ–‡ã€‚æ‚¨éœ€è¦æ ¹æ“šæ³•å¾‹æ¢æ–‡ä¸­çš„å¹¾å¥é€£çºŒå¥å­ç”Ÿæˆå¾ŒçºŒå•é¡Œã€‚

ä»»å‹™ï¼šç‚ºä»¥ä¸‹æ³•å¾‹æ¢æ–‡ç”Ÿæˆå¾ŒçºŒå•é¡Œï¼Œé€™äº›å•é¡Œçš„ç­”æ¡ˆä¸åœ¨çµ¦å®šçš„å¥å­ä¸­æ‰¾åˆ°ã€‚

å¾ŒçºŒå•é¡Œå®šç¾©ï¼šç­”æ¡ˆä¸åœ¨çµ¦å®šçš„å¥å­ä¸­æ‰¾åˆ°ã€‚ç­”æ¡ˆå¯ä»¥å¾çµ¦å®šå¥å­ä¹‹å‰æˆ–ä¹‹å¾Œçš„ä¸Šä¸‹æ–‡ã€æ¶µè“‹ç›¸åŒæ³•å¾‹äº‹ä»¶çš„ç›¸é—œæ³•æ¢ã€æˆ–å¾çµ¦å®šå¥å­ä¸­é—œéµè©çš„é‚è¼¯ã€å› æœæˆ–æ™‚é–“å»¶ä¼¸ä¸­æ¨æ–·å‡ºä¾†ã€‚

æ³•å¾‹æ¢æ–‡å…§å®¹ï¼š
{node.contextualized_text}

è¦æ±‚ï¼š
1. æ³•å¾‹è¦ç´ ï¼šæ¯å€‹å•é¡Œå¿…é ˆåŒ…å«ç‰¹å®šçš„æ³•å¾‹è¦ç´ ï¼ˆæ¢æ–‡è™Ÿã€é©ç”¨ç¯„åœã€æ³•å¾‹å¾Œæœã€æ¬Šåˆ©ç¾©å‹™ã€æ§‹æˆè¦ä»¶ç­‰ï¼‰æˆ–å…¶ä»–é—œéµç‰¹å¾µï¼Œä»¥æ¸›å°‘æ­§ç¾©ä¸¦ç¢ºä¿å•é¡Œçš„ç¨ç«‹æ€§ã€‚
2. å¤šæ¨£æ€§å’Œå®¢è§€æ€§ï¼šä¸åŒçš„å¾ŒçºŒå•é¡Œæ‡‰é—œæ³¨é€™äº›å¥å­æ‰€ä»£è¡¨çš„æ•´é«”æ³•å¾‹äº‹ä»¶çš„å¤šæ¨£åŒ–ã€å®¢è§€æ–¹é¢ï¼Œç¢ºä¿å¤šæ¨£æ€§å’Œä»£è¡¨æ€§ã€‚å„ªå…ˆè€ƒæ…®å®¢è§€å•é¡Œã€‚
3. å› æœå’Œé‚è¼¯é—œä¿‚ï¼šæ ¹æ“šçµ¦å®šçš„å¥å­ï¼Œç”Ÿæˆæ¶‰åŠå› æœé—œä¿‚ã€ä¸¦è¡Œé—œä¿‚ã€åºåˆ—ã€é€²ç¨‹ã€é€£æ¥å’Œå…¶ä»–é‚è¼¯æ–¹é¢çš„å•é¡Œã€‚å¯æ¢ç´¢çš„é ˜åŸŸåŒ…æ‹¬ä½†ä¸é™æ–¼ï¼šæ³•å¾‹äº‹ä»¶çš„èƒŒæ™¯ã€ä¿¡æ¯ã€åŸå› ã€å½±éŸ¿ã€æ„ç¾©ã€ç™¼å±•è¶¨å‹¢æˆ–ç›¸é—œå€‹äººçš„è§€é»ã€‚

æ•¸é‡è¦æ±‚ï¼š
- æœ€å°‘ç”Ÿæˆ {self.config.min_outgoing_questions} å€‹å•é¡Œ
- æœ€å¤šç”Ÿæˆ {self.config.max_outgoing_questions} å€‹å•é¡Œ
- è«‹æ ¹æ“šæ³•å¾‹æ¢æ–‡çš„è¤‡é›œåº¦å’Œå»¶ä¼¸æ€§ï¼Œç”Ÿæˆé©ç•¶æ•¸é‡çš„å•é¡Œä¾†å……åˆ†æ¢ç´¢é‚è¼¯é€£æ¥
- å¦‚æœæ¢æ–‡å…§å®¹ç°¡å–®ï¼Œç”Ÿæˆè¼ƒå°‘å•é¡Œï¼›å¦‚æœæ¢æ–‡å…§å®¹è¤‡é›œä¸”æœ‰å¤šå€‹å»¶ä¼¸é»ï¼Œç”Ÿæˆè¼ƒå¤šå•é¡Œ

ç¯„ä¾‹ï¼š
å¥å­åˆ—è¡¨ï¼š["ç¬¬å…«æ¢ï¼šè‘—ä½œæ¬Šäººäº«æœ‰ä¸‹åˆ—æ¬Šåˆ©ï¼šä¸€ã€é‡è£½æ¬Šï¼›äºŒã€å…¬é–‹æ’­é€æ¬Šï¼›ä¸‰ã€å…¬é–‹å‚³è¼¸æ¬Šã€‚"]
ç­”æ¡ˆç¯„ä¾‹ï¼š
{{
    "Question List": [
        "è‘—ä½œæ¬Šäººå¦‚ä½•è¡Œä½¿é‡è£½æ¬Šï¼Ÿ",
        "é‡è£½æ¬Šçš„ä¿è­·æœŸé™æ˜¯å¤šä¹…ï¼Ÿ",
        "é•åé‡è£½æ¬Šæœƒæœ‰ä»€éº¼æ³•å¾‹å¾Œæœï¼Ÿ",
        "å…¬é–‹æ’­é€æ¬Šèˆ‡å…¬é–‹å‚³è¼¸æ¬Šæœ‰ä»€éº¼å€åˆ¥ï¼Ÿ",
        "è‘—ä½œæ¬Šäººå¦‚ä½•è­‰æ˜å…¶æ¬Šåˆ©å—åˆ°ä¾µå®³ï¼Ÿ"
    ]
}}

è«‹ä»¥JSONæ ¼å¼è¿”å›ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
{{
    "Question List": [
        "å•é¡Œ1",
        "å•é¡Œ2",
        "å•é¡Œ3"
    ]
}}

è«‹ç¢ºä¿JSONæ ¼å¼æ­£ç¢ºï¼Œé¿å…ä¸å¿…è¦çš„è½‰ç¾©ã€æ›è¡Œæˆ–ç©ºæ ¼ã€‚æ‚¨é‚„æ‡‰è©²ç‰¹åˆ¥æ³¨æ„ç¢ºä¿ï¼Œé™¤äº†JSONå’Œåˆ—è¡¨æ ¼å¼æœ¬èº«ä½¿ç”¨é›™å¼•è™Ÿ(")å¤–ï¼Œå…¶ä»–æ‰€æœ‰é›™å¼•è™Ÿçš„å¯¦ä¾‹éƒ½æ‡‰æ›¿æ›ç‚ºå–®å¼•è™Ÿã€‚ä¾‹å¦‚ï¼Œä½¿ç”¨'è‘—ä½œæ¬Šæ³•'è€Œä¸æ˜¯"è‘—ä½œæ¬Šæ³•"ã€‚
è«‹ç¢ºä¿JSONæ ¼å¼æ­£ç¢ºï¼Œä¸è¦åŒ…å«ä»»ä½•å…¶ä»–æ–‡å­—ã€‚
"""
        
        try:
            response = await self.llm_client.generate_async(prompt)
            
            # è§£æJSONéŸ¿æ‡‰
            if response.strip().startswith('{'):
                result = json.loads(response.strip())
                questions = result.get('Question List', [])
                
                # æš«æ™‚ç§»é™¤å•é¡Œæ•¸é‡é©—è­‰
                # questions = self._validate_question_count(
                #     questions, 
                #     self.config.min_outgoing_questions, 
                #     self.config.max_outgoing_questions,
                #     "outgoing"
                # )
                
                # é©—è­‰å•é¡Œè³ªé‡
                validated_questions = self._validate_questions(questions, "outgoing")
                return validated_questions
            else:
                # å¦‚æœéŸ¿æ‡‰ä¸æ˜¯JSONæ ¼å¼ï¼Œå˜—è©¦æå–å•é¡Œ
                questions = self._extract_questions_from_text(response)
                # æš«æ™‚ç§»é™¤å•é¡Œæ•¸é‡é©—è­‰
                # questions = self._validate_question_count(
                #     questions, 
                #     self.config.min_outgoing_questions, 
                #     self.config.max_outgoing_questions,
                #     "outgoing"
                # )
                return questions
                
        except Exception as e:
            print(f"âŒ ç”Ÿæˆå‹•æ…‹å¤–å‘å•é¡Œå¤±æ•—: {e}")
            # è¿”å›é»˜èªå•é¡Œ
            return self._generate_default_outgoing_questions(node)
    
    async def _generate_fixed_outgoing_questions(self, node: LegalNode, target_count: int = None) -> List[str]:
        """å›ºå®šæ•¸é‡ç”Ÿæˆå¤–å‘å•é¡Œ - å‘å¾Œå…¼å®¹"""
        if target_count is None:
            target_count = self.config.max_pseudo_queries_per_node
        
        prompt = f"""
æ‚¨æ˜¯ä¸€ä½æ³•å¾‹å°ˆå®¶ï¼Œæ“…é•·æå‡ºæ·±åˆ»å•é¡Œä¸¦ç²¾é€šä¸­æ–‡ã€‚æ‚¨éœ€è¦æ ¹æ“šæ³•å¾‹æ¢æ–‡ä¸­çš„å¹¾å¥é€£çºŒå¥å­ç”Ÿæˆå¾ŒçºŒå•é¡Œã€‚

ä»»å‹™ï¼šç‚ºä»¥ä¸‹æ³•å¾‹æ¢æ–‡ç”Ÿæˆå¾ŒçºŒå•é¡Œï¼Œé€™äº›å•é¡Œçš„ç­”æ¡ˆä¸åœ¨çµ¦å®šçš„å¥å­ä¸­æ‰¾åˆ°ã€‚

å¾ŒçºŒå•é¡Œå®šç¾©ï¼šç­”æ¡ˆä¸åœ¨çµ¦å®šçš„å¥å­ä¸­æ‰¾åˆ°ã€‚ç­”æ¡ˆå¯ä»¥å¾çµ¦å®šå¥å­ä¹‹å‰æˆ–ä¹‹å¾Œçš„ä¸Šä¸‹æ–‡ã€æ¶µè“‹ç›¸åŒæ³•å¾‹äº‹ä»¶çš„ç›¸é—œæ³•æ¢ã€æˆ–å¾çµ¦å®šå¥å­ä¸­é—œéµè©çš„é‚è¼¯ã€å› æœæˆ–æ™‚é–“å»¶ä¼¸ä¸­æ¨æ–·å‡ºä¾†ã€‚

æ³•å¾‹æ¢æ–‡å…§å®¹ï¼š
{node.contextualized_text}

è¦æ±‚ï¼š
1. æ³•å¾‹è¦ç´ ï¼šæ¯å€‹å•é¡Œå¿…é ˆåŒ…å«ç‰¹å®šçš„æ³•å¾‹è¦ç´ ï¼ˆæ¢æ–‡è™Ÿã€é©ç”¨ç¯„åœã€æ³•å¾‹å¾Œæœã€æ¬Šåˆ©ç¾©å‹™ã€æ§‹æˆè¦ä»¶ç­‰ï¼‰æˆ–å…¶ä»–é—œéµç‰¹å¾µï¼Œä»¥æ¸›å°‘æ­§ç¾©ä¸¦ç¢ºä¿å•é¡Œçš„ç¨ç«‹æ€§ã€‚
2. å¤šæ¨£æ€§å’Œå®¢è§€æ€§ï¼šä¸åŒçš„å¾ŒçºŒå•é¡Œæ‡‰é—œæ³¨é€™äº›å¥å­æ‰€ä»£è¡¨çš„æ•´é«”æ³•å¾‹äº‹ä»¶çš„å¤šæ¨£åŒ–ã€å®¢è§€æ–¹é¢ï¼Œç¢ºä¿å¤šæ¨£æ€§å’Œä»£è¡¨æ€§ã€‚å„ªå…ˆè€ƒæ…®å®¢è§€å•é¡Œã€‚
3. å› æœå’Œé‚è¼¯é—œä¿‚ï¼šæ ¹æ“šçµ¦å®šçš„å¥å­ï¼Œç”Ÿæˆæ¶‰åŠå› æœé—œä¿‚ã€ä¸¦è¡Œé—œä¿‚ã€åºåˆ—ã€é€²ç¨‹ã€é€£æ¥å’Œå…¶ä»–é‚è¼¯æ–¹é¢çš„å•é¡Œã€‚å¯æ¢ç´¢çš„é ˜åŸŸåŒ…æ‹¬ä½†ä¸é™æ–¼ï¼šæ³•å¾‹äº‹ä»¶çš„èƒŒæ™¯ã€ä¿¡æ¯ã€åŸå› ã€å½±éŸ¿ã€æ„ç¾©ã€ç™¼å±•è¶¨å‹¢æˆ–ç›¸é—œå€‹äººçš„è§€é»ã€‚

ç¯„ä¾‹ï¼š
å¥å­åˆ—è¡¨ï¼š["ç¬¬å…«æ¢ï¼šè‘—ä½œæ¬Šäººäº«æœ‰ä¸‹åˆ—æ¬Šåˆ©ï¼šä¸€ã€é‡è£½æ¬Šï¼›äºŒã€å…¬é–‹æ’­é€æ¬Šï¼›ä¸‰ã€å…¬é–‹å‚³è¼¸æ¬Šã€‚"]

ç­”æ¡ˆç¯„ä¾‹ï¼š
{{
    "Question List": [
        "è‘—ä½œæ¬Šäººå¦‚ä½•è¡Œä½¿é‡è£½æ¬Šï¼Ÿ",
        "é•åé‡è£½æ¬Šæœƒæœ‰ä»€éº¼æ³•å¾‹å¾Œæœï¼Ÿ"
    ]
}}

è«‹ç”Ÿæˆ{target_count}å€‹å•é¡Œï¼Œåš´æ ¼éµå¾ªJSONæ ¼å¼ï¼Œé¿å…ä¸å¿…è¦çš„è½‰ç¾©ã€æ›è¡Œæˆ–ç©ºæ ¼ã€‚æ‚¨é‚„æ‡‰è©²ç‰¹åˆ¥æ³¨æ„ç¢ºä¿ï¼Œé™¤äº†JSONå’Œåˆ—è¡¨æ ¼å¼æœ¬èº«ä½¿ç”¨é›™å¼•è™Ÿ(")å¤–ï¼Œå…¶ä»–æ‰€æœ‰é›™å¼•è™Ÿçš„å¯¦ä¾‹éƒ½æ‡‰æ›¿æ›ç‚ºå–®å¼•è™Ÿã€‚ä¾‹å¦‚ï¼Œä½¿ç”¨'è‘—ä½œæ¬Šæ³•'è€Œä¸æ˜¯"è‘—ä½œæ¬Šæ³•"ã€‚

è«‹ä»¥JSONæ ¼å¼è¿”å›ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
{{
    "Question List": [
        "å•é¡Œ1",
        "å•é¡Œ2"
    ]
}}

è«‹ç¢ºä¿JSONæ ¼å¼æ­£ç¢ºï¼Œä¸è¦åŒ…å«ä»»ä½•å…¶ä»–æ–‡å­—ã€‚
"""
        
        try:
            response = await self.llm_client.generate_async(prompt)
            
            # è§£æJSONéŸ¿æ‡‰
            if response.strip().startswith('{'):
                result = json.loads(response.strip())
                questions = result.get('Question List', [])
                
                # é©—è­‰å•é¡Œè³ªé‡
                validated_questions = self._validate_questions(questions, "outgoing")
                # æ ¹æ“šé…ç½®æ±ºå®šæ˜¯å¦é™åˆ¶æ•¸é‡
                if self.config.use_dynamic_question_count:
                    # å‹•æ…‹æ¨¡å¼ï¼šä¸é™åˆ¶æ•¸é‡ï¼Œè®“LLMæ±ºå®š
                    return validated_questions
                else:
                    # å›ºå®šæ¨¡å¼ï¼šä½¿ç”¨é…ç½®çš„æ•¸é‡é™åˆ¶
                    return validated_questions[:self.config.max_pseudo_queries_per_node]
            else:
                # å¦‚æœéŸ¿æ‡‰ä¸æ˜¯JSONæ ¼å¼ï¼Œå˜—è©¦æå–å•é¡Œ
                questions = self._extract_questions_from_text(response)
                # æ ¹æ“šé…ç½®æ±ºå®šæ˜¯å¦é™åˆ¶æ•¸é‡
                if self.config.use_dynamic_question_count:
                    # å‹•æ…‹æ¨¡å¼ï¼šä¸é™åˆ¶æ•¸é‡ï¼Œè®“LLMæ±ºå®š
                    return questions
                else:
                    # å›ºå®šæ¨¡å¼ï¼šä½¿ç”¨é…ç½®çš„æ•¸é‡é™åˆ¶
                    return questions[:self.config.max_pseudo_queries_per_node]
                
        except Exception as e:
            print(f"âŒ ç”Ÿæˆå¤–å‘å•é¡Œå¤±æ•—: {e}")
            # è¿”å›é»˜èªå•é¡Œ
            return self._generate_default_outgoing_questions(node)
    
    def _validate_questions(self, questions: List[str], question_type: str) -> List[str]:
        """é©—è­‰å•é¡Œè³ªé‡"""
        validated = []
        
        for question in questions:
            if isinstance(question, str) and len(question.strip()) > 10:
                # åŸºæœ¬é©—è­‰ï¼šå•é¡Œé•·åº¦ã€åŒ…å«å•è™Ÿç­‰
                question = question.strip()
                if question.endswith('ï¼Ÿ') or question.endswith('?'):
                    validated.append(question)
                else:
                    # å¦‚æœæ²’æœ‰å•è™Ÿï¼Œæ·»åŠ ä¸€å€‹
                    validated.append(question + 'ï¼Ÿ')
        
        return validated
    
    def _validate_question_count(self, questions: List[str], min_count: int, max_count: int, question_type: str) -> List[str]:
        """é©—è­‰å•é¡Œæ•¸é‡æ˜¯å¦ç¬¦åˆè¦æ±‚"""
        if not questions:
            print(f"âš ï¸ {question_type}å•é¡Œç‚ºç©ºï¼Œä½¿ç”¨é»˜èªå•é¡Œ")
            return self._get_default_questions(question_type, min_count)
        
        question_count = len(questions)
        
        if question_count < min_count:
            print(f"âš ï¸ {question_type}å•é¡Œæ•¸é‡ä¸è¶³ï¼ˆ{question_count} < {min_count}ï¼‰ï¼Œè£œå……é»˜èªå•é¡Œ")
            # è£œå……é»˜èªå•é¡Œåˆ°æœ€å°‘æ•¸é‡
            default_questions = self._get_default_questions(question_type, min_count - question_count)
            questions.extend(default_questions)
        elif question_count > max_count:
            print(f"âš ï¸ {question_type}å•é¡Œæ•¸é‡éå¤šï¼ˆ{question_count} > {max_count}ï¼‰ï¼Œæˆªå–å‰{max_count}å€‹")
            questions = questions[:max_count]
        
        print(f"âœ… {question_type}å•é¡Œæ•¸é‡é©—è­‰å®Œæˆï¼š{len(questions)}å€‹å•é¡Œ")
        return questions
    
    def _get_default_questions(self, question_type: str, count: int) -> List[str]:
        """ç²å–é»˜èªå•é¡Œ"""
        if question_type == "incoming":
            return [
                f"æ­¤æ¢æ–‡çš„ä¸»è¦å…§å®¹æ˜¯ä»€éº¼ï¼Ÿ",
                f"æ­¤æ¢æ–‡è¦å®šäº†å“ªäº›æ³•å¾‹è¦ä»¶ï¼Ÿ",
                f"æ­¤æ¢æ–‡çš„é©ç”¨ç¯„åœæ˜¯ä»€éº¼ï¼Ÿ"
            ][:count]
        else:  # outgoing
            return [
                f"é•åæ­¤æ¢æ–‡æœƒæœ‰ä»€éº¼æ³•å¾‹å¾Œæœï¼Ÿ",
                f"æ­¤æ¢æ–‡èˆ‡å…¶ä»–ç›¸é—œæ³•æ¢æœ‰ä»€éº¼é—œä¿‚ï¼Ÿ",
                f"æ­¤æ¢æ–‡åœ¨å¯¦å‹™ä¸­å¦‚ä½•æ‡‰ç”¨ï¼Ÿ",
                f"æ­¤æ¢æ–‡çš„ç«‹æ³•ç›®çš„æ˜¯ä»€éº¼ï¼Ÿ"
            ][:count]
    
    def _extract_questions_from_text(self, text: str) -> List[str]:
        """å¾æ–‡æœ¬ä¸­æå–å•é¡Œ"""
        questions = []
        lines = text.split('\n')
        
        for line in lines:
            line = line.strip()
            if ('ï¼Ÿ' in line or '?' in line) and len(line) > 10:
                # æ¸…ç†è¡Œå…§å®¹
                line = line.replace('- ', '').replace('* ', '').replace('1. ', '').replace('2. ', '').replace('3. ', '')
                questions.append(line)
        
        # æ ¹æ“šé…ç½®æ±ºå®šæ˜¯å¦é™åˆ¶æ•¸é‡
        if self.config.use_dynamic_question_count:
            # å‹•æ…‹æ¨¡å¼ï¼šä¸é™åˆ¶æ•¸é‡ï¼Œè®“LLMæ±ºå®š
            return questions
        else:
            # å›ºå®šæ¨¡å¼ï¼šä½¿ç”¨é…ç½®çš„æ•¸é‡é™åˆ¶
            return questions[:self.config.max_pseudo_queries_per_node]
    
    def _generate_default_incoming_questions(self, node: LegalNode) -> List[str]:
        """ç”Ÿæˆé»˜èªå…§å‘å•é¡Œ"""
        article_num = node.article_number
        
        return [
            f"{article_num}çš„ä¸»è¦å…§å®¹æ˜¯ä»€éº¼ï¼Ÿ",
            f"{article_num}è¦å®šäº†å“ªäº›æ³•å¾‹è¦ä»¶ï¼Ÿ",
            f"æ ¹æ“š{article_num}ï¼Œç›¸é—œçš„å®šç¾©æ˜¯ä»€éº¼ï¼Ÿ",
            f"{article_num}çš„é©ç”¨ç¯„åœæ˜¯ä»€éº¼ï¼Ÿ",
            f"{article_num}è¦å®šäº†ä»€éº¼æ³•å¾‹å¾Œæœï¼Ÿ"
        ][:self.config.max_pseudo_queries_per_node]
    
    def _generate_default_outgoing_questions(self, node: LegalNode) -> List[str]:
        """ç”Ÿæˆé»˜èªå¤–å‘å•é¡Œ"""
        article_num = node.article_number
        
        return [
            f"é•å{article_num}æœƒæœ‰ä»€éº¼æ³•å¾‹å¾Œæœï¼Ÿ",
            f"å¦‚ä½•ç”³è«‹{article_num}è¦å®šçš„æ¬Šåˆ©ï¼Ÿ",
            f"{article_num}èˆ‡å…¶ä»–æ³•æ¢æœ‰ä»€éº¼é—œè¯ï¼Ÿ",
            f"åœ¨ä»€éº¼æƒ…æ³ä¸‹é©ç”¨{article_num}ï¼Ÿ",
            f"{article_num}çš„å¯¦å‹™æ“ä½œç¨‹åºæ˜¯ä»€éº¼ï¼Ÿ"
        ][:self.config.max_pseudo_queries_per_node]
    
    def _extract_keywords_from_query(self, query_content: str) -> Set[str]:
        """å¾æŸ¥è©¢å…§å®¹ä¸­æå–é—œéµè©"""
        # æ³•å¾‹é ˜åŸŸåœç”¨è©
        stop_words = {
            'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº', 'éƒ½', 'ä¸€', 'ä¸€å€‹', 'ä¸Š', 'ä¹Ÿ', 'å¾ˆ', 'åˆ°', 'èªª', 'è¦', 'å»', 'ä½ ', 'æœƒ', 'è‘—', 'æ²’æœ‰', 'çœ‹', 'å¥½', 'è‡ªå·±', 'é€™',
            'ä»€éº¼', 'å¦‚ä½•', 'ç‚ºä»€éº¼', 'å“ªäº›', 'ä»€éº¼æ™‚å€™', 'å“ªè£¡', 'å¤šå°‘', 'æ€éº¼', 'æ˜¯å¦', 'èƒ½å¦', 'å¯ä»¥', 'æ‡‰è©²', 'å¿…é ˆ', 'éœ€è¦', 'è¦æ±‚', 'è¦å®š', 'æ¢æ–‡', 'æ³•å¾‹', 'æ³•æ¢', 'æ¢æ¬¾', 'é …ç›®', 'å…§å®¹', 'å®šç¾©', 'é©ç”¨', 'ç¯„åœ', 'å¾Œæœ', 'ç¨‹åº', 'æ¬Šåˆ©', 'ç¾©å‹™', 'è²¬ä»»', 'è™•ç½°', 'ç½°å‰‡', 'é•å', 'ç”³è«‹', 'å¯¦å‹™', 'æ“ä½œ', 'é—œè¯', 'æƒ…æ³', 'æ¢ä»¶', 'æ¨™æº–', 'åŸå‰‡', 'æ–¹æ³•', 'æ–¹å¼', 'æµç¨‹', 'æ­¥é©Ÿ', 'éç¨‹', 'çµæœ', 'æ•ˆæœ', 'å½±éŸ¿', 'æ„ç¾©', 'ä½œç”¨', 'åŠŸèƒ½', 'ç‰¹é»', 'æ€§è³ª', 'é¡å‹', 'ç¨®é¡', 'åˆ†é¡', 'å€åˆ¥', 'å·®ç•°', 'ç›¸åŒ', 'ä¸åŒ', 'é¡ä¼¼', 'ç›¸é—œ', 'ç„¡é—œ', 'é‡è¦', 'ä¸»è¦', 'åŸºæœ¬', 'æ ¸å¿ƒ', 'é—œéµ', 'å¿…è¦', 'å……åˆ†', 'æœ‰æ•ˆ', 'ç„¡æ•ˆ', 'åˆæ³•', 'éæ³•', 'æ­£ç•¶', 'ä¸æ­£ç•¶', 'åˆç†', 'ä¸åˆç†', 'é©ç•¶', 'ä¸é©ç•¶', 'æ­£ç¢º', 'éŒ¯èª¤', 'æº–ç¢º', 'ä¸æº–ç¢º', 'å®Œæ•´', 'ä¸å®Œæ•´', 'æ¸…æ¥š', 'ä¸æ¸…æ¥š', 'æ˜ç¢º', 'ä¸æ˜ç¢º', 'å…·é«”', 'æŠ½è±¡', 'è©³ç´°', 'ç°¡ç•¥', 'å…¨é¢', 'ç‰‡é¢', 'å®¢è§€', 'ä¸»è§€', 'å…¬æ­£', 'ä¸å…¬æ­£', 'å…¬å¹³', 'ä¸å…¬å¹³', 'å¹³ç­‰', 'ä¸å¹³ç­‰', 'è‡ªç”±', 'é™åˆ¶', 'ä¿è­·', 'ä¿éšœ', 'ç¶­è­·', 'ä¿ƒé€²', 'ç™¼å±•', 'æ”¹å–„', 'æé«˜', 'é™ä½', 'å¢åŠ ', 'æ¸›å°‘', 'æ“´å¤§', 'ç¸®å°', 'åŠ å¼·', 'å‰Šå¼±', 'å®Œå–„', 'æ”¹é€²', 'å„ªåŒ–', 'èª¿æ•´', 'ä¿®æ”¹', 'è®Šæ›´', 'æ›´æ–°', 'å‡ç´š', 'é™ç´š', 'æå‡', 'ä¸‹é™', 'ä¸Šå‡', 'ä¸‹é™', 'å¢é•·', 'ä¸‹é™', 'ä¸Šå‡', 'ä¸‹é™', 'æé«˜', 'é™ä½', 'å¢åŠ ', 'æ¸›å°‘', 'æ“´å¤§', 'ç¸®å°', 'åŠ å¼·', 'å‰Šå¼±', 'å®Œå–„', 'æ”¹é€²', 'å„ªåŒ–', 'èª¿æ•´', 'ä¿®æ”¹', 'è®Šæ›´', 'æ›´æ–°', 'å‡ç´š', 'é™ç´š', 'æå‡', 'ä¸‹é™', 'ä¸Šå‡', 'ä¸‹é™', 'å¢é•·', 'ä¸‹é™', 'ä¸Šå‡', 'ä¸‹é™'
        }
        
        # 1. åˆ†è©
        words = jieba.lcut(query_content)
        
        # 2. éæ¿¾åœç”¨è©å’ŒçŸ­è©
        keywords = set()
        for word in words:
            word = word.strip()
            if (len(word) >= 2 and 
                word not in stop_words and 
                not re.match(r'^[0-9]+$', word) and
                not re.match(r'^[a-zA-Z]+$', word) and
                not re.match(r'^[^\u4e00-\u9fff]+$', word)):  # åªä¿ç•™ä¸­æ–‡å­—ç¬¦
                keywords.add(word)
        
        return keywords

class EdgeConnector:
    """é‚Šé€£æ¥å™¨"""
    
    def __init__(self, config: HopRAGConfig = DEFAULT_CONFIG):
        self.config = config
    
    async def connect_edges(self, nodes: Dict[str, LegalNode], embedding_model) -> Dict[str, List[Dict[str, Any]]]:
        """é€£æ¥ç¯€é»é‚Š - æ”¯æŒå‹•æ…‹é‚Šæ•¸é™åˆ¶å’Œé‚è¼¯é—œä¿‚é‚Š"""
        print("ğŸ”— é–‹å§‹é‚ŠåŒ¹é…å’Œé€£æ¥...")
        
        # è¨ˆç®—å‹•æ…‹é‚Šæ•¸é™åˆ¶ï¼ˆåŸºæ–¼è«–æ–‡O(n log n)è¦æ±‚ï¼‰
        n = len(nodes)
        if self.config.use_dynamic_edge_limit:
            dynamic_limit = int(self.config.edge_limit_factor * n * np.log2(n + 1))
            print(f"ğŸ“Š å‹•æ…‹é‚Šæ•¸é™åˆ¶: O(n log n) = {dynamic_limit} (n={n})")
        else:
            dynamic_limit = self.config.max_edges_per_node * n
            print(f"ğŸ“Š å›ºå®šé‚Šæ•¸é™åˆ¶: {dynamic_limit}")
        
        # Step 1: ç‚ºæ‰€æœ‰å½æŸ¥è©¢ç”Ÿæˆembedding
        await self._generate_pseudo_query_embeddings(nodes, embedding_model)
        
        # Step 2: æ§‹å»ºé‚è¼¯é—œä¿‚é‚Š
        print("ğŸ—ï¸ æ§‹å»ºé‚è¼¯é—œä¿‚é‚Š...")
        logical_edges = self._build_logical_edges(nodes)
        
        # Step 3: åŸ·è¡Œèªç¾©ç›¸ä¼¼åº¦é‚ŠåŒ¹é…ç®—æ³•
        print("ğŸ” åŸ·è¡Œèªç¾©ç›¸ä¼¼åº¦é‚ŠåŒ¹é…...")
        semantic_edges = await self._perform_edge_matching(nodes)
        
        # Step 4: åˆä½µé‚è¼¯é‚Šå’Œèªç¾©é‚Š
        print("ğŸ”„ åˆä½µé‚è¼¯é—œä¿‚é‚Šå’Œèªç¾©é‚Š...")
        edges = self._merge_edge_types(logical_edges, semantic_edges)
        
        # Step 5: æ‡‰ç”¨é‚Šæ•¸é™åˆ¶
        edges = self._apply_edge_limit(edges, dynamic_limit)
        
        # çµ±è¨ˆæ··åˆæª¢ç´¢ä¿¡æ¯
        total_edges = sum(len(edge_list) for edge_list in edges.values())
        logical_count = sum(len(edge_list) for edge_list in logical_edges.values())
        semantic_count = sum(len(edge_list) for edge_list in semantic_edges.values())
        
        print(f"âœ… é‚Šé€£æ¥å®Œæˆï¼Œå…±å»ºç«‹ {total_edges} æ¢é‚Š")
        print(f"   - é‚è¼¯é—œä¿‚é‚Š: {logical_count} æ¢")
        print(f"   - èªç¾©ç›¸ä¼¼åº¦é‚Š: {semantic_count} æ¢")
        
        if self.config.use_hybrid_retrieval:
            print(f"ğŸ”— æ··åˆæª¢ç´¢çµ±è¨ˆ:")
            print(f"   - Jaccardæ¬Šé‡: {self.config.jaccard_weight}")
            print(f"   - é¤˜å¼¦æ¬Šé‡: {self.config.cosine_weight}")
            print(f"   - è©å½™é–¾å€¼: {self.config.lexical_threshold}")
            print(f"   - èªç¾©é–¾å€¼: {self.config.semantic_threshold}")
        
        return edges
    
    def _apply_edge_limit(self, edges: Dict[str, List[Dict[str, Any]]], total_limit: int) -> Dict[str, List[Dict[str, Any]]]:
        """æ‡‰ç”¨é‚Šæ•¸é™åˆ¶ - åŸºæ–¼è«–æ–‡O(n log n)è¦æ±‚"""
        current_total = sum(len(edge_list) for edge_list in edges.values())
        
        if current_total <= total_limit:
            print(f"âœ… é‚Šæ•¸æœªè¶…éé™åˆ¶: {current_total} <= {total_limit}")
            return edges
        
        print(f"âš ï¸ é‚Šæ•¸è¶…éé™åˆ¶: {current_total} > {total_limit}ï¼Œé–‹å§‹ä¿®å‰ª...")
        
        # æ”¶é›†æ‰€æœ‰é‚Šä¸¦æŒ‰ç›¸ä¼¼åº¦æ’åº
        all_edges = []
        for node_id, edge_list in edges.items():
            for edge in edge_list:
                edge['source_node'] = node_id
                all_edges.append(edge)
        
        # æŒ‰ç›¸ä¼¼åº¦åˆ†æ•¸æ’åºï¼ˆé™åºï¼‰
        all_edges.sort(key=lambda x: x.get('similarity_score', 0), reverse=True)
        
        # ä¿ç•™å‰total_limitæ¢é‚Š
        selected_edges = all_edges[:total_limit]
        
        # é‡æ–°çµ„ç¹”é‚Šå­—å…¸
        limited_edges = {}
        for edge in selected_edges:
            source_node = edge.pop('source_node')
            if source_node not in limited_edges:
                limited_edges[source_node] = []
            limited_edges[source_node].append(edge)
        
        # ç¢ºä¿æ‰€æœ‰ç¯€é»éƒ½æœ‰é‚Šåˆ—è¡¨ï¼ˆå³ä½¿ç‚ºç©ºï¼‰
        for node_id in edges.keys():
            if node_id not in limited_edges:
                limited_edges[node_id] = []
        
        print(f"âœ… é‚Šæ•¸ä¿®å‰ªå®Œæˆ: {current_total} -> {sum(len(edge_list) for edge_list in limited_edges.values())}")
        
        return limited_edges
    
    async def _generate_pseudo_query_embeddings(self, nodes: Dict[str, LegalNode], embedding_model):
        """ç‚ºæ‰€æœ‰å½æŸ¥è©¢ç”Ÿæˆembedding"""
        print("ğŸ“Š é–‹å§‹ç”Ÿæˆå½æŸ¥è©¢embeddingå‘é‡...")
        
        all_queries = []
        query_mapping = {}
        
        # æ”¶é›†æ‰€æœ‰å½æŸ¥è©¢
        for node_id, node in nodes.items():
            for pseudo_query in node.pseudo_queries.get("outgoing", []):
                all_queries.append(pseudo_query.content)
                query_mapping[pseudo_query.content] = (node_id, "outgoing", pseudo_query)
                
            for pseudo_query in node.pseudo_queries.get("incoming", []):
                all_queries.append(pseudo_query.content)
                query_mapping[pseudo_query.content] = (node_id, "incoming", pseudo_query)
        
        if not all_queries:
            print("âš ï¸ æ²’æœ‰å½æŸ¥è©¢éœ€è¦ç”Ÿæˆembedding")
            return
        
        print(f"ğŸ“ˆ ç¸½å…±éœ€è¦ç”Ÿæˆ {len(all_queries)} å€‹embeddingå‘é‡")
        print(f"â±ï¸ é è¨ˆéœ€è¦ 1-2 åˆ†é˜")
        
        # æ‰¹é‡ç”Ÿæˆembedding
        try:
            start_time = time.time()
            if hasattr(embedding_model, 'encode_async'):
                embeddings = await embedding_model.encode_async(all_queries)
            else:
                embeddings = embedding_model.encode(all_queries)
            
            embedding_time = time.time() - start_time
            print(f"âœ… Embeddingç”Ÿæˆå®Œæˆï¼è€—æ™‚: {embedding_time:.1f} ç§’")
            
            # å°‡embeddingåˆ†é…å›å½æŸ¥è©¢
            for i, query_content in enumerate(all_queries):
                node_id, query_type, pseudo_query = query_mapping[query_content]
                pseudo_query.embedding = embeddings[i]
                
        except Exception as e:
            print(f"âŒ ç”Ÿæˆå½æŸ¥è©¢embeddingå¤±æ•—: {e}")
    
    async def _perform_edge_matching(self, nodes: Dict[str, LegalNode]) -> Dict[str, List[Dict[str, Any]]]:
        """åŸ·è¡Œé‚ŠåŒ¹é…ç®—æ³•"""
        print("ğŸ”— åŸ·è¡Œé‚ŠåŒ¹é…ç®—æ³•...")
        
        node_ids = list(nodes.keys())
        edges = {}
        
        for i, node_a in enumerate(node_ids):
            outgoing_queries_a = nodes[node_a].pseudo_queries.get("outgoing", [])
            edges[node_a] = []
            
            for j, node_b in enumerate(node_ids):
                if i == j:
                    continue
                    
                incoming_queries_b = nodes[node_b].pseudo_queries.get("incoming", [])
                
                # è¨ˆç®—æœ€ä½³ç›¸ä¼¼åº¦
                best_similarity = 0
                best_outgoing_query = None
                best_incoming_query = None
                
                for out_query in outgoing_queries_a:
                    for in_query in incoming_queries_b:
                        if (out_query.embedding is not None and in_query.embedding is not None and
                            out_query.keywords and in_query.keywords):
                            
                            # è¨ˆç®—å„ç¨®ç›¸ä¼¼åº¦
                            jaccard_sim = self._calculate_jaccard_similarity(out_query.keywords, in_query.keywords)
                            cosine_sim = self._calculate_cosine_similarity(out_query.embedding, in_query.embedding)
                            
                            # è¨ˆç®—ç¶œåˆç›¸ä¼¼åº¦
                            similarity = self._calculate_similarity(out_query, in_query)
                            
                            # æ··åˆæª¢ç´¢éæ¿¾ï¼šæª¢æŸ¥è©å½™å’Œèªç¾©é–¾å€¼ï¼ˆåœ¨è¨ˆç®—ç›¸ä¼¼åº¦å¾Œï¼‰
                            if self.config.use_hybrid_retrieval:
                                if (jaccard_sim < self.config.lexical_threshold or 
                                    cosine_sim < self.config.semantic_threshold):
                                    continue  # è·³éä¸æ»¿è¶³é–¾å€¼çš„çµ„åˆ
                            
                            if similarity > best_similarity:
                                best_similarity = similarity
                                best_outgoing_query = out_query
                                best_incoming_query = in_query
                
                # å¦‚æœç›¸ä¼¼åº¦è¶…éé–¾å€¼ï¼Œå»ºç«‹é‚Š
                if best_similarity >= self.config.similarity_threshold:
                    # å‰µå»ºå®Œæ•´çš„é‚Šç‰¹å¾µ
                    edge_feature = self._create_edge_feature(best_outgoing_query, best_incoming_query)
                    
                    edge_attr = {
                        'to_node': node_b,
                        'pseudo_query': f"{best_outgoing_query.content} -> {best_incoming_query.content}",
                        'similarity_score': best_similarity,
                        'jaccard_score': edge_feature.jaccard_score,
                        'cosine_score': edge_feature.cosine_score,
                        'mixed_score': edge_feature.mixed_score,
                        'combined_keywords': list(edge_feature.combined_keywords),
                        'edge_query': edge_feature.query,
                        'edge_embedding': edge_feature.embedding.tolist() if edge_feature.embedding is not None else None,
                        'outgoing_query_id': best_outgoing_query.query_id,
                        'incoming_query_id': best_incoming_query.query_id,
                        'edge_type': self._determine_edge_type(node_a, node_b).value  # è½‰æ›ç‚ºå­—ç¬¦ä¸²
                    }
                    
                    edges[node_a].append(edge_attr)
                    
                    # é™åˆ¶æ¯å€‹ç¯€é»çš„æœ€å¤§å‡ºé‚Šæ•¸é‡
                    if len(edges[node_a]) >= self.config.max_edges_per_node:
                        break
            
            if i % 10 == 0:
                print(f"  è™•ç†é€²åº¦: {i+1}/{len(node_ids)} ç¯€é»")
        
        return edges
    
    def _calculate_similarity(self, query_a: PseudoQuery, query_b: PseudoQuery) -> float:
        """è¨ˆç®—å…©å€‹å½æŸ¥è©¢çš„æ··åˆç›¸ä¼¼åº¦ï¼ˆJaccard + é¤˜å¼¦ç›¸ä¼¼åº¦ï¼‰"""
        # 1. è¨ˆç®—Jaccardç›¸ä¼¼åº¦ï¼ˆè©å½™å±¤é¢ï¼‰
        jaccard_sim = self._calculate_jaccard_similarity(query_a.keywords, query_b.keywords)
        
        # 2. è¨ˆç®—é¤˜å¼¦ç›¸ä¼¼åº¦ï¼ˆèªç¾©å±¤é¢ï¼‰
        cosine_sim = self._calculate_cosine_similarity(query_a.embedding, query_b.embedding)
        
        # 3. æ··åˆæª¢ç´¢ï¼šæ ¹æ“šé…ç½®æ¬Šé‡è¨ˆç®—
        if self.config.use_hybrid_retrieval:
            # ä½¿ç”¨é…ç½®çš„æ¬Šé‡
            mixed_similarity = (jaccard_sim * self.config.jaccard_weight + 
                              cosine_sim * self.config.cosine_weight)
        else:
            # ä½¿ç”¨è«–æ–‡å…¬å¼ï¼šå…©è€…å¹³å‡å€¼
            mixed_similarity = (jaccard_sim + cosine_sim) / 2
        
        return float(mixed_similarity)
    
    def _calculate_jaccard_similarity(self, keywords_a: Set[str], keywords_b: Set[str]) -> float:
        """è¨ˆç®—Jaccardç›¸ä¼¼åº¦"""
        if not keywords_a or not keywords_b:
            return 0.0
        
        intersection = len(keywords_a.intersection(keywords_b))
        union = len(keywords_a.union(keywords_b))
        
        if union == 0:
            return 0.0
        
        return intersection / union
    
    def _calculate_cosine_similarity(self, embedding_a: np.ndarray, embedding_b: np.ndarray) -> float:
        """è¨ˆç®—é¤˜å¼¦ç›¸ä¼¼åº¦"""
        if embedding_a is None or embedding_b is None:
            return 0.0
        
        dot_product = np.dot(embedding_a, embedding_b)
        norm_a = np.linalg.norm(embedding_a)
        norm_b = np.linalg.norm(embedding_b)
        
        if norm_a == 0 or norm_b == 0:
            return 0.0
        
        return dot_product / (norm_a * norm_b)
    
    def _create_edge_feature(self, outgoing_query: PseudoQuery, incoming_query: PseudoQuery) -> EdgeFeature:
        """å‰µå»ºé‚Šç‰¹å¾µ - æŒ‰ç…§è«–æ–‡è¦æ±‚ e_s,t* = (q_t*,j*^-, k_t*,j*^- âˆª k_s,i^+, v_t*,j*^-)"""
        
        # åˆä½µé—œéµè©é›†åˆï¼šk_t*,j*^- âˆª k_s,i^+
        combined_keywords = incoming_query.keywords.union(outgoing_query.keywords)
        
        # è¨ˆç®—å„ç¨®ç›¸ä¼¼åº¦åˆ†æ•¸
        jaccard_score = self._calculate_jaccard_similarity(
            outgoing_query.keywords, incoming_query.keywords
        )
        cosine_score = self._calculate_cosine_similarity(
            outgoing_query.embedding, incoming_query.embedding
        )
        mixed_score = (jaccard_score + cosine_score) / 2
        
        return EdgeFeature(
            query=incoming_query.content,  # q_t*,j*^-
            combined_keywords=combined_keywords,  # k_t*,j*^- âˆª k_s,i^+
            embedding=incoming_query.embedding,  # v_t*,j*^-
            jaccard_score=jaccard_score,
            cosine_score=cosine_score,
            mixed_score=mixed_score
        )
    
    def _determine_edge_type(self, from_node: str, to_node: str) -> EdgeType:
        """ç¢ºå®šé‚Šçš„é¡å‹ - ä½¿ç”¨æ–°çš„å±¤ç´šå‘½å"""
        # é€™è£¡éœ€è¦æ ¹æ“šå¯¦éš›çš„ç¯€é»é¡å‹ä¾†åˆ¤æ–·
        # æª¢æŸ¥ç¯€é»IDä¸­çš„å±¤ç´šæ¨™è­˜
        if 'basic_unit_component' in from_node and 'basic_unit_component' in to_node:
            return EdgeType.COMPONENT_TO_COMPONENT
        elif 'basic_unit' in from_node and 'basic_unit' in to_node and 'component' not in from_node and 'component' not in to_node:
            return EdgeType.BASIC_UNIT_TO_BASIC_UNIT
        elif 'basic_unit' in from_node and 'component' not in from_node and 'basic_unit_component' in to_node:
            return EdgeType.BASIC_UNIT_TO_COMPONENT
        elif 'basic_unit_component' in from_node and 'basic_unit' in to_node and 'component' not in to_node:
            return EdgeType.COMPONENT_TO_BASIC_UNIT
        else:
            # é»˜èªç‚ºbasic_unitä¹‹é–“çš„é‚Š
            return EdgeType.BASIC_UNIT_TO_BASIC_UNIT
    
    def _build_logical_edges(self, nodes: Dict[str, LegalNode]) -> Dict[str, List[Dict[str, Any]]]:
        """æ§‹å»ºåŸºæ–¼é‚è¼¯é—œä¿‚çš„é‚Š"""
        logical_edges = {node_id: [] for node_id in nodes.keys()}
        
        # 1. æ§‹å»ºå±¤æ¬¡çµæ§‹é—œä¿‚é‚Š
        hierarchical_edges = self._build_hierarchical_edges(nodes)
        self._merge_edges(logical_edges, hierarchical_edges)
        
        # 2. æ§‹å»ºå¼•ç”¨é—œä¿‚é‚Š
        reference_edges = self._build_reference_edges(nodes)
        self._merge_edges(logical_edges, reference_edges)
        
        # 3. æ§‹å»ºä¸»é¡Œç›¸ä¼¼æ€§é‚Šï¼ˆåŸºæ–¼æ³•å¾‹æ¦‚å¿µï¼‰
        topical_edges = self._build_topical_edges(nodes)
        self._merge_edges(logical_edges, topical_edges)
        
        return logical_edges
    
    def _build_hierarchical_edges(self, nodes: Dict[str, LegalNode]) -> Dict[str, List[Dict[str, Any]]]:
        """æ§‹å»ºå±¤æ¬¡çµæ§‹é—œä¿‚é‚Š"""
        hierarchical_edges = {node_id: [] for node_id in nodes.keys()}
        
        # æŒ‰æ³•å¾‹åç¨±åˆ†çµ„
        law_groups = {}
        for node_id, node in nodes.items():
            if node.law_name not in law_groups:
                law_groups[node.law_name] = []
            law_groups[node.law_name].append((node_id, node))
        
        for law_name, law_nodes in law_groups.items():
            # æŒ‰æ¢æ–‡è™Ÿæ’åº
            law_nodes.sort(key=lambda x: self._extract_article_number(x[1].article_number))
            
            # é€£æ¥ç›¸é„°æ¢æ–‡
            for i in range(len(law_nodes) - 1):
                current_id, current_node = law_nodes[i]
                next_id, next_node = law_nodes[i + 1]
                
                # å‰µå»ºé›™å‘é‚Š
                edge = {
                    "target": next_id,
                    "edge_type": EdgeType.BASIC_UNIT_TO_BASIC_UNIT.value,
                    "similarity": 0.8,  # é«˜ç›¸ä¼¼åº¦ï¼Œå› ç‚ºæ˜¯åŒä¸€æ³•å¾‹çš„ç›¸é„°æ¢æ–‡
                    "edge_reason": "hierarchical_adjacent_articles",
                    "metadata": {
                        "law_name": law_name,
                        "current_article": current_node.article_number,
                        "next_article": next_node.article_number,
                        "relationship": "adjacent_articles"
                    }
                }
                hierarchical_edges[current_id].append(edge)
                
                # åå‘é‚Š
                reverse_edge = {
                    "target": current_id,
                    "edge_type": EdgeType.BASIC_UNIT_TO_BASIC_UNIT.value,
                    "similarity": 0.8,
                    "edge_reason": "hierarchical_adjacent_articles",
                    "metadata": {
                        "law_name": law_name,
                        "current_article": next_node.article_number,
                        "previous_article": current_node.article_number,
                        "relationship": "adjacent_articles"
                    }
                }
                hierarchical_edges[next_id].append(reverse_edge)
        
        return hierarchical_edges
    
    def _build_reference_edges(self, nodes: Dict[str, LegalNode]) -> Dict[str, List[Dict[str, Any]]]:
        """æ§‹å»ºå¼•ç”¨é—œä¿‚é‚Šï¼ˆåŸºæ–¼æ¢æ–‡å…§å®¹ä¸­çš„å¼•ç”¨ï¼‰"""
        reference_edges = {node_id: [] for node_id in nodes.keys()}
        
        # æå–å¼•ç”¨æ¨¡å¼çš„æ­£å‰‡è¡¨é”å¼
        reference_patterns = [
            r'ç¬¬\s*(\d+)\s*æ¢',  # ç¬¬Xæ¢
            r'ç¬¬\s*(\d+)\s*é …',  # ç¬¬Xé …
            r'å‰é …',  # å‰é …
            r'å‰æ¢',  # å‰æ¢
            r'æœ¬æ¢',  # æœ¬æ¢
            r'æœ¬é …',  # æœ¬é …
            r'ä¾\s*ç¬¬\s*(\d+)\s*æ¢',  # ä¾ç¬¬Xæ¢
        ]
        
        for source_id, source_node in nodes.items():
            content = source_node.content
            
            for pattern in reference_patterns:
                matches = re.finditer(pattern, content)
                for match in matches:
                    referenced_number = match.group(1) if match.groups() else None
                    
                    # æŸ¥æ‰¾è¢«å¼•ç”¨çš„ç¯€é»
                    for target_id, target_node in nodes.items():
                        if (target_id != source_id and 
                            target_node.law_name == source_node.law_name):
                            
                            # æª¢æŸ¥æ˜¯å¦å¼•ç”¨è©²æ¢æ–‡
                            if referenced_number and referenced_number == target_node.article_number:
                                edge = {
                                    "target": target_id,
                                    "edge_type": EdgeType.BASIC_UNIT_TO_BASIC_UNIT.value,
                                    "similarity": 0.9,  # é«˜ç›¸ä¼¼åº¦ï¼Œå› ç‚ºæ˜¯æ˜ç¢ºå¼•ç”¨
                                    "edge_reason": "reference_relationship",
                                    "metadata": {
                                        "law_name": source_node.law_name,
                                        "source_article": source_node.article_number,
                                        "referenced_article": target_node.article_number,
                                        "reference_pattern": pattern,
                                        "match_text": match.group()
                                    }
                                }
                                reference_edges[source_id].append(edge)
        
        return reference_edges
    
    def _build_topical_edges(self, nodes: Dict[str, LegalNode]) -> Dict[str, List[Dict[str, Any]]]:
        """æ§‹å»ºä¸»é¡Œç›¸ä¼¼æ€§é‚Šï¼ˆåŸºæ–¼æ³•å¾‹æ¦‚å¿µå’Œé—œéµè©ï¼‰"""
        topical_edges = {node_id: [] for node_id in nodes.keys()}
        
        # æ³•å¾‹æ¦‚å¿µé—œéµè©
        legal_concepts = {
            'è‘—ä½œæ¬Š': ['è‘—ä½œæ¬Š', 'ç‰ˆæ¬Š', 'å‰µä½œ', 'ä½œå“', 'ä½œè€…', 'è‘—ä½œäºº'],
            'å•†æ¨™': ['å•†æ¨™', 'æ¨™ç« ', 'è¨»å†Š', 'å“ç‰Œ', 'è­˜åˆ¥'],
            'å°ˆåˆ©': ['å°ˆåˆ©', 'ç™¼æ˜', 'æŠ€è¡“', 'å‰µæ–°', 'å°ˆåˆ©æ¬Š'],
            'ä¾µæ¬Š': ['ä¾µæ¬Š', 'ä¾µå®³', 'é•å', 'ä¸æ³•', 'æå®³'],
            'æˆæ¬Š': ['æˆæ¬Š', 'è¨±å¯', 'åŒæ„', 'å…è¨±', 'æˆèˆ‡'],
            'è³ å„Ÿ': ['è³ å„Ÿ', 'æå®³', 'è£œå„Ÿ', 'ç½°é‡‘', 'ç½°æ¬¾'],
            'ç¨‹åº': ['ç¨‹åº', 'æµç¨‹', 'æ‰‹çºŒ', 'ç”³è«‹', 'å¯©æŸ¥']
        }
        
        # ç‚ºæ¯å€‹ç¯€é»æå–æ¦‚å¿µæ¨™ç±¤
        node_concepts = {}
        for node_id, node in nodes.items():
            concepts = []
            content_lower = node.content.lower()
            
            for concept, keywords in legal_concepts.items():
                if any(keyword in content_lower for keyword in keywords):
                    concepts.append(concept)
            
            node_concepts[node_id] = concepts
        
        # åŸºæ–¼æ¦‚å¿µç›¸ä¼¼æ€§æ§‹å»ºé‚Š
        for source_id, source_concepts in node_concepts.items():
            for target_id, target_concepts in node_concepts.items():
                if source_id != target_id:
                    # è¨ˆç®—æ¦‚å¿µé‡ç–Šåº¦
                    common_concepts = set(source_concepts) & set(target_concepts)
                    if common_concepts:
                        similarity = len(common_concepts) / max(len(source_concepts), len(target_concepts))
                        
                        if similarity >= 0.3:  # è‡³å°‘30%çš„æ¦‚å¿µé‡ç–Š
                            edge = {
                                "target": target_id,
                                "edge_type": EdgeType.BASIC_UNIT_TO_BASIC_UNIT.value,
                                "similarity": similarity,
                                "edge_reason": "topical_similarity",
                                "metadata": {
                                    "common_concepts": list(common_concepts),
                                    "source_concepts": source_concepts,
                                    "target_concepts": target_concepts,
                                    "similarity_score": similarity
                                }
                            }
                            topical_edges[source_id].append(edge)
        
        return topical_edges
    
    def _merge_edges(self, target_edges: Dict[str, List[Dict[str, Any]]], 
                    source_edges: Dict[str, List[Dict[str, Any]]]):
        """åˆä½µé‚Šåˆ°ç›®æ¨™å­—å…¸"""
        for node_id, edges in source_edges.items():
            target_edges[node_id].extend(edges)
    
    def _merge_edge_types(self, logical_edges: Dict[str, List[Dict[str, Any]]], 
                         semantic_edges: Dict[str, List[Dict[str, Any]]]) -> Dict[str, List[Dict[str, Any]]]:
        """åˆä½µé‚è¼¯é‚Šå’Œèªç¾©é‚Šï¼Œé¿å…é‡è¤‡"""
        merged_edges = {node_id: [] for node_id in logical_edges.keys()}
        
        # æ·»åŠ é‚è¼¯é‚Š
        for node_id, edges in logical_edges.items():
            merged_edges[node_id].extend(edges)
        
        # æ·»åŠ èªç¾©é‚Šï¼Œä½†é¿å…é‡è¤‡
        for node_id, semantic_edge_list in semantic_edges.items():
            # ç²å–ç¾æœ‰ç›®æ¨™ç¯€é»ï¼ˆé‚è¼¯é‚Šä½¿ç”¨"target"ï¼Œèªç¾©é‚Šä½¿ç”¨"to_node"ï¼‰
            existing_targets = set()
            for edge in merged_edges[node_id]:
                if "target" in edge:
                    existing_targets.add(edge["target"])
                elif "to_node" in edge:
                    existing_targets.add(edge["to_node"])
            
            for edge in semantic_edge_list:
                # ç²å–èªç¾©é‚Šçš„ç›®æ¨™ç¯€é»
                semantic_target = edge.get("to_node") or edge.get("target")
                if semantic_target and semantic_target not in existing_targets:
                    # çµ±ä¸€é‚Šæ ¼å¼ï¼Œå°‡"to_node"è½‰æ›ç‚º"target"
                    normalized_edge = edge.copy()
                    if "to_node" in normalized_edge and "target" not in normalized_edge:
                        normalized_edge["target"] = normalized_edge.pop("to_node")
                    
                    # ç¢ºä¿æœ‰ç›¸ä¼¼åº¦åˆ†æ•¸
                    if "similarity_score" in normalized_edge:
                        normalized_edge["similarity"] = normalized_edge["similarity_score"]
                    
                    merged_edges[node_id].append(normalized_edge)
        
        return merged_edges
    
    def _extract_article_number(self, article_str: str) -> int:
        """æå–æ¢æ–‡è™Ÿç¢¼ç”¨æ–¼æ’åº"""
        if not article_str:
            return 0
        
        # æå–æ•¸å­—
        numbers = re.findall(r'\d+', article_str)
        if numbers:
            return int(numbers[0])
        return 0

class PassageGraphBuilder:
    """æ®µè½åœ–æ§‹å»ºå™¨"""
    
    def __init__(self, config: HopRAGConfig = DEFAULT_CONFIG):
        self.config = config
        self.pseudo_query_generator = None
        self.edge_connector = None
        self.embedding_model = None
        
    def set_components(self, pseudo_query_generator: PseudoQueryGenerator, 
                      edge_connector: EdgeConnector, embedding_model):
        """è¨­ç½®çµ„ä»¶"""
        self.pseudo_query_generator = pseudo_query_generator
        self.edge_connector = edge_connector
        self.embedding_model = embedding_model
        
    async def build_graph(self, multi_level_chunks: Dict[str, Dict[str, List[Dict]]]) -> Tuple[Dict[str, LegalNode], Dict[str, List[Dict[str, Any]]]]:
        """æ§‹å»ºå®Œæ•´çš„çŸ¥è­˜åœ–è­œ"""
        print("ğŸ—ï¸ é–‹å§‹æ§‹å»ºHopRAGçŸ¥è­˜åœ–è­œ...")
        
        # Step 1: å‰µå»ºç¯€é»
        nodes = await self._create_nodes_from_chunks(multi_level_chunks)
        
        # Step 2: ç”Ÿæˆå½æŸ¥è©¢
        await self._generate_pseudo_queries(nodes)
        
        # Step 3: æ§‹å»ºåœ–é‚Š
        edges = await self.edge_connector.connect_edges(nodes, self.embedding_model)
        
        print("âœ… HopRAGçŸ¥è­˜åœ–è­œæ§‹å»ºå®Œæˆï¼")
        return nodes, edges
    
    async def _create_nodes_from_chunks(self, multi_level_chunks: Dict[str, Dict[str, List[Dict]]]) -> Dict[str, LegalNode]:
        """å¾chunkså‰µå»ºç¯€é»"""
        print("ğŸ“ å‰µå»ºç¯€é»...")
        
        nodes = {}
        
        for doc_id, levels in multi_level_chunks.items():
            # è™•ç†æ¢ç´šç¯€é» (basic_unitå±¤æ¬¡)
            if 'basic_unit' in levels:
                for chunk_idx, chunk in enumerate(levels['basic_unit']):
                    try:
                        # ç¢ºä¿chunkçµæ§‹å®Œæ•´
                        if not isinstance(chunk, dict):
                            print(f"âŒ Chunk {chunk_idx} ä¸æ˜¯å­—å…¸é¡å‹: {type(chunk)}")
                            continue
                        
                        # ç¢ºä¿metadataå­˜åœ¨
                        if 'metadata' not in chunk:
                            chunk['metadata'] = {}
                        
                        # ç¢ºä¿contentå­˜åœ¨
                        if 'content' not in chunk:
                            print(f"âŒ Chunk {chunk_idx} æ²’æœ‰contentæ¬„ä½")
                            continue
                        
                        # ç”Ÿæˆnode_idï¼Œå¦‚æœmetadataä¸­æ²’æœ‰idæ¬„ä½
                        node_id = chunk['metadata'].get('id', f"{doc_id}_basic_unit_{chunk_idx}")
                        
                        basic_unit_node = LegalNode(
                            node_id=node_id,
                            node_type=NodeType.BASIC_UNIT,
                            content=chunk['content'],
                            contextualized_text=chunk['content'],
                            law_name=chunk['metadata'].get('law_name', ''),
                            article_number=chunk['metadata'].get('article_label', ''),
                            metadata=chunk['metadata']
                        )
                        nodes[basic_unit_node.node_id] = basic_unit_node
                        
                    except Exception as e:
                        print(f"âŒ å‰µå»ºbasic_unitç¯€é»å¤±æ•— (chunk {chunk_idx}): {e}")
                        continue
            
            # è™•ç†é …ç´šç¯€é» (basic_unit_componentå±¤æ¬¡)
            if 'basic_unit_component' in levels:
                for chunk_idx, chunk in enumerate(levels['basic_unit_component']):
                    try:
                        # ç¢ºä¿chunkçµæ§‹å®Œæ•´
                        if not isinstance(chunk, dict):
                            print(f"âŒ Component Chunk {chunk_idx} ä¸æ˜¯å­—å…¸é¡å‹: {type(chunk)}")
                            continue
                        
                        # ç¢ºä¿metadataå­˜åœ¨
                        if 'metadata' not in chunk:
                            chunk['metadata'] = {}
                        
                        # ç¢ºä¿contentå­˜åœ¨
                        if 'content' not in chunk:
                            print(f"âŒ Component Chunk {chunk_idx} æ²’æœ‰contentæ¬„ä½")
                            continue
                        
                        # ç”Ÿæˆnode_idï¼Œå¦‚æœmetadataä¸­æ²’æœ‰idæ¬„ä½
                        node_id = chunk['metadata'].get('id', f"{doc_id}_basic_unit_component_{chunk_idx}")
                        
                        component_node = LegalNode(
                            node_id=node_id,
                            node_type=NodeType.BASIC_UNIT_COMPONENT,
                            content=chunk['content'],
                            contextualized_text=chunk['content'],
                            law_name=chunk['metadata'].get('law_name', ''),
                            article_number=chunk['metadata'].get('article_label', ''),
                            item_number=chunk['metadata'].get('item_label', ''),
                            parent_article_id=chunk['metadata'].get('parent_article_id'),
                            metadata=chunk['metadata']
                        )
                        nodes[component_node.node_id] = component_node
                        
                    except Exception as e:
                        print(f"âŒ å‰µå»ºbasic_unit_componentç¯€é»å¤±æ•— (chunk {chunk_idx}): {e}")
                        continue
        
        print(f"âœ… ç¯€é»å‰µå»ºå®Œæˆï¼Œå…± {len(nodes)} å€‹ç¯€é»")
        return nodes
    
    async def _generate_pseudo_queries(self, nodes: Dict[str, LegalNode]):
        """ç‚ºæ‰€æœ‰ç¯€é»ç”Ÿæˆå½æŸ¥è©¢ï¼ˆæ”¯æŒä¸¦è¡Œæ‰¹é‡è™•ç†ï¼‰"""
        print("ğŸ¤– é–‹å§‹ç”Ÿæˆå½æŸ¥è©¢...")
        
        node_list = list(nodes.values())
        total_nodes = len(node_list)
        start_time = time.time()
        
        # ğŸš€ ä¸¦è¡Œæ‰¹é‡è™•ç†é…ç½®
        batch_size = 10  # æ¯æ‰¹è™•ç†10å€‹ç¯€é»
        use_parallel = True  # âœ… å•Ÿç”¨ä¸¦è¡Œè™•ç†ï¼ˆLLMæ–‡æœ¬ç”Ÿæˆï¼Œé€Ÿç‡é™åˆ¶è¼ƒå¯¬æ¾ï¼‰
        # æ³¨æ„ï¼šé€™è£¡æ˜¯LLMç”Ÿæˆæ–‡æœ¬ï¼ˆå½æŸ¥è©¢ï¼‰ï¼Œä¸æ˜¯Embedding
        # Gemini LLMé€Ÿç‡é™åˆ¶ï¼š15 RPM â‰ˆ 0.25 req/ç§’ï¼Œä¸¦è¡Œ10å€‹ä¸æœƒè¶…é™
        
        print(f"ğŸ“Š ç¸½å…±éœ€è¦è™•ç† {total_nodes} å€‹ç¯€é»")
        if use_parallel:
            print(f"âš¡ ä½¿ç”¨ä¸¦è¡Œè™•ç†æ¨¡å¼ï¼Œæ‰¹æ¬¡å¤§å°: {batch_size}")
            print(f"ğŸš€ é è¨ˆåŠ é€Ÿæ¯”: {batch_size}x")
            print(f"â±ï¸ é è¨ˆç¸½æ™‚é–“: {total_nodes * 2.5 / 60 / batch_size:.1f} åˆ†é˜ï¼ˆä¸²è¡Œéœ€ {total_nodes * 2.5 / 60:.1f} åˆ†é˜ï¼‰")
        else:
            print(f"â³ ä½¿ç”¨ä¸²è¡Œè™•ç†æ¨¡å¼")
            print(f"â±ï¸ é è¨ˆæ¯å€‹ç¯€é»éœ€è¦ 2-3 ç§’ï¼ˆåŒ…å«LLMèª¿ç”¨ï¼‰")
            print(f"ğŸ• é è¨ˆç¸½æ™‚é–“: {total_nodes * 2.5 / 60:.1f} åˆ†é˜")
        print("=" * 60)
        
        if use_parallel:
            # ğŸš€ ä¸¦è¡Œæ‰¹é‡è™•ç†
            for batch_idx in range(0, total_nodes, batch_size):
                batch_start = batch_idx
                batch_end = min(batch_idx + batch_size, total_nodes)
                batch_nodes = node_list[batch_start:batch_end]
                
                batch_start_time = time.time()
                
                # ä¸¦è¡Œè™•ç†ç•¶å‰æ‰¹æ¬¡
                tasks = [
                    self.pseudo_query_generator.generate_pseudo_queries_for_node(node)
                    for node in batch_nodes
                ]
                
                try:
                    results = await asyncio.gather(*tasks, return_exceptions=True)
                    
                    # æª¢æŸ¥éŒ¯èª¤
                    for idx, result in enumerate(results):
                        if isinstance(result, Exception):
                            print(f"âŒ ç¯€é» {batch_nodes[idx].node_id} å¤±æ•—: {result}")
                    
                    batch_time = time.time() - batch_start_time
                    
                    # è¨ˆç®—é€²åº¦
                    progress = batch_end / total_nodes * 100
                    elapsed_time = time.time() - start_time
                    avg_time_per_batch = elapsed_time / ((batch_idx // batch_size) + 1)
                    remaining_batches = (total_nodes - batch_end) / batch_size
                    estimated_remaining_time = remaining_batches * avg_time_per_batch
                    
                    print(f"ğŸ“ˆ æ‰¹æ¬¡ {batch_idx//batch_size + 1}: {batch_end}/{total_nodes} ({progress:.1f}%) | "
                          f"æ‰¹æ¬¡è€—æ™‚: {batch_time:.1f}s | "
                          f"å¹³å‡: {batch_time/len(batch_nodes):.1f}s/ç¯€é» | "
                          f"å‰©é¤˜: {estimated_remaining_time/60:.1f}åˆ†é˜")
                    
                except Exception as e:
                    print(f"âŒ æ‰¹æ¬¡è™•ç†å¤±æ•—: {e}")
                    continue
        else:
            # â³ ä¸²è¡Œè™•ç†ï¼ˆåŸæ–¹å¼ï¼‰
            for i, node in enumerate(node_list):
                try:
                    node_start_time = time.time()
                    await self.pseudo_query_generator.generate_pseudo_queries_for_node(node)
                    node_time = time.time() - node_start_time
                    
                    # è¨ˆç®—é€²åº¦å’Œå‰©é¤˜æ™‚é–“
                    progress = (i + 1) / total_nodes * 100
                    elapsed_time = time.time() - start_time
                    avg_time_per_node = elapsed_time / (i + 1)
                    remaining_nodes = total_nodes - (i + 1)
                    estimated_remaining_time = remaining_nodes * avg_time_per_node
                    
                    # æ¯5å€‹ç¯€é»é¡¯ç¤ºä¸€æ¬¡é€²åº¦
                    if (i + 1) % 5 == 0 or i == 0:
                        print(f"ğŸ“ˆ é€²åº¦: {i+1}/{total_nodes} ({progress:.1f}%) | "
                              f"ç¯€é»: {node.node_id[:20]}... | "
                              f"è€—æ™‚: {node_time:.1f}s | "
                              f"å‰©é¤˜: {estimated_remaining_time/60:.1f}åˆ†é˜")
                    
                    # æ¯20å€‹ç¯€é»é¡¯ç¤ºè©³ç´°çµ±è¨ˆ
                    if (i + 1) % 20 == 0:
                        print(f"ğŸ“Š çµ±è¨ˆ: å¹³å‡ {avg_time_per_node:.1f}s/ç¯€é» | "
                              f"å·²ç”¨æ™‚: {elapsed_time/60:.1f}åˆ†é˜ | "
                              f"é è¨ˆå®Œæˆ: {estimated_remaining_time/60:.1f}åˆ†é˜")
                        
                except Exception as e:
                    print(f"âŒ ç¯€é» {node.node_id} å½æŸ¥è©¢ç”Ÿæˆå¤±æ•—: {e}")
                    continue
        
        total_time = time.time() - start_time
        print("=" * 60)
        print(f"âœ… å½æŸ¥è©¢ç”Ÿæˆå®Œæˆï¼ç¸½è€—æ™‚: {total_time/60:.1f} åˆ†é˜")
        print(f"ğŸ“Š å¹³å‡æ¯å€‹ç¯€é»: {total_time/total_nodes:.1f} ç§’")
        if use_parallel:
            serial_time = total_nodes * 2.5 / 60
            speedup = serial_time / (total_time / 60) if total_time > 0 else 1
            print(f"âš¡ å¯¦éš›åŠ é€Ÿæ¯”: {speedup:.1f}xï¼ˆä¸¦è¡Œ vs ä¸²è¡Œï¼‰")
    
    
    
    def _extract_article_number(self, article_str: str) -> int:
        """æå–æ¢æ–‡è™Ÿç¢¼ç”¨æ–¼æ’åº"""
        if not article_str:
            return 0
        
        # æå–æ•¸å­—
        numbers = re.findall(r'\d+', article_str)
        if numbers:
            return int(numbers[0])
        return 0
