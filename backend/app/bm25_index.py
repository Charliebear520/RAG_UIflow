"""
BM25ÈóúÈçµÂ≠óÁ¥¢ÂºïÊ®°ÁµÑ
"""

import os
import pickle
import jieba
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass
import re

try:
    from rank_bm25 import BM25Okapi
    BM25_AVAILABLE = True
except ImportError:
    BM25Okapi = None
    BM25_AVAILABLE = False


@dataclass
class BM25IndexInfo:
    """BM25Á¥¢Âºï‰ø°ÊÅØ"""
    total_documents: int
    vocabulary_size: int
    avg_doc_length: float
    k1: float
    b: float
    metadata: Dict[str, Any]


class BM25KeywordIndex:
    """BM25ÈóúÈçµÂ≠óÁ¥¢ÂºïÈ°û"""
    
    def __init__(self, k1: float = 1.2, b: float = 0.75):
        self.k1 = k1  # Ë©ûÈ†ªÈ£ΩÂíåÂèÉÊï∏
        self.b = b    # Èï∑Â∫¶Ê≠£Ë¶èÂåñÂèÉÊï∏
        self.bm25_index = None
        self.index_info: Optional[BM25IndexInfo] = None
        self.chunk_ids: List[str] = []
        self.chunk_doc_ids: List[str] = []
        self.chunks_flat: List[str] = []
        
        # Â§öÂ±§Ê¨°BM25Á¥¢Âºï
        self.multi_level_bm25_indices: Dict[str, Any] = {}
        self.multi_level_chunk_ids: Dict[str, List[str]] = {}
        self.multi_level_chunk_doc_ids: Dict[str, List[str]] = {}
        self.multi_level_chunks_flat: Dict[str, List[str]] = {}
        self.multi_level_index_info: Dict[str, BM25IndexInfo] = {}
        
        # ÊåÅ‰πÖÂåñË®≠ÁΩÆ
        self.data_dir = "data"
        self.ensure_data_dir()
    
    def ensure_data_dir(self):
        """Á¢∫‰øùÊï∏ÊìöÁõÆÈåÑÂ≠òÂú®"""
        if not os.path.exists(self.data_dir):
            os.makedirs(self.data_dir)
            print(f"‚úÖ ÂâµÂª∫BM25Êï∏ÊìöÁõÆÈåÑ: {self.data_dir}")
    
    def _preprocess_text(self, text: str) -> List[str]:
        """ÊñáÊú¨È†êËôïÁêÜÂíåÂàÜË©û"""
        if not text:
            return []
        
        # Ê∏ÖÁêÜÊñáÊú¨
        text = re.sub(r'[^\u4e00-\u9fff\u3400-\u4dbf\w\s]', ' ', text)
        text = re.sub(r'\s+', ' ', text).strip()
        
        # ‰ΩøÁî®jiebaÂàÜË©û
        if jieba:
            words = jieba.lcut(text)
            # ÈÅéÊøæÂÅúÁî®Ë©ûÂíåÁü≠Ë©û
            words = [word for word in words if len(word) > 1 and word.strip()]
        else:
            # Á∞°ÂñÆÁöÑÁ©∫Ê†ºÂàÜÂâ≤
            words = text.split()
        
        return words
    
    def _preprocess_documents(self, documents: List[str]) -> List[List[str]]:
        """ÊâπÈáèÈ†êËôïÁêÜÊñáÊ™î"""
        processed_docs = []
        for doc in documents:
            processed_docs.append(self._preprocess_text(doc))
        return processed_docs
    
    def build_index(self, documents: List[str], chunk_ids: List[str], 
                   chunk_doc_ids: List[str]) -> None:
        """ÊßãÂª∫BM25Á¥¢Âºï"""
        if not BM25_AVAILABLE:
            raise RuntimeError("BM25 not available. Please install rank-bm25.")
        
        if not documents:
            return
        
        # È†êËôïÁêÜÊñáÊ™î
        processed_docs = self._preprocess_documents(documents)
        
        # ÂâµÂª∫BM25Á¥¢Âºï
        self.bm25_index = BM25Okapi(processed_docs, k1=self.k1, b=self.b)
        
        # Â≠òÂÑ≤ÂÖÉÊï∏Êìö
        self.chunks_flat = documents
        self.chunk_ids = chunk_ids
        self.chunk_doc_ids = chunk_doc_ids
        
        # Ë®àÁÆóÁ¥¢Âºï‰ø°ÊÅØ
        vocab_size = len(self.bm25_index.idf) if hasattr(self.bm25_index, 'idf') else 0
        avg_doc_length = sum(len(doc) for doc in processed_docs) / len(processed_docs) if processed_docs else 0
        
        self.index_info = BM25IndexInfo(
            total_documents=len(documents),
            vocabulary_size=vocab_size,
            avg_doc_length=avg_doc_length,
            k1=self.k1,
            b=self.b,
            metadata={"created_at": None}
        )
        
        print(f"‚úÖ ÊßãÂª∫BM25Á¥¢Âºï: {len(documents)} ÂÄãÊñáÊ™î, Ë©ûÂΩôÈáè: {vocab_size}")
    
    def build_multi_level_index(self, level_name: str, documents: List[str], 
                               chunk_ids: List[str], chunk_doc_ids: List[str]) -> None:
        """ÊßãÂª∫Â§öÂ±§Ê¨°BM25Á¥¢Âºï"""
        if not BM25_AVAILABLE:
            raise RuntimeError("BM25 not available. Please install rank-bm25.")
        
        if not documents:
            return
        
        # È†êËôïÁêÜÊñáÊ™î
        processed_docs = self._preprocess_documents(documents)
        
        # ÂâµÂª∫BM25Á¥¢Âºï
        bm25_index = BM25Okapi(processed_docs, k1=self.k1, b=self.b)
        
        # Â≠òÂÑ≤Á¥¢ÂºïÂíåÂÖÉÊï∏Êìö
        self.multi_level_bm25_indices[level_name] = bm25_index
        self.multi_level_chunks_flat[level_name] = documents
        self.multi_level_chunk_ids[level_name] = chunk_ids
        self.multi_level_chunk_doc_ids[level_name] = chunk_doc_ids
        
        # Ë®àÁÆóÁ¥¢Âºï‰ø°ÊÅØ
        vocab_size = len(bm25_index.idf) if hasattr(bm25_index, 'idf') else 0
        avg_doc_length = sum(len(doc) for doc in processed_docs) / len(processed_docs) if processed_docs else 0
        
        self.multi_level_index_info[level_name] = BM25IndexInfo(
            total_documents=len(documents),
            vocabulary_size=vocab_size,
            avg_doc_length=avg_doc_length,
            k1=self.k1,
            b=self.b,
            metadata={"level": level_name}
        )
        
        print(f"‚úÖ ÊßãÂª∫Â±§Ê¨° '{level_name}' BM25Á¥¢Âºï: {len(documents)} ÂÄãÊñáÊ™î")
    
    def search(self, query: str, k: int = 10) -> Tuple[List[int], List[float]]:
        """ÊêúÁ¥¢ÊúÄÁõ∏ÈóúÁöÑÊñáÊ™î"""
        if self.bm25_index is None:
            return [], []
        
        if not query:
            return [], []
        
        # È†êËôïÁêÜÊü•Ë©¢
        query_tokens = self._preprocess_text(query)
        if not query_tokens:
            return [], []
        
        # Ë®àÁÆóBM25ÂàÜÊï∏
        scores = self.bm25_index.get_scores(query_tokens)
        
        # Áç≤ÂèñÂâçkÂÄãÁµêÊûú
        if len(scores) == 0:
            return [], []
        
        # ÊéíÂ∫è‰∏¶Áç≤ÂèñÁ¥¢Âºï
        scored_indices = [(i, score) for i, score in enumerate(scores)]
        scored_indices.sort(key=lambda x: x[1], reverse=True)
        
        top_k = scored_indices[:k]
        indices = [idx for idx, _ in top_k]
        scores_list = [score for _, score in top_k]
        
        return indices, scores_list
    
    def search_multi_level(self, level_name: str, query: str, k: int = 10) -> Tuple[List[int], List[float]]:
        """ÊêúÁ¥¢ÊåáÂÆöÂ±§Ê¨°ÁöÑÊúÄÁõ∏ÈóúÊñáÊ™î"""
        if level_name not in self.multi_level_bm25_indices:
            return [], []
        
        bm25_index = self.multi_level_bm25_indices[level_name]
        
        if not query:
            return [], []
        
        # È†êËôïÁêÜÊü•Ë©¢
        query_tokens = self._preprocess_text(query)
        if not query_tokens:
            return [], []
        
        # Ë®àÁÆóBM25ÂàÜÊï∏
        scores = bm25_index.get_scores(query_tokens)
        
        # Áç≤ÂèñÂâçkÂÄãÁµêÊûú
        if len(scores) == 0:
            return [], []
        
        # ÊéíÂ∫è‰∏¶Áç≤ÂèñÁ¥¢Âºï
        scored_indices = [(i, score) for i, score in enumerate(scores)]
        scored_indices.sort(key=lambda x: x[1], reverse=True)
        
        top_k = scored_indices[:k]
        indices = [idx for idx, _ in top_k]
        scores_list = [score for _, score in top_k]
        
        return indices, scores_list
    
    def get_chunk_by_index(self, index: int) -> Optional[Dict[str, Any]]:
        """Ê†πÊìöÁ¥¢ÂºïÁç≤Âèñchunk‰ø°ÊÅØ"""
        if index < 0 or index >= len(self.chunk_ids):
            return None
        
        return {
            "chunk_id": self.chunk_ids[index],
            "doc_id": self.chunk_doc_ids[index],
            "content": self.chunks_flat[index]
        }
    
    def get_multi_level_chunk_by_index(self, level_name: str, index: int) -> Optional[Dict[str, Any]]:
        """Ê†πÊìöÁ¥¢ÂºïÁç≤ÂèñÂ§öÂ±§Ê¨°chunk‰ø°ÊÅØ"""
        if level_name not in self.multi_level_chunk_ids:
            return None
        
        chunk_ids = self.multi_level_chunk_ids[level_name]
        if index < 0 or index >= len(chunk_ids):
            return None
        
        return {
            "chunk_id": chunk_ids[index],
            "doc_id": self.multi_level_chunk_doc_ids[level_name][index],
            "content": self.multi_level_chunks_flat[level_name][index]
        }
    
    def get_available_levels(self) -> List[str]:
        """Áç≤ÂèñÂèØÁî®ÁöÑÂ±§Ê¨°"""
        return list(self.multi_level_bm25_indices.keys())
    
    def has_index(self) -> bool:
        """Ê™¢Êü•ÊòØÂê¶ÊúâÁ¥¢Âºï"""
        return self.bm25_index is not None
    
    def has_multi_level_index(self) -> bool:
        """Ê™¢Êü•ÊòØÂê¶ÊúâÂ§öÂ±§Ê¨°Á¥¢Âºï"""
        return len(self.multi_level_bm25_indices) > 0
    
    def save_data(self) -> None:
        """‰øùÂ≠òBM25Á¥¢ÂºïÂíåÂÖÉÊï∏Êìö"""
        try:
            # ‰øùÂ≠òÊ®ôÊ∫ñÁ¥¢Âºï
            if self.bm25_index is not None:
                with open(os.path.join(self.data_dir, "bm25_index.pkl"), "wb") as f:
                    pickle.dump(self.bm25_index, f)
            
            # ‰øùÂ≠òÂ§öÂ±§Ê¨°Á¥¢Âºï
            for level_name, bm25_index in self.multi_level_bm25_indices.items():
                with open(os.path.join(self.data_dir, f"bm25_index_{level_name}.pkl"), "wb") as f:
                    pickle.dump(bm25_index, f)
            
            # ‰øùÂ≠òÂÖÉÊï∏Êìö
            metadata = {
                "index_info": self.index_info.__dict__ if self.index_info else None,
                "chunk_ids": self.chunk_ids,
                "chunk_doc_ids": self.chunk_doc_ids,
                "chunks_flat": self.chunks_flat,
                "k1": self.k1,
                "b": self.b,
                "multi_level_index_info": {k: v.__dict__ for k, v in self.multi_level_index_info.items()},
                "multi_level_chunk_ids": self.multi_level_chunk_ids,
                "multi_level_chunk_doc_ids": self.multi_level_chunk_doc_ids,
                "multi_level_chunks_flat": self.multi_level_chunks_flat
            }
            
            with open(os.path.join(self.data_dir, "bm25_metadata.pkl"), "wb") as f:
                pickle.dump(metadata, f)
            
            print(f"‚úÖ BM25Êï∏ÊìöÂ∑≤‰øùÂ≠òÂà∞ {self.data_dir}")
            
        except Exception as e:
            print(f"‚ùå ‰øùÂ≠òBM25Êï∏ÊìöÂ§±Êïó: {e}")
    
    def load_data(self) -> None:
        """ËºâÂÖ•BM25Á¥¢ÂºïÂíåÂÖÉÊï∏Êìö"""
        try:
            # ËºâÂÖ•ÂÖÉÊï∏Êìö
            metadata_file = os.path.join(self.data_dir, "bm25_metadata.pkl")
            if not os.path.exists(metadata_file):
                print(f"üìÅ BM25ÂÖÉÊï∏ÊìöÊñá‰ª∂‰∏çÂ≠òÂú®: {metadata_file}")
                return
            
            with open(metadata_file, "rb") as f:
                metadata = pickle.load(f)
            
            # ÊÅ¢Âæ©Ê®ôÊ∫ñÁ¥¢Âºï
            index_file = os.path.join(self.data_dir, "bm25_index.pkl")
            if os.path.exists(index_file):
                with open(index_file, "rb") as f:
                    self.bm25_index = pickle.load(f)
                self.index_info = BM25IndexInfo(**metadata["index_info"]) if metadata["index_info"] else None
            
            # ÊÅ¢Âæ©Â§öÂ±§Ê¨°Á¥¢Âºï
            for level_name in metadata.get("multi_level_index_info", {}).keys():
                level_index_file = os.path.join(self.data_dir, f"bm25_index_{level_name}.pkl")
                if os.path.exists(level_index_file):
                    with open(level_index_file, "rb") as f:
                        self.multi_level_bm25_indices[level_name] = pickle.load(f)
            
            # ÊÅ¢Âæ©ÂÖÉÊï∏Êìö
            self.chunk_ids = metadata.get("chunk_ids", [])
            self.chunk_doc_ids = metadata.get("chunk_doc_ids", [])
            self.chunks_flat = metadata.get("chunks_flat", [])
            self.k1 = metadata.get("k1", self.k1)
            self.b = metadata.get("b", self.b)
            
            # ÊÅ¢Âæ©Â§öÂ±§Ê¨°ÂÖÉÊï∏Êìö
            self.multi_level_index_info = {
                k: BM25IndexInfo(**v) for k, v in metadata.get("multi_level_index_info", {}).items()
            }
            self.multi_level_chunk_ids = metadata.get("multi_level_chunk_ids", {})
            self.multi_level_chunk_doc_ids = metadata.get("multi_level_chunk_doc_ids", {})
            self.multi_level_chunks_flat = metadata.get("multi_level_chunks_flat", {})
            
            print(f"‚úÖ BM25Êï∏ÊìöÂ∑≤Âæû {self.data_dir} ËºâÂÖ•")
            print(f"   üìÑ Ê®ôÊ∫ñÊñáÊ™î: {self.index_info.total_documents if self.index_info else 0} ÂÄã")
            print(f"   üèóÔ∏è Â§öÂ±§Ê¨°ÊñáÊ™î: {len(self.multi_level_bm25_indices)} ÂÄãÂ±§Ê¨°")
            
        except Exception as e:
            print(f"‚ùå ËºâÂÖ•BM25Êï∏ÊìöÂ§±Êïó: {e}")
    
    def reset_index(self) -> None:
        """ÈáçÁΩÆÊâÄÊúâÁ¥¢Âºï"""
        self.bm25_index = None
        self.index_info = None
        self.chunk_ids = []
        self.chunk_doc_ids = []
        self.chunks_flat = []
        
        self.multi_level_bm25_indices = {}
        self.multi_level_chunk_ids = {}
        self.multi_level_chunk_doc_ids = {}
        self.multi_level_chunks_flat = {}
        self.multi_level_index_info = {}
        
        print("üóëÔ∏è BM25Á¥¢ÂºïÊï∏ÊìöÂ∑≤ÈáçÁΩÆ")
    
    def get_stats(self) -> Dict[str, Any]:
        """Áç≤ÂèñÁµ±Ë®à‰ø°ÊÅØ"""
        stats = {
            "bm25_available": BM25_AVAILABLE,
            "standard_index": {
                "has_index": self.bm25_index is not None,
                "total_documents": self.index_info.total_documents if self.index_info else 0,
                "vocabulary_size": self.index_info.vocabulary_size if self.index_info else 0,
                "avg_doc_length": self.index_info.avg_doc_length if self.index_info else 0,
                "k1": self.k1,
                "b": self.b
            },
            "multi_level_indices": {
                level: {
                    "total_documents": info.total_documents,
                    "vocabulary_size": info.vocabulary_size,
                    "avg_doc_length": info.avg_doc_length,
                    "k1": info.k1,
                    "b": info.b
                }
                for level, info in self.multi_level_index_info.items()
            }
        }
        return stats
