"""
HopRAGå¤šè·³æª¢ç´¢å™¨æ¨¡çµ„
åŒ…å«InitialRetrieverã€GraphTraverserã€LLMReasoner
"""

import asyncio
from typing import Dict, List, Any, Optional, Tuple, Set
import networkx as nx
from collections import Counter, deque
import numpy as np

from .hoprag_config import HopRAGConfig, DEFAULT_CONFIG
from .hoprag_graph_builder import LegalNode

class InitialRetriever:
    """åˆå§‹æª¢ç´¢å™¨"""
    
    def __init__(self, config: HopRAGConfig = DEFAULT_CONFIG):
        self.config = config
        
    async def retrieve_initial_nodes(self, query: str, base_results: List[Dict[str, Any]]) -> List[str]:
        """å¾åŸºç¤æª¢ç´¢çµæœä¸­æå–åˆå§‹ç¯€é»"""
        print(f"ğŸ” åˆå§‹æª¢ç´¢ï¼šæŸ¥è©¢ '{query}'")
        
        initial_nodes = []
        for result in base_results:
            # å˜—è©¦å¾çµæœä¸­æå–node_id
            node_id = result.get('node_id') or result.get('id') or result.get('chunk_id')
            if node_id:
                initial_nodes.append(node_id)
        
        # é™åˆ¶åˆå§‹ç¯€é»æ•¸é‡
        initial_nodes = initial_nodes[:self.config.initial_retrieve_k]
        
        print(f"âœ… åˆå§‹æª¢ç´¢å®Œæˆï¼Œç²å¾— {len(initial_nodes)} å€‹åˆå§‹ç¯€é»")
        return initial_nodes
    
    def get_retrieval_stats(self, initial_nodes: List[str]) -> Dict[str, Any]:
        """ç²å–æª¢ç´¢çµ±è¨ˆä¿¡æ¯"""
        return {
            "num_initial_nodes": len(initial_nodes),
            "retrieval_strategy": self.config.base_strategy,
            "max_initial_nodes": self.config.initial_retrieve_k
        }

class LLMReasoner:
    """LLMæ¨ç†å™¨"""
    
    def __init__(self, llm_client, config: HopRAGConfig = DEFAULT_CONFIG):
        self.llm_client = llm_client
        self.config = config
        
    async def reason_about_relevance(self, query: str, current_node: str, 
                                   neighbor: Dict[str, Any]) -> bool:
        """ä½¿ç”¨LLMæ¨ç†åˆ¤æ–·é„°å±…ç¯€é»çš„ç›¸é—œæ€§"""
        try:
            # æ§‹å»ºæ¨ç†æç¤º
            reasoning_prompt = self._build_reasoning_prompt(
                query, current_node, neighbor
            )
            
            # èª¿ç”¨LLMé€²è¡Œæ¨ç†
            decision = await self.llm_client.generate_async(reasoning_prompt)
            
            # è§£ææ±ºç­–
            return self._is_relevant_decision(decision)
            
        except Exception as e:
            print(f"âŒ LLMæ¨ç†å¤±æ•—: {e}")
            # å¦‚æœLLMå¤±æ•—ï¼ŒåŸºæ–¼ç›¸ä¼¼åº¦åˆ†æ•¸åˆ¤æ–·
            return neighbor.get('similarity_score', 0) > 0.8
    
    def _build_reasoning_prompt(self, query: str, current_node: str, neighbor: Dict[str, Any]) -> str:
        """æ§‹å»ºLLMæ¨ç†æç¤º - æŒ‰ç…§Figure 8è¦æ±‚å¯¦ç¾"""
        current_content = neighbor.get('current_content', '')
        neighbor_content = neighbor.get('contextualized_text', '')
        pseudo_query = neighbor.get('pseudo_query', '')
        
        prompt = f"""
æ‚¨æ˜¯ä¸€å€‹æ³•å¾‹å•ç­”æ©Ÿå™¨äººã€‚æˆ‘å°‡æä¾›æ‚¨ä¸€å€‹ä¸»å•é¡Œï¼Œæ¶‰åŠå¤šå€‹æ³•å¾‹æ¢æ–‡ä¿¡æ¯ï¼Œä»¥åŠä¸€å€‹é¡å¤–çš„è¼”åŠ©å•é¡Œã€‚æ‚¨çš„ä»»å‹™æ˜¯å›ç­”ä¸»å•é¡Œï¼Œä½†ç”±æ–¼ä¸»å•é¡Œæ¶‰åŠå¾ˆå¤šæ‚¨å¯èƒ½ä¸çŸ¥é“çš„ä¿¡æ¯ï¼Œæ‚¨æœ‰æ©Ÿæœƒä½¿ç”¨è¼”åŠ©å•é¡Œä¾†æ”¶é›†æ‚¨éœ€è¦çš„ä¿¡æ¯ã€‚ä½†æ˜¯ï¼Œè¼”åŠ©å•é¡Œå¯èƒ½ä¸¦ä¸ç¸½æ˜¯æœ‰ç”¨çš„ï¼Œå› æ­¤æ‚¨éœ€è¦è©•ä¼°è¼”åŠ©å•é¡Œèˆ‡ä¸»å•é¡Œçš„é—œä¿‚ï¼Œä»¥ç¢ºå®šæ˜¯å¦ä½¿ç”¨å®ƒã€‚

æ‚¨éœ€è¦è©•ä¼°è¼”åŠ©å•é¡Œæ˜¯å¦å®Œå…¨ç„¡é—œã€é–“æ¥ç›¸é—œï¼Œæˆ–ç›¸é—œä¸”å¿…è¦æ–¼å›ç­”ä¸»å•é¡Œã€‚æ‚¨åªèƒ½è¿”å›é€™ä¸‰ç¨®çµæœä¹‹ä¸€ã€‚

è«‹æ³¨æ„ï¼Œä¸»å•é¡Œå°‡æ¶‰åŠå¤šå€‹èƒŒæ™¯å¥å­ï¼Œé€™æ„å‘³è‘—å›ç­”ä¸»å•é¡Œéœ€è¦çµåˆå’Œæ¨ç†å¤šå€‹ä¿¡æ¯ç‰‡æ®µã€‚ä½†æ˜¯ï¼Œæ‚¨ä¸çŸ¥é“å“ªäº›å…·é«”å¥å­æ˜¯å›ç­”ä¸»å•é¡Œæ‰€å¿…éœ€çš„ã€‚æ‚¨çš„ä»»å‹™æ˜¯è©•ä¼°çµ¦å®šçš„è¼”åŠ©å•é¡Œæ˜¯å¦ç›¸é—œä¸”å¿…è¦ã€é–“æ¥ç›¸é—œï¼Œæˆ–å®Œå…¨ç„¡é—œæ–¼å›ç­”ä¸»å•é¡Œã€‚

çµæœ1ï¼š[å®Œå…¨ç„¡é—œ]ã€‚åœ¨é€™ç¨®æƒ…æ³ä¸‹ï¼Œæ‚¨ç¢ºå®šå³ä½¿æ²’æœ‰è¼”åŠ©å•é¡Œçš„ä¿¡æ¯ï¼Œæ‚¨ä»ç„¶å¯ä»¥å›ç­”ä¸»å•é¡Œï¼Œæˆ–è€…è¼”åŠ©å•é¡Œä¸­çš„ä¿¡æ¯èˆ‡ä¸»å•é¡Œçš„ç­”æ¡ˆå®Œå…¨ç„¡é—œã€‚

çµæœ2ï¼š[é–“æ¥ç›¸é—œ]ã€‚åœ¨é€™ç¨®æƒ…æ³ä¸‹ï¼Œæ‚¨ç™¼ç¾è¼”åŠ©å•é¡Œèˆ‡ä¸»å•é¡Œç›¸é—œï¼Œä½†å…¶ç­”æ¡ˆä¸æ˜¯å›ç­”ä¸»å•é¡Œæ‰€éœ€çš„å¤šå€‹ä¿¡æ¯ç‰‡æ®µçš„ä¸€éƒ¨åˆ†ã€‚è¼”åŠ©å•é¡Œé—œæ³¨ç›¸é—œä¸»é¡Œï¼Œä½†ä¸æä¾›å›ç­”ä¸»å•é¡Œæ‰€éœ€çš„é—œéµä¿¡æ¯ã€‚

çµæœ3ï¼š[ç›¸é—œä¸”å¿…è¦]ã€‚åœ¨é€™ç¨®æƒ…æ³ä¸‹ï¼Œæ‚¨ç™¼ç¾è¼”åŠ©å•é¡Œæ˜¯ä¸»å•é¡Œçš„å­å•é¡Œï¼Œé€™æ„å‘³è‘—å¦‚æœä¸å›ç­”è¼”åŠ©å•é¡Œï¼Œæ‚¨å°‡ç„¡æ³•å›ç­”ä¸»å•é¡Œã€‚è¼”åŠ©å•é¡Œæä¾›çš„ä¿¡æ¯æ˜¯å›ç­”ä¸»å•é¡Œæ‰€å¿…éœ€çš„ã€‚

ç¯„ä¾‹1ï¼š
ä¸»å•é¡Œï¼šå¼µä¸‰é•åè‘—ä½œæ¬Šæ³•ç¬¬å…«æ¢çš„é‡è£½æ¬Šè¦å®šï¼Œæœƒé¢è‡¨ä»€éº¼æ³•å¾‹å¾Œæœï¼Ÿ
è¼”åŠ©å•é¡Œï¼šç¾åœ‹ç¸½çµ±ç™¼è¡¨åœ‹æƒ…å’¨æ–‡çš„ç›®çš„æ˜¯ä»€éº¼ï¼Ÿ
åœ¨é€™ç¨®æƒ…æ³ä¸‹ï¼Œç¶“éä»”ç´°è€ƒæ…®ï¼Œæ‚¨ç™¼ç¾è¼”åŠ©å•é¡Œç„¡åŠ©æ–¼å›ç­”ä¸»å•é¡Œã€‚è¼”åŠ©å•é¡Œèˆ‡ä¸»å•é¡Œå®Œå…¨ç„¡é—œã€‚æ‚¨çš„å›æ‡‰æ‡‰è©²æ˜¯ï¼š
{{"Decision": "Completely Irrelevant"}}

ç¯„ä¾‹2ï¼š
ä¸»å•é¡Œï¼šå¼µä¸‰é•åè‘—ä½œæ¬Šæ³•ç¬¬å…«æ¢çš„é‡è£½æ¬Šè¦å®šï¼Œæœƒé¢è‡¨ä»€éº¼æ³•å¾‹å¾Œæœï¼Ÿ
è¼”åŠ©å•é¡Œï¼šé€™å€‹æ³•å¾‹å¾Œæœå°è©²åœ°å€çš„äºŒæ¬¡å‰µä½œæœ‰ä»€éº¼æ„ç¾©ï¼Ÿ
åœ¨é€™ç¨®æƒ…æ³ä¸‹ï¼Œæ‚¨æ³¨æ„åˆ°ä¸»å•é¡Œå’Œè¼”åŠ©å•é¡Œéƒ½æ¶‰åŠé¡ä¼¼çš„ä¸»é¡Œï¼Œä½†è¼”åŠ©å•é¡Œé—œæ³¨çš„æ˜¯æ³•å¾‹å¾Œæœçš„æ„ç¾©ï¼Œè€Œä¸»å•é¡Œè©¢å•çš„æ˜¯å…·é«”çš„æ³•å¾‹å¾Œæœã€‚ç¶“éä»”ç´°è€ƒæ…®ï¼Œæ‚¨ç™¼ç¾è¼”åŠ©å•é¡Œæ˜¯ç›¸é—œçš„ï¼Œä½†å…¶ç­”æ¡ˆä¸æä¾›å›ç­”ä¸»å•é¡Œæ‰€éœ€çš„ä»»ä½•é—œéµä¿¡æ¯ã€‚æ‚¨çš„å›æ‡‰æ‡‰è©²æ˜¯ï¼š
{{"Decision": "Indirectly relevant"}}

ç¯„ä¾‹3ï¼š
ä¸»å•é¡Œï¼šå¼µä¸‰é•åè‘—ä½œæ¬Šæ³•ç¬¬å…«æ¢çš„é‡è£½æ¬Šè¦å®šï¼Œæœƒé¢è‡¨ä»€éº¼æ³•å¾‹å¾Œæœï¼Ÿ
è¼”åŠ©å•é¡Œï¼šè‘—ä½œæ¬Šæ³•ç¬¬å…«æ¢è¦å®šäº†å“ªäº›æ¬Šåˆ©ï¼Ÿ
åœ¨é€™ç¨®æƒ…æ³ä¸‹ï¼Œç¶“éä»”ç´°è€ƒæ…®ï¼Œæ‚¨ç™¼ç¾è¼”åŠ©å•é¡Œç¢ºå¯¦èˆ‡ä¸»å•é¡Œç›¸é—œã€‚è¼”åŠ©å•é¡Œæ˜¯ä¸»å•é¡Œçš„å­å•é¡Œï¼Œæä¾›å›ç­”ä¸»å•é¡Œæ‰€éœ€çš„å¿…è¦ä¿¡æ¯ã€‚å¦‚æœä¸å›ç­”è¼”åŠ©å•é¡Œï¼Œæ‚¨å°‡ç„¡æ³•å›ç­”ä¸»å•é¡Œã€‚æ‚¨çš„å›æ‡‰æ‡‰è©²æ˜¯ï¼š
{{"Decision": "Relevant and Necessary"}}

ç¾åœ¨è«‹åš´æ ¼éµå¾ªJSONæ ¼å¼ï¼Œé¿å…ä¸å¿…è¦çš„è½‰ç¾©ã€æ›è¡Œæˆ–ç©ºæ ¼ã€‚æ‚¨é‚„æ‡‰è©²ç‰¹åˆ¥æ³¨æ„ç¢ºä¿ï¼Œé™¤äº†JSONå’Œåˆ—è¡¨æ ¼å¼æœ¬èº«ä½¿ç”¨é›™å¼•è™Ÿ(")å¤–ï¼Œå…¶ä»–æ‰€æœ‰é›™å¼•è™Ÿçš„å¯¦ä¾‹éƒ½æ‡‰æ›¿æ›ç‚ºå–®å¼•è™Ÿã€‚ä¾‹å¦‚ï¼Œä½¿ç”¨'è‘—ä½œæ¬Šæ³•'è€Œä¸æ˜¯"è‘—ä½œæ¬Šæ³•"ã€‚

ä¸»å•é¡Œï¼š{query}
è¼”åŠ©å•é¡Œï¼š{pseudo_query}

æ±ºç­–ï¼š"""
        
        return prompt
    
    def _is_relevant_decision(self, decision: str) -> bool:
        """åˆ¤æ–·LLMæ±ºç­–æ˜¯å¦è¡¨ç¤ºç›¸é—œ"""
        import json
        
        try:
            # å˜—è©¦è§£æJSONæ ¼å¼
            if decision.strip().startswith('{'):
                result = json.loads(decision.strip())
                decision_value = result.get('Decision', '').lower()
                return ("relevant and necessary" in decision_value or 
                        "indirectly relevant" in decision_value)
        except:
            pass
        
        # å¦‚æœJSONè§£æå¤±æ•—ï¼Œä½¿ç”¨åŸå§‹é‚è¼¯
        decision_lower = decision.lower().strip()
        return ("relevant and necessary" in decision_lower or 
                "indirectly relevant" in decision_lower)
    
    async def batch_reason_about_relevance(self, query: str, current_node: str,
                                         neighbors: List[Dict[str, Any]]) -> List[bool]:
        """æ‰¹é‡æ¨ç†é„°å±…ç¯€é»çš„ç›¸é—œæ€§"""
        tasks = []
        for neighbor in neighbors:
            task = self.reason_about_relevance(query, current_node, neighbor)
            tasks.append(task)
        
        # ä¸¦è¡ŒåŸ·è¡Œæ¨ç†
        results = await asyncio.gather(*tasks, return_exceptions=True)
        
        # è™•ç†ç•°å¸¸çµæœ
        processed_results = []
        for result in results:
            if isinstance(result, Exception):
                print(f"âŒ æ‰¹é‡æ¨ç†ä¸­å‡ºç¾ç•°å¸¸: {result}")
                processed_results.append(False)
            else:
                processed_results.append(result)
        
        return processed_results

class GraphTraverser:
    """åœ–éæ­·å™¨"""
    
    def __init__(self, config: HopRAGConfig = DEFAULT_CONFIG):
        self.config = config
        self.llm_reasoner = None
        
    def set_llm_reasoner(self, llm_reasoner: LLMReasoner):
        """è¨­ç½®LLMæ¨ç†å™¨"""
        self.llm_reasoner = llm_reasoner
        
    async def traverse_graph(self, query: str, initial_nodes: List[str], 
                           graph: nx.DiGraph, nodes: Dict[str, LegalNode]) -> Dict[int, List[str]]:
        """åŸºæ–¼æ¨ç†çš„åœ–éæ­·"""
        print(f"ğŸ” é–‹å§‹HopRAGåœ–éæ­·ï¼ŒæŸ¥è©¢: '{query}'")
        
        hop_results = {0: initial_nodes}
        current_nodes = initial_nodes
        
        for hop in range(1, self.config.max_hops + 1):
            print(f"  ç¬¬ {hop} è·³æª¢ç´¢...")
            next_nodes = []
            
            for node_id in current_nodes:
                # ç²å–é„°å±…ç¯€é»
                neighbors = self._get_neighbors_with_edges(node_id, graph, nodes)
                
                # ä½¿ç”¨LLMæ¨ç†åˆ¤æ–·ç›¸é—œæ€§
                if self.llm_reasoner and neighbors:
                    relevant_neighbors = await self._filter_by_llm_reasoning(
                        query, node_id, neighbors
                    )
                    next_nodes.extend(relevant_neighbors)
            
            # å»é‡å’Œé™åˆ¶æ•¸é‡
            next_nodes = list(set(next_nodes))[:self.config.top_k_per_hop]
            hop_results[hop] = next_nodes
            current_nodes = next_nodes
            
            if not current_nodes:
                print(f"  ç¬¬ {hop} è·³å¾Œç„¡æ›´å¤šç›¸é—œç¯€é»ï¼Œåœæ­¢éæ­·")
                break
        
        # çµ±è¨ˆä¿¡æ¯
        total_nodes_found = sum(len(nodes) for nodes in hop_results.values())
        print(f"âœ… åœ–éæ­·å®Œæˆï¼Œå…±æ‰¾åˆ° {total_nodes_found} å€‹ç›¸é—œç¯€é»")
        
        return hop_results
    
    def _get_neighbors_with_edges(self, node_id: str, graph: nx.DiGraph, 
                                nodes: Dict[str, LegalNode]) -> List[Dict[str, Any]]:
        """ç²å–ç¯€é»çš„é„°å±…åŠå…¶é‚Šä¿¡æ¯"""
        neighbors = []
        
        if node_id in graph:
            for neighbor_id in graph.successors(node_id):
                if neighbor_id in nodes:
                    neighbor_node = nodes[neighbor_id]
                    edge_data = graph[node_id][neighbor_id]
                    
                    neighbor_info = {
                        'node_id': neighbor_id,
                        'node_type': neighbor_node.node_type.value,
                        'content': neighbor_node.content,
                        'contextualized_text': neighbor_node.contextualized_text,
                        'pseudo_query': edge_data.get('pseudo_query', ''),
                        'similarity_score': edge_data.get('similarity_score', 0.0),
                        'edge_type': edge_data.get('edge_type', '')
                    }
                    
                    neighbors.append(neighbor_info)
        
        return neighbors
    
    async def _filter_by_llm_reasoning(self, query: str, current_node: str, 
                                     neighbors: List[Dict[str, Any]]) -> List[str]:
        """ä½¿ç”¨LLMæ¨ç†éæ¿¾é„°å±…ç¯€é»"""
        if not self.llm_reasoner:
            # å¦‚æœæ²’æœ‰LLMæ¨ç†å™¨ï¼ŒåŸºæ–¼ç›¸ä¼¼åº¦åˆ†æ•¸éæ¿¾
            relevant_neighbors = [
                neighbor['node_id'] for neighbor in neighbors 
                if neighbor.get('similarity_score', 0) > 0.7
            ]
            return relevant_neighbors
        
        # æ‰¹é‡æ¨ç†
        relevance_decisions = await self.llm_reasoner.batch_reason_about_relevance(
            query, current_node, neighbors
        )
        
        # ç¯©é¸ç›¸é—œçš„é„°å±…
        relevant_neighbors = []
        for neighbor, is_relevant in zip(neighbors, relevance_decisions):
            if is_relevant:
                relevant_neighbors.append(neighbor['node_id'])
        
        return relevant_neighbors
    
    def get_traversal_stats(self, hop_results: Dict[int, List[str]]) -> Dict[str, Any]:
        """ç²å–éæ­·çµ±è¨ˆä¿¡æ¯"""
        total_nodes = sum(len(nodes) for nodes in hop_results.values())
        hop_counts = {f"hop_{hop}": len(nodes) for hop, nodes in hop_results.items()}
        
        return {
            "total_nodes_found": total_nodes,
            "max_hops_reached": max(hop_results.keys()) if hop_results else 0,
            "hop_distribution": hop_counts,
            "config_max_hops": self.config.max_hops,
            "config_top_k_per_hop": self.config.top_k_per_hop
        }

class HopRetriever:
    """å¤šè·³æª¢ç´¢å™¨ä¸»é¡"""
    
    def __init__(self, config: HopRAGConfig = DEFAULT_CONFIG):
        self.config = config
        
        # åˆå§‹åŒ–çµ„ä»¶
        self.initial_retriever = InitialRetriever(config)
        self.graph_traverser = GraphTraverser(config)
        self.algorithm1_traverser = None
        self.llm_reasoner = None
        
    def set_llm_reasoner(self, llm_reasoner: LLMReasoner):
        """è¨­ç½®LLMæ¨ç†å™¨"""
        self.llm_reasoner = llm_reasoner
        self.graph_traverser.set_llm_reasoner(llm_reasoner)
    
    def set_algorithm1_traverser(self, algorithm1_traverser):
        """è¨­ç½®æ¼”ç®—æ³•1éæ­·å™¨"""
        self.algorithm1_traverser = algorithm1_traverser
        
    async def retrieve(self, query: str, base_results: List[Dict[str, Any]], 
                      graph: nx.DiGraph, nodes: Dict[str, LegalNode]) -> Dict[int, List[str]]:
        """åŸ·è¡Œå¤šè·³æª¢ç´¢"""
        print(f"ğŸš€ é–‹å§‹HopRAGå¤šè·³æª¢ç´¢ï¼ŒæŸ¥è©¢: '{query}'")
        
        # Step 1: åˆå§‹æª¢ç´¢
        initial_nodes = await self.initial_retriever.retrieve_initial_nodes(query, base_results)
        
        if not initial_nodes:
            print("âš ï¸ æ²’æœ‰æ‰¾åˆ°åˆå§‹ç¯€é»")
            return {0: []}
        
        # Step 2: åœ–éæ­·
        hop_results = await self.graph_traverser.traverse_graph(
            query, initial_nodes, graph, nodes
        )
        
        # Step 3: çµ±è¨ˆä¿¡æ¯
        initial_stats = self.initial_retriever.get_retrieval_stats(initial_nodes)
        traversal_stats = self.graph_traverser.get_traversal_stats(hop_results)
        
        print(f"ğŸ“Š æª¢ç´¢çµ±è¨ˆ: {initial_stats}")
        print(f"ğŸ“Š éæ­·çµ±è¨ˆ: {traversal_stats}")
        
        return hop_results
    
    def get_retrieval_summary(self, hop_results: Dict[int, List[str]]) -> Dict[str, Any]:
        """ç²å–æª¢ç´¢æ‘˜è¦"""
        total_nodes = sum(len(nodes) for nodes in hop_results.values())
        
        return {
            "total_nodes_retrieved": total_nodes,
            "hops_performed": len(hop_results) - 1,  # æ¸›å»åˆå§‹è·³
            "nodes_per_hop": {f"hop_{hop}": len(nodes) for hop, nodes in hop_results.items()},
            "config_used": {
                "max_hops": self.config.max_hops,
                "top_k_per_hop": self.config.top_k_per_hop,
                "base_strategy": self.config.base_strategy
            }
        }
    
    async def retrieve_with_algorithm1(self, query: str, graph: nx.DiGraph, 
                                     nodes: Dict[str, LegalNode], 
                                     top_k: int = 5, n_hop: int = 4) -> List[str]:
        """ä½¿ç”¨æ¼”ç®—æ³•1é€²è¡Œæª¢ç´¢"""
        if not self.algorithm1_traverser:
            print("âŒ æ¼”ç®—æ³•1éæ­·å™¨æœªè¨­ç½®ï¼Œä½¿ç”¨æ¨™æº–æª¢ç´¢")
            return await self.retrieve(query, [], graph, nodes)
        
        print(f"ğŸš€ ä½¿ç”¨æ¼”ç®—æ³•1é€²è¡Œæª¢ç´¢")
        return await self.algorithm1_traverser.reasoning_augmented_traversal(
            query, graph, nodes, top_k, n_hop
        )

class Algorithm1Traverser:
    """æ¼”ç®—æ³•1ï¼šåŸºæ–¼æ¨ç†çš„åœ–éæ­· - å®Œæ•´å¯¦ç¾"""
    
    def __init__(self, llm_client, embedding_model, config: HopRAGConfig = DEFAULT_CONFIG):
        self.llm_client = llm_client
        self.embedding_model = embedding_model
        self.config = config
        
    async def reasoning_augmented_traversal(self, query: str, graph: nx.DiGraph, 
                                          nodes: Dict[str, LegalNode], 
                                          top_k: int, n_hop: int) -> List[str]:
        """
        æ¼”ç®—æ³•1ï¼šåŸºæ–¼æ¨ç†çš„åœ–éæ­· - å¢å¼·ç‰ˆï¼ˆæ”¯æŒnhopåˆ†æï¼‰
        
        Args:
            query: åŸå§‹æŸ¥è©¢ q
            graph: åœ–çµæ§‹ G
            nodes: ç¯€é»å­—å…¸
            top_k: æœ€çµ‚è¿”å›çš„é ‚ç´šçµæœæ•¸é‡
            n_hop: åœ–éæ­·çš„æœ€å¤§è·³æ•¸
            
        Returns:
            C: æœ€çµ‚ä¿®å‰ªå¾Œçš„ç›¸é—œä¸Šä¸‹æ–‡é›†åˆ
        """
        print(f"ğŸš€ é–‹å§‹æ¼”ç®—æ³•1ï¼šåŸºæ–¼æ¨ç†çš„åœ–éæ­·")
        print(f"   æŸ¥è©¢: '{query}'")
        print(f"   æœ€å¤§è·³æ•¸: {n_hop}")
        print(f"   ç›®æ¨™çµæœæ•¸: {top_k}")
        
        # åˆå§‹åŒ–nhopåˆ†ææ•¸æ“š
        nhop_stats = {
            'llm_calls': 0,
            'queue_lengths': [],
            'new_nodes_per_hop': [],
            'total_nodes_visited': 0,
            'early_stop_triggered': False
        }
        
        # Step 1: v_q <- EMB(q) - æŸ¥è©¢åµŒå…¥
        v_q = await self._get_query_embedding(query)
        
        # Step 2: k_q <- NER(q) - å‘½åå¯¦é«”è­˜åˆ¥
        k_q = await self._extract_entities(query)
        
        # Step 3: C_queue <- Retrieve(v_q, k_q, G) - åˆå§‹æª¢ç´¢
        C_queue = await self._initial_retrieve(v_q, k_q, graph, nodes)
        
        # Step 4: C_count <- Counter(C_queue) - è¨ˆæ•¸å™¨åˆå§‹åŒ–
        C_count = Counter(C_queue)
        
        print(f"âœ… åˆå§‹æª¢ç´¢å®Œæˆï¼Œç²å¾— {len(C_queue)} å€‹åˆå§‹ç¯€é»")
        nhop_stats['queue_lengths'].append(len(C_queue))
        
        # Step 5-16: å»£åº¦å„ªå…ˆéæ­·å¾ªç’°
        for i in range(1, n_hop + 1):
            print(f"  ç¬¬ {i} è·³æª¢ç´¢...")
            
            # è¨˜éŒ„è·³èºå‰çš„éšŠåˆ—é•·åº¦
            initial_queue_length = len(C_queue)
            new_nodes_this_hop = 0
            
            # å‰µå»ºç•¶å‰è·³çš„éšŠåˆ—å‰¯æœ¬
            current_queue = deque(C_queue)
            queue_size = len(current_queue)
            
            # Step 6: for j <- 1, 2, ..., |C_queue| do
            for j in range(queue_size):
                # Step 7: v_j <- C_queue.dequeue()
                v_j = current_queue.popleft()
                
                # Step 8: v_k <- Reason({<v_j, e_j,k, v_k>})
                v_k = await self._reason_next_node(query, v_j, graph, nodes)
                
                # è¿½è¹¤LLMå‘¼å«æ¬¡æ•¸
                if self.config.nhop_cost_tracking:
                    nhop_stats['llm_calls'] += 1
                
                if v_k is not None:
                    # Step 9-13: æ›´æ–°è¨ˆæ•¸å™¨
                    if v_k not in C_count:
                        # Step 10-11: æ–°ç¯€é»åŠ å…¥éšŠåˆ—å’Œè¨ˆæ•¸å™¨
                        C_queue.append(v_k)
                        C_count[v_k] = 1
                        new_nodes_this_hop += 1
                    else:
                        # Step 13: å·²å­˜åœ¨ç¯€é»è¨ˆæ•¸åŠ 1
                        C_count[v_k] += 1
            
            # è¨˜éŒ„æœ¬è·³çµ±è¨ˆ
            nhop_stats['new_nodes_per_hop'].append(new_nodes_this_hop)
            nhop_stats['queue_lengths'].append(len(C_queue))
            nhop_stats['total_nodes_visited'] = len(C_count)
            
            print(f"    æ–°å¢ç¯€é»: {new_nodes_this_hop}, éšŠåˆ—é•·åº¦: {len(C_queue)}")
            
            # æ—©æœŸåœæ­¢æª¢æŸ¥ï¼ˆåŸºæ–¼è«–æ–‡è§€å¯Ÿï¼šç¬¬5è·³éšŠåˆ—é•·åº¦åƒ…1.23ï¼‰
            if (self.config.enable_nhop_analysis and 
                len(C_queue) <= self.config.queue_length_threshold and 
                i >= 2):  # è‡³å°‘åŸ·è¡Œ2è·³
                print(f"  ğŸ›‘ æ—©æœŸåœæ­¢ï¼šéšŠåˆ—é•·åº¦ {len(C_queue)} <= é–¾å€¼ {self.config.queue_length_threshold}")
                nhop_stats['early_stop_triggered'] = True
                break
            
            # é™åˆ¶éšŠåˆ—å¤§å°ï¼Œé¿å…çˆ†ç‚¸æ€§å¢é•·
            if len(C_queue) > self.config.top_k_per_hop * 2:
                # æŒ‰è¨ˆæ•¸æ’åºï¼Œä¿ç•™æœ€é‡è¦çš„ç¯€é»
                sorted_nodes = sorted(C_count.items(), key=lambda x: x[1], reverse=True)
                C_queue = deque([node_id for node_id, _ in sorted_nodes[:self.config.top_k_per_hop * 2]])
                print(f"    éšŠåˆ—æˆªæ–·ï¼šä¿ç•™å‰ {len(C_queue)} å€‹ç¯€é»")
        
        # Step 17: C <- Prune(C_count, v_q, k_q, top_k) - çµæœä¿®å‰ª
        C = await self._prune_results(C_count, v_q, k_q, top_k, nodes)
        
        # è¼¸å‡ºnhopåˆ†æçµæœ
        if self.config.enable_nhop_analysis:
            self._print_nhop_analysis(nhop_stats, n_hop)
        
        print(f"âœ… æ¼”ç®—æ³•1å®Œæˆï¼Œè¿”å› {len(C)} å€‹æœ€çµ‚çµæœ")
        return C
    
    def _print_nhop_analysis(self, nhop_stats: Dict[str, Any], max_nhop: int):
        """æ‰“å°nhopåˆ†æçµæœ"""
        print(f"\nğŸ“Š nhopæ€§èƒ½åˆ†æå ±å‘Š")
        print(f"=" * 50)
        
        # åŸºæœ¬çµ±è¨ˆ
        print(f"ğŸ”¢ åŸºæœ¬çµ±è¨ˆ:")
        print(f"   ç¸½LLMå‘¼å«æ¬¡æ•¸: {nhop_stats['llm_calls']}")
        print(f"   ç¸½è¨ªå•ç¯€é»æ•¸: {nhop_stats['total_nodes_visited']}")
        print(f"   æ—©æœŸåœæ­¢: {'æ˜¯' if nhop_stats['early_stop_triggered'] else 'å¦'}")
        
        # éšŠåˆ—é•·åº¦åˆ†æ
        print(f"\nğŸ“ˆ éšŠåˆ—é•·åº¦è®ŠåŒ–:")
        for i, length in enumerate(nhop_stats['queue_lengths']):
            hop_label = "åˆå§‹" if i == 0 else f"ç¬¬{i}è·³"
            print(f"   {hop_label}: {length} å€‹ç¯€é»")
        
        # æ¯è·³æ–°å¢ç¯€é»åˆ†æ
        print(f"\nğŸ†• æ¯è·³æ–°å¢ç¯€é»:")
        for i, new_nodes in enumerate(nhop_stats['new_nodes_per_hop']):
            print(f"   ç¬¬{i+1}è·³: {new_nodes} å€‹æ–°ç¯€é»")
        
        # æˆæœ¬æ•ˆç›Šåˆ†æ
        if nhop_stats['llm_calls'] > 0:
            efficiency = nhop_stats['total_nodes_visited'] / nhop_stats['llm_calls']
            print(f"\nğŸ’° æˆæœ¬æ•ˆç›Šåˆ†æ:")
            print(f"   ç¯€é»/LLMå‘¼å«æ¯”: {efficiency:.2f}")
            print(f"   å¹³å‡æ¯è·³LLMå‘¼å«: {nhop_stats['llm_calls'] / len(nhop_stats['new_nodes_per_hop']):.1f}")
        
        # èˆ‡è«–æ–‡æ•¸æ“šå°æ¯”
        print(f"\nğŸ“š èˆ‡è«–æ–‡æ•¸æ“šå°æ¯”:")
        print(f"   è«–æ–‡nhop=4å¹³å‡LLMå‘¼å«: ~38.53æ¬¡")
        print(f"   ç•¶å‰LLMå‘¼å«æ¬¡æ•¸: {nhop_stats['llm_calls']}æ¬¡")
        if nhop_stats['llm_calls'] > 0:
            ratio = nhop_stats['llm_calls'] / 38.53
            print(f"   ç›¸å°è«–æ–‡æ¯”ä¾‹: {ratio:.2f}x")
        
        # å»ºè­°
        print(f"\nğŸ’¡ å„ªåŒ–å»ºè­°:")
        if nhop_stats['early_stop_triggered']:
            print(f"   âœ… æ—©æœŸåœæ­¢ç”Ÿæ•ˆï¼Œç¯€çœäº†è¨ˆç®—æˆæœ¬")
        if len(nhop_stats['queue_lengths']) > 1:
            final_queue = nhop_stats['queue_lengths'][-1]
            if final_queue <= 2:
                print(f"   âœ… æœ€çµ‚éšŠåˆ—é•·åº¦({final_queue})ç¬¦åˆè«–æ–‡è§€å¯Ÿ(â‰¤1.23)")
            else:
                print(f"   âš ï¸ æœ€çµ‚éšŠåˆ—é•·åº¦({final_queue})è¼ƒé«˜ï¼Œå¯è€ƒæ…®èª¿æ•´é–¾å€¼")
        
        print(f"=" * 50)
    
    async def _get_query_embedding(self, query: str) -> np.ndarray:
        """Step 1: v_q <- EMB(q) - æŸ¥è©¢åµŒå…¥"""
        try:
            if hasattr(self.embedding_model, 'encode_async'):
                embedding = await self.embedding_model.encode_async([query])
            else:
                embedding = self.embedding_model.encode([query])
            return embedding[0]
        except Exception as e:
            print(f"âŒ æŸ¥è©¢åµŒå…¥å¤±æ•—: {e}")
            return np.zeros(768)  # è¿”å›é›¶å‘é‡ä½œç‚ºfallback
    
    async def _extract_entities(self, query: str) -> Set[str]:
        """Step 2: k_q <- NER(q) - å‘½åå¯¦é«”è­˜åˆ¥"""
        import jieba
        import re
        
        # ç°¡å–®çš„å¯¦é«”æå–ï¼ˆå¯ä»¥å¾ŒçºŒæ”¹é€²ç‚ºçœŸæ­£çš„NERï¼‰
        words = jieba.lcut(query)
        
        # éæ¿¾åœç”¨è©ï¼Œä¿ç•™æœ‰æ„ç¾©çš„è©å½™
        stop_words = {'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº', 'éƒ½', 'ä¸€', 'ä¸€å€‹', 'ä¸Š', 'ä¹Ÿ', 'å¾ˆ', 'åˆ°', 'èªª', 'è¦', 'å»', 'ä½ ', 'æœƒ', 'è‘—', 'æ²’æœ‰', 'çœ‹', 'å¥½', 'è‡ªå·±', 'é€™', 'ä»€éº¼', 'å¦‚ä½•', 'ç‚ºä»€éº¼', 'å“ªäº›', 'ä»€éº¼æ™‚å€™', 'å“ªè£¡', 'å¤šå°‘', 'æ€éº¼', 'æ˜¯å¦', 'èƒ½å¦', 'å¯ä»¥', 'æ‡‰è©²', 'å¿…é ˆ', 'éœ€è¦', 'è¦æ±‚'}
        
        entities = set()
        for word in words:
            word = word.strip()
            if (len(word) >= 2 and 
                word not in stop_words and 
                not re.match(r'^[0-9]+$', word) and
                not re.match(r'^[a-zA-Z]+$', word)):
                entities.add(word)
        
        return entities
    
    async def _initial_retrieve(self, v_q: np.ndarray, k_q: Set[str], 
                              graph: nx.DiGraph, nodes: Dict[str, LegalNode]) -> List[str]:
        """Step 3: C_queue <- Retrieve(v_q, k_q, G) - åˆå§‹æª¢ç´¢"""
        # è¨ˆç®—æ‰€æœ‰ç¯€é»èˆ‡æŸ¥è©¢çš„ç›¸ä¼¼åº¦
        similarities = []
        
        for node_id, node in nodes.items():
            if hasattr(node, 'embedding') and node.embedding is not None:
                # èªç¾©ç›¸ä¼¼åº¦
                semantic_sim = np.dot(v_q, node.embedding) / (
                    np.linalg.norm(v_q) * np.linalg.norm(node.embedding)
                )
                
                # è©å½™ç›¸ä¼¼åº¦ï¼ˆå¦‚æœæœ‰é—œéµè©ï¼‰
                lexical_sim = 0.0
                if hasattr(node, 'keywords') and node.keywords and k_q:
                    intersection = len(node.keywords.intersection(k_q))
                    union = len(node.keywords.union(k_q))
                    if union > 0:
                        lexical_sim = intersection / union
                
                # æ··åˆç›¸ä¼¼åº¦
                mixed_sim = (semantic_sim + lexical_sim) / 2
                similarities.append((node_id, mixed_sim))
        
        # æŒ‰ç›¸ä¼¼åº¦æ’åºï¼Œå–å‰kå€‹
        similarities.sort(key=lambda x: x[1], reverse=True)
        initial_nodes = [node_id for node_id, _ in similarities[:self.config.initial_retrieve_k]]
        
        return initial_nodes
    
    async def _reason_next_node(self, query: str, current_node: str, 
                              graph: nx.DiGraph, nodes: Dict[str, LegalNode]) -> Optional[str]:
        """Step 8: v_k <- Reason({<v_j, e_j,k, v_k>}) - LLMæ¨ç†ä¸‹ä¸€å€‹ç¯€é»"""
        if current_node not in graph:
            return None
        
        # ç²å–ç•¶å‰ç¯€é»çš„æ‰€æœ‰é„°å±…
        neighbors = list(graph.successors(current_node))
        if not neighbors:
            return None
        
        # å¦‚æœåªæœ‰ä¸€å€‹é„°å±…ï¼Œç›´æ¥è¿”å›
        if len(neighbors) == 1:
            return neighbors[0]
        
        # æ§‹å»ºæ¨ç†æç¤º
        current_node_data = nodes.get(current_node)
        if not current_node_data:
            return None
        
        # æ”¶é›†é„°å±…ä¿¡æ¯
        neighbor_info = []
        for neighbor_id in neighbors:
            neighbor_data = nodes.get(neighbor_id)
            if neighbor_data:
                edge_data = graph[current_node][neighbor_id]
                neighbor_info.append({
                    'node_id': neighbor_id,
                    'content': neighbor_data.content[:200],  # é™åˆ¶é•·åº¦
                    'pseudo_query': edge_data.get('pseudo_query', ''),
                    'similarity_score': edge_data.get('similarity_score', 0.0)
                })
        
        # æ§‹å»ºLLMæ¨ç†æç¤º
        prompt = self._build_reasoning_prompt(query, current_node_data, neighbor_info)
        
        try:
            # èª¿ç”¨LLMé€²è¡Œæ¨ç†
            response = await self.llm_client.generate_async(prompt)
            
            # è§£æLLMéŸ¿æ‡‰ï¼Œæå–æœ€ç›¸é—œçš„ç¯€é»ID
            selected_node = self._parse_reasoning_response(response, neighbor_info)
            return selected_node
            
        except Exception as e:
            print(f"âŒ LLMæ¨ç†å¤±æ•—: {e}")
            # Fallback: è¿”å›ç›¸ä¼¼åº¦æœ€é«˜çš„é„°å±…
            best_neighbor = max(neighbor_info, key=lambda x: x['similarity_score'])
            return best_neighbor['node_id']
    
    def _build_reasoning_prompt(self, query: str, current_node: LegalNode, 
                              neighbor_info: List[Dict[str, Any]]) -> str:
        """æ§‹å»ºLLMæ¨ç†æç¤º"""
        current_content = current_node.content[:300]  # é™åˆ¶é•·åº¦
        
        # æ§‹å»ºé„°å±…é¸é …
        options = []
        for i, neighbor in enumerate(neighbor_info):
            options.append(f"é¸é …{i+1}: {neighbor['node_id']}\n"
                          f"å…§å®¹: {neighbor['content']}\n"
                          f"é€£æ¥å•é¡Œ: {neighbor['pseudo_query']}\n"
                          f"ç›¸ä¼¼åº¦: {neighbor['similarity_score']:.3f}\n")
        
        prompt = f"""
æ‚¨æ˜¯ä¸€å€‹æ³•å¾‹å•ç­”æ©Ÿå™¨äººã€‚æˆ‘éœ€è¦æ‚¨å¹«åŠ©é¸æ“‡æœ€ç›¸é—œçš„ä¸‹ä¸€å€‹ç¯€é»ã€‚

ä¸»å•é¡Œ: {query}

ç•¶å‰ç¯€é»å…§å®¹: {current_content}

å¯é¸çš„ä¸‹ä¸€å€‹ç¯€é»:
{chr(10).join(options)}

è«‹åŸºæ–¼ä»¥ä¸‹æ¨™æº–é¸æ“‡æœ€ç›¸é—œçš„ç¯€é»ï¼š
1. èˆ‡ä¸»å•é¡Œçš„é‚è¼¯é—œè¯æ€§
2. é€£æ¥å•é¡Œçš„ç›¸é—œæ€§
3. å…§å®¹çš„ç›¸é—œæ€§

è«‹åªè¿”å›é¸é …ç·¨è™Ÿï¼ˆå¦‚ï¼šé¸é …1ã€é¸é …2ç­‰ï¼‰ï¼Œä¸è¦åŒ…å«å…¶ä»–æ–‡å­—ã€‚
"""
        
        return prompt
    
    def _parse_reasoning_response(self, response: str, neighbor_info: List[Dict[str, Any]]) -> str:
        """è§£æLLMæ¨ç†éŸ¿æ‡‰"""
        response = response.strip().lower()
        
        # å˜—è©¦æå–é¸é …ç·¨è™Ÿ
        for i in range(len(neighbor_info)):
            if f"é¸é …{i+1}" in response or f"option{i+1}" in response or str(i+1) in response:
                return neighbor_info[i]['node_id']
        
        # å¦‚æœç„¡æ³•è§£æï¼Œè¿”å›ç›¸ä¼¼åº¦æœ€é«˜çš„
        best_neighbor = max(neighbor_info, key=lambda x: x['similarity_score'])
        return best_neighbor['node_id']
    
    async def _prune_results(self, C_count: Counter, v_q: np.ndarray, k_q: Set[str], 
                           top_k: int, nodes: Dict[str, LegalNode]) -> List[str]:
        """
        Step 17: C <- Prune(C_count, v_q, k_q, top_k) - çµæœä¿®å‰ª
        æŒ‰ç…§è«–æ–‡è¦æ±‚å¯¦ç¾æœ‰ç”¨æ€§åº¦é‡ï¼šH_i = (SIM(v_i, q) + IMP(v_i, C_count)) / 2
        """
        print(f"ğŸ” é–‹å§‹ä¿®å‰ªéšæ®µï¼Œå€™é¸ç¯€é»æ•¸: {len(C_count)}")
        
        # è¨ˆç®—æ¯å€‹ç¯€é»çš„æœ‰ç”¨æ€§åº¦é‡ H_i
        helpfulness_scores = []
        
        # è¨ˆç®—ç¸½è¨ªå•æ¬¡æ•¸ï¼ˆç”¨æ–¼IMPè¨ˆç®—ï¼‰
        total_visits = sum(C_count.values())
        
        for node_id, count in C_count.items():
            node = nodes.get(node_id)
            if not node:
                continue
            
            # 1. è¨ˆç®— IMP(v_i, C_count) = C_count[v_i] / Î£ C_count[v_j] (è¦ç¯„åŒ–è¨ªå•æ¬¡æ•¸)
            imp_score = count / total_visits if total_visits > 0 else 0
            
            # 2. è¨ˆç®— SIM(v_i, q) = æ®µè½èˆ‡æŸ¥è©¢çš„å¹³å‡è©å½™+èªç¾©ç›¸ä¼¼åº¦
            sim_score = self._calculate_sim_score(node, v_q, k_q)
            
            # 3. è¨ˆç®—æœ‰ç”¨æ€§åº¦é‡ H_i = (SIM(v_i, q) + IMP(v_i, C_count)) / 2
            helpfulness_score = (sim_score + imp_score) / 2
            
            helpfulness_scores.append((node_id, helpfulness_score, sim_score, imp_score))
        
        # æŒ‰æœ‰ç”¨æ€§åˆ†æ•¸æ’åºï¼Œå–å‰top_kå€‹
        helpfulness_scores.sort(key=lambda x: x[1], reverse=True)
        pruned_results = [node_id for node_id, _, _, _ in helpfulness_scores[:top_k]]
        
        # è¼¸å‡ºä¿®å‰ªçµ±è¨ˆä¿¡æ¯
        print(f"âœ… ä¿®å‰ªå®Œæˆï¼Œä¿ç•™ {len(pruned_results)} å€‹ç¯€é»")
        if helpfulness_scores:
            best_score = helpfulness_scores[0][1]
            worst_score = helpfulness_scores[-1][1]
            print(f"   æœ‰ç”¨æ€§åˆ†æ•¸ç¯„åœ: {worst_score:.3f} - {best_score:.3f}")
        
        return pruned_results
    
    def _calculate_sim_score(self, node: LegalNode, v_q: np.ndarray, k_q: Set[str]) -> float:
        """
        è¨ˆç®— SIM(v_i, q) = æ®µè½èˆ‡æŸ¥è©¢çš„å¹³å‡è©å½™+èªç¾©ç›¸ä¼¼åº¦
        """
        # èªç¾©ç›¸ä¼¼åº¦ï¼ˆé¤˜å¼¦ç›¸ä¼¼åº¦ï¼‰
        semantic_score = 0.0
        if hasattr(node, 'embedding') and node.embedding is not None:
            semantic_score = np.dot(v_q, node.embedding) / (
                np.linalg.norm(v_q) * np.linalg.norm(node.embedding)
            )
        
        # è©å½™ç›¸ä¼¼åº¦ï¼ˆJaccardç›¸ä¼¼åº¦ï¼‰
        lexical_score = 0.0
        if hasattr(node, 'keywords') and node.keywords and k_q:
            intersection = len(node.keywords.intersection(k_q))
            union = len(node.keywords.union(k_q))
            if union > 0:
                lexical_score = intersection / union
        
        # å¹³å‡è©å½™+èªç¾©ç›¸ä¼¼åº¦
        sim_score = (semantic_score + lexical_score) / 2
        
        return sim_score
