from __future__ import annotations

import io
import os
import uuid
from dataclasses import dataclass
import re
from typing import List, Optional, Dict, Any, Tuple
import json
from datetime import datetime
import asyncio
import time
from concurrent.futures import ThreadPoolExecutor

from fastapi import FastAPI, UploadFile, File, Form, HTTPException, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from pydantic import BaseModel
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

from .models import ChunkConfig, MetadataOptions, MultiLevelFusionRequest
from .hybrid_search import hybrid_rank, HybridConfig
from .store import InMemoryStore
from .query_classifier import query_classifier, get_query_analysis
from .result_fusion import MultiLevelResultFusion, FusionConfig, fuse_multi_level_results
from .hoprag_system_modular import HopRAGSystem
from .hoprag_clients import HopRAGClientManager
from .hoprag_config import HopRAGConfig, DEFAULT_CONFIG
try:
    from rank_bm25 import BM25Okapi  # type: ignore
    BM25_AVAILABLE = True
except ImportError:
    BM25Okapi = None  # type: ignore
    BM25_AVAILABLE = False
from dotenv import load_dotenv
try:
    from PyPDF2 import PdfReader
    PYPDF2_AVAILABLE = True
except ImportError:
    PdfReader = None
    PYPDF2_AVAILABLE = False
import pdfplumber
try:
    import jieba  # type: ignore
    import jieba.analyse  # type: ignore
    jieba.initialize()
except ImportError:
    jieba = None  # type: ignore

try:
    import google.generativeai as genai  # type: ignore
    GEMINI_AVAILABLE = True
except ImportError:  # pragma: no cover - optional dependency
    genai = None  # type: ignore
    GEMINI_AVAILABLE = False

try:
    from sentence_transformers import SentenceTransformer  # type: ignore
    SENTENCE_TRANSFORMERS_AVAILABLE = True
except ImportError:  # pragma: no cover - optional dependency
    SentenceTransformer = None  # type: ignore
    SENTENCE_TRANSFORMERS_AVAILABLE = False

# Embedding ç¶­åº¦é…ç½®
# Gemini: æ”¯æ´ 128-3072ï¼Œå»ºè­° 768/1536/3072
# BGE-M3: å›ºå®š 1024 æˆ– 3072ï¼ˆå–æ±ºæ–¼é…ç½®ï¼‰
EMBEDDING_DIMENSION = 3072  # ğŸ¯ çµ±ä¸€é…ç½®ï¼šæ”¹é€™è£¡å°±èƒ½æ”¹å…¨éƒ¨

load_dotenv()


def get_env_bool(name: str, default: bool = False) -> bool:
    v = os.getenv(name)
    if v is None:
        return default
    return v.lower() in {"1", "true", "yes", "on"}


GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY") or "AIzaSyC3hF9d-BWVQRjTd_uzo4grF9upIDsZhEI"
USE_GEMINI_EMBEDDING = True  # å¼ºåˆ¶ä½¿ç”¨ Gemini
USE_GEMINI_COMPLETION = True
USE_BGE_M3_EMBEDDING = False  # å¼ºåˆ¶ä¸ä½¿ç”¨ BGE-M3

# èª¿è©¦ä¿¡æ¯
print(f"ğŸ”§ Embedding é…ç½®:")
print(f"   USE_GEMINI_EMBEDDING: {USE_GEMINI_EMBEDDING}")
print(f"   GOOGLE_API_KEY: {'å·²è¨­ç½®' if GOOGLE_API_KEY else 'æœªè¨­ç½®'}")
print(f"   GEMINI_API_KEY: {'å·²è¨­ç½®' if os.getenv('GEMINI_API_KEY') else 'æœªè¨­ç½®'}")
print(f"   USE_BGE_M3_EMBEDDING: {USE_BGE_M3_EMBEDDING}")
print(f"   GOOGLE_EMBEDDING_MODEL: {os.getenv('GOOGLE_EMBEDDING_MODEL', 'gemini-embedding-001')}")
print(f"   USE_GEMINI_COMPLETION: {USE_GEMINI_COMPLETION}")

try:
    import httpx
except Exception:  # pragma: no cover
    httpx = None


# ---- Simple in-memory store (demo only) ----
@dataclass
class DocRecord:
    id: str
    filename: str
    text: str
    chunks: List[str]
    chunk_size: int
    overlap: int
    json_data: Optional[Dict[str, Any]] = None  # å­˜å„²çµæ§‹åŒ–JSONæ•¸æ“š
    structured_chunks: Optional[List[Dict[str, Any]]] = None  # å­˜å„²çµæ§‹åŒ–chunks
    generated_questions: Optional[List[str]] = None  # å­˜å„²ç”Ÿæˆçš„å•é¡Œ








from .store import InMemoryStore
store = InMemoryStore()

# åˆå§‹åŒ–HopRAGç³»çµ±ï¼ˆæ¨¡çµ„åŒ–æ¶æ§‹ï¼‰
hoprag_client_manager = HopRAGClientManager()
hoprag_system = HopRAGSystem(
    llm_client=hoprag_client_manager.get_llm_client(),
    embedding_model=hoprag_client_manager.get_embedding_client(),
    config=DEFAULT_CONFIG
)


app = FastAPI(title="RAG Visualizer API", version="0.1.0")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # During dev; restrict in prod
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# æš«æ™‚åœç”¨routes.pyçš„åŒ…å«ï¼Œé¿å…å¾ªç’°å°å…¥å•é¡Œ
from .routes import router
app.include_router(router, prefix="/api")


class ChunkRequest(BaseModel):
    doc_id: str
    chunk_size: int = 500
    overlap: int = 50
    strategy: str = "fixed_size"
    use_json_structure: bool = False
    
    # ç­–ç•¥ç‰¹å®šåƒæ•¸
    hierarchical_params: Optional[Dict[str, Any]] = None
    adaptive_params: Optional[Dict[str, Any]] = None
    hybrid_params: Optional[Dict[str, Any]] = None
    semantic_params: Optional[Dict[str, Any]] = None
    rcts_hierarchical_params: Optional[Dict[str, Any]] = None
    structured_hierarchical_params: Optional[Dict[str, Any]] = None


class EmbedRequest(BaseModel):
    doc_ids: Optional[List[str]] = None  # if None, embed all


class RetrieveRequest(BaseModel):
    query: str
    k: int = 5


class GenerateRequest(BaseModel):
    query: str
    top_k: int = 5


# MetadataOptions å·²ç§»è‡³ models.py


# è©•æ¸¬ç›¸é—œçš„æ•¸æ“šæ¨¡å‹
# ChunkConfig å·²ç§»è‡³ models.py


class EvaluationMetrics(BaseModel):
    precision_omega: float  # PrecisionÎ© - æœ€å¤§æº–ç¢ºç‡
    precision_at_k: Dict[int, float]  # k -> precision score
    recall_at_k: Dict[int, float]  # k -> recall score
    chunk_count: int
    avg_chunk_length: float
    length_variance: float


class EvaluationResult(BaseModel):
    config: Dict[str, Any]  # æ”¹ç‚ºå­—å…¸ä»¥æ”¯æ´å‹•æ…‹åƒæ•¸
    metrics: EvaluationMetrics
    test_queries: List[str]
    retrieval_results: Dict[str, List[Dict]]  # query -> results
    timestamp: datetime


@dataclass
class EvaluationTask:
    id: str
    doc_id: str
    configs: List[ChunkConfig]
    test_queries: List[str]
    k_values: List[int]
    status: str  # "pending", "running", "completed", "failed"
    results: List[EvaluationResult]
    created_at: datetime
    progress: float = 0.0  # æ–°å¢ï¼šé€²åº¦ 0.0 to 1.0
    completed_at: Optional[datetime] = None
    error_message: Optional[str] = None
    strategy: str = "fixed_size"  # æ–°å¢ï¼šåˆ†å‰²ç­–ç•¥


class EvaluationStore:
    def __init__(self) -> None:
        self.tasks: Dict[str, EvaluationTask] = {}
        self.executor = ThreadPoolExecutor(max_workers=2)

    def create_task(self, doc_id: str, configs: List[ChunkConfig], 
                   test_queries: List[str], k_values: List[int], 
                   strategy: str = "fixed_size") -> str:
        task_id = str(uuid.uuid4())
        task = EvaluationTask(
            id=task_id,
            doc_id=doc_id,
            configs=configs,
            test_queries=test_queries,
            k_values=k_values,
            strategy=strategy,
            status="pending",
            results=[],
            created_at=datetime.now()
        )
        self.tasks[task_id] = task
        return task_id

    def get_task(self, task_id: str) -> Optional[EvaluationTask]:
        return self.tasks.get(task_id)

    def update_task_status(self, task_id: str, status: str, 
                          results: Optional[List[EvaluationResult]] = None,
                          error_message: Optional[str] = None,
                          progress: Optional[float] = None):
        if task_id in self.tasks:
            self.tasks[task_id].status = status
            if results is not None:
                self.tasks[task_id].results = results
            if error_message is not None:
                self.tasks[task_id].error_message = error_message
            if progress is not None:
                self.tasks[task_id].progress = progress
            if status == "completed":
                self.tasks[task_id].completed_at = datetime.now()
                self.tasks[task_id].progress = 1.0


# å‰µå»ºè©•ä¼°å­˜å„²å¯¦ä¾‹
eval_store = EvaluationStore()


class FixedSizeEvaluationRequest(BaseModel):
    doc_id: str
    chunk_sizes: List[int] = [300, 500, 800]
    overlap_ratios: List[float] = [0.0, 0.1, 0.2]
    strategy: str = "fixed_size"  # æ–°å¢ï¼šåˆ†å‰²ç­–ç•¥
    test_queries: List[str] = [
        "è‘—ä½œæ¬Šçš„å®šç¾©æ˜¯ä»€éº¼ï¼Ÿ",
        "ä»€éº¼æƒ…æ³ä¸‹å¯ä»¥åˆç†ä½¿ç”¨ä»–äººä½œå“ï¼Ÿ",
        "ä¾µçŠ¯è‘—ä½œæ¬Šçš„æ³•å¾‹å¾Œæœæ˜¯ä»€éº¼ï¼Ÿ",
        "è‘—ä½œæ¬Šçš„ä¿è­·æœŸé™æ˜¯å¤šä¹…ï¼Ÿ",
        "å¦‚ä½•ç”³è«‹è‘—ä½œæ¬Šç™»è¨˜ï¼Ÿ"
    ]
    k_values: List[int] = [1, 3, 5, 10]
    
    # ç­–ç•¥ç‰¹å®šåƒæ•¸é¸é … - é è¨­åŒ…å«æ‰€æœ‰æ’åˆ—çµ„åˆ
    chunk_by_options: List[str] = ["article", "item", "section", "chapter"]  # çµæ§‹åŒ–å±¤æ¬¡åˆ†å‰²é¸é …
    preserve_structure_options: List[bool] = [True, False]  # RCTSå±¤æ¬¡åˆ†å‰²é¸é …
    level_depth_options: List[int] = [2, 3, 4]  # å±¤æ¬¡åˆ†å‰²é¸é …
    similarity_threshold_options: List[float] = [0.5, 0.6, 0.7]  # èªç¾©åˆ†å‰²é¸é …
    semantic_threshold_options: List[float] = [0.6, 0.7, 0.8]  # LLMèªç¾©åˆ†å‰²é¸é …
    switch_threshold_options: List[float] = [0.3, 0.5, 0.7]  # æ··åˆåˆ†å‰²é¸é …
    min_chunk_size_options: List[int] = [100, 200, 300]  # å±¤æ¬¡åˆ†å‰²é¸é …
    context_window_options: List[int] = [50, 100, 150]  # èªç¾©åˆ†å‰²é¸é …
    step_size_options: List[int] = [200, 250, 300]  # æ»‘å‹•è¦–çª—é¸é …
    window_size_options: List[int] = [400, 500, 600, 800]  # æ»‘å‹•è¦–çª—é¸é …
    boundary_aware_options: List[bool] = [True, False]  # æ»‘å‹•è¦–çª—é¸é …
    preserve_sentences_options: List[bool] = [True, False]  # æ»‘å‹•è¦–çª—é¸é …
    min_chunk_size_options_sw: List[int] = [50, 100, 150]  # æ»‘å‹•è¦–çª—å°ˆç”¨é¸é …
    max_chunk_size_options_sw: List[int] = [800, 1000, 1200]  # æ»‘å‹•è¦–çª—å°ˆç”¨é¸é …
    secondary_size_options: List[int] = [300, 400, 500]  # æ··åˆåˆ†å‰²é¸é …  # ç”¨æ–¼è¨ˆç®—recall@K


class GenerateQuestionsRequest(BaseModel):
    doc_id: str
    num_questions: int = 10
    question_types: List[str] = ["æ¡ˆä¾‹æ‡‰ç”¨", "æƒ…å¢ƒåˆ†æ", "å¯¦å‹™è™•ç†", "æ³•å¾‹å¾Œæœ", "åˆè¦åˆ¤æ–·"]  # å•é¡Œé¡å‹
    difficulty_levels: List[str] = ["åŸºç¤", "é€²éš", "æ‡‰ç”¨"]  # é›£åº¦ç­‰ç´š


class GeneratedQuestion(BaseModel):
    question: str
    references: List[str]  # ç›¸é—œæ³•è¦æ¢æ–‡
    question_type: str
    difficulty: str
    keywords: List[str]
    estimated_tokens: int


class QuestionGenerationResult(BaseModel):
    doc_id: str
    total_questions: int
    questions: List[GeneratedQuestion]
    generation_time: float
    timestamp: datetime


def generate_unique_id(law_name: str, chapter: str, section: str, article: str, item: Optional[str] = None) -> str:
    """ç”Ÿæˆid"""
    # æ¸…ç†æ³•è¦åç¨±
    law_clean = re.sub(r'[^\w]', '', law_name.lower())
    law_clean = re.sub(r'æ³•è¦åç¨±|æ³•|æ¢ä¾‹', '', law_clean)
    
    # æå–ç« ç¯€
    chapter_num = re.search(r'ç¬¬([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒ0-9]+)ç« ', chapter)
    chapter_num = chapter_num.group(1) if chapter_num else "0"
    
    # æå–ç¯€
    section_num = re.search(r'ç¬¬([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒ0-9]+)ç¯€', section)
    section_num = section_num.group(1) if section_num else "0"
    
    # æå–æ¢æ–‡
    article_num = re.search(r'ç¬¬([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒ0-9]+)æ¢', article)
    article_num = article_num.group(1) if article_num else "0"
    
    # çµ„åˆID
    parts = [law_clean, f"ch{chapter_num}", f"sec{section_num}", f"art{article_num}"]
    if item:
        parts.append(f"item{item}")
    
    return "-".join(parts)


def extract_keywords_with_gemini(text: str, top_k: int = 5) -> List[str]:
    """ä½¿ç”¨Geminiæ¨¡å‹æå–é—œéµè©"""
    if not GEMINI_AVAILABLE:
        return extract_keywords_fallback(text, top_k)
    
    try:
        # å„ªå…ˆä½¿ç”¨ GOOGLE_API_KEYï¼Œå¦‚æœæ²’æœ‰å‰‡ä½¿ç”¨ GEMINI_API_KEY
        api_key = GOOGLE_API_KEY or os.getenv('GEMINI_API_KEY')
        if not api_key:
            return extract_keywords_fallback(text, top_k)
        
        # Configure API key using getattr to avoid static export issues
        cfg = getattr(genai, "configure", None)
        if callable(cfg):
            cfg(api_key=api_key)  # type: ignore[misc]
        ModelCls = getattr(genai, "GenerativeModel", None)
        if ModelCls is None:
            return extract_keywords_fallback(text, top_k)
        model = ModelCls('gemini-2.0-flash-exp')
        
        prompt = f"""
        è«‹å¾ä»¥ä¸‹æ³•å¾‹æ¢æ–‡å…§å®¹ä¸­æå–{top_k}å€‹æœ€é‡è¦çš„é—œéµè©ã€‚
        é—œéµè©æ‡‰è©²æ˜¯æ³•å¾‹è¡“èªã€é‡è¦æ¦‚å¿µæˆ–æ ¸å¿ƒå…§å®¹ã€‚
        è«‹åªè¿”å›é—œéµè©ï¼Œç”¨é€—è™Ÿåˆ†éš”ï¼Œä¸è¦å…¶ä»–è§£é‡‹ã€‚
        
        æ¢æ–‡å…§å®¹ï¼š
        {text}
        """
        
        response = model.generate_content(prompt)
        keywords_text = response.text.strip()
        
        # è§£æé—œéµè©
        keywords = [kw.strip() for kw in keywords_text.split(',') if kw.strip()]
        return keywords[:top_k]
        
    except Exception as e:
        print(f"Geminié—œéµè©æå–å¤±æ•—: {e}")
        return extract_keywords_fallback(text, top_k)


def extract_keywords_fallback(text: str, top_k: int = 5) -> List[str]:
    """å‚™ç”¨é—œéµè©æå–æ–¹æ³•"""
    if jieba is None:
        # å¦‚æœjiebaä¸å¯ç”¨ï¼Œä½¿ç”¨ç°¡å–®çš„æ­£å‰‡è¡¨é”å¼
        words = re.findall(r'[\u4e00-\u9fff]+', text)
        return list(set(words))[:top_k]
    
    try:
        # ä½¿ç”¨jiebaæå–é—œéµè©ï¼›éƒ¨åˆ†ç‰ˆæœ¬å‹åˆ¥ç‚º List[Tuple[str, float]] | List[str]
        from typing import cast, List as _List
        kws = jieba.analyse.extract_tags(text, topK=top_k, withWeight=False)  # type: ignore[call-arg]
        keywords = cast(_List[str], list(kws))
        return keywords[:top_k] if keywords else []
    except:
        # å¦‚æœjiebaå¤±æ•—ï¼Œä½¿ç”¨ç°¡å–®çš„æ­£å‰‡è¡¨é”å¼
        words = re.findall(r'[\u4e00-\u9fff]+', text)
        return list(set(words))[:top_k]


def extract_keywords(text: str, top_k: int = 5) -> List[str]:
    """æå–é—œéµè© - å„ªå…ˆä½¿ç”¨Geminiï¼Œå‚™ç”¨jieba"""
    return extract_keywords_with_gemini(text, top_k)


def extract_cross_references(text: str) -> List[str]:
    """æå–äº¤å‰å¼•ç”¨"""
    references = []
    
    # åŒ¹é…ã€Œç¬¬Xæ¢ã€
    article_refs = re.findall(r'ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒ0-9]+æ¢', text)
    references.extend(article_refs)
    
    # åŒ¹é…ã€Œç¬¬Xé …ã€
    item_refs = re.findall(r'ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒ0-9]+é …', text)
    references.extend(item_refs)
    
    # åŒ¹é…ã€Œç¬¬Xæ¬¾ã€
    clause_refs = re.findall(r'ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒ0-9]+æ¬¾', text)
    references.extend(clause_refs)
    
    # åŒ¹é…ã€Œå‰é …ã€ã€Œå‰æ¢ã€ã€Œæº–ç”¨ä¹‹ã€ç­‰
    if re.search(r'å‰é …|å‰æ¢|æº–ç”¨ä¹‹|ä¾.*è¦å®š|æ¯”ç…§.*è¾¦ç†|é©ç”¨.*è¦å®š', text):
        references.append("internal_reference")
    
    # åŒ¹é…ã€Œæœ¬æ³•ã€ã€Œæœ¬æ¢ä¾‹ã€ç­‰è‡ªå¼•ç”¨
    if re.search(r'æœ¬æ³•|æœ¬æ¢ä¾‹|æœ¬è¦å‰‡|æœ¬è¾¦æ³•', text):
        references.append("self_reference")
    
    # åŒ¹é…ã€Œå…¶ä»–æ³•å¾‹ã€ã€Œç›¸é—œæ³•è¦ã€ç­‰å¤–éƒ¨å¼•ç”¨
    if re.search(r'å…¶ä»–æ³•å¾‹|ç›¸é—œæ³•è¦|å…¶ä»–æ³•è¦|å…¶ä»–æ¢ä¾‹', text):
        references.append("external_reference")
    
    return list(set(references))


def preprocess_text(text: str) -> List[str]:
    """
    æ–‡æœ¬é è™•ç†ï¼šåˆ†è©ã€å»åœç”¨è©ã€æ¸…ç†
    """
    if not text:
        return []
    
    # ä½¿ç”¨jiebaåˆ†è©
    if jieba:
        words = jieba.lcut(text)
    else:
        # ç°¡å–®çš„å­—ç¬¦ç´šåˆ†è©ä½œç‚ºå‚™é¸
        words = list(text)
    
    # ä¸­æ–‡åœç”¨è©åˆ—è¡¨ï¼ˆæ³•å¾‹æ–‡æª”å°ˆç”¨ï¼Œè¼ƒå°‘éæ¿¾ï¼‰
    stop_words = {
        'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'éƒ½', 'ä¸€', 'ä¸Š', 'ä¹Ÿ', 'å¾ˆ', 'åˆ°', 'è¦', 'å»', 'æœƒ', 'è‘—', 'æ²’æœ‰', 'çœ‹', 'å¥½', 'è‡ªå·±', 'é€™', 'é‚£', 'å®ƒ', 'ä»–', 'å¥¹', 'æˆ‘å€‘', 'ä½ å€‘', 'ä»–å€‘', 'å¥¹å€‘', 'å®ƒå€‘', 'ä»€éº¼', 'æ€éº¼', 'ç‚ºä»€éº¼', 'å“ªè£¡', 'ä»€éº¼æ™‚å€™', 'å¤šå°‘', 'å¹¾å€‹', 'ä¸€äº›', 'æ‰€æœ‰', 'æ¯å€‹', 'ä»»ä½•', 'å¦‚æœ', 'å› ç‚º', 'æ‰€ä»¥', 'ä½†æ˜¯', 'ç„¶å¾Œ', 'æˆ–è€…', 'è€Œä¸”', 'é›–ç„¶', 'ä¸é', 'åªæ˜¯', 'å°±æ˜¯', 'é‚„æ˜¯', 'å·²ç¶“', 'æ­£åœ¨', 'å°‡è¦', 'å¯ä»¥', 'æ‡‰è©²', 'å¿…é ˆ', 'éœ€è¦', 'æƒ³è¦', 'å¸Œæœ›', 'å–œæ­¡', 'ä¸å–œæ­¡', 'çŸ¥é“', 'ä¸çŸ¥é“', 'æ˜ç™½', 'ä¸æ˜ç™½', 'è¨˜å¾—', 'å¿˜è¨˜', 'é–‹å§‹', 'çµæŸ', 'ç¹¼çºŒ', 'åœæ­¢', 'å®Œæˆ', 'åš', 'åšé', 'æ­£åœ¨åš', 'å°‡è¦åš', 'è¢«', 'æŠŠ', 'çµ¦', 'å°', 'å‘', 'å¾', 'åˆ°', 'åœ¨', 'æ–¼', 'ç‚º', 'ä»¥', 'ç”¨', 'é€šé', 'æ ¹æ“š', 'æŒ‰ç…§', 'ä¾ç…§', 'é—œæ–¼', 'å°æ–¼', 'è‡³æ–¼', 'é™¤äº†', 'åŒ…æ‹¬', 'ä»¥åŠ', 'èˆ‡', 'æˆ–', 'ä½†', 'ç„¶è€Œ', 'å› æ­¤', 'æ–¼æ˜¯', 'ç„¶å¾Œ', 'æ¥è‘—', 'æœ€å¾Œ', 'é¦–å…ˆ', 'å…¶æ¬¡', 'å†æ¬¡', 'å¦å¤–', 'æ­¤å¤–', 'ä¸¦ä¸”', 'åŒæ™‚', 'ä¸€èµ·', 'åˆ†åˆ¥', 'å„è‡ª', 'å…±åŒ', 'å–®ç¨', 'ç¨ç«‹', 'ç›¸é—œ', 'ç„¡é—œ', 'é‡è¦', 'ä¸é‡è¦', 'ä¸»è¦', 'æ¬¡è¦', 'åŸºæœ¬', 'æ ¹æœ¬', 'æ ¸å¿ƒ', 'é—œéµ', 'å¿…è¦', 'ä¸å¿…è¦', 'å¯èƒ½', 'ä¸å¯èƒ½', 'ä¸€å®š', 'ä¸ä¸€å®š', 'è‚¯å®š', 'ä¸è‚¯å®š', 'ç¢ºå®š', 'ä¸ç¢ºå®š', 'æ¸…æ¥š', 'ä¸æ¸…æ¥š', 'æ˜ç¢º', 'ä¸æ˜ç¢º', 'å…·é«”', 'ä¸å…·é«”', 'è©³ç´°', 'ä¸è©³ç´°', 'ç°¡å–®', 'è¤‡é›œ', 'å®¹æ˜“', 'å›°é›£', 'æ–¹ä¾¿', 'ä¸æ–¹ä¾¿', 'å¿«é€Ÿ', 'æ…¢é€Ÿ', 'é«˜æ•ˆ', 'ä½æ•ˆ', 'æœ‰æ•ˆ', 'ç„¡æ•ˆ', 'æˆåŠŸ', 'å¤±æ•—', 'æ­£ç¢º', 'éŒ¯èª¤', 'å°', 'éŒ¯', 'å¥½', 'å£', 'å„ª', 'åŠ£', 'é«˜', 'ä½', 'å¤§', 'å°', 'å¤š', 'å°‘', 'é•·', 'çŸ­', 'å¯¬', 'çª„', 'åš', 'è–„', 'æ·±', 'æ·º', 'æ–°', 'èˆŠ', 'å¹´è¼•', 'è€', 'æ—©', 'æ™š', 'å¿«', 'æ…¢', 'ç†±', 'å†·', 'æš–', 'æ¶¼', 'ä¹¾', 'æ¿•', 'äº®', 'æš—', 'æ˜', 'æ¸…', 'æ¿', 'éœ', 'å‹•', 'å®‰', 'å±', 'å¹³', 'é™¡', 'ç›´', 'å½', 'åœ“', 'æ–¹', 'å°–', 'éˆ', 'è»Ÿ', 'ç¡¬', 'è¼•', 'é‡', 'å¼·', 'å¼±', 'ç·Š', 'é¬†', 'å¯†', 'ç–', 'æ»¿', 'ç©º', 'å¯¦', 'è™›', 'çœŸ', 'å‡', 'æ­£', 'è² ', 'åŠ ', 'æ¸›', 'ä¹˜', 'é™¤', 'ç­‰æ–¼', 'ä¸ç­‰æ–¼', 'å¤§æ–¼', 'å°æ–¼', 'å¤§æ–¼ç­‰æ–¼', 'å°æ–¼ç­‰æ–¼', 'å’Œ', 'å·®', 'ç©', 'å•†', 'é¤˜', 'å€', 'åˆ†', 'æ¯”', 'ç‡', 'æ¯”ä¾‹', 'ç™¾åˆ†', 'åƒåˆ†', 'è¬åˆ†', 'å„„åˆ†', 'å…†åˆ†', 'äº¬åˆ†', 'å“åˆ†', 'ç§­åˆ†', 'ç©°åˆ†', 'æºåˆ†', 'æ¾—åˆ†', 'æ­£åˆ†', 'è¼‰åˆ†', 'æ¥µåˆ†', 'æ†æ²³æ²™åˆ†', 'é˜¿åƒ§ç¥‡åˆ†', 'é‚£ç”±ä»–åˆ†', 'ä¸å¯æ€è­°åˆ†', 'ç„¡é‡å¤§æ•¸åˆ†'
    }
    
    # éæ¿¾åœç”¨è©å’ŒçŸ­è©
    filtered_words = []
    for word in words:
        word = word.strip()
        if len(word) > 1 and word not in stop_words and not word.isdigit():
            filtered_words.append(word)
    
    return filtered_words


def calculate_tfidf_importance(texts: List[str], target_text: str) -> float:
    """
    ä½¿ç”¨TF-IDFè¨ˆç®—æ–‡æœ¬é‡è¦æ€§
    """
    if not texts or not target_text:
        return 1.0
    
    try:
        # é è™•ç†æ‰€æœ‰æ–‡æœ¬
        processed_texts = [' '.join(preprocess_text(text)) for text in texts]
        processed_target = ' '.join(preprocess_text(target_text))
        
        if not processed_target:
            return 1.0
        
        # è¨ˆç®—TF-IDF
        vectorizer = TfidfVectorizer(
            max_features=1000,
            ngram_range=(1, 2),
            min_df=1,
            max_df=0.95
        )
        
        # æ“¬åˆæ‰€æœ‰æ–‡æœ¬
        all_texts = processed_texts + [processed_target]
        tfidf_matrix = vectorizer.fit_transform(all_texts)
        
        # ç²å–ç›®æ¨™æ–‡æœ¬çš„TF-IDFå‘é‡ï¼ˆé¿å…ç¨€ç–çŸ©é™£çš„åˆ‡ç‰‡è­¦å‘Šï¼‰
        target_vector = tfidf_matrix.getrow(tfidf_matrix.shape[0] - 1)
        
        # è¨ˆç®—èˆ‡å…¶ä»–æ–‡æœ¬çš„å¹³å‡ç›¸ä¼¼åº¦
        similarities = cosine_similarity(target_vector, tfidf_matrix[:-1])
        avg_similarity = similarities.mean()
        
        # è¨ˆç®—TF-IDFåˆ†æ•¸ï¼ˆè©é »-é€†æ–‡æª”é »ç‡ï¼‰
        tfidf_scores = target_vector.toarray()[0]
        tfidf_sum = tfidf_scores.sum()
        
        # ç¶œåˆè©•åˆ†ï¼šTF-IDFåˆ†æ•¸ + ç›¸ä¼¼åº¦
        importance = (tfidf_sum * 0.7 + avg_similarity * 0.3) * 10
        
        # æ¨™æº–åŒ–åˆ°1-5ç¯„åœ
        importance = max(0.1, min(5.0, importance))
        
        return round(importance, 2)
        
    except Exception as e:
        print(f"TF-IDFè¨ˆç®—éŒ¯èª¤: {e}")
        return 1.0


def calculate_bm25_importance(texts: List[str], target_text: str) -> float:
    """
    ä½¿ç”¨BM25è¨ˆç®—æ–‡æœ¬é‡è¦æ€§
    """
    if not texts or not target_text or not BM25_AVAILABLE:
        return 1.0
    
    try:
        # é è™•ç†æ‰€æœ‰æ–‡æœ¬
        processed_texts = [preprocess_text(text) for text in texts]
        processed_target = preprocess_text(target_text)
        
        if not processed_target:
            return 1.0
        
        # åˆå§‹åŒ–BM25
        bm25 = BM25Okapi(processed_texts)
        
        # è¨ˆç®—BM25åˆ†æ•¸
        scores = bm25.get_scores(processed_target)
        
        if len(scores) == 0:
            return 1.0
        
        # è¨ˆç®—å¹³å‡åˆ†æ•¸
        avg_score = scores.mean()
        
        # è¨ˆç®—æœ€é«˜åˆ†æ•¸
        max_score = scores.max()
        
        # ç¶œåˆè©•åˆ†ï¼šå¹³å‡åˆ†æ•¸ + æœ€é«˜åˆ†æ•¸
        importance = (avg_score * 0.6 + max_score * 0.4) * 2
        
        # æ¨™æº–åŒ–åˆ°1-5ç¯„åœ
        importance = max(0.1, min(5.0, importance))
        
        return round(importance, 2)
        
    except Exception as e:
        print(f"BM25è¨ˆç®—éŒ¯èª¤: {e}")
        return 1.0


def calculate_importance(chapter: str, section: str, article: str, content: str = "", all_articles: List[Dict] = None) -> float:
    """
    è¨ˆç®—é‡è¦æ€§æ¬Šé‡ - ä½¿ç”¨TF-IDFå’ŒBM25å‹•æ…‹è¨ˆç®—
    
    åƒæ•¸:
    - chapter: ç« ç¯€åç¨±
    - section: ç¯€åç¨±  
    - article: æ¢æ–‡åç¨±
    - content: æ¢æ–‡å…§å®¹
    - all_articles: æ‰€æœ‰æ¢æ–‡åˆ—è¡¨ï¼Œç”¨æ–¼è¨ˆç®—ç›¸å°é‡è¦æ€§
    """
    # åŸºç¤æ¬Šé‡
    base_weight = 1.0
    
    # å¦‚æœæ²’æœ‰å…§å®¹æˆ–æ‰€æœ‰æ¢æ–‡ï¼Œä½¿ç”¨éœæ…‹æ¬Šé‡
    if not content or not all_articles:
        return calculate_static_importance(chapter, section, article)
    
    try:
        # æº–å‚™æ‰€æœ‰æ¢æ–‡çš„æ–‡æœ¬
        all_texts = []
        for art in all_articles:
            text = f"{art.get('article', '')} {art.get('content', '')}"
            if text.strip():
                all_texts.append(text)
        
        if len(all_texts) < 2:
            return calculate_static_importance(chapter, section, article)
        
        # ç›®æ¨™æ–‡æœ¬
        target_text = f"{article} {content}"
        
        # è¨ˆç®—TF-IDFé‡è¦æ€§
        tfidf_importance = calculate_tfidf_importance(all_texts, target_text)
        
        # è¨ˆç®—BM25é‡è¦æ€§
        bm25_importance = calculate_bm25_importance(all_texts, target_text)
        
        # ç¶œåˆè©•åˆ†ï¼šTF-IDF 60% + BM25 40%
        dynamic_weight = tfidf_importance * 0.6 + bm25_importance * 0.4
        
        # çµåˆéœæ…‹æ¬Šé‡ï¼ˆ30%ï¼‰å’Œå‹•æ…‹æ¬Šé‡ï¼ˆ70%ï¼‰
        final_weight = base_weight * 0.3 + dynamic_weight * 0.7
        
        return round(final_weight, 2)
        
    except Exception as e:
        print(f"å‹•æ…‹é‡è¦æ€§è¨ˆç®—éŒ¯èª¤: {e}")
        return calculate_static_importance(chapter, section, article)


def calculate_static_importance(chapter: str, section: str, article: str) -> float:
    """
    éœæ…‹é‡è¦æ€§æ¬Šé‡è¨ˆç®—ï¼ˆå‚™ç”¨æ–¹æ³•ï¼‰
    """
    weight = 1.0
    
    # ç¸½å‰‡ç« ç¯€æ¬Šé‡æ›´é«˜ (åŸºç¤æ€§æ¢æ–‡)
    if "ç¸½å‰‡" in chapter or "ç¬¬ä¸€ç« " in chapter or "é€šå‰‡" in chapter:
        weight *= 1.5
    
    # å®šç¾©æ€§æ¢æ–‡æ¬Šé‡æ›´é«˜ (æ ¸å¿ƒæ¦‚å¿µ)
    if "å®šç¾©" in article or "ç”¨è©" in article or "é‡‹ç¾©" in article:
        weight *= 1.3
    
    # ç½°å‰‡ç« ç¯€æ¬Šé‡è¼ƒé«˜ (æ³•å¾‹å¾Œæœ)
    if "ç½°å‰‡" in chapter or "ç½°" in chapter or "è™•ç½°" in chapter:
        weight *= 1.2
    
    # æ–½è¡Œç´°å‰‡æ¬Šé‡è¼ƒä½ (ç¨‹åºæ€§æ¢æ–‡)
    if "æ–½è¡Œ" in chapter or "ç¨‹åº" in chapter or "æµç¨‹" in chapter:
        weight *= 0.8
    
    # é™„å‰‡æ¬Šé‡æœ€ä½ (è£œå……æ€§æ¢æ–‡)
    if "é™„å‰‡" in chapter or "é™„" in chapter:
        weight *= 0.7
    
    # é€šå‰‡ç¯€æ¬Šé‡è¼ƒé«˜
    if "é€šå‰‡" in section:
        weight *= 1.2
    
    return round(weight, 2)




def extract_spans_with_pdfplumber(pdf_file, text_content: str, full_text: str = "") -> List[Dict[str, Any]]:
    """ä½¿ç”¨pdfplumberæå–æ–‡å­—ç‰‡æ®µç¯„åœ"""
    spans = []
    
    try:
        # é‡ç½®æ–‡ä»¶æŒ‡é‡
        pdf_file.seek(0)
        
        with pdfplumber.open(pdf_file) as pdf:
            # é¦–å…ˆåœ¨æ•´å€‹æ–‡æª”ä¸­æŸ¥æ‰¾å…§å®¹
            all_text = ""
            page_texts = []
            
            for page_num, page in enumerate(pdf.pages, 1):
                page_text = page.extract_text() or ""
                page_texts.append(page_text)
                all_text += page_text + "\n"
            
            # åœ¨å®Œæ•´æ–‡æœ¬ä¸­æŸ¥æ‰¾å…§å®¹
            if text_content.strip():
                # æ¸…ç†æ–‡æœ¬å…§å®¹ï¼Œå»é™¤å¤šé¤˜ç©ºç™½
                clean_content = re.sub(r'\s+', ' ', text_content.strip())
                clean_all_text = re.sub(r'\s+', ' ', all_text)
                
                start_idx = clean_all_text.find(clean_content)
                if start_idx != -1:
                    end_idx = start_idx + len(clean_content)
                    
                    # è¨ˆç®—åœ¨å“ªå€‹é é¢
                    page_num = 1
                    current_pos = 0
                    for i, page_text in enumerate(page_texts):
                        clean_page_text = re.sub(r'\s+', ' ', page_text)
                        page_len = len(clean_page_text)
                        
                        if current_pos <= start_idx < current_pos + page_len:
                            page_num = i + 1
                            # è¨ˆç®—åœ¨è©²é é¢å…§çš„ç›¸å°ä½ç½®
                            page_start = start_idx - current_pos
                            page_end = page_start + len(clean_content)
                            
                            spans.append({
                                "start_char": start_idx,
                                "end_char": end_idx,
                                "page_start_char": page_start,
                                "page_end_char": page_end,
                                "text": clean_content[:100] + "..." if len(clean_content) > 100 else clean_content,
                                "page": page_num,
                                "confidence": 1.0
                            })
                            break
                        current_pos += page_len + 1  # +1 for newline
                
                # å¦‚æœæ²’æ‰¾åˆ°å®Œæ•´åŒ¹é…ï¼Œå˜—è©¦éƒ¨åˆ†åŒ¹é…
                if not spans and len(clean_content) > 10:
                    # å˜—è©¦åŒ¹é…å‰20å€‹å­—ç¬¦
                    partial_content = clean_content[:20]
                    start_idx = clean_all_text.find(partial_content)
                    if start_idx != -1:
                        end_idx = start_idx + len(clean_content)
                        
                        # è¨ˆç®—é é¢ä½ç½®
                        page_num = 1
                        current_pos = 0
                        for i, page_text in enumerate(page_texts):
                            clean_page_text = re.sub(r'\s+', ' ', page_text)
                            page_len = len(clean_page_text)
                            
                            if current_pos <= start_idx < current_pos + page_len:
                                page_num = i + 1
                                page_start = start_idx - current_pos
                                page_end = page_start + len(clean_content)
                                
                                spans.append({
                                    "start_char": start_idx,
                                    "end_char": end_idx,
                                    "page_start_char": page_start,
                                    "page_end_char": page_end,
                                    "text": clean_content[:100] + "..." if len(clean_content) > 100 else clean_content,
                                    "page": page_num,
                                    "confidence": 0.8,
                                    "note": "partial_match"
                                })
                                break
                            current_pos += page_len + 1
            
            # å¦‚æœé‚„æ˜¯æ²’æ‰¾åˆ°ï¼Œä½¿ç”¨é—œéµè©åŒ¹é…
            if not spans and text_content.strip():
                keywords = re.findall(r'[\u4e00-\u9fff]+', text_content)
                if keywords:
                    # æ‰¾åˆ°åŒ…å«æœ€å¤šé—œéµè©çš„é é¢
                    best_page = 1
                    best_score = 0
                    
                    for page_num, page_text in enumerate(page_texts, 1):
                        clean_page_text = re.sub(r'\s+', ' ', page_text)
                        score = sum(1 for keyword in keywords if keyword in clean_page_text)
                        if score > best_score:
                            best_score = score
                            best_page = page_num
                    
                    if best_score > 0:
                        spans.append({
                            "start_char": 0,
                            "end_char": len(text_content),
                            "page_start_char": 0,
                            "page_end_char": len(text_content),
                            "text": text_content[:100] + "..." if len(text_content) > 100 else text_content,
                            "page": best_page,
                            "confidence": 0.5,
                            "note": "keyword_match",
                            "matched_keywords": [kw for kw in keywords if kw in page_texts[best_page-1]]
                        })
                        
    except Exception as e:
        print(f"Error extracting spans: {e}")
    
    return spans


def get_text_position_in_document(full_text: str, target_text: str) -> Dict[str, Any]:
    """ç²å–æ–‡æœ¬åœ¨æ–‡æª”ä¸­çš„ä½ç½®ä¿¡æ¯"""
    if not target_text.strip():
        return {"start": 0, "end": 0, "found": False}
    
    # æ¸…ç†æ–‡æœ¬å…§å®¹
    clean_target = re.sub(r'\s+', ' ', target_text.strip())
    clean_full = re.sub(r'\s+', ' ', full_text)
    
    start_idx = clean_full.find(clean_target)
    if start_idx != -1:
        return {
            "start": start_idx,
            "end": start_idx + len(clean_target),
            "found": True,
            "confidence": 1.0
        }
    
    # å˜—è©¦éƒ¨åˆ†åŒ¹é…
    if len(clean_target) > 10:
        partial = clean_target[:15]
        start_idx = clean_full.find(partial)
        if start_idx != -1:
            return {
                "start": start_idx,
                "end": start_idx + len(clean_target),
                "found": True,
                "confidence": 0.7,
                "note": "partial_match"
            }
    
    return {"start": 0, "end": 0, "found": False, "confidence": 0.0}


def get_page_range_for_text(pdf_file, target_text: str) -> Dict[str, int]:
    """ç²å–æ–‡æœ¬åœ¨PDFä¸­çš„é ç¢¼ç¯„åœ"""
    try:
        # é‡ç½®æ–‡ä»¶æŒ‡é’ˆ
        pdf_file.seek(0)
        
        with pdfplumber.open(pdf_file) as pdf:
            start_page = None
            end_page = None
            
            for page_num, page in enumerate(pdf.pages, 1):
                page_text = page.extract_text() or ""
                clean_page_text = re.sub(r'\s+', ' ', page_text)
                clean_target = re.sub(r'\s+', ' ', target_text.strip())
                
                if clean_target in clean_page_text:
                    if start_page is None:
                        start_page = page_num
                    end_page = page_num
                elif len(clean_target) > 10:
                    # å˜—è©¦éƒ¨åˆ†åŒ¹é…
                    partial = clean_target[:20]
                    if partial in clean_page_text:
                        if start_page is None:
                            start_page = page_num
                        end_page = page_num
            
            if start_page is not None:
                return {"start": start_page, "end": end_page or start_page}
            else:
                return {"start": 1, "end": 1}  # é»˜èªå€¼
                
    except Exception as e:
        print(f"Error getting page range: {e}")
        return {"start": 1, "end": 1}  # é»˜è®¤å€¼




@app.get("/health")
def health():
    return {"ok": True}


@app.post("/upload")
async def upload(file: UploadFile = File(...)):
    content = await file.read()
    doc_id = str(uuid.uuid4())
    
    # æª¢æŸ¥æ–‡ä»¶é¡å‹ä¸¦ç›¸æ‡‰è™•ç†
    if file.filename and file.filename.lower().endswith('.pdf'):
        # è™•ç†PDFæ–‡ä»¶
        try:
            import io
            if pdfplumber:
                # ä½¿ç”¨pdfplumberè§£æPDF
                pdf_file = io.BytesIO(content)
                text = ""
                with pdfplumber.open(pdf_file) as pdf:
                    for page in pdf.pages:
                        page_text = page.extract_text()
                        if page_text:
                            text += page_text + "\n"
            else:
                # å‚™ç”¨æ–¹æ¡ˆï¼šä½¿ç”¨PyPDF2
                pdf_file = io.BytesIO(content)
                if PYPDF2_AVAILABLE:
                    pdf_reader = PdfReader(pdf_file)
                    text = ""
                    for page in pdf_reader.pages:
                        text += page.extract_text() + "\n"
                else:
                    # å¦‚æœæ²’æœ‰PDFè§£æåº«ï¼Œè¿”å›éŒ¯èª¤
                    return JSONResponse(
                        status_code=400, 
                        content={"error": "PDF parsing libraries not available. Please install pdfplumber or PyPDF2."}
                    )
        except Exception as e:
            return JSONResponse(
                status_code=400, 
                content={"error": f"Failed to parse PDF: {str(e)}"}
            )
    else:
        # è™•ç†æ–‡æœ¬æ–‡ä»¶
        try:
            text = content.decode("utf-8", errors="ignore")
        except Exception:
            text = str(content)
    
    # æ¸…ç†æ–‡æœ¬
    text = text.strip()
    if not text:
        return JSONResponse(
            status_code=400, 
            content={"error": "No text content found in the file"}
        )
    
    store.docs[doc_id] = DocRecord(
        id=doc_id,
        filename=file.filename,
        text=text,
        json_data=None,  # åˆå§‹ç‚ºNoneï¼Œå¾ŒçºŒé€šé/update-jsonç«¯é»æ›´æ–°
        chunks=[],
        chunk_size=0,
        overlap=0,
    )
    # When uploading new docs, prior embeddings are invalid
    store.reset_embeddings()
    return {"doc_id": doc_id, "filename": file.filename, "num_chars": len(text)}


@app.post("/api/update-json")
async def update_json(request: dict):
    """æ›´æ–°æ–‡æª”çš„JSONçµæ§‹åŒ–æ•¸æ“š"""
    doc_id = request.get("doc_id")
    json_data = request.get("json_data")
    
    if not doc_id or not json_data:
        return JSONResponse(
            status_code=400,
            content={"error": "doc_id and json_data are required"}
        )
    
    if doc_id not in store.docs:
        return JSONResponse(
            status_code=404,
            content={"error": "Document not found"}
        )
    
    # æ›´æ–°æ–‡æª”çš„JSONæ•¸æ“š
    store.docs[doc_id].json_data = json_data
    
    # é‡ç½®ç›¸é—œç‹€æ…‹ï¼Œå› ç‚ºJSONæ•¸æ“šæ”¹è®Šå¯èƒ½å½±éŸ¿chunking
    store.docs[doc_id].chunks = []
    store.docs[doc_id].chunk_size = 0
    store.docs[doc_id].overlap = 0
    store.reset_embeddings()
    
    return {"success": True, "message": "JSON data updated successfully"}


def sliding_window_chunks(text: str, chunk_size: int, overlap: int) -> List[str]:
    """å›ºå®šå¤§å°æ»‘å‹•çª—å£åˆ†å‰²"""
    if chunk_size <= 0:
        return [text]
    if overlap >= chunk_size:
        overlap = max(0, chunk_size - 1)
    chunks: List[str] = []
    start = 0
    n = len(text)
    while start < n:
        end = min(n, start + chunk_size)
        chunks.append(text[start:end])
        if end == n:
            break
        start = end - overlap
    return chunks


def hierarchical_chunks(text: str, max_chunk_size: int, min_chunk_size: int, overlap: int, level_depth: int) -> List[str]:
    """å±¤æ¬¡åŒ–åˆ†å‰²ç­–ç•¥"""
    if max_chunk_size <= 0:
        return [text]
    
    # é¦–å…ˆæŒ‰æ®µè½åˆ†å‰²
    paragraphs = text.split('\n\n')
    chunks = []
    
    for para in paragraphs:
        if len(para) <= max_chunk_size:
            chunks.append(para)
        else:
            # å¦‚æœæ®µè½å¤ªé•·ï¼ŒæŒ‰å¥å­åˆ†å‰²
            sentences = para.split('ã€‚')
            current_chunk = ""
            
            for sentence in sentences:
                if len(current_chunk + sentence) <= max_chunk_size:
                    current_chunk += sentence + "ã€‚"
                else:
                    if current_chunk and len(current_chunk) >= min_chunk_size:
                        chunks.append(current_chunk.strip())
                    current_chunk = sentence + "ã€‚"
            
            if current_chunk and len(current_chunk) >= min_chunk_size:
                chunks.append(current_chunk.strip())
    
    # æ‡‰ç”¨é‡ç–Š
    if overlap > 0:
        overlapped_chunks = []
        for i, chunk in enumerate(chunks):
            overlapped_chunks.append(chunk)
            if i < len(chunks) - 1 and len(chunk) > overlap:
                # æ·»åŠ é‡ç–Šéƒ¨åˆ†
                overlap_text = chunk[-overlap:]
                next_chunk = chunks[i + 1]
                if len(next_chunk) > overlap:
                    overlapped_chunks.append(overlap_text + next_chunk[overlap:])
        return overlapped_chunks
    
    return chunks


def adaptive_chunks(text: str, target_size: int, tolerance: int, overlap: int, semantic_threshold: float) -> List[str]:
    """è‡ªé©æ‡‰åˆ†å‰²ç­–ç•¥"""
    if target_size <= 0:
        return [text]
    
    chunks = []
    start = 0
    n = len(text)
    
    while start < n:
        # å˜—è©¦æ‰¾åˆ°æœ€ä½³åˆ†å‰²é»
        end = min(n, start + target_size)
        
        # å¦‚æœæ¥è¿‘ç›®æ¨™å¤§å°ï¼Œå°‹æ‰¾èªç¾©é‚Šç•Œ
        if end - start >= target_size - tolerance:
            # å°‹æ‰¾å¥è™Ÿã€æ®µè½ç­‰èªç¾©é‚Šç•Œ
            for i in range(end, max(start + target_size - tolerance, start), -1):
                if i < n and text[i] in ['ã€‚', '\n', 'ï¼', 'ï¼Ÿ']:
                    end = i + 1
                    break
        
        chunk = text[start:end].strip()
        if chunk:
            chunks.append(chunk)
        
        if end == n:
            break
        
        # è¨ˆç®—ä¸‹ä¸€å€‹chunkçš„èµ·å§‹ä½ç½®ï¼ˆè€ƒæ…®é‡ç–Šï¼‰
        start = max(start + 1, end - overlap)
    
    return chunks


def hybrid_chunks(text: str, primary_size: int, secondary_size: int, overlap: int, switch_threshold: float) -> List[str]:
    """æ··åˆåˆ†å‰²ç­–ç•¥"""
    if primary_size <= 0:
        return [text]
    
    chunks = []
    start = 0
    n = len(text)
    
    while start < n:
        # æ±ºå®šä½¿ç”¨ä¸»è¦å¤§å°é‚„æ˜¯æ¬¡è¦å¤§å°
        remaining_text = text[start:]
        avg_sentence_length = len(remaining_text) / max(1, remaining_text.count('ã€‚'))
        
        if avg_sentence_length > primary_size * switch_threshold:
            chunk_size = secondary_size
        else:
            chunk_size = primary_size
        
        end = min(n, start + chunk_size)
        chunk = text[start:end].strip()
        
        if chunk:
            chunks.append(chunk)
        
        if end == n:
            break
        
        start = max(start + 1, end - overlap)
    
    return chunks


def semantic_chunks(text: str, target_size: int, similarity_threshold: float, overlap: int, context_window: int) -> List[str]:
    """èªç¾©åˆ†å‰²ç­–ç•¥"""
    if target_size <= 0:
        return [text]
    
    # ç°¡åŒ–å¯¦ç¾ï¼šæŒ‰å¥å­åˆ†å‰²ï¼Œç„¶å¾Œåˆä½µç›¸ä¼¼çš„å¥å­
    sentences = text.split('ã€‚')
    chunks = []
    current_chunk = ""
    
    for sentence in sentences:
        if not sentence.strip():
            continue
            
        sentence = sentence.strip() + "ã€‚"
        
        # å¦‚æœç•¶å‰chunkåŠ ä¸Šæ–°å¥å­ä¸è¶…éç›®æ¨™å¤§å°
        if len(current_chunk + sentence) <= target_size:
            current_chunk += sentence
        else:
            # ä¿å­˜ç•¶å‰chunk
            if current_chunk:
                chunks.append(current_chunk.strip())
            
            # é–‹å§‹æ–°chunk
            current_chunk = sentence
    
    # æ·»åŠ æœ€å¾Œä¸€å€‹chunk
    if current_chunk:
        chunks.append(current_chunk.strip())
    
    return chunks


def json_structured_chunks(json_data: Dict[str, Any], chunk_size: int, overlap: int) -> List[Dict[str, Any]]:
    """
    åŸºæ–¼JSONçµæ§‹çš„æ™ºèƒ½åˆ†å‰²
    ä¿ç•™æ³•å¾‹æ–‡æª”çš„çµæ§‹åŒ–ä¿¡æ¯
    æ”¯æŒå–®ä¸€æ³•å¾‹æ–‡æª”å’Œå¤šæ³•å¾‹æ–‡æª”æ ¼å¼
    """
    if not json_data or chunk_size <= 0:
        return []
    
    chunks = []
    
    # æª¢æŸ¥æ˜¯å¦ç‚ºå¤šæ³•å¾‹æ–‡æª”æ ¼å¼
    if "laws" in json_data:
        # å¤šæ³•å¾‹æ–‡æª”æ ¼å¼
        laws = json_data.get("laws", [])
        for law in laws:
            law_chunks = process_single_law(law, chunk_size, overlap)
            chunks.extend(law_chunks)
    else:
        # å–®ä¸€æ³•å¾‹æ–‡æª”æ ¼å¼
        law_chunks = process_single_law(json_data, chunk_size, overlap)
        chunks.extend(law_chunks)
    
    return chunks


def process_single_law(law_data: Dict[str, Any], chunk_size: int, overlap: int) -> List[Dict[str, Any]]:
    """
    è™•ç†å–®ä¸€æ³•å¾‹æ–‡æª”
    """
    chunks = []
    law_name = law_data.get("law_name", "æœªå‘½åæ³•è¦")
    
    def create_chunk(content: str, metadata: Dict[str, Any], chunk_id: str) -> Dict[str, Any]:
        """å‰µå»ºåŒ…å«metadataçš„chunk"""
        return {
            "chunk_id": chunk_id,
            "content": content,
            "metadata": {
                "id": metadata.get("id", ""),
                "spans": metadata.get("spans", {}),
                "page_range": metadata.get("page_range", {})
            }
        }
    
    def process_article(article: Dict[str, Any], chapter: str, section: str) -> List[Dict[str, Any]]:
        """è™•ç†å–®å€‹æ¢æ–‡"""
        article_chunks = []
        article_title = article.get("article", "")
        article_content = article.get("content", "")
        items = article.get("items", [])
        
        # è™•ç†æ¢æ–‡ä¸»é«”
        if article_content:
            # å¦‚æœæ¢æ–‡å…§å®¹è¼ƒçŸ­ï¼Œç›´æ¥ä½œç‚ºä¸€å€‹chunk
            if len(article_content) <= chunk_size:
                metadata = {
                    "id": article.get("metadata", {}).get("id", ""),
                    "spans": article.get("metadata", {}).get("spans", {}),
                    "page_range": article.get("metadata", {}).get("page_range", {})
                }
                chunk_id = f"{article_title}_main"
                article_chunks.append(create_chunk(article_content, metadata, chunk_id))
            else:
                # æ¢æ–‡å…§å®¹è¼ƒé•·ï¼Œéœ€è¦åˆ†å‰²
                text_chunks = sliding_window_chunks(article_content, chunk_size, overlap)
                for i, chunk_text in enumerate(text_chunks):
                    metadata = {
                        "id": article.get("metadata", {}).get("id", ""),
                        "spans": article.get("metadata", {}).get("spans", {}),
                        "page_range": article.get("metadata", {}).get("page_range", {})
                    }
                    chunk_id = f"{article_title}_part_{i+1}"
                    article_chunks.append(create_chunk(chunk_text, metadata, chunk_id))
        
        # è™•ç†æ¢æ–‡é …ç›® - æ”¯æ´æ–°çµæ§‹ (paragraphs) å’ŒèˆŠçµæ§‹ (items)
        paragraphs = article.get("paragraphs", [])
        items = article.get("items", [])
        
        # ä½¿ç”¨ paragraphs å¦‚æœå­˜åœ¨ï¼Œå¦å‰‡ä½¿ç”¨ items
        items_to_process = paragraphs if paragraphs else items
        
        for item in items_to_process:
            # æ”¯æ´æ–°çµæ§‹çš„éµå
            item_title = item.get("paragraph", item.get("item", ""))
            item_content = item.get("content", "")
            
            # è™•ç†é …ç›®ä¸»é«”
            if item_content:
                if len(item_content) <= chunk_size:
                    metadata = {
                        "id": item.get("metadata", {}).get("id", ""),
                        "spans": item.get("metadata", {}).get("spans", {}),
                        "page_range": item.get("metadata", {}).get("page_range", {})
                    }
                    chunk_id = f"{article_title}_{item_title}_main"
                    article_chunks.append(create_chunk(item_content, metadata, chunk_id))
                else:
                    # é …ç›®å…§å®¹è¼ƒé•·ï¼Œéœ€è¦åˆ†å‰²
                    text_chunks = sliding_window_chunks(item_content, chunk_size, overlap)
                    for i, chunk_text in enumerate(text_chunks):
                        metadata = {
                            "id": item.get("metadata", {}).get("id", ""),
                            "spans": item.get("metadata", {}).get("spans", {}),
                            "page_range": item.get("metadata", {}).get("page_range", {})
                        }
                        chunk_id = f"{article_title}_{item_title}_part_{i+1}"
                        article_chunks.append(create_chunk(chunk_text, metadata, chunk_id))
            
            # è™•ç†å­é …ç›® - æ”¯æ´æ–°çµæ§‹ (subparagraphs) å’ŒèˆŠçµæ§‹ (sub_items)
            subparagraphs = item.get("subparagraphs", [])
            sub_items = item.get("sub_items", [])
            
            # ä½¿ç”¨ subparagraphs å¦‚æœå­˜åœ¨ï¼Œå¦å‰‡ä½¿ç”¨ sub_items
            sub_items_to_process = subparagraphs if subparagraphs else sub_items
            
            for sub_item in sub_items_to_process:
                # æ”¯æ´æ–°çµæ§‹çš„éµå
                sub_item_title = sub_item.get("subparagraph", sub_item.get("sub_item", ""))
                sub_item_content = sub_item.get("content", "")
                
                if sub_item_content:
                    if len(sub_item_content) <= chunk_size:
                        metadata = {
                            "id": sub_item.get("metadata", {}).get("id", ""),
                            "spans": sub_item.get("metadata", {}).get("spans", {}),
                            "page_range": sub_item.get("metadata", {}).get("page_range", {})
                        }
                        chunk_id = f"{article_title}_{item_title}_{sub_item_title}"
                        article_chunks.append(create_chunk(sub_item_content, metadata, chunk_id))
                    else:
                        # å­é …ç›®å…§å®¹è¼ƒé•·ï¼Œéœ€è¦åˆ†å‰²
                        text_chunks = sliding_window_chunks(sub_item_content, chunk_size, overlap)
                        for i, chunk_text in enumerate(text_chunks):
                            metadata = {
                                "id": sub_item.get("metadata", {}).get("id", ""),
                                "spans": sub_item.get("metadata", {}).get("spans", {}),
                                "page_range": sub_item.get("metadata", {}).get("page_range", {})
                            }
                            chunk_id = f"{article_title}_{item_title}_{sub_item_title}_part_{i+1}"
                            article_chunks.append(create_chunk(chunk_text, metadata, chunk_id))
                
                # è™•ç†ç¬¬ä¸‰å±¤é …ç›® (items)
                third_level_items = sub_item.get("items", [])
                for third_item in third_level_items:
                    third_item_title = third_item.get("item", "")
                    third_item_content = third_item.get("content", "")
                    
                    if third_item_content:
                        if len(third_item_content) <= chunk_size:
                            metadata = {
                                "id": third_item.get("metadata", {}).get("id", ""),
                                "spans": third_item.get("metadata", {}).get("spans", {}),
                                "page_range": third_item.get("metadata", {}).get("page_range", {})
                            }
                            chunk_id = f"{article_title}_{item_title}_{sub_item_title}_{third_item_title}"
                            article_chunks.append(create_chunk(third_item_content, metadata, chunk_id))
                        else:
                            # ç¬¬ä¸‰å±¤é …ç›®å…§å®¹è¼ƒé•·ï¼Œéœ€è¦åˆ†å‰²
                            text_chunks = sliding_window_chunks(third_item_content, chunk_size, overlap)
                            for i, chunk_text in enumerate(text_chunks):
                                metadata = {
                                    "id": third_item.get("metadata", {}).get("id", ""),
                                    "spans": third_item.get("metadata", {}).get("spans", {}),
                                    "page_range": third_item.get("metadata", {}).get("page_range", {})
                                }
                                chunk_id = f"{article_title}_{item_title}_{sub_item_title}_{third_item_title}_part_{i+1}"
                                article_chunks.append(create_chunk(chunk_text, metadata, chunk_id))
        
        return article_chunks
    
    # éæ­·æ‰€æœ‰ç« ç¯€
    chapters = law_data.get("chapters", [])
    for chapter in chapters:
        chapter_title = chapter.get("chapter", "")
        sections = chapter.get("sections", [])
        
        for section in sections:
            section_title = section.get("section", "")
            articles = section.get("articles", [])
            
            for article in articles:
                article_chunks = process_article(article, chapter_title, section_title)
                chunks.extend(article_chunks)
    
    return chunks


# è©•æ¸¬ç›¸é—œå‡½æ•¸
def calculate_precision_at_k(retrieved_chunks: List[str], query: str, k: int) -> float:
    """
    è¨ˆç®—Precision@K - æª¢ç´¢å‡ºä¾†çš„tokensä¸­ï¼Œæœ‰å¤šå°‘æ˜¯çœŸæ­£ç›¸é—œçš„
    """
    if not retrieved_chunks or k <= 0:
        return 0.0
    
    # å–å‰kå€‹çµæœ
    top_k_chunks = retrieved_chunks[:k]
    
    # æ”¹é€²çš„é—œéµè©åŒ¹é…æ–¹æ³• - ä½¿ç”¨å­—ç¬¦ç´šåŒ¹é…
    query_chars = set(query.replace(' ', '').replace('ï¼Ÿ', '').replace('ï¼', '').replace('ï¼Œ', '').replace('ã€‚', ''))
    if not query_chars:
        return 0.0
    
    relevant_count = 0
    for chunk in top_k_chunks:
        chunk_chars = set(chunk.replace(' ', '').replace('ï¼Œ', '').replace('ã€‚', '').replace('ï¼›', '').replace('ï¼š', ''))
        # å¦‚æœæŸ¥è©¢ä¸­çš„å­—ç¬¦æœ‰50%ä»¥ä¸Šå‡ºç¾åœ¨chunkä¸­ï¼Œèªç‚ºç›¸é—œ
        overlap_chars = query_chars & chunk_chars
        if len(overlap_chars) >= len(query_chars) * 0.5:
            relevant_count += 1
    
    return relevant_count / len(top_k_chunks)


def calculate_precision_omega(retrieved_chunks: List[str], query: str) -> float:
    """
    è¨ˆç®—PrecisionÎ© - å‡è¨­Recallæ˜¯æ»¿åˆ†ï¼Œæœ€å¤§çš„æº–ç¢ºç‡æ˜¯å¤šå°‘
    """
    if not retrieved_chunks:
        return 0.0
    
    # æ”¹é€²çš„é—œéµè©åŒ¹é…æ–¹æ³• - ä½¿ç”¨å­—ç¬¦ç´šåŒ¹é…
    query_chars = set(query.replace(' ', '').replace('ï¼Ÿ', '').replace('ï¼', '').replace('ï¼Œ', '').replace('ã€‚', ''))
    if not query_chars:
        return 0.0
    
    relevant_count = 0
    for chunk in retrieved_chunks:
        chunk_chars = set(chunk.replace(' ', '').replace('ï¼Œ', '').replace('ã€‚', '').replace('ï¼›', '').replace('ï¼š', ''))
        # å¦‚æœæŸ¥è©¢ä¸­çš„å­—ç¬¦æœ‰30%ä»¥ä¸Šå‡ºç¾åœ¨chunkä¸­ï¼Œèªç‚ºç›¸é—œ
        overlap_chars = query_chars & chunk_chars
        if len(overlap_chars) >= len(query_chars) * 0.3:
            relevant_count += 1
    
    return relevant_count / len(retrieved_chunks)


def calculate_recall_at_k(retrieved_chunks: List[str], query: str, k: int, 
                         ground_truth_chunks: List[str] = None) -> float:
    """
    è¨ˆç®—Recall@K - åœ¨å‰Kå€‹æª¢ç´¢çµæœä¸­å‘½ä¸­ç›¸é—œchunkçš„æ¯”ä¾‹
    """
    if not retrieved_chunks or k <= 0:
        return 0.0
    
    # å–å‰kå€‹çµæœ
    top_k_chunks = retrieved_chunks[:k]
    
    # å¦‚æœæ²’æœ‰ground truthï¼Œä½¿ç”¨é—œéµè©åŒ¹é…ä½œç‚ºè¿‘ä¼¼
    if ground_truth_chunks is None:
        # æ”¹é€²çš„é—œéµè©åŒ¹é…æ–¹æ³• - ä½¿ç”¨å­—ç¬¦ç´šåŒ¹é…
        query_chars = set(query.replace(' ', '').replace('ï¼Ÿ', '').replace('ï¼', '').replace('ï¼Œ', '').replace('ã€‚', ''))
        if not query_chars:
            return 0.0
        
        # é¦–å…ˆè¨ˆç®—ç¸½ç›¸é—œæ–‡æª”æ•¸é‡ï¼ˆéœ€è¦å¾æ‰€æœ‰chunksä¸­è¨ˆç®—ï¼Œä¸åªæ˜¯top_kï¼‰
        # ä½†ç”±æ–¼æˆ‘å€‘æ²’æœ‰è¨ªå•æ‰€æœ‰chunksï¼Œæˆ‘å€‘éœ€è¦ä¸€å€‹è¿‘ä¼¼æ–¹æ³•
        # é€™è£¡æˆ‘å€‘å‡è¨­ç¸½ç›¸é—œæ–‡æª”æ•¸é‡ç­‰æ–¼æª¢ç´¢åˆ°çš„ç›¸é—œæ–‡æª”æ•¸é‡ï¼ˆé€™æ˜¯ä¸€å€‹è¿‘ä¼¼ï¼‰
        retrieved_relevant_count = 0
        for chunk in top_k_chunks:
            chunk_chars = set(chunk.replace(' ', '').replace('ï¼Œ', '').replace('ã€‚', '').replace('ï¼›', '').replace('ï¼š', ''))
            # å¦‚æœæŸ¥è©¢ä¸­çš„å­—ç¬¦æœ‰50%ä»¥ä¸Šå‡ºç¾åœ¨chunkä¸­ï¼Œèªç‚ºç›¸é—œ
            overlap_chars = query_chars & chunk_chars
            if len(overlap_chars) >= len(query_chars) * 0.5:
                retrieved_relevant_count += 1
        
        # ç”±æ–¼ç„¡æ³•æº–ç¢ºè¨ˆç®—ç¸½ç›¸é—œæ–‡æª”æ•¸é‡ï¼Œæˆ‘å€‘ä½¿ç”¨ä¸€å€‹ä¿å®ˆçš„ä¼°è¨ˆ
        # å‡è¨­ç¸½ç›¸é—œæ–‡æª”æ•¸é‡è‡³å°‘ç­‰æ–¼æª¢ç´¢åˆ°çš„ç›¸é—œæ–‡æª”æ•¸é‡
        total_relevant_estimate = max(retrieved_relevant_count, 1)
        
        return retrieved_relevant_count / total_relevant_estimate
    
    # ä½¿ç”¨ground truthè¨ˆç®— - é€™è£¡ground_truth_chunkså¯¦éš›ä¸Šæ˜¯æ‰€æœ‰chunks
    # é¦–å…ˆè¨ˆç®—æ‰€æœ‰chunksä¸­ç›¸é—œçš„æ•¸é‡
    query_chars = set(query.replace(' ', '').replace('ï¼Ÿ', '').replace('ï¼', '').replace('ï¼Œ', '').replace('ã€‚', ''))
    if not query_chars:
        return 0.0
    
    total_relevant_count = 0
    for chunk in ground_truth_chunks:
        chunk_chars = set(chunk.replace(' ', '').replace('ï¼Œ', '').replace('ã€‚', '').replace('ï¼›', '').replace('ï¼š', ''))
        overlap_chars = query_chars & chunk_chars
        if len(overlap_chars) >= len(query_chars) * 0.3:
            total_relevant_count += 1
    
    # è¨ˆç®—æª¢ç´¢åˆ°çš„ç›¸é—œchunksæ•¸é‡
    retrieved_relevant_count = 0
    for chunk in top_k_chunks:
        chunk_chars = set(chunk.replace(' ', '').replace('ï¼Œ', '').replace('ã€‚', '').replace('ï¼›', '').replace('ï¼š', ''))
        overlap_chars = query_chars & chunk_chars
        if len(overlap_chars) >= len(query_chars) * 0.3:
            retrieved_relevant_count += 1
    
    return retrieved_relevant_count / total_relevant_count if total_relevant_count > 0 else 0


def calculate_faithfulness(chunks: List[str]) -> float:
    """
    è¨ˆç®—å¿ å¯¦åº¦ - è©•ä¼°chunkæ˜¯å¦ä¿æŒå®Œæ•´èªç¾©
    åŸºæ–¼å¥å­å®Œæ•´æ€§ã€æ®µè½é‚Šç•Œç­‰
    """
    if not chunks:
        return 0.0
    
    total_score = 0.0
    
    for chunk in chunks:
        score = 1.0
        
        # æª¢æŸ¥å¥å­å®Œæ•´æ€§
        sentences = re.split(r'[ã€‚ï¼ï¼Ÿ]', chunk)
        incomplete_sentences = sum(1 for s in sentences if s.strip() and not s.endswith(('ã€‚', 'ï¼', 'ï¼Ÿ')))
        if len(sentences) > 1:
            score *= (1.0 - incomplete_sentences / len(sentences))
        
        # æª¢æŸ¥æ®µè½å®Œæ•´æ€§
        if chunk.startswith(('ç¬¬', 'æ¢', 'é …', 'æ¬¾')) and not chunk.endswith(('ã€‚', 'ï¼', 'ï¼Ÿ')):
            score *= 0.8
        
        total_score += score
    
    return total_score / len(chunks)


def calculate_fragmentation_score(chunks: List[str], original_text: str) -> float:
    """
    è¨ˆç®—ç¢ç‰‡åŒ–ç¨‹åº¦ - è©•ä¼°æ–‡æœ¬è¢«åˆ†å‰²çš„ç´°ç¢ç¨‹åº¦
    è¿”å›å€¼è¶Šé«˜è¡¨ç¤ºç¢ç‰‡åŒ–è¶Šåš´é‡
    """
    if not chunks or not original_text:
        return 0.0
    
    # è¨ˆç®—å¹³å‡chunké•·åº¦ç›¸å°æ–¼åŸæ–‡çš„æ¯”ä¾‹
    avg_chunk_length = sum(len(chunk) for chunk in chunks) / len(chunks)
    length_ratio = avg_chunk_length / len(original_text)
    
    # è¨ˆç®—chunkæ•¸é‡
    chunk_count_ratio = len(chunks) / (len(original_text) / 500)  # ä»¥500å­—ç¬¦ç‚ºåŸºæº–
    
    # ç¶œåˆè©•åˆ†
    fragmentation = (1.0 - length_ratio) * 0.6 + chunk_count_ratio * 0.4
    
    return min(1.0, max(0.0, fragmentation))


def generate_questions_with_gemini(text_content: str, num_questions: int, 
                                 question_types: List[str], difficulty_levels: List[str]) -> List[GeneratedQuestion]:
    """
    ä½¿ç”¨Geminiç”Ÿæˆç¹é«”ä¸­æ–‡æ³•å¾‹è€ƒå¤é¡Œ
    åƒè€ƒihoweræ–‡ç« çš„åšæ³•ï¼Œå¾æ–‡æœ¬ä¸­éš¨æ©Ÿé¸æ“‡å…§å®¹ç”Ÿæˆå•é¡Œ
    """
    if not GEMINI_AVAILABLE:
        return generate_questions_fallback(text_content, num_questions)
    
    try:
        # å„ªå…ˆä½¿ç”¨ GOOGLE_API_KEYï¼Œå¦‚æœæ²’æœ‰å‰‡ä½¿ç”¨ GEMINI_API_KEY
        api_key = GOOGLE_API_KEY or os.getenv('GEMINI_API_KEY')
        if not api_key:
            print("è­¦å‘Šï¼šGOOGLE_API_KEY å’Œ GEMINI_API_KEY éƒ½æœªè¨­ç½®ï¼Œä½¿ç”¨å‚™ç”¨æ–¹æ³•")
            return generate_questions_fallback(text_content, num_questions)
        
        cfg = getattr(genai, "configure", None)
        if callable(cfg):
            cfg(api_key=api_key)  # type: ignore[misc]
        ModelCls = getattr(genai, "GenerativeModel", None)
        if ModelCls is None:
            print("è­¦å‘Šï¼šç„¡æ³•ç²å– GenerativeModel é¡ï¼Œä½¿ç”¨å‚™ç”¨æ–¹æ³•")
            return generate_questions_fallback(text_content, num_questions)
        model = ModelCls('gemini-2.0-flash-exp')
        
        # å¾æ–‡æœ¬ä¸­éš¨æ©Ÿé¸æ“‡4000 tokensçš„å…§å®¹ï¼ˆæ¨¡æ“¬ihowerçš„åšæ³•ï¼‰
        import random
        text_chunks = text_content.split('\n')
        random.shuffle(text_chunks)
        
        # é¸æ“‡è¶³å¤ çš„å…§å®¹ä¾†ç”Ÿæˆå•é¡Œ
        selected_content = ""
        current_tokens = 0
        max_tokens = 4000
        
        for chunk in text_chunks:
            if current_tokens + len(chunk) > max_tokens:
                break
            selected_content += chunk + "\n"
            current_tokens += len(chunk)
        
        if not selected_content.strip():
            selected_content = text_content[:2000]  # å‚™ç”¨æ–¹æ¡ˆ
        
        prompt = f"""
ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„æ³•å¾‹æ•™è‚²å°ˆå®¶ï¼Œè«‹æ ¹æ“šä»¥ä¸‹æ³•å¾‹æ–‡æœ¬å…§å®¹ï¼Œç”Ÿæˆ{num_questions}é“ç¹é«”ä¸­æ–‡è€ƒå¤é¡Œã€‚

é‡è¦è¦æ±‚ï¼š
1. æ‰€æœ‰å•é¡Œå¿…é ˆä½¿ç”¨ç¹é«”ä¸­æ–‡ï¼ˆå°ç£ç”¨æ³•ï¼‰
2. å•é¡Œé¡å‹æ‡‰åŒ…å«ï¼š{', '.join(question_types)}ï¼Œéš¨æ©Ÿåˆ†é…ä½†ç¢ºä¿å¤šæ¨£æ€§
3. é›£åº¦ç­‰ç´šæ‡‰åŒ…å«ï¼š{', '.join(difficulty_levels)}ï¼Œéš¨æ©Ÿåˆ†é…ï¼ŒåŸºç¤å•é¡Œèšç„¦å–®ä¸€æ¦‚å¿µï¼Œé€²éšå•é¡Œæ¶‰åŠå¤šæ¦‚å¿µï¼Œæ‡‰ç”¨å•é¡Œæ¨¡æ“¬å¯¦å‹™å ´æ™¯
4. æ¯é“é¡Œç›®éƒ½è¦æ¨™æ˜ç›¸é—œçš„æ³•è¦æ¢æ–‡
5. å•é¡Œæ‡‰è©²åŸºæ–¼æ–‡æœ¬ä¸­çš„å…·é«”å…§å®¹ï¼Œä¸æ˜¯æ³›æ³›è€Œè«‡
6. å•é¡Œæ‡‰è©²æœ‰æ˜ç¢ºçš„ç­”æ¡ˆï¼Œå¯ä»¥åœ¨æ–‡æœ¬ä¸­æ‰¾åˆ°ä¾æ“š

æ ¸å¿ƒè¨­è¨ˆåŸå‰‡ï¼š
7. é‡é»ï¼šé¿å…ç´”ç²¹çš„æ¢æ–‡èƒŒèª¦é¡Œç›®ï¼Œæ”¹ç‚ºå¯¦éš›ç”Ÿæ´»æ¡ˆä¾‹æ‡‰ç”¨é¡Œ
8. å•é¡Œæ‡‰è©²è¨­è¨ˆæˆæƒ…å¢ƒå¼æ¡ˆä¾‹ï¼Œè®“å­¸ç”Ÿæ€è€ƒå¦‚ä½•åœ¨å¯¦éš›ç”Ÿæ´»ä¸­æ‡‰ç”¨æ³•å¾‹æ¦‚å¿µ
9. ä½¿ç”¨ã€Œå¦‚æœ...é‚£éº¼...ã€æˆ–ã€Œç•¶...æ™‚...ã€çš„æƒ…å¢ƒè¨­å®š
10. æä¾›å…·é«”çš„ç”Ÿæ´»å ´æ™¯ï¼ˆå¦‚ï¼šç¶²è·¯ä½¿ç”¨ã€å‰µä½œåˆ†äº«ã€å•†æ¥­æ´»å‹•ç­‰ï¼‰
11. è©¢å•ã€Œæ‡‰è©²å¦‚ä½•è™•ç†ã€ã€ã€Œæ˜¯å¦ç¬¦åˆæ³•å¾‹è¦å®šã€ã€ã€Œæœƒç”¢ç”Ÿä»€éº¼å¾Œæœã€ç­‰
12. é¿å…ç›´æ¥å•ã€Œç¬¬Xæ¢è¦å®šä»€éº¼ã€é€™é¡èƒŒèª¦é¡Œ

æ–‡æœ¬å…§å®¹ï¼š
{selected_content}

è«‹ä»¥JSONæ ¼å¼è¿”å›çµæœï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
{{
  "questions": [
    {{
      "question": "å•é¡Œå…§å®¹",
      "references": ["ç¬¬Xæ¢", "ç¬¬Yæ¢ç¬¬Zé …"],
      "question_type": "æ¡ˆä¾‹æ‡‰ç”¨/æƒ…å¢ƒåˆ†æ/å¯¦å‹™è™•ç†/æ³•å¾‹å¾Œæœ/åˆè¦åˆ¤æ–·",
      "difficulty": "åŸºç¤/é€²éš/æ‡‰ç”¨",
      "keywords": ["é—œéµè©1", "é—œéµè©2"],
      "estimated_tokens": ä¼°ç®—çš„tokenæ•¸é‡
    }}
  ]
}}

è«‹ç¢ºä¿ç”Ÿæˆçš„å•é¡Œéƒ½æ˜¯å¯¦éš›ç”Ÿæ´»æ¡ˆä¾‹æ‡‰ç”¨é¡Œï¼Œé¿å…æ¢æ–‡èƒŒèª¦ï¼Œè®“å­¸ç”Ÿèƒ½å¤ æ€è€ƒå¦‚ä½•åœ¨çœŸå¯¦æƒ…å¢ƒä¸­æ‡‰ç”¨æ³•å¾‹çŸ¥è­˜ã€‚
"""
        
        response = model.generate_content(prompt)
        response_text = response.text.strip()
        
        # è§£æJSONéŸ¿æ‡‰
        try:
            # æ¸…ç†éŸ¿æ‡‰æ–‡æœ¬ï¼Œç§»é™¤å¯èƒ½çš„markdownæ ¼å¼
            if response_text.startswith('```json'):
                response_text = response_text[7:]
            if response_text.endswith('```'):
                response_text = response_text[:-3]
            
            import json
            result = json.loads(response_text)
            
            questions = []
            for q_data in result.get('questions', []):
                question = GeneratedQuestion(
                    question=q_data.get('question', ''),
                    references=q_data.get('references', []),
                    question_type=q_data.get('question_type', ''),
                    difficulty=q_data.get('difficulty', ''),
                    keywords=q_data.get('keywords', []),
                    estimated_tokens=q_data.get('estimated_tokens', 0)
                )
                questions.append(question)
            
            return questions[:num_questions]  # ç¢ºä¿ä¸è¶…éè«‹æ±‚æ•¸é‡
            
        except json.JSONDecodeError as e:
            print(f"JSONè§£æéŒ¯èª¤: {e}")
            print(f"éŸ¿æ‡‰å…§å®¹: {response_text[:500]}...")  # åªé¡¯ç¤ºå‰500å­—ç¬¦
            return generate_questions_fallback(text_content, num_questions)
        
    except Exception as e:
        print(f"Geminiå•é¡Œç”Ÿæˆå¤±æ•—: {e}")
        return generate_questions_fallback(text_content, num_questions)


def generate_questions_fallback(text_content: str, num_questions: int) -> List[GeneratedQuestion]:
    """
    å‚™ç”¨å•é¡Œç”Ÿæˆæ–¹æ³•
    """
    questions = []
    
    # ç°¡å–®çš„æ­£å‰‡è¡¨é”å¼æå–æ³•æ¢
    import re
    articles = re.findall(r'ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒ0-9]+æ¢[^ã€‚]*ã€‚', text_content)
    
    print(f"å‚™ç”¨æ–¹æ³•ï¼šå¾æ–‡æœ¬ä¸­æ‰¾åˆ° {len(articles)} å€‹æ³•æ¢")
    
    # ç”ŸæˆåŸºç¤å•é¡Œ
    question_templates = [
        ("{article}çš„å®šç¾©æ˜¯ä»€éº¼ï¼Ÿ", "å®šç¾©", "åŸºç¤"),
        ("{article}çš„é©ç”¨æ¢ä»¶ç‚ºä½•ï¼Ÿ", "æ¢ä»¶", "åŸºç¤"),
        ("é•å{article}çš„æ³•å¾‹å¾Œæœæ˜¯ä»€éº¼ï¼Ÿ", "å¾Œæœ", "é€²éš"),
        ("{article}çš„ç”³è«‹ç¨‹åºç‚ºä½•ï¼Ÿ", "ç¨‹åº", "é€²éš"),
        ("{article}çš„ä¿è­·æœŸé™æ˜¯å¤šä¹…ï¼Ÿ", "æœŸé™", "åŸºç¤"),
    ]
    
    if articles:
        # å¦‚æœæœ‰æ³•æ¢ï¼ŒåŸºæ–¼æ³•æ¢ç”Ÿæˆå•é¡Œ
        for i in range(min(num_questions, len(articles))):
            article = articles[i % len(articles)]
            template, q_type, difficulty = question_templates[i % len(question_templates)]
            
            # æå–æ¢æ–‡è™Ÿç¢¼
            article_match = re.search(r'ç¬¬([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒ0-9]+)æ¢', article)
            article_num = article_match.group(1) if article_match else str(i+1)
            
            question = GeneratedQuestion(
                question=template.format(article=f"ç¬¬{article_num}æ¢"),
                references=[f"ç¬¬{article_num}æ¢"],
                question_type=q_type,
                difficulty=difficulty,
                keywords=extract_keywords(article, 3),
                estimated_tokens=len(article) + 50
            )
            questions.append(question)
    else:
        # å¦‚æœæ²’æœ‰æ‰¾åˆ°æ³•æ¢ï¼Œç”Ÿæˆé€šç”¨å•é¡Œ
        print("è­¦å‘Šï¼šæ²’æœ‰æ‰¾åˆ°æ³•æ¢ï¼Œç”Ÿæˆé€šç”¨å•é¡Œ")
        generic_questions = [
            "è«‹èªªæ˜æœ¬æ³•å¾‹æ–‡æª”çš„ä¸»è¦å…§å®¹å’Œç›®çš„ï¼Ÿ",
            "æœ¬æ³•å¾‹æ–‡æª”é©ç”¨æ–¼å“ªäº›æƒ…æ³ï¼Ÿ",
            "é•åæœ¬æ³•å¾‹è¦å®šæœƒç”¢ç”Ÿä»€éº¼å¾Œæœï¼Ÿ",
            "å¦‚ä½•ç”³è«‹æœ¬æ³•å¾‹è¦å®šçš„ç›¸é—œæ¬Šåˆ©ï¼Ÿ",
            "æœ¬æ³•å¾‹è¦å®šçš„ä¿è­·æœŸé™æ˜¯å¤šä¹…ï¼Ÿ"
        ]
        
        for i in range(min(num_questions, len(generic_questions))):
            question = GeneratedQuestion(
                question=generic_questions[i],
                references=["ç›¸é—œæ³•æ¢"],
                question_type="åŸºç¤æ¦‚å¿µ",
                difficulty="åŸºç¤",
                keywords=extract_keywords(text_content[:200], 3),
                estimated_tokens=100
            )
            questions.append(question)
    
    print(f"å‚™ç”¨æ–¹æ³•ç”Ÿæˆäº† {len(questions)} å€‹å•é¡Œ")
    return questions


def evaluate_chunk_config(doc: DocRecord, config: ChunkConfig, 
                         test_queries: List[str], k_values: List[int], 
                         strategy: str = "fixed_size") -> EvaluationResult:
    """
    è©•ä¼°å–®å€‹chunké…ç½®
    """
    # æ ¹æ“šç­–ç•¥ç”Ÿæˆchunksï¼Œå‚³éç­–ç•¥ç‰¹å®šåƒæ•¸
    if strategy == "fixed_size":
        chunks = sliding_window_chunks(doc.text, config.chunk_size, config.overlap)
    elif strategy == "hierarchical":
        from .chunking import chunk_text
        chunks = chunk_text(doc.text, strategy="hierarchical", 
                           max_chunk_size=config.chunk_size, 
                           overlap_ratio=config.overlap_ratio,
                           min_chunk_size=config.min_chunk_size,
                           level_depth=config.level_depth)
    elif strategy == "rcts_hierarchical":
        from .chunking import chunk_text
        chunks = chunk_text(doc.text, strategy="rcts_hierarchical", 
                           max_chunk_size=config.chunk_size, 
                           overlap_ratio=config.overlap_ratio,
                           preserve_structure=config.preserve_structure)
    elif strategy == "structured_hierarchical":
        from .chunking import chunk_text
        chunks = chunk_text(doc.text, strategy="structured_hierarchical", 
                           json_data=doc.json_data, 
                           max_chunk_size=config.chunk_size, 
                           overlap_ratio=config.overlap_ratio,
                           chunk_by=config.chunk_by)
    elif strategy == "semantic":
        from .chunking import chunk_text
        chunks = chunk_text(doc.text, strategy="semantic", 
                           max_chunk_size=config.chunk_size, 
                           similarity_threshold=config.similarity_threshold,
                           context_window=config.context_window,
                           overlap_ratio=config.overlap_ratio)
    elif strategy == "sliding_window":
        from .chunking import chunk_text
        chunks = chunk_text(doc.text, strategy="sliding_window", 
                           window_size=config.window_size, 
                           step_size=config.step_size,
                           overlap_ratio=config.overlap_ratio,
                           boundary_aware=config.boundary_aware,
                           min_chunk_size_sw=config.min_chunk_size_sw,
                           max_chunk_size_sw=config.max_chunk_size_sw,
                           preserve_sentences=config.preserve_sentences)
    elif strategy == "llm_semantic":
        from .chunking import chunk_text
        chunks = chunk_text(doc.text, strategy="llm_semantic", 
                           max_chunk_size=config.chunk_size, 
                           semantic_threshold=config.semantic_threshold,
                           context_window=config.context_window,
                           overlap_ratio=config.overlap_ratio)
    elif strategy == "hybrid":
        from .chunking import chunk_text
        chunks = chunk_text(doc.text, strategy="hybrid", 
                           primary_size=config.chunk_size, 
                           secondary_size=config.secondary_size,
                           switch_threshold=config.switch_threshold,
                           overlap_ratio=config.overlap_ratio)
    else:
        # é»˜èªä½¿ç”¨å›ºå®šå¤§å°åˆ†å¡Š
        chunks = sliding_window_chunks(doc.text, config.chunk_size, config.overlap)

    # è¨ˆç®—åŸºæœ¬çµ±è¨ˆ
    chunk_count = len(chunks)
    avg_chunk_length = sum(len(c) for c in chunks) / chunk_count if chunk_count else 0.0
    lengths = [len(c) for c in chunks]
    length_variance = (
        sum((l - avg_chunk_length) ** 2 for l in lengths) / chunk_count if chunk_count else 0.0
    )

    # ä½¿ç”¨TF-IDFç‚ºæ¯å€‹æŸ¥è©¢åšæª¢ç´¢æ‰“åˆ†ï¼ˆä¸­æ–‡ç”¨è‡ªå®šç¾©åˆ†è©ï¼‰
    def to_tokens(s: str) -> str:
        toks = preprocess_text(s)
        return " ".join(toks) if toks else s

    processed_chunks = [to_tokens(c) for c in chunks]
    # è‹¥æ–‡æª”éçŸ­ï¼Œé¿å…vectorizerå ±éŒ¯
    if not processed_chunks:
        processed_chunks = [""]

    vectorizer = TfidfVectorizer(
        analyzer="word",
        token_pattern=r"[^\s]+",
        max_features=5000,
        min_df=1,
        max_df=0.98,
        ngram_range=(1, 2),
    )
    X = vectorizer.fit_transform(processed_chunks)

    retrieval_results: Dict[str, List[Dict]] = {}
    precision_at_k_scores: Dict[int, List[float]] = {k: [] for k in k_values}
    recall_at_k_scores: Dict[int, List[float]] = {k: [] for k in k_values}
    precision_omega_scores: List[float] = []

    def compute_pr(retrieved_indices: List[int], relevant_set: set[int], k: int) -> Tuple[float, float]:
        if k <= 0:
            return 0.0, 0.0
        topk = retrieved_indices[:k]
        hit = sum(1 for i in topk if i in relevant_set)
        precision = hit / k
        recall = hit / max(1, len(relevant_set))
        return precision, recall

    max_k = max(k_values) if k_values else 10

    for query in test_queries:
        q = to_tokens(query)
        if not q.strip():
            q = query or ""

        q_vec = vectorizer.transform([q])
        # ä½™å¼¦ç›¸ä¼¼åº¦
        sims = cosine_similarity(q_vec, X).ravel()
        ranked_idx = sims.argsort()[::-1].tolist()

        # å®šç¾©ç›¸é—œé›†ï¼šåˆ†æ•¸é”åˆ°æœ€ä½³åˆ†æ•¸çš„æŸä¸€æ¯”ä¾‹é–¾å€¼ï¼ˆä¾‹å¦‚0.7ï¼‰ä¸”>0
        best = float(sims[ranked_idx[0]]) if ranked_idx else 0.0
        threshold = best * 0.7 if best > 0 else 0.0
        relevant_set = {i for i, s in enumerate(sims) if s >= threshold and s > 0}
        # é˜²æ­¢ç©ºé›†åˆå°è‡´recallç„¡æ„ç¾©ï¼Œè‹¥å…¨éƒ¨ç‚º0åˆ†ï¼Œå‰‡èªç‚ºæ²’æœ‰ç›¸é—œæ–‡æª”
        # è‹¥åªæœ‰æ¥µå°‘æ•¸éé›¶ï¼Œè‡³å°‘ä¿ç•™top1ç‚ºç›¸é—œ
        if best > 0 and not relevant_set:
            relevant_set = {ranked_idx[0]}

        # ä¿å­˜å‰max_kå€‹æª¢ç´¢çµæœä¾›å¯©æŸ¥
        retrieval_results[query] = [
            {
                "chunk_index": i,
                "score": float(sims[i]),
                "content": (chunks[i][:200] + "...") if len(chunks[i]) > 200 else chunks[i],
            }
            for i in ranked_idx[:max_k]
        ]

        # æŒ‡æ¨™è¨ˆç®—
        for k in k_values:
            p, r = compute_pr(ranked_idx, relevant_set, k)
            precision_at_k_scores[k].append(p)
            recall_at_k_scores[k].append(r)

        # PrecisionÎ©: ç†æƒ³æƒ…æ³ä¸‹ï¼ˆæœ€å„ªæ’åºï¼‰åœ¨k=max_kæ™‚å¯é”åˆ°çš„ç²¾åº¦
        # = min(|R|, max_k) / max_k
        precision_omega_scores.append(
            min(len(relevant_set), max_k) / max_k if max_k > 0 else 0.0
        )

    # èšåˆå¹³å‡
    avg_precision_omega = sum(precision_omega_scores) / len(precision_omega_scores) if precision_omega_scores else 0.0
    avg_precision_at_k = {k: (sum(v) / len(v) if v else 0.0) for k, v in precision_at_k_scores.items()}
    avg_recall_at_k = {k: (sum(v) / len(v) if v else 0.0) for k, v in recall_at_k_scores.items()}

    metrics = EvaluationMetrics(
        precision_omega=avg_precision_omega,
        precision_at_k=avg_precision_at_k,
        recall_at_k=avg_recall_at_k,
        chunk_count=chunk_count,
        avg_chunk_length=avg_chunk_length,
        length_variance=length_variance,
    )

    # å‰µå»ºè©³ç´°çš„é…ç½®ä¿¡æ¯ï¼ŒåŒ…å«æ‰€æœ‰ç­–ç•¥ç‰¹å®šåƒæ•¸
    detailed_config = {
        "chunk_size": config.chunk_size,
        "overlap": config.overlap,
        "overlap_ratio": config.overlap_ratio,
        "strategy": strategy,
    }
    
    # æ ¹æ“šç­–ç•¥æ·»åŠ ç‰¹å®šåƒæ•¸
    if strategy == "structured_hierarchical":
        detailed_config["chunk_by"] = config.chunk_by
    elif strategy == "rcts_hierarchical":
        detailed_config["preserve_structure"] = config.preserve_structure
    elif strategy == "hierarchical":
        detailed_config["level_depth"] = config.level_depth
        detailed_config["min_chunk_size"] = config.min_chunk_size
    elif strategy == "semantic":
        detailed_config["similarity_threshold"] = config.similarity_threshold
        detailed_config["context_window"] = config.context_window
    elif strategy == "llm_semantic":
        detailed_config["semantic_threshold"] = config.semantic_threshold
        detailed_config["context_window"] = config.context_window
    elif strategy == "sliding_window":
        detailed_config["window_size"] = config.window_size
        detailed_config["step_size"] = config.step_size
        detailed_config["boundary_aware"] = config.boundary_aware
        detailed_config["preserve_sentences"] = config.preserve_sentences
        detailed_config["min_chunk_size_sw"] = config.min_chunk_size_sw
        detailed_config["max_chunk_size_sw"] = config.max_chunk_size_sw
    elif strategy == "hybrid":
        detailed_config["switch_threshold"] = config.switch_threshold
        detailed_config["secondary_size"] = config.secondary_size

    return EvaluationResult(
        config=detailed_config,
        metrics=metrics,
        test_queries=test_queries,
        retrieval_results=retrieval_results,
        timestamp=datetime.now(),
    )


@app.post("/api/chunk")
def chunk(req: ChunkRequest):
    doc = store.docs.get(req.doc_id)
    if not doc:
        return JSONResponse(status_code=404, content={"error": "doc not found"})
    
    # å°å…¥æ–°çš„chunkingæ¨¡çµ„
    from .chunking import chunk_text
    
    # æ ¹æ“šä¸åŒç­–ç•¥é€²è¡Œåˆ†å¡Š
    strategy = req.strategy
    use_json_structure = req.use_json_structure
    
    # å¦‚æœå•Ÿç”¨JSONçµæ§‹åŒ–åˆ†å‰²ä¸”æœ‰JSONæ•¸æ“šï¼Œå„ªå…ˆä½¿ç”¨JSONçµæ§‹åŒ–åˆ†å‰²
    if use_json_structure and doc.json_data:
        structured_chunks = json_structured_chunks(doc.json_data, req.chunk_size, req.overlap)
        # æå–ç´”æ–‡æœ¬chunksç”¨æ–¼å¾ŒçºŒè™•ç†
        chunks = [chunk["content"] for chunk in structured_chunks]
        # å­˜å„²çµæ§‹åŒ–chunksåˆ°æ–‡æª”ä¸­
        doc.structured_chunks = structured_chunks
    else:
        # ä½¿ç”¨æ–°çš„chunkingæ¨¡çµ„
        chunk_kwargs = {
            "chunk_size": req.chunk_size,
            "overlap": req.overlap,
        }
        
        # æ ¹æ“šç­–ç•¥æ·»åŠ ç‰¹å®šåƒæ•¸
        if strategy == 'hierarchical' and req.hierarchical_params:
            chunk_kwargs.update({
                "max_chunk_size": req.chunk_size,
                "min_chunk_size": req.hierarchical_params.get('min_chunk_size', req.chunk_size // 2),
                "overlap_ratio": req.overlap / req.chunk_size if req.chunk_size > 0 else 0.1,
                "level_depth": req.hierarchical_params.get('level_depth', 2)
            })
        elif strategy == 'rcts_hierarchical' and req.rcts_hierarchical_params:
            chunk_kwargs.update({
                "max_chunk_size": req.chunk_size,
                "overlap_ratio": req.rcts_hierarchical_params.get('overlap_ratio', 0.1),
                "preserve_structure": req.rcts_hierarchical_params.get('preserve_structure', True)
            })
        elif strategy == 'structured_hierarchical' and req.structured_hierarchical_params:
            chunk_kwargs.update({
                "max_chunk_size": req.chunk_size,
                "overlap_ratio": req.structured_hierarchical_params.get('overlap_ratio', 0.1),
                "chunk_by": req.structured_hierarchical_params.get('chunk_by', 'article')
            })
        elif strategy == 'adaptive' and req.adaptive_params:
            chunk_kwargs.update({
                "target_size": req.chunk_size,
                "tolerance": req.adaptive_params.get('tolerance', req.chunk_size // 10),
                "semantic_threshold": req.adaptive_params.get('semantic_threshold', 0.7)
            })
        elif strategy == 'hybrid' and req.hybrid_params:
            chunk_kwargs.update({
                "primary_size": req.chunk_size,
                "secondary_size": req.hybrid_params.get('secondary_size', req.chunk_size // 2),
                "switch_threshold": req.hybrid_params.get('switch_threshold', 0.8)
            })
        elif strategy == 'semantic' and req.semantic_params:
            chunk_kwargs.update({
                "target_size": req.chunk_size,
                "similarity_threshold": req.semantic_params.get('similarity_threshold', 0.6),
                "context_window": req.semantic_params.get('context_window', 100)
            })
        
        # ä½¿ç”¨æ–°çš„chunkingæ¨¡çµ„
        chunks = chunk_text(doc.text, strategy=strategy, json_data=doc.json_data, **chunk_kwargs)
        
        # æ¸…ç©ºçµæ§‹åŒ–chunks
        doc.structured_chunks = []
    
    # è¨ˆç®—è©³ç´°æŒ‡æ¨™
    chunk_lengths = [len(chunk) for chunk in chunks]
    avg_length = sum(chunk_lengths) / len(chunk_lengths) if chunks else 0
    length_variance = 0
    if len(chunk_lengths) > 1:
        variance = sum((length - avg_length) ** 2 for length in chunk_lengths) / len(chunk_lengths)
        length_variance = variance / avg_length if avg_length > 0 else 0
    
    doc.chunks = chunks
    doc.chunk_size = req.chunk_size
    doc.overlap = req.overlap
    # invalidates embeddings for safety
    store.reset_embeddings()
    
    return {
        "doc_id": doc.id, 
        "num_chunks": len(chunks), 
        "chunk_size": req.chunk_size, 
        "overlap": req.overlap,
        "strategy": strategy,
        "sample": chunks[:3],  # å‰3å€‹chunksä½œç‚ºé è¦½
        "all_chunks": chunks,  # æ‰€æœ‰chunks
        "metrics": {
            "avg_length": round(avg_length, 2),
            "length_variance": round(length_variance, 3),
            "min_length": min(chunk_lengths) if chunks else 0,
            "max_length": max(chunk_lengths) if chunks else 0,
            "overlap_rate": req.overlap / req.chunk_size if req.chunk_size > 0 else 0
        }
    }


async def embed_gemini(texts: List[str]) -> List[List[float]]:
    """Call Google Generative API (Gemini) embeddings endpoint using API key.

    Note: This uses the REST endpoint pattern with the API key in query params.
    """
    if not httpx:
        raise RuntimeError("httpx not available")
    if not GOOGLE_API_KEY:
        raise RuntimeError("GOOGLE_API_KEY not set")
    # Gemini embedding model: gemini-embedding-001 (ç¶­åº¦å¯é…ç½®: 128-3072)
    model = "gemini-embedding-001"
    # ä½¿ç”¨æ­£ç¢ºçš„ API ç«¯é»æ ¼å¼
    url = f"https://generativelanguage.googleapis.com/v1beta/models/{model}:embedContent"
    headers = {
        "x-goog-api-key": GOOGLE_API_KEY,
        "Content-Type": "application/json"
    }
    out: List[List[float]] = []
    total_texts = len(texts)
    print(f"ğŸ”§ é–‹å§‹Gemini embeddingè™•ç†ï¼Œå…± {total_texts} å€‹æ–‡æœ¬")
    
    async with httpx.AsyncClient(timeout=60) as client:
        # é€å€‹è™•ç†æ–‡æœ¬ï¼ˆGemini API éœ€è¦å–®å€‹è«‹æ±‚ï¼‰
        for i, text in enumerate(texts):
            try:
                # æª¢æŸ¥æ–‡æœ¬é•·åº¦ï¼ŒGemini APIæœ‰é•·åº¦é™åˆ¶
                # Gemini embedding API æ”¯æŒæœ€å¤š 2048 tokensï¼Œç´„ 10000-20000 å­—ç¬¦ï¼ˆä¸­æ–‡ï¼‰
                MAX_CHARS = 20000
                original_length = len(text)
                if original_length > MAX_CHARS:
                    text = text[:MAX_CHARS]
                    print(f"âš ï¸ æ–‡æœ¬éé•·({original_length}å­—ç¬¦)ï¼Œå·²æˆªæ–·åˆ°{MAX_CHARS}å­—ç¬¦")
                
                payload = {
                    "model": f"models/{model}",
                    "content": {"parts": [{"text": text}]},
                    "output_dimensionality": EMBEDDING_DIMENSION  # ä½¿ç”¨å…¨å±€é…ç½®çš„ç¶­åº¦
                }
                r = await client.post(url, headers=headers, json=payload)
                
                if r.status_code == 400:
                    print(f"âŒ Gemini API 400éŒ¯èª¤ï¼Œæ–‡æœ¬å…§å®¹å¯èƒ½æœ‰å•é¡Œ: {text[:100]}...")
                    # ä½¿ç”¨éš¨æ©Ÿå‘é‡ä½œç‚ºfallback
                    import numpy as np
                    fallback_vector = np.random.randn(EMBEDDING_DIMENSION).astype(np.float32).tolist()
                    out.append(fallback_vector)
                    continue
                
                r.raise_for_status()
                data = r.json()
                
                # èª¿è©¦ï¼šæ‰“å°å®Œæ•´çš„APIéŸ¿æ‡‰çµæ§‹
                if i == 0:  # åªåœ¨ç¬¬ä¸€æ¬¡æ‰“å°
                    print(f"ğŸ“‹ Gemini APIéŸ¿æ‡‰çµæ§‹: {list(data.keys())}")
                    if "embedding" in data:
                        print(f"ğŸ“‹ Embeddingçµæ§‹: {list(data['embedding'].keys())}")
                
                # æ ¹æ“šå®˜æ–¹æ–‡æª”ï¼ŒéŸ¿æ‡‰æ ¼å¼æ˜¯ {"embedding": {"values": [...]}}
                embedding_values = data.get("embedding", {}).get("values", [])
                
                if not embedding_values:
                    print(f"âŒ ç²å–åˆ°çš„embeddingç‚ºç©ºï¼Œä½¿ç”¨fallbackå‘é‡")
                    print(f"âŒ å®Œæ•´éŸ¿æ‡‰: {data}")
                    import numpy as np
                    fallback_vector = np.random.randn(EMBEDDING_DIMENSION).astype(np.float32).tolist()
                    out.append(fallback_vector)
                else:
                    # èª¿è©¦ï¼šæ‰“å°å¯¦éš›è¿”å›çš„ç¶­åº¦
                    actual_dimension = len(embedding_values)
                    if i == 0:  # åªåœ¨ç¬¬ä¸€æ¬¡æ‰“å°
                        print(f"âœ… Geminiè¿”å›çš„å‘é‡ç¶­åº¦: {actual_dimension}")
                    if actual_dimension != EMBEDDING_DIMENSION:
                        print(f"âš ï¸ è­¦å‘Šï¼šGeminiè¿”å›çš„å‘é‡ç¶­åº¦ç‚º {actual_dimension}ï¼Œèˆ‡é…ç½®çš„{EMBEDDING_DIMENSION}ä¸åŒ")
                        print(f"âš ï¸ é€™å¯èƒ½æœƒå°è‡´èˆ‡ä¹‹å‰å­˜å„²çš„embeddingç¶­åº¦ä¸åŒ¹é…")
                    out.append(embedding_values)
                
                # é¡¯ç¤ºé€²åº¦
                progress = ((i + 1) / total_texts) * 100
                print(f"ğŸ“Š Gemini embeddingé€²åº¦: {i + 1}/{total_texts} ({progress:.1f}%)")
                
            except Exception as e:
                print(f"âŒ è™•ç†ç¬¬{i+1}å€‹æ–‡æœ¬æ™‚å‡ºéŒ¯: {e}")
                # ä½¿ç”¨éš¨æ©Ÿå‘é‡ä½œç‚ºfallback
                import numpy as np
                fallback_vector = np.random.randn(EMBEDDING_DIMENSION).astype(np.float32).tolist()
                out.append(fallback_vector)
                continue
    
    print(f"âœ… Gemini embeddingå®Œæˆï¼Œå…±è™•ç† {len(out)} å€‹å‘é‡")
    return out


def embed_bge_m3(texts: List[str]) -> List[List[float]]:
    """ä½¿ç”¨ BGE-M3 æ¨¡å‹é€²è¡Œ embedding"""
    if not SENTENCE_TRANSFORMERS_AVAILABLE:
        raise RuntimeError("sentence-transformers not available")
    
    try:
        total_texts = len(texts)
        print(f"ğŸ”§ é–‹å§‹BGE-M3 embeddingè™•ç†ï¼Œå…± {total_texts} å€‹æ–‡æœ¬")
        
        # è¼‰å…¥ BGE-M3 æ¨¡å‹
        model = SentenceTransformer('BAAI/bge-m3')
        
        # æ‰¹é‡è™•ç†æ–‡æœ¬
        embeddings = model.encode(texts, batch_size=32, show_progress_bar=True)
        
        # è½‰æ›ç‚ºåˆ—è¡¨æ ¼å¼
        result = embeddings.tolist()
        print(f"âœ… BGE-M3 embeddingå®Œæˆï¼Œå…±è™•ç† {len(result)} å€‹å‘é‡")
        return result
        
    except Exception as e:
        raise RuntimeError(f"BGE-M3 embedding failed: {e}")


@app.post("/api/embed")
async def embed(req: EmbedRequest):
    print(f"ğŸ” Embedå‡½æ•°è¢«è°ƒç”¨ï¼Œè¯·æ±‚: {req}")
    # gather chunks across selected docs
    selected = req.doc_ids or list(store.docs.keys())
    print(f"ğŸ” é€‰ä¸­çš„æ–‡æ¡£: {selected}")
    all_chunks: List[str] = []
    chunk_doc_ids: List[str] = []
    for d in selected:
        doc = store.docs.get(d)
        if doc and doc.chunks:
            all_chunks.extend(doc.chunks)
            chunk_doc_ids.extend([doc.id] * len(doc.chunks))

    if not all_chunks:
        return JSONResponse(status_code=400, content={"error": "no chunks to embed"})

    # èª¿è©¦ä¿¡æ¯
    print(f"ğŸ” Embedding èª¿è©¦ä¿¡æ¯:")
    print(f"   USE_GEMINI_EMBEDDING: {USE_GEMINI_EMBEDDING}")
    print(f"   GOOGLE_API_KEY: {'å·²è¨­ç½®' if GOOGLE_API_KEY else 'æœªè¨­ç½®'}")
    print(f"   USE_BGE_M3_EMBEDDING: {USE_BGE_M3_EMBEDDING}")
    print(f"   SENTENCE_TRANSFORMERS_AVAILABLE: {SENTENCE_TRANSFORMERS_AVAILABLE}")
    
    # å˜—è©¦ä½¿ç”¨ Gemini embeddingï¼ˆä¸»è¦é¸é …ï¼‰
    if USE_GEMINI_EMBEDDING and GOOGLE_API_KEY:
        try:
            vectors = await embed_gemini(all_chunks)
            store.embeddings = vectors
            store.chunk_doc_ids = chunk_doc_ids
            store.chunks_flat = all_chunks
            return {
                "provider": "gemini", 
                "model": "gemini-embedding-001",
                "num_vectors": len(vectors),
                "dimension": len(vectors[0]) if vectors else 0
            }
        except Exception as e:
            print(f"Gemini embedding failed: {e}")
            # å¦‚æœ Gemini å¤±æ•—ï¼Œå˜—è©¦ BGE-M3
    
    # å˜—è©¦ä½¿ç”¨ BGE-M3 embeddingï¼ˆå‚™ç”¨é¸é …ï¼‰
    if USE_BGE_M3_EMBEDDING and SENTENCE_TRANSFORMERS_AVAILABLE:
        try:
            vectors = embed_bge_m3(all_chunks)
            store.embeddings = vectors
            store.chunk_doc_ids = chunk_doc_ids
            store.chunks_flat = all_chunks
            return {
                "provider": "bge-m3", 
                "model": "BAAI/bge-m3",
                "num_vectors": len(vectors),
                "dimension": len(vectors[0]) if vectors else 0
            }
        except Exception as e:
            print(f"BGE-M3 embedding failed: {e}")
    
    # æ²’æœ‰å¯ç”¨çš„ embedding æ–¹æ³•
    return JSONResponse(
        status_code=500, 
        content={
            "error": "No embedding method available. Please configure Gemini API key or BGE-M3 model."
        }
    )


def convert_structured_to_multi_level(structured_chunks):
    """å°‡çµæ§‹åŒ–åˆ†å¡Šè½‰æ›ç‚ºè«–æ–‡ä¸­çš„å…­å€‹ç²’åº¦ç´šåˆ¥æ ¼å¼ï¼Œç¢ºä¿ä¸Šä¸‹æ–‡é€£è²«æ€§"""
    # è«–æ–‡ä¸­çš„å…­å€‹å±¤æ¬¡
    six_level_chunks = {
        'document': [],                    # 1. æ–‡ä»¶å±¤ç´š (Document Level)
        'document_component': [],          # 2. æ–‡ä»¶çµ„æˆéƒ¨åˆ†å±¤ç´š (Document Component Level)
        'basic_unit_hierarchy': [],        # 3. åŸºæœ¬å–®ä½å±¤æ¬¡çµæ§‹å±¤ç´š (Basic Unit Hierarchy Level)
        'basic_unit': [],                  # 4. åŸºæœ¬å–®ä½å±¤ç´š (Basic Unit Level)
        'basic_unit_component': [],        # 5. åŸºæœ¬å–®ä½çµ„æˆéƒ¨åˆ†å±¤ç´š (Basic Unit Component Level)
        'enumeration': []                  # 6. åˆ—èˆ‰å±¤ç´š (Enumeration Level)
    }
    
    for chunk in structured_chunks:
        content = chunk.get('content', '')
        metadata = chunk.get('metadata', {})
        
        # å„ªå…ˆä½¿ç”¨metadataä¸­çš„levelä¿¡æ¯ï¼ˆå¤šå±¤æ¬¡çµæ§‹åŒ–åˆ†å¡Šæœƒè¨­ç½®é€™å€‹ï¼‰
        if 'level' in metadata:
            chunk_by = metadata['level']
        else:
            chunk_by = metadata.get('chunk_by', 'article')
        
        # å°‡å¤§å¯«çš„levelåç¨±è½‰æ›ç‚ºå°å¯«ï¼ˆå…¼å®¹MultiLevelStructuredChunkingç”Ÿæˆçš„æ ¼å¼ï¼‰
        chunk_by = chunk_by.lower()
        
        # æ ¹æ“šchunk_byå’Œå…§å®¹ç‰¹å¾µåˆ†é¡åˆ°å°æ‡‰å±¤æ¬¡
        level_name, semantic_features = classify_chunk_to_level(content, metadata, chunk_by)
        
        # è™•ç†ä¸Šä¸‹æ–‡é€£è²«æ€§ï¼šç‚ºåˆ—èˆ‰å…ƒç´ æ·»åŠ çˆ¶ç´šä¸Šä¸‹æ–‡
        final_content = content
        if level_name == 'enumeration' and chunk_by == 'item':
            # æª¢æŸ¥æ˜¯å¦å·²ç¶“åŒ…å«çˆ¶ç´šå…§å®¹ï¼ˆé€šéæª¢æŸ¥æ˜¯å¦åŒ…å«æ¢æ–‡ä¸»æ–‡ï¼‰
            if not has_parent_context(content, metadata):
                # å˜—è©¦å¾å…¶ä»–chunksä¸­æ‰¾åˆ°çˆ¶ç´šæ¢æ–‡å…§å®¹
                parent_content = find_parent_article_content(structured_chunks, metadata)
                if parent_content:
                    final_content = f"{parent_content}\n{content}"
                    # æ›´æ–°èªç¾©ç‰¹å¾µä»¥åæ˜ ä¸Šä¸‹æ–‡é€£è²«æ€§
                    semantic_features['has_parent_context'] = True
                    semantic_features['parent_content_length'] = len(parent_content)
        
        if level_name in six_level_chunks:
            six_level_chunks[level_name].append({
                'content': final_content,
                'original_content': content,  # ä¿ç•™åŸå§‹å…§å®¹
                'metadata': {
                    **metadata,
                    'semantic_level': level_name,
                    'semantic_features': semantic_features,
                    'target_queries': get_target_queries_for_level(level_name),
                    'has_context_consistency': level_name == 'enumeration' and final_content != content
                }
            })
    
    return six_level_chunks


def classify_chunk_to_level(content: str, metadata: dict, chunk_by: str) -> tuple:
    """æ ¹æ“šå…§å®¹å’Œå…ƒæ•¸æ“šå°‡chunkåˆ†é¡åˆ°åˆé©çš„å±¤æ¬¡ - å°æ‡‰è«–æ–‡ä¸­çš„å…­å€‹ç²’åº¦ç´šåˆ¥"""
    import re
    
    # æ ¹æ“šè«–æ–‡å®šç¾©çš„å…­å€‹ç²’åº¦ç´šåˆ¥æ˜ å°„
    level_mapping = {
        # 1) law_name â†’ document
        'law': 'document',
        # 2) chapter â†’ document_component
        'chapter': 'document_component',
        # 3) section â†’ basic_unit_hierarchy
        'section': 'basic_unit_hierarchy',
        # 4) article â†’ basic_unit
        'article': 'basic_unit',
        # 5) paragraph/é … â†’ basic_unit_component
        'paragraph': 'basic_unit_component',
        # 6) subparagraph/æ¬¾ â†’ enumerationï¼›item/ç›® â†’ enumeration
        'subparagraph': 'enumeration',
        'item': 'enumeration'
    }
    
    # é¦–å…ˆæ ¹æ“šchunk_byç¢ºå®šåŸºæœ¬å±¤æ¬¡
    base_level = level_mapping.get(chunk_by, 'basic_unit')
    
    # åŸºæ–¼å…§å®¹ç‰¹å¾µé€²è¡Œèªç¾©åˆ†æ
    semantic_features = analyze_chunk_semantics(content)
    
    # æ ¹æ“šèªç¾©ç‰¹å¾µå’Œå…§å®¹é•·åº¦é€²è¡Œç²¾ç´°èª¿æ•´
    # ä»¥ä½ æŒ‡å®šçš„å›ºå®šæ˜ å°„ç‚ºä¸»ï¼›åªä¿ç•™å°‘é‡åˆç†åŒ–ï¼ˆä¾‹å¦‚ article çš„å®šç¾©æ€§é•·æ–‡å¯æ­¸åˆ° basic_unit_componentï¼‰
    if chunk_by == 'article':
        level = 'basic_unit' if not (semantic_features['is_definition'] and len(content) > 200) else 'basic_unit_component'
    elif chunk_by in ('paragraph',):
        # é …ï¼ˆparagraphï¼‰å›ºå®šç‚º basic_unit_component
        level = 'basic_unit_component'
    elif chunk_by in ('subparagraph', 'item'):
        # æ¬¾/ç›®å›ºå®šç‚º enumerationï¼ˆæ³¨æ„ï¼šæ­¤è™•çš„ item ä»£è¡¨ã€Œç›®ã€ï¼‰
        level = 'enumeration'
    elif chunk_by == 'chapter':
        level = 'document_component'
    elif chunk_by == 'section':
        level = 'basic_unit_hierarchy'
    elif chunk_by == 'law':
        level = 'document'
    else:
        level = base_level
    
    return level, semantic_features


def analyze_chunk_semantics(content: str) -> dict:
    """åˆ†æchunkçš„èªç¾©ç‰¹å¾µ"""
    import re
    
    features = {
        'is_definition': False,
        'is_procedural': False,
        'is_enumeration': False,
        'is_normative': False,
        'has_article_reference': False,
        'concept_density': 0.0,
        'legal_keywords': []
    }
    
    content_lower = content.lower()
    
    # æª¢æŸ¥å®šç¾©æ€§å…§å®¹
    definition_patterns = [
        r'æœ¬æ³•æ‰€ç¨±.*?æ˜¯æŒ‡',
        r'.*?æŒ‡.*?è€…',
        r'.*?ç‚º.*?è€…',
        r'å®šç¾©.*?ç‚º',
        r'æ‰€è¬‚.*?ä¿‚æŒ‡'
    ]
    for pattern in definition_patterns:
        if re.search(pattern, content):
            features['is_definition'] = True
            break
    
    # æª¢æŸ¥ç¨‹åºæ€§å…§å®¹
    procedural_patterns = [
        r'æ‡‰.*?ç”³è«‹',
        r'å¾—.*?è¾¦ç†',
        r'ä¾.*?ç¨‹åº',
        r'å¦‚ä½•.*?',
        r'ç¨‹åº.*?',
        r'æµç¨‹.*?'
    ]
    for pattern in procedural_patterns:
        if re.search(pattern, content):
            features['is_procedural'] = True
            break
    
    # æª¢æŸ¥åˆ—èˆ‰æ€§å…§å®¹
    enumeration_patterns = [
        r'[ï¼ˆ(]\d+[ï¼‰)]',
        r'[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+[ã€ï¼]',
        r'\d+[ã€ï¼]',
        r'ç¬¬.*?é …',
        r'ç¬¬.*?æ¬¾'
    ]
    for pattern in enumeration_patterns:
        if re.search(pattern, content):
            features['is_enumeration'] = True
            break
    
    # æª¢æŸ¥è¦ç¯„æ€§å…§å®¹
    normative_patterns = [
        r'æ‡‰.*?',
        r'å¾—.*?',
        r'ä¸å¾—.*?',
        r'ç¦æ­¢.*?',
        r'è¦å®š.*?'
    ]
    for pattern in normative_patterns:
        if re.search(pattern, content):
            features['is_normative'] = True
            break
    
    # æª¢æŸ¥æ³•æ¢å¼•ç”¨
    if re.search(r'ç¬¬\s*\d+\s*æ¢', content):
        features['has_article_reference'] = True
    
    # è¨ˆç®—æ¦‚å¿µå¯†åº¦
    legal_keywords = ['æœ¬æ³•', 'æ¢æ–‡', 'è¦å®š', 'æ¬Šåˆ©', 'ç¾©å‹™', 'ç”³è«‹', 'è¾¦ç†', 'ç¨‹åº', 'å®šç¾©', 'ç¯„åœ', 'è²¬ä»»', 'æ¬ŠåŠ›', 'è·æ¬Š', 'è·è²¬', 'æ³•å¾‹', 'æ³•è¦', 'æ¢ä¾‹']
    keyword_count = sum(1 for keyword in legal_keywords if keyword in content)
    features['concept_density'] = keyword_count / max(len(content.split()), 1)
    features['legal_keywords'] = [kw for kw in legal_keywords if kw in content]
    
    return features


def get_target_queries_for_level(level_name: str) -> list:
    """æ ¹æ“šå±¤æ¬¡è¿”å›ç›®æ¨™æŸ¥è©¢é—œéµè©"""
    query_mapping = {
        'document': ['æ•´éƒ¨', 'å…¨æ–‡', 'æ•´å€‹', 'å…¨éƒ¨'],
        'document_component': ['ç« ', 'éƒ¨åˆ†', 'ç·¨', 'ç¯‡'],
        'basic_unit_hierarchy': ['ç¯€', 'æ¨™é¡Œ', 'ç« ç¯€'],
        'basic_unit': ['ç¬¬.*æ¢', 'æ¢æ–‡', 'æ³•æ¢'],
        'basic_unit_component': ['æ®µè½', 'ä¸»æ–‡', 'å…§å®¹', 'å®šç¾©'],
        'enumeration': ['é …', 'ç›®', 'æ¬¾', 'å­é …']
    }
    return query_mapping.get(level_name, ['ç¬¬.*æ¢'])


def has_parent_context(content: str, metadata: dict) -> bool:
    """æª¢æŸ¥å…§å®¹æ˜¯å¦å·²ç¶“åŒ…å«çˆ¶ç´šä¸Šä¸‹æ–‡"""
    import re
    
    # æª¢æŸ¥æ˜¯å¦åŒ…å«æ¢æ–‡ä¸»æ–‡çš„ç‰¹å¾µ
    article_main_patterns = [
        r'æœ¬æ³•.*?å®šç¾©',
        r'æœ¬æ³•.*?è¦å®š',
        r'æœ¬æ³•.*?ç”¨è©',
        r'æ‡‰.*?ç”³è«‹',
        r'å¾—.*?è¾¦ç†',
        r'ä¾.*?ç¨‹åº'
    ]
    
    # å¦‚æœå…§å®¹é•·åº¦è¼ƒçŸ­ä¸”ä¸åŒ…å«æ¢æ–‡ä¸»æ–‡ç‰¹å¾µï¼Œå¯èƒ½ç¼ºå°‘çˆ¶ç´šä¸Šä¸‹æ–‡
    if len(content) < 200:
        for pattern in article_main_patterns:
            if re.search(pattern, content):
                return True
        return False
    
    return True


def find_parent_article_content(structured_chunks: list, current_metadata: dict) -> str:
    """å¾çµæ§‹åŒ–chunksä¸­æ‰¾åˆ°çˆ¶ç´šæ¢æ–‡å…§å®¹"""
    current_article = current_metadata.get('article', '')
    current_chapter = current_metadata.get('chapter', '')
    current_section = current_metadata.get('section', '')
    
    # æŸ¥æ‰¾å°æ‡‰çš„æ¢æ–‡chunk
    for chunk in structured_chunks:
        chunk_metadata = chunk.get('metadata', {})
        chunk_by = chunk_metadata.get('chunk_by', '')
        
        # æ‰¾åˆ°å°æ‡‰çš„æ¢æ–‡chunk
        if (chunk_by == 'article' and 
            chunk_metadata.get('article', '') == current_article and
            chunk_metadata.get('chapter', '') == current_chapter and
            chunk_metadata.get('section', '') == current_section):
            
            content = chunk.get('content', '')
            # æå–æ¢æ–‡ä¸»æ–‡éƒ¨åˆ†ï¼ˆæ’é™¤é …ç›®å…§å®¹ï¼‰
            lines = content.split('\n')
            main_content_lines = []
            
            for line in lines:
                line = line.strip()
                # å¦‚æœé‡åˆ°é …ç›®æ¨™è¨˜ï¼Œåœæ­¢æå–ä¸»æ–‡
                if re.match(r'^[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+[ã€ï¼]', line) or re.match(r'^\d+[ã€ï¼]', line):
                    break
                # åŒ…å«æ¡æ–‡æ ‡é¢˜å’Œä¸»æ–‡å†…å®¹ï¼Œä½†æ’é™¤ç»“æ„ä¿¡æ¯
                if line and not line.startswith('ã€') and not line.startswith('ç« ') and not line.startswith('ç¯€'):
                    main_content_lines.append(line)
            
            return '\n'.join(main_content_lines)
    
    return ""


@app.post("/api/multi-level-embed")
async def multi_level_embed(req: EmbedRequest):
    """å¤šå±¤æ¬¡embeddingç«¯é» - ç‚ºè«–æ–‡ä¸­çš„å…­å€‹ç²’åº¦ç´šåˆ¥å‰µå»ºç¨ç«‹çš„embedding"""
    print(f"ğŸ” å¤šå±‚çº§Embeddingå‡½æ•°è¢«è°ƒç”¨ï¼Œè¯·æ±‚: {req}")
    print(f"ğŸ” é…ç½®æ£€æŸ¥:")
    print(f"   USE_GEMINI_EMBEDDING: {USE_GEMINI_EMBEDDING}")
    print(f"   GOOGLE_API_KEY: {'å·²è¨­ç½®' if GOOGLE_API_KEY else 'æœªè¨­ç½®'}")
    print(f"   USE_BGE_M3_EMBEDDING: {USE_BGE_M3_EMBEDDING}")
    # æ”¶é›†é¸å®šæ–‡æª”çš„å¤šå±¤æ¬¡chunks
    selected = req.doc_ids or list(store.docs.keys())
    all_multi_level_chunks = {}
    
    for doc_id in selected:
        doc = store.docs.get(doc_id)
        if doc and hasattr(doc, 'multi_level_chunks') and doc.multi_level_chunks:
            all_multi_level_chunks[doc_id] = doc.multi_level_chunks
        elif doc and ((hasattr(doc, 'structured_chunks') and doc.structured_chunks) or (hasattr(doc, 'json_data') and doc.json_data)):
            # è‹¥å·²æœ‰çµæ§‹åŒ–chunksæˆ–æœ‰jsonçµæ§‹ï¼Œå„ªå…ˆåŸºæ–¼JSONç”Ÿæˆå®Œæ•´å…­å±¤ï¼Œé¿å…åªå‰©æ¢ç´š
            print(f"ğŸ”„ åŸºæ–¼JSONç”Ÿæˆå…­å€‹ç²’åº¦ç´šåˆ¥æ ¼å¼ï¼Œæ–‡æª”: {doc.filename}")
            try:
                from .chunking import MultiLevelStructuredChunking
                ml_chunker = MultiLevelStructuredChunking()
                # ç›´æ¥å¾ JSON ç”¢ç”Ÿå¤šå±¤ç´šå¸¶ span çš„åˆ—è¡¨
                raw_multi_level_list = ml_chunker.chunk_with_span(doc.text, json_data=getattr(doc, 'json_data', None))
                # çµ±ä¸€è½‰ç‚ºå…­å±¤å­—å…¸çµæ§‹
                converted_chunks = convert_structured_to_multi_level(raw_multi_level_list)
            except Exception as e:
                print(f"âš ï¸ åŸºæ–¼JSONç”Ÿæˆå¤šå±¤ç´šå¤±æ•—ï¼Œå›é€€ç”¨structured_chunksè½‰æ›: {e}")
                converted_chunks = convert_structured_to_multi_level(doc.structured_chunks or [])

            all_multi_level_chunks[doc_id] = converted_chunks
            # ä¿å­˜åˆ°æ–‡æª”
            doc.multi_level_chunks = converted_chunks
            doc.chunking_strategy = "structured_to_multi_level"
            store.add_doc(doc)
    
    if not all_multi_level_chunks:
        return JSONResponse(
            status_code=400, 
            content={"error": "No multi-level chunks available. Please run structured hierarchical chunking or multi-level semantic chunking first."}
        )
    
    # è«–æ–‡ä¸­çš„å…­å€‹å±¤æ¬¡
    six_levels = [
        'document',                    # 1. æ–‡ä»¶å±¤ç´š
        'document_component',          # 2. æ–‡ä»¶çµ„æˆéƒ¨åˆ†å±¤ç´š
        'basic_unit_hierarchy',        # 3. åŸºæœ¬å–®ä½å±¤æ¬¡çµæ§‹å±¤ç´š
        'basic_unit',                  # 4. åŸºæœ¬å–®ä½å±¤ç´š
        'basic_unit_component',        # 5. åŸºæœ¬å–®ä½çµ„æˆéƒ¨åˆ†å±¤ç´š
        'enumeration'                  # 6. åˆ—èˆ‰å±¤ç´š
    ]
    
    # ç‚ºæ¯å€‹å±¤æ¬¡å‰µå»ºç¨ç«‹çš„embedding
    level_results = {}
    total_vectors = 0
    total_levels = len(six_levels)
    completed_levels = 0
    
    print(f"ğŸš€ é–‹å§‹å¤šå±¤æ¬¡embeddingè™•ç†ï¼Œå…± {total_levels} å€‹å±¤æ¬¡")
    
    for level_idx, level_name in enumerate(six_levels):
        level_chunks = []
        level_doc_ids = []
        
        # æ”¶é›†è©²å±¤æ¬¡çš„æ‰€æœ‰chunks
        for doc_id, multi_chunks in all_multi_level_chunks.items():
            if level_name in multi_chunks:
                for chunk_data in multi_chunks[level_name]:
                    if isinstance(chunk_data, dict) and 'content' in chunk_data:
                        level_chunks.append(chunk_data['content'])
                        level_doc_ids.append(doc_id)
        
        if not level_chunks:
            print(f"âš ï¸ å±¤æ¬¡ '{level_name}' æ²’æœ‰å¯ç”¨çš„chunks")
            completed_levels += 1
            progress = (completed_levels / total_levels) * 100
            print(f"ğŸ“Š é€²åº¦: {completed_levels}/{total_levels} ({progress:.1f}%)")
            continue
        
        print(f"ğŸ” é–‹å§‹ç‚ºå±¤æ¬¡ '{level_name}' å‰µå»ºembeddingï¼Œå…± {len(level_chunks)} å€‹chunks")
        
            # ç‚ºè©²å±¤æ¬¡å‰µå»ºembedding
        try:
            print(f"â³ æ­£åœ¨è™•ç†å±¤æ¬¡ '{level_name}' çš„embedding...")
            if USE_GEMINI_EMBEDDING and GOOGLE_API_KEY:
                vectors = await embed_gemini(level_chunks)
                provider = "gemini"
                model = "gemini-embedding-001"
            elif USE_BGE_M3_EMBEDDING and SENTENCE_TRANSFORMERS_AVAILABLE:
                vectors = embed_bge_m3(level_chunks)
                provider = "bge-m3"
                model = "BAAI/bge-m3"
            else:
                print(f"âŒ å±¤æ¬¡ '{level_name}' embeddingå¤±æ•—ï¼šæ²’æœ‰å¯ç”¨çš„embeddingæ–¹æ³•")
                completed_levels += 1
                progress = (completed_levels / total_levels) * 100
                print(f"ğŸ“Š é€²åº¦: {completed_levels}/{total_levels} ({progress:.1f}%)")
                continue
            
            # å­˜å„²è©²å±¤æ¬¡çš„embeddingå’Œå…ƒæ•¸æ“š
            metadata = {
                "provider": provider,
                "model": model,
                "dimension": len(vectors[0]) if vectors else 0
            }
            store.set_multi_level_embeddings(level_name, vectors, level_chunks, level_doc_ids, metadata)
            
            level_results[level_name] = {
                "provider": provider,
                "model": model,
                "num_vectors": len(vectors),
                "dimension": len(vectors[0]) if vectors else 0,
                "num_chunks": len(level_chunks),
                "level_description": get_level_description(level_name)
            }
            
            total_vectors += len(vectors)
            completed_levels += 1
            progress = (completed_levels / total_levels) * 100
            print(f"âœ… å±¤æ¬¡ '{level_name}' embeddingå®Œæˆï¼š{len(vectors)} å€‹å‘é‡")
            print(f"ğŸ“Š é€²åº¦: {completed_levels}/{total_levels} ({progress:.1f}%)")
            
        except Exception as e:
            print(f"âŒ å±¤æ¬¡ '{level_name}' embeddingå¤±æ•—ï¼š{e}")
            # ä½¿ç”¨éš¨æ©Ÿå‘é‡ä½œç‚ºfallback
            try:
                import numpy as np
                fallback_vectors = np.random.randn(len(level_chunks), EMBEDDING_DIMENSION).astype(np.float32).tolist()
                metadata = {
                    "provider": "fallback_random",
                    "model": f"random_{EMBEDDING_DIMENSION}d",
                    "dimension": EMBEDDING_DIMENSION
                }
                store.set_multi_level_embeddings(level_name, fallback_vectors, level_chunks, level_doc_ids, metadata)
                
                level_results[level_name] = {
                    "provider": "fallback_random",
                    "model": f"random_{EMBEDDING_DIMENSION}d",
                    "num_vectors": len(fallback_vectors),
                    "dimension": EMBEDDING_DIMENSION,
                    "num_chunks": len(level_chunks),
                    "level_description": get_level_description(level_name),
                    "error": f"Original embedding failed, using fallback: {str(e)}"
                }
                
                total_vectors += len(fallback_vectors)
                print(f"âš ï¸ å±¤æ¬¡ '{level_name}' ä½¿ç”¨fallbackå‘é‡ï¼š{len(fallback_vectors)} å€‹")
                
            except Exception as fallback_error:
                print(f"âŒ å±¤æ¬¡ '{level_name}' fallbackä¹Ÿå¤±æ•—ï¼š{fallback_error}")
                level_results[level_name] = {
                    "error": f"Both original and fallback failed: {str(e)} | {str(fallback_error)}",
                    "num_chunks": len(level_chunks),
                    "level_description": get_level_description(level_name)
                }
            
            completed_levels += 1
            progress = (completed_levels / total_levels) * 100
            print(f"ğŸ“Š é€²åº¦: {completed_levels}/{total_levels} ({progress:.1f}%)")
    
    print(f"ğŸ‰ å¤šå±¤æ¬¡embeddingè™•ç†å®Œæˆï¼ç¸½å…±è™•ç†äº† {total_vectors} å€‹å‘é‡ï¼ŒæˆåŠŸå®Œæˆ {completed_levels}/{total_levels} å€‹å±¤æ¬¡")
    
    if not level_results:
        return JSONResponse(
            status_code=500,
            content={"error": "Failed to create embeddings for any level"}
        )
    
    return {
        "message": "Six-level embeddings created successfully",
        "total_vectors": total_vectors,
        "levels": level_results,
        "available_levels": list(level_results.keys()),
        "level_descriptions": {
            level: get_level_description(level) for level in six_levels
        }
    }


def get_level_description(level_name: str) -> str:
    """ç²å–å±¤æ¬¡æè¿°"""
    descriptions = {
        'document': 'æ–‡ä»¶å±¤ç´š (Document Level) - æ•´å€‹æ³•å¾‹æ–‡æª”',
        'document_component': 'æ–‡ä»¶çµ„æˆéƒ¨åˆ†å±¤ç´š (Document Component Level) - æ–‡æª”çš„ä¸»è¦çµ„æˆéƒ¨åˆ†',
        'basic_unit_hierarchy': 'åŸºæœ¬å–®ä½å±¤æ¬¡çµæ§‹å±¤ç´š (Basic Unit Hierarchy Level) - æ›¸ç±ã€æ¨™é¡Œã€ç« ç¯€',
        'basic_unit': 'åŸºæœ¬å–®ä½å±¤ç´š (Basic Unit Level) - æ–‡ç« /æ¢æ–‡ (article)',
        'basic_unit_component': 'åŸºæœ¬å–®ä½çµ„æˆéƒ¨åˆ†å±¤ç´š (Basic Unit Component Level) - å¼·åˆ¶æ€§ä¸»æ–‡æˆ–æ®µè½',
        'enumeration': 'åˆ—èˆ‰å±¤ç´š (Enumeration Level) - é …ç›®ã€å­é …'
    }
    return descriptions.get(level_name, f"æœªçŸ¥å±¤æ¬¡: {level_name}")


def rank_with_dense_vectors(query: str, k: int):
    """ä½¿ç”¨å¯†é›†å‘é‡é€²è¡Œç›¸ä¼¼åº¦è¨ˆç®—ï¼ˆæ”¯æŒ Gemini å’Œ BGE-M3ï¼‰"""
    import numpy as np
    # ç¢ºä¿embeddingsæ˜¯numpyæ•¸çµ„æ ¼å¼
    if store.embeddings is None:
        raise ValueError("No embeddings available")
    if isinstance(store.embeddings, list):
        vecs = np.array(store.embeddings, dtype=float)
    else:
        vecs = np.array(store.embeddings, dtype=float)  # type: ignore[assignment]
    
    # æ ¹æ“šç•¶å‰é…ç½®é¸æ“‡æŸ¥è©¢å‘é‡åŒ–æ–¹æ³•
    if USE_GEMINI_EMBEDDING and GOOGLE_API_KEY:
        try:
            qvec = np.array(asyncio_run(embed_gemini([query]))[0], dtype=float)
        except Exception as e:
            print(f"Gemini query embedding failed: {e}")
            # å¦‚æœ Gemini å¤±æ•—ï¼Œå˜—è©¦ BGE-M3
            if USE_BGE_M3_EMBEDDING and SENTENCE_TRANSFORMERS_AVAILABLE:
                try:
                    qvec = np.array(embed_bge_m3([query])[0], dtype=float)
                except Exception as e2:
                    print(f"BGE-M3 query embedding failed: {e2}")
                    raise RuntimeError("Both Gemini and BGE-M3 query embedding failed")
            else:
                raise RuntimeError("Gemini query embedding failed and BGE-M3 not available")
    elif USE_BGE_M3_EMBEDDING and SENTENCE_TRANSFORMERS_AVAILABLE:
        try:
            qvec = np.array(embed_bge_m3([query])[0], dtype=float)
        except Exception as e:
            print(f"BGE-M3 query embedding failed: {e}")
            raise RuntimeError("BGE-M3 query embedding failed")
    else:
        raise RuntimeError("No dense embedding method available")
    
    # normalize
    vecs_norm = vecs / (np.linalg.norm(vecs, axis=1, keepdims=True) + 1e-8)
    q_norm = qvec / (np.linalg.norm(qvec) + 1e-8)
    sims = vecs_norm @ q_norm
    idxs = np.argsort(-sims)[:k]
    return idxs.tolist(), sims[idxs].tolist()


def calculate_query_qa_similarity(query: str, qa_query: str) -> float:
    """è¨ˆç®—æŸ¥è©¢èˆ‡QAçš„ç›¸ä¼¼åº¦"""
    import re
    
    query_lower = query.lower().strip()
    qa_lower = qa_query.lower().strip()
    
    # æ–¹æ³•1: å®Œå…¨åŒ¹é…
    if query_lower == qa_lower:
        return 1.0
    
    # æ–¹æ³•2: åŒ…å«åŒ¹é…
    if query_lower in qa_lower or qa_lower in query_lower:
        return 0.9
    
    # æ–¹æ³•3: æ³•æ¢è™Ÿç¢¼åŒ¹é…
    query_article_match = re.search(r'ç¬¬\s*(\d+(?:ä¹‹\d+)?)\s*æ¢', query_lower)
    qa_article_match = re.search(r'ç¬¬\s*(\d+(?:ä¹‹\d+)?)\s*æ¢', qa_lower)
    
    if query_article_match and qa_article_match:
        query_article = query_article_match.group(1)
        qa_article = qa_article_match.group(1)
        if query_article == qa_article:
            return 0.8
    
    # æ–¹æ³•4: æ³•å¾‹åç¨±åŒ¹é…
    law_names = ["è‘—ä½œæ¬Šæ³•", "å•†æ¨™æ³•", "å°ˆåˆ©æ³•", "æ°‘æ³•", "åˆ‘æ³•"]
    query_laws = [law for law in law_names if law in query_lower]
    qa_laws = [law for law in law_names if law in qa_lower]
    
    if query_laws and qa_laws and any(law in qa_laws for law in query_laws):
        return 0.7
    
    # æ–¹æ³•5: é—œéµè©é‡ç–Š
    query_words = set(re.findall(r'\w+', query_lower))
    qa_words = set(re.findall(r'\w+', qa_lower))
    
    if query_words and qa_words:
        overlap = len(query_words.intersection(qa_words))
        union = len(query_words.union(qa_words))
        jaccard_similarity = overlap / union if union > 0 else 0
        return jaccard_similarity * 0.6  # é™ä½æ¬Šé‡
    
    return 0.0


def expand_query_with_legal_domain(query: str) -> Dict[str, Any]:
    """ä½¿ç”¨é ˜åŸŸå°ˆå±¬è©åº«é€²è¡ŒæŸ¥è©¢æ“´å±•"""
    
    # å¦‚æœæœ‰æ³•å¾‹æ¨ç†å¼•æ“ï¼Œå„ªå…ˆä½¿ç”¨
    if legal_reasoning_engine:
        try:
            analysis = legal_reasoning_engine.analyze_query(query)
            expanded_query = legal_reasoning_engine.get_expanded_query(query)
            
            return {
                "original_query": query,
                "expanded_query": expanded_query,
                "expansion_ratio": len(expanded_query.split()) / len(query.split()),
                "domain_matches": analysis["concept_mappings"],
                "detected_domains": ["copyright"] if analysis["detected_concepts"] else [],
                "applicable_articles": analysis["applicable_articles"],
                "reasoning_explanation": analysis["reasoning_explanation"],
                "confidence_scores": analysis["confidence_scores"],
                "reasoning_engine_used": True
            }
        except Exception as e:
            print(f"âš ï¸ æ³•å¾‹æ¨ç†å¼•æ“åŸ·è¡Œå¤±æ•—: {e}")
            # å›é€€åˆ°åŸæœ‰æ–¹æ³•
    
    # é ˜åŸŸå°ˆå±¬è©åº« - æ³•å¾‹æ¦‚å¿µæ˜ å°„
    legal_domain_dict = {
        # è‘—ä½œæ¬Šæ³•å°ˆå±¬è©å½™
        "copyright": {
                   "æ ¸å¿ƒæ¦‚å¿µ": {
                       "é‡è£½": ["è¤‡è£½", "æŠ„è¥²", "ç›œç‰ˆ", "ç¿»å°", "å½±å°", "æƒæ", "ä¸‹è¼‰", "ä¿å­˜", "ç›´æ¥è¤‡è£½"],
                       "æ”¹ä½œ": ["æ”¹å¯«", "æ”¹ç·¨", "ä¿®æ”¹", "è¡ç”Ÿ", "å‰µä½œ", "é‡æ–°å‰µä½œ", "äºŒæ¬¡å‰µä½œ", "ç”¨è‡ªå·±çš„èªæ°£", "æ”¹å¯«æˆè‡ªå·±çš„èªæ°£", "æ”¹å¯«æˆ", "èªæ°£", "ç¿»è­¯", "è­¯", "è½‰è­¯", "ä¸­è­¯", "è‹±è­¯", "æ—¥è­¯"],
                       "æ•£å¸ƒ": ["åˆ†äº«", "å‚³æ’­", "ç™¼å¸ƒ", "ä¸Šå‚³", "è½‰è¼‰", "è½‰ç™¼", "å‚³é€"],
                "å…¬é–‹å‚³è¼¸": ["ä¸Šç¶²", "ç¶²è·¯å‚³æ’­", "ç·šä¸Šåˆ†äº«", "ä¸²æµ", "ç›´æ’­"],
                "å…¬é–‹æ¼”å‡º": ["è¡¨æ¼”", "æ¼”å¥", "æ¼”å”±", "æ’­æ”¾", "æ”¾æ˜ "],
                "å…¬é–‹å±•ç¤º": ["å±•è¦½", "å±•ç¤º", "é™³åˆ—", "å±•å‡º"],
                "å‡ºç§Ÿ": ["ç§Ÿå€Ÿ", "ç§Ÿè³ƒ", "å‡ºå€Ÿ"],
                "ä¾µå®³": ["é•å", "ä¾µçŠ¯", "æå®³", "é•æ³•", "ä¸æ³•"],
                       "åˆç†ä½¿ç”¨": ["å¼•ç”¨", "è©•è«–", "æ•™å­¸", "ç ”ç©¶", "å ±å°", "å­¸è¡“"],
                       "æ•™è‚²ç”¨é€”": ["èª²å ‚", "å­¸æ ¡", "æ•™è‚²", "æ•™å­¸", "æˆèª²", "å­¸ç”Ÿ", "æ’­æ”¾", "å½±ç‰‡", "youtube", "å½±ç‰‡"],
                       "è‘—ä½œè²¡ç”¢æ¬Š": ["ç‰ˆæ¬Š", "è²¡ç”¢æ¬Š", "ç¶“æ¿Ÿæ¬Šåˆ©"],
                       "è‘—ä½œäººæ ¼æ¬Š": ["ç½²åæ¬Š", "å®Œæ•´æ€§æ¬Š", "åè­½æ¬Š"],
                "å…¬é–‹ç™¼è¡¨": ["ç™¼è¡¨", "å‡ºç‰ˆ", "å…¬é–‹", "ç™¼å¸ƒ"],
                "å‰µä½œ": ["è£½ä½œ", "ç”¢ç”Ÿ", "å®Œæˆ", "å¯«ä½œ", "ç¹ªè£½"],
                "è‘—ä½œ": ["ä½œå“", "å‰µä½œ", "è—è¡“å“", "æ–‡å­¸", "éŸ³æ¨‚", "ç¾è¡“", "æ”å½±"],
                "è‘—ä½œäºº": ["ä½œè€…", "å‰µä½œè€…", "è—è¡“å®¶", "ä½œå®¶"],
            },
            "æ³•å¾‹æ¢æ–‡": {
                "ç¬¬3æ¢": ["å®šç¾©", "æ¦‚å¿µ", "è§£é‡‹", "èªªæ˜", "ä½•è¬‚"],
                "ç¬¬10æ¢": ["è‘—ä½œæ¬Šå–å¾—", "å®Œæˆæ™‚", "äº«æœ‰", "ç”¢ç”Ÿ"],
                "ç¬¬22æ¢": ["é‡è£½æ¬Š", "è¤‡è£½æ¬Š"],
                "ç¬¬26æ¢": ["å…¬é–‹æ¼”å‡ºæ¬Š"],
                "ç¬¬26-1æ¢": ["å…¬é–‹å‚³è¼¸æ¬Š"],
                "ç¬¬28æ¢": ["æ”¹ä½œæ¬Š", "è¡ç”Ÿè‘—ä½œ", "ç¿»è­¯", "è­¯", "è½‰è­¯", "ä¸­è­¯", "è‹±è­¯", "æ—¥è­¯", "æ”¹ä½œ", "æ”¹ç·¨", "ä¿®æ”¹", "è¡ç”Ÿ", "å‰µä½œ", "é‡æ–°å‰µä½œ", "äºŒæ¬¡å‰µä½œ"],
                "ç¬¬28-1æ¢": ["æ•£å¸ƒæ¬Š"],
                "ç¬¬29æ¢": ["å‡ºç§Ÿæ¬Š"],
                       "ç¬¬44æ¢": ["å¸æ³•ç¨‹åº", "é‡è£½"],
                       "ç¬¬46æ¢": ["å­¸æ ¡", "æˆèª²", "æ•™å­¸", "é‡è£½"],
                       "ç¬¬47æ¢": ["æ•™è‚²", "å­¸æ ¡", "å…¬é–‹æ’­é€", "å…¬é–‹å‚³è¼¸"],
                       "ç¬¬65æ¢": ["åˆç†ä½¿ç”¨", "ä¾‹å¤–", "é™åˆ¶"],
                       "ç¬¬87æ¢": ["è¦–ç‚ºä¾µå®³", "ç¦æ­¢è¡Œç‚º"],
                       "ç¬¬91æ¢": ["é‡è£½ç½ª", "åˆ‘ç½°", "ç½°é‡‘"],
            }
        },
        
        # å•†æ¨™æ³•å°ˆå±¬è©å½™
        "trademark": {
            "æ ¸å¿ƒæ¦‚å¿µ": {
                "å•†æ¨™": ["æ¨™èªŒ", "æ¨™è­˜", "å“ç‰Œ", "å•†è™Ÿ", "logo", "æ¨™è¨˜"],
                "è¨»å†Š": ["ç”³è«‹", "ç™»è¨˜", "æ ¸å‡†", "å–å¾—"],
                "ä»¿å†’": ["å‡å†’", "å½é€ ", "ä»¿è£½", "å±±å¯¨", "ç›œç”¨"],
                "æ··æ·†": ["ç›¸ä¼¼", "è¿‘ä¼¼", "èª¤èª", "æ··åŒ"],
                "ä½¿ç”¨": ["ä½¿ç”¨", "ç¶“ç‡Ÿ", "éŠ·å”®", "å»£å‘Š"],
                "å°ˆç”¨æ¬Š": ["ç¨å ", "æ’ä»–", "å°ˆæœ‰"],
                "ä¾µå®³": ["ä¾µæ¬Š", "é•å", "æå®³"],
            },
            "æ³•å¾‹æ¢æ–‡": {
                "ç¬¬2æ¢": ["å®šç¾©", "å•†æ¨™", "æœå‹™æ¨™ç« "],
                "ç¬¬5æ¢": ["è¨»å†Š", "ç”³è«‹", "æ ¸å‡†"],
                "ç¬¬29æ¢": ["è¿‘ä¼¼", "æ··æ·†", "é¡ä¼¼"],
                "ç¬¬68æ¢": ["ä¾µå®³", "ä¾µæ¬Š", "ç¦æ­¢"],
                "ç¬¬95æ¢": ["åˆ‘ç½°", "ä»¿å†’ç½ª"],
            }
        },
        
        # å°ˆåˆ©æ³•å°ˆå±¬è©å½™
        "patent": {
            "æ ¸å¿ƒæ¦‚å¿µ": {
                "ç™¼æ˜": ["å‰µæ–°", "æŠ€è¡“", "æ”¹è‰¯", "è¨­è¨ˆ"],
                "å°ˆåˆ©": ["å°ˆåˆ©æ¬Š", "ç¨å æ¬Š"],
                "æ–°ç©æ€§": ["æ–°", "æœªå…¬é–‹", "é¦–å‰µ"],
                "é€²æ­¥æ€§": ["éé¡¯è€Œæ˜“è¦‹", "æŠ€è¡“é€²æ­¥"],
                "ç”¢æ¥­åˆ©ç”¨æ€§": ["å¯¦ç”¨", "å¯è¡Œ", "è£½é€ "],
                "ç”³è«‹": ["æå‡º", "æäº¤", "ç”³å ±"],
                "æ ¸å‡†": ["é€šé", "æˆæ¬Š", "å…¬å‘Š"],
            }
        }
    }
    
    # æŸ¥è©¢æ“´å±•é‚è¼¯
    expanded_terms = set()
    # æ”¹é€²æŸ¥è©¢åˆ†å‰²ï¼Œè™•ç†ä¸­æ–‡å’Œæ¨™é»ç¬¦è™Ÿ
    import re
    # ç§»é™¤æ¨™é»ç¬¦è™Ÿï¼Œç„¶å¾Œåˆ†å‰²
    cleaned_query = re.sub(r'[ï¼Œã€‚ï¼Ÿï¼ã€ï¼›ï¼šï¼Ÿ]', ' ', query.lower())
    # ä½¿ç”¨ç©ºæ ¼å’Œæ¨™é»ç¬¦è™Ÿåˆ†å‰²
    original_terms = set(re.split(r'[\sï¼Œã€‚ï¼Ÿï¼ã€ï¼›ï¼šï¼Ÿ]+', cleaned_query))
    # ç§»é™¤ç©ºå­—ç¬¦ä¸²
    original_terms = {term for term in original_terms if term.strip()}
    domain_matches = []
    
    # 1. è­˜åˆ¥æŸ¥è©¢é ˜åŸŸ
    detected_domains = []
    if any(term in query for term in ["è‘—ä½œæ¬Š", "ç‰ˆæ¬Š", "è‘—ä½œ", "å‰µä½œ", "é‡è£½", "æ”¹ä½œ", "èª²å ‚", "æ•™è‚²", "æ•™å­¸", "å­¸æ ¡", "æ’­æ”¾", "å½±ç‰‡", "youtube", "æˆæ¬Š"]):
        detected_domains.append("copyright")
    if any(term in query for term in ["å•†æ¨™", "å“ç‰Œ", "æ¨™èªŒ", "ä»¿å†’"]):
        detected_domains.append("trademark")
    if any(term in query for term in ["å°ˆåˆ©", "ç™¼æ˜", "æŠ€è¡“", "å‰µæ–°"]):
        detected_domains.append("patent")
    
    # 2. æŸ¥è©¢æ“´å±•
    for domain in detected_domains:
        if domain in legal_domain_dict:
            domain_data = legal_domain_dict[domain]
            
            # æ ¸å¿ƒæ¦‚å¿µæ“´å±•
            for legal_concept, synonyms in domain_data["æ ¸å¿ƒæ¦‚å¿µ"].items():
                # ç›´æ¥æª¢æŸ¥æŸ¥è©¢ä¸­æ˜¯å¦åŒ…å«åŒç¾©è©
                for synonym in synonyms:
                    if synonym in query:
                        expanded_terms.update(synonyms)
                        expanded_terms.add(legal_concept)
                        domain_matches.append(f"{synonym}â†’{legal_concept}")
                # ä¹Ÿæª¢æŸ¥æŸ¥è©¢ä¸­æ˜¯å¦åŒ…å«æ¦‚å¿µæœ¬èº«
                if legal_concept in query:
                    expanded_terms.update(synonyms)
                    expanded_terms.add(legal_concept)
                    domain_matches.append(f"æŸ¥è©¢â†’{legal_concept}")
            
            # æ³•å¾‹æ¢æ–‡æ“´å±•
            for article, keywords in domain_data["æ³•å¾‹æ¢æ–‡"].items():
                for term in original_terms:
                    if term in keywords:
                        expanded_terms.update(keywords)
                        expanded_terms.add(article)
                        domain_matches.append(f"{term}â†’{article}")
                # ä¹Ÿæª¢æŸ¥æŸ¥è©¢ä¸­æ˜¯å¦åŒ…å«æ¢æ–‡é—œéµå­—
                for keyword in keywords:
                    if keyword in query:
                        expanded_terms.update(keywords)
                        expanded_terms.add(article)
                        domain_matches.append(f"{keyword}â†’{article}")
    
    # 3. ç”Ÿæˆæ“´å±•æŸ¥è©¢
    expanded_query_terms = list(original_terms.union(expanded_terms))
    expanded_query = " ".join(expanded_query_terms)
    
    return {
        "original_query": query,
        "expanded_query": expanded_query,
        "detected_domains": detected_domains,
        "expanded_terms": list(expanded_terms),
        "domain_matches": domain_matches,
        "expansion_ratio": len(expanded_terms) / len(original_terms) if original_terms else 0
    }


def detect_content_hierarchy(content: str) -> str:
    """åŸºæ–¼å…§å®¹åˆ†ææª¢æ¸¬å±¤æ¬¡ç´šåˆ¥"""
    import re
    
    # æª¢æ¸¬æ³•æ¢ç´šåˆ¥ï¼ˆåŒ…å«"ç¬¬Xæ¢"ï¼‰
    if re.search(r'ç¬¬\s*\d+\s*æ¢', content):
        return "article"
    
    # æª¢æ¸¬ç¯€ç´šåˆ¥ï¼ˆåŒ…å«"ç¬¬Xç¯€"æˆ–"ç¬¬Xç« "ï¼‰
    if re.search(r'ç¬¬\s*\d+\s*[ç¯€ç« ]', content):
        return "section"
    
    # æª¢æ¸¬ç« ç´šåˆ¥ï¼ˆåŒ…å«"ç¬¬Xç« "æˆ–"ç¸½å‰‡"ã€"é™„å‰‡"ç­‰ï¼‰
    if re.search(r'ç¬¬\s*\d+\s*ç« |ç¸½å‰‡|é™„å‰‡', content):
        return "chapter"
    
    # æª¢æ¸¬æ˜¯å¦ç‚ºå…·é«”æ³•å¾‹æ¢æ–‡å…§å®¹
    if re.search(r'æ¢æ–‡|è¦å®š|æ¬Šåˆ©|ç¾©å‹™|ç¦æ­¢|è™•ç½°', content):
        return "article"
    
    # é»˜èªç‚ºä¸€èˆ¬å…§å®¹
    return "general"


def calculate_hierarchical_relevance(query: str, result: Dict) -> Dict[str, Any]:
    """è¨ˆç®—å±¤æ¬¡åŒ–ç›¸é—œæ€§åˆ†æ•¸ - åŸºæ–¼è«–æ–‡çš„Aboutnessæ¦‚å¿µå’Œå…§å®¹åˆ†æ"""
    content = result.get("content", "")
    metadata = result.get("metadata", {})
    
    # åŸºæ–¼å…§å®¹åˆ†ææª¢æ¸¬å±¤æ¬¡ç´šåˆ¥ï¼ˆå‚™ç”¨æ–¹æ¡ˆï¼‰
    content_hierarchy = detect_content_hierarchy(content)
    
    # æå–å±¤æ¬¡ä¿¡æ¯ï¼ˆå„ªå…ˆä½¿ç”¨metadataï¼Œå‚™ç”¨å…§å®¹åˆ†æï¼‰
    hierarchy_level = "article"  # é»˜èªå±¤ç´š
    if metadata and metadata.get("article"):
        hierarchy_level = "article"
    elif metadata and metadata.get("section"):
        hierarchy_level = "section"
    elif metadata and metadata.get("chapter"):
        hierarchy_level = "chapter"
    else:
        # ä½¿ç”¨å…§å®¹åˆ†æçµæœ
        hierarchy_level = content_hierarchy
    
    # Aboutnessåˆ†æ - è­˜åˆ¥æ–‡æœ¬çš„ä¸»è¦ä¸»é¡Œ
    aboutness_score = 0.0
    aboutness_keywords = []
    
    # æ³•å¾‹æ¦‚å¿µaboutness
    legal_concepts = ["è‘—ä½œæ¬Š", "ç‰ˆæ¬Š", "ä¾µæ¬Š", "é‡è£½", "æ”¹ä½œ", "æ•£å¸ƒ", "åˆç†ä½¿ç”¨", "æˆæ¬Š", "å•†æ¨™", "å°ˆåˆ©"]
    for concept in legal_concepts:
        if concept in content:
            aboutness_score += 1.0
            aboutness_keywords.append(concept)
    
    # çµæ§‹å±¤ç´šæ¬Šé‡ï¼ˆåŸºæ–¼è«–æ–‡çš„å¤šå±¤æ¬¡æ–¹æ³•ï¼‰
    hierarchy_weights = {
        "article": 1.0,    # æ³•æ¢ç´šåˆ¥ - æœ€é«˜ç²¾åº¦
        "section": 0.8,    # ç¯€ç´šåˆ¥ - ä¸­ç­‰ç²¾åº¦
        "chapter": 0.6,    # ç« ç´šåˆ¥ - è¼ƒä½ç²¾åº¦ä½†å»£åº¦æ›´å¤§
        "general": 0.4     # ä¸€èˆ¬å…§å®¹ - æœ€ä½æ¬Šé‡
    }
    
    hierarchy_weight = hierarchy_weights.get(hierarchy_level, 1.0)
    
    return {
        "aboutness_score": aboutness_score,
        "aboutness_keywords": aboutness_keywords,
        "hierarchy_level": hierarchy_level,
        "hierarchy_weight": hierarchy_weight,
        "content_hierarchy": content_hierarchy  # å…§å®¹åˆ†æçš„çµæœ
    }


def calculate_retrieval_metrics(query: str, results: List[Dict], k: int) -> Dict[str, float]:
    """è¨ˆç®—æª¢ç´¢æŒ‡æ¨™ P@K å’Œ R@K - æ•´åˆæŸ¥è©¢æ“´å±•ã€æ™ºèƒ½ç›¸é—œæ€§åˆ¤æ–·å’Œå¤šå±¤æ¬¡æª¢ç´¢"""
    try:
        print(f"ğŸ” é–‹å§‹è¨ˆç®—æª¢ç´¢æŒ‡æ¨™ï¼ŒæŸ¥è©¢: '{query}', k={k}")
        
        if not results:
            print("âŒ æ²’æœ‰æª¢ç´¢çµæœ")
            return {"p_at_k": 0.0, "r_at_k": 0.0, "note": "No retrieval results"}
        
        # 1. æŸ¥è©¢æ“´å±•è™•ç†
        query_expansion = expand_query_with_legal_domain(query)
        expanded_query = query_expansion["expanded_query"]
        detected_domains = query_expansion["detected_domains"]
        domain_matches = query_expansion["domain_matches"]
        
        print(f"ğŸ” æŸ¥è©¢æ“´å±•: åŸæŸ¥è©¢='{query}'")
        print(f"ğŸ” æ“´å±•æŸ¥è©¢: '{expanded_query}'")
        print(f"ğŸ” æª¢æ¸¬é ˜åŸŸ: {detected_domains}")
        print(f"ğŸ” é ˜åŸŸæ˜ å°„: {domain_matches[:5]}...")  # åªé¡¯ç¤ºå‰5å€‹
        
        # åŸºæ–¼æŸ¥è©¢å…§å®¹å’Œæª¢ç´¢çµæœè¨ˆç®—ç›¸é—œæ€§
        relevant_chunks = []
        query_lower = query.lower()
        expanded_query_lower = expanded_query.lower()
        
        # æå–æŸ¥è©¢ä¸­çš„é—œéµä¿¡æ¯
        import re
        
        # æå–æ³•æ¢è™Ÿç¢¼
        article_patterns = [
            r'ç¬¬\s*(\d+)\s*æ¢',
            r'æ¢\s*(\d+)',
            r'article\s*(\d+)',
        ]
        
        article_numbers = []
        for pattern in article_patterns:
            matches = re.findall(pattern, query)
            article_numbers.extend([int(m) for m in matches])
        
        # æå–æ³•å¾‹åç¨±
        law_keywords = []
        law_patterns = ['è‘—ä½œæ¬Šæ³•', 'å•†æ¨™æ³•', 'å°ˆåˆ©æ³•', 'æ°‘æ³•', 'åˆ‘æ³•']
        for law in law_patterns:
            if law in query:
                law_keywords.append(law)
        
        # æª¢æ¸¬æŸ¥è©¢é¡å‹
        has_explicit_article = len(article_numbers) > 0
        query_type = "explicit_article" if has_explicit_article else "semantic_query"
        
        print(f"ğŸ“‹ æŸ¥è©¢åˆ†æ: é¡å‹={query_type}, æ³•æ¢è™Ÿç¢¼={article_numbers}, æ³•å¾‹é—œéµå­—={law_keywords}")
        
        # åˆ¤æ–·æ¯å€‹æª¢ç´¢çµæœçš„ç›¸é—œæ€§ï¼ˆæ•´åˆæŸ¥è©¢æ“´å±•å’Œå¤šå±¤æ¬¡æª¢ç´¢ï¼‰
        for i, result in enumerate(results):
            content = result.get("content", "")
            content_lower = content.lower()
            
            relevance_score = 0
            relevance_reasons = []
            
            # è¨ˆç®—å±¤æ¬¡åŒ–ç›¸é—œæ€§ï¼ˆåŸºæ–¼è«–æ–‡çš„å¤šå±¤æ¬¡æ–¹æ³•ï¼‰
            hierarchical_analysis = calculate_hierarchical_relevance(query, result)
            aboutness_score = hierarchical_analysis["aboutness_score"]
            hierarchy_weight = hierarchical_analysis["hierarchy_weight"]
            hierarchy_level = hierarchical_analysis["hierarchy_level"]
            
            # å±¤æ¬¡åŒ–ç›¸é—œæ€§åŠ åˆ†
            if aboutness_score > 0:
                relevance_score += aboutness_score * hierarchy_weight * 0.5  # é©åº¦æ¬Šé‡
                relevance_reasons.append(f"å±¤æ¬¡åŒ–aboutness({hierarchy_level}):{aboutness_score:.1f}")
            
            # 1. æ³•æ¢è™Ÿç¢¼åŒ¹é…ï¼ˆæ¬Šé‡æœ€é«˜ï¼Œåƒ…é©ç”¨æ–¼æ˜ç¢ºæ³•æ¢æŸ¥è©¢ï¼‰
            if has_explicit_article:
                for article_num in article_numbers:
                    if f'ç¬¬{article_num}æ¢' in content or f'ç¬¬ {article_num} æ¢' in content:
                        relevance_score += 4  # æé«˜æ¬Šé‡
                        relevance_reasons.append(f"ç²¾ç¢ºåŒ¹é…æ³•æ¢{article_num}")
                        break
            
            # 2. æ³•å¾‹åç¨±åŒ¹é…
            for law in law_keywords:
                if law in content:
                    relevance_score += 2
                    relevance_reasons.append(f"åŒ¹é…æ³•å¾‹{law}")
            
            # 3. æŸ¥è©¢æ“´å±•åŒ¹é…ï¼ˆæ–°å¢ï¼‰
            expanded_words = set(expanded_query_lower.split())
            content_words = set(content_lower.split())
            expanded_matches = expanded_words.intersection(content_words)
            
            if len(expanded_matches) > 0:
                # è¨ˆç®—æ“´å±•åŒ¹é…çš„æ¬Šé‡
                expansion_weight = min(len(expanded_matches) * 0.8, 3.0)  # æœ€å¤š3åˆ†
                relevance_score += expansion_weight
                relevance_reasons.append(f"æ“´å±•åŒ¹é…{len(expanded_matches)}å€‹è©:{list(expanded_matches)[:3]}")
            
            # 4. é ˜åŸŸå°ˆå±¬æ¦‚å¿µåŒ¹é…ï¼ˆæ–°å¢ï¼‰- æ”¹é€²é‚è¼¯
            for domain_match in domain_matches[:5]:  # é™åˆ¶åŒ¹é…æ•¸é‡
                concept = domain_match.split("â†’")[-1]
                if concept in content:
                    # å°æ–¼"æ”¹ä½œ"æ¦‚å¿µï¼Œçµ¦äºˆæ›´é«˜æ¬Šé‡
                    if concept == "æ”¹ä½œ":
                        relevance_score += 2.5  # é«˜æ¬Šé‡
                        relevance_reasons.append(f"æ ¸å¿ƒæ¦‚å¿µ:{concept}")
                    else:
                        relevance_score += 1.5
                        relevance_reasons.append(f"é ˜åŸŸæ¦‚å¿µ:{concept}")
            
            # 5. ç›´æ¥æ¦‚å¿µåŒ¹é…ï¼ˆæ–°å¢ï¼‰
            if "æ”¹ä½œ" in content and ("æ”¹å¯«" in query_lower or "èªæ°£" in query_lower):
                relevance_score += 3.0  # æœ€é«˜æ¬Šé‡
                relevance_reasons.append("ç›´æ¥æ¦‚å¿µåŒ¹é…:æ”¹ä½œ")
            
            # 6. åŸå§‹æŸ¥è©¢é—œéµè©åŒ¹é…
            query_words = set(query_lower.split())
            original_matches = query_words.intersection(content_words)
            
            if len(original_matches) >= 1:
                relevance_score += len(original_matches) * 0.3  # è¼ƒä½æ¬Šé‡ï¼Œé¿å…é‡è¤‡è¨ˆç®—
                relevance_reasons.append(f"åŸå§‹åŒ¹é…{len(original_matches)}å€‹è©:{list(original_matches)}")
            
            # 7. ç›¸ä¼¼åº¦åˆ†æ•¸ï¼ˆèª¿æ•´é–¾å€¼ï¼‰
            if 'score' in result:
                if result['score'] > 0.3:  # é€²ä¸€æ­¥é™ä½é–¾å€¼
                    relevance_score += result['score'] * 1.5  # é©åº¦æ¬Šé‡
                    relevance_reasons.append(f"ç›¸ä¼¼åº¦{result['score']:.2f}")
            
            # 8. èªç¾©æŸ¥è©¢çš„ç‰¹æ®ŠåŠ åˆ†ï¼ˆæ›´åš´æ ¼çš„æ¢ä»¶ï¼‰
            if not has_explicit_article and relevance_score > 1.0:  # åªæœ‰ç•¶åŸºç¤åˆ†æ•¸å¤ é«˜æ™‚æ‰åŠ åˆ†
                relevance_score += 0.3  # é€²ä¸€æ­¥é™ä½é¡å¤–åŠ åˆ†
                relevance_reasons.append("èªç¾©æŸ¥è©¢åŠ åˆ†")
            
            # å‹•æ…‹é–¾å€¼ï¼šæ›´å¯¬é¬†çš„æ¨™æº–ä»¥è­˜åˆ¥ç›¸é—œå…§å®¹
            if has_explicit_article:
                base_threshold = 3.0  # æ˜ç¢ºæ³•æ¢æŸ¥è©¢çš„é–¾å€¼
            else:
                base_threshold = 1.5  # èªç¾©æŸ¥è©¢çš„é–¾å€¼ï¼Œç¢ºä¿èƒ½è­˜åˆ¥ç›¸é—œå…§å®¹
            
            # å¦‚æœæœ‰æŸ¥è©¢æ“´å±•ï¼Œé©åº¦é™ä½é–¾å€¼
            if query_expansion["expansion_ratio"] > 2.0:  # ç•¶æœ‰é¡¯è‘—æ“´å±•æ™‚
                base_threshold *= 0.7  # æ›´ç©æ¥µçš„èª¿æ•´
            
            if relevance_score >= base_threshold:
                relevant_chunks.append(i)
                print(f"   âœ… Chunk {i+1} ç›¸é—œ (åˆ†æ•¸:{relevance_score:.1f}): {relevance_reasons} - {content[:50]}...")
            else:
                print(f"   âŒ Chunk {i+1} ä¸ç›¸é—œ (åˆ†æ•¸:{relevance_score:.1f}): {content[:50]}...")
        
        print(f"ğŸ“Š æ‰¾åˆ° {len(relevant_chunks)} å€‹ç›¸é—œchunks: {relevant_chunks}")
        
        # è¨ˆç®—P@Kå’ŒR@K
        top_k_results = results[:k]
        relevant_in_top_k = 0
        
        for i, result in enumerate(top_k_results):
            if i in relevant_chunks:
                relevant_in_top_k += 1
        
        p_at_k = relevant_in_top_k / k if k > 0 else 0.0
        r_at_k = relevant_in_top_k / len(relevant_chunks) if relevant_chunks else 0.0
        
        print(f"ğŸ“ˆ è©•æ¸¬çµæœ: P@{k}={p_at_k:.3f}, R@{k}={r_at_k:.3f}")
        
        return {
            "p_at_k": p_at_k,
            "r_at_k": r_at_k,
            "relevant_chunks_count": len(relevant_chunks),
            "relevant_chunks_indices": relevant_chunks,
            "query_analysis": {
                "query_type": query_type,
                "article_numbers": article_numbers,
                "law_keywords": law_keywords,
                "total_results": len(results),
                "threshold_used": base_threshold,
                "expansion_ratio": query_expansion["expansion_ratio"]
            },
            "query_expansion": {
                "original_query": query,
                "expanded_query": expanded_query,
                "detected_domains": detected_domains,
                "expansion_ratio": query_expansion["expansion_ratio"],
                "domain_matches": domain_matches[:10]  # é™åˆ¶è¿”å›æ•¸é‡
            },
            "note": f"æ™ºèƒ½åˆ†æ({query_type}+æŸ¥è©¢æ“´å±•)ï¼Œæ‰¾åˆ°{len(relevant_chunks)}å€‹ç›¸é—œçµæœ"
        }
        
    except Exception as e:
        print(f"âŒ è¨ˆç®—æª¢ç´¢æŒ‡æ¨™æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        import traceback
        traceback.print_exc()
        return {"p_at_k": 0.0, "r_at_k": 0.0, "error": str(e)}


def load_qa_data() -> List[Dict]:
    """è¼‰å…¥ QA æ•¸æ“š"""
    try:
        import json
        import os
        
        # å˜—è©¦è¼‰å…¥ä¸åŒçš„ QA æ–‡ä»¶ï¼ˆæŒ‰å„ªå…ˆç´šæ’åºï¼‰
        qa_files = [
            "QA/qa_gold.json",  # å„ªå…ˆä½¿ç”¨qa_gold.json
            "QA/copyright.json",
            "QA/copyright_p.json",
            "QA/copyright_n.json"
        ]
        
        # ç²å–é …ç›®æ ¹ç›®éŒ„
        current_dir = os.path.dirname(__file__)
        project_root = os.path.join(current_dir, "..", "..")
        project_root = os.path.abspath(project_root)
        
        print(f"ğŸ” æ­£åœ¨è¼‰å…¥QAæ•¸æ“šï¼Œé …ç›®æ ¹ç›®éŒ„: {project_root}")
        
        for qa_file in qa_files:
            qa_path = os.path.join(project_root, qa_file)
            print(f"   å˜—è©¦è¼‰å…¥: {qa_path}")
            
            if os.path.exists(qa_path):
                try:
                    with open(qa_path, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                        if isinstance(data, list) and len(data) > 0:
                            print(f"âœ… æˆåŠŸè¼‰å…¥ {qa_file}ï¼Œå…± {len(data)} æ¢QAæ•¸æ“š")
                            return data
                        else:
                            print(f"âš ï¸  {qa_file} æ ¼å¼ä¸æ­£ç¢ºæˆ–ç‚ºç©º")
                except Exception as e:
                    print(f"âŒ è¼‰å…¥ {qa_file} å¤±æ•—: {e}")
            else:
                print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {qa_path}")
        
        print("âŒ æ‰€æœ‰QAæ–‡ä»¶éƒ½ç„¡æ³•è¼‰å…¥")
        return []
    except Exception as e:
        print(f"âŒ è¼‰å…¥ QA æ•¸æ“šå¤±æ•—: {e}")
        return []


def is_article_match(chunk_content: str, article_number: int, article_suffix: int = None) -> bool:
    """æª¢æŸ¥chunkå…§å®¹æ˜¯å¦åŒ…å«æŒ‡å®šçš„æ³•æ¢è™Ÿç¢¼"""
    import re
    
    if not chunk_content or article_number is None:
        return False
    
    # æ¨™æº–åŒ–æ–‡æœ¬
    content = chunk_content.replace(" ", "").replace("ã€€", "")
    
    if article_suffix is not None:
        # ç¬¬10æ¢ä¹‹1 æˆ– ç¬¬10-1æ¢ æ ¼å¼
        patterns = [
            rf"ç¬¬\s*{article_number}\s*æ¢\s*(?:ä¹‹|-)\s*{article_suffix}",
            rf"ç¬¬\s*{article_number}\s*æ¢\s*ä¹‹\s*{article_suffix}",
            rf"ç¬¬\s*{article_number}\s*æ¢\s*-\s*{article_suffix}",
            rf"ç¬¬\s*{article_number}\s*æ¢ä¹‹{article_suffix}",
            rf"ç¬¬\s*{article_number}\s*æ¢-{article_suffix}"
        ]
    else:
        # ç¬¬3æ¢ æ ¼å¼ï¼ˆä¸åŒ…å«ä¹‹æˆ–-ï¼‰
        patterns = [
            rf"ç¬¬\s*{article_number}\s*æ¢(?![\dä¹‹-])",
            rf"ç¬¬\s*{article_number}\s*æ¢$",
            rf"ç¬¬\s*{article_number}\s*æ¢[^0-9ä¹‹-]"
        ]
    
    for pattern in patterns:
        if re.search(pattern, content):
            return True
    
    return False


def is_law_match(chunk_content: str, law_name: str) -> bool:
    """æª¢æŸ¥chunkå…§å®¹æ˜¯å¦åŒ…å«æŒ‡å®šçš„æ³•å¾‹åç¨±"""
    if not law_name or not chunk_content:
        return True  # å¦‚æœæ²’æœ‰æŒ‡å®šæ³•å¾‹åç¨±ï¼Œä¸é€²è¡ŒåŒ¹é…
    
    # æ³•å¾‹åç¨±è®Šé«”æ˜ å°„
    law_variants = {
        "è‘—ä½œæ¬Šæ³•": ["è‘—ä½œæ¬Šæ³•", "è‘—ä½œæ¬Š", "ç‰ˆæ¬Šæ³•", "ç‰ˆæ¬Š"],
        "å•†æ¨™æ³•": ["å•†æ¨™æ³•", "å•†æ¨™"],
        "å°ˆåˆ©æ³•": ["å°ˆåˆ©æ³•", "å°ˆåˆ©"],
        "æ°‘æ³•": ["æ°‘æ³•", "æ°‘äº‹"],
        "åˆ‘æ³•": ["åˆ‘æ³•", "åˆ‘äº‹"]
    }
    
    variants = law_variants.get(law_name, [law_name])
    content_lower = chunk_content.lower()
    
    # å¦‚æœchunkä¸­åŒ…å«ä»»ä½•æ³•å¾‹åç¨±è®Šé«”ï¼Œå‰‡åŒ¹é…
    if any(variant in content_lower for variant in variants):
        return True
    
    # å¦‚æœchunkä¸­æ²’æœ‰æ˜ç¢ºçš„æ³•å¾‹åç¨±ï¼Œä½†åŒ…å«æ³•æ¢è™Ÿç¢¼ï¼Œä¹Ÿèªç‚ºåŒ¹é…
    # é€™æ˜¯å› ç‚ºæ³•æ¢å…§å®¹æœ¬èº«å¯èƒ½ä¸åŒ…å«æ³•å¾‹åç¨±
    import re
    if re.search(r'ç¬¬\s*\d+\s*æ¢', content_lower):
        return True
    
    return False


def is_relevant_chunk(chunk_content: str, gold_info: Dict[str, Any]) -> bool:
    """åˆ¤æ–·chunkæ˜¯å¦èˆ‡goldæ¨™æº–ç›¸é—œ"""
    if not chunk_content or not gold_info:
        return False
    
    # æ³•æ¢è™Ÿç¢¼åŒ¹é…ï¼ˆå¿…é ˆï¼‰
    article_number = gold_info.get("article_number")
    article_suffix = gold_info.get("article_suffix")
    
    if article_number is None:
        return False  # æ²’æœ‰æ³•æ¢è™Ÿç¢¼ï¼Œç„¡æ³•åˆ¤æ–·ç›¸é—œæ€§
    
    article_match = is_article_match(chunk_content, article_number, article_suffix)
    if not article_match:
        return False
    
    # æ³•å¾‹åç¨±åŒ¹é…ï¼ˆåŠ åˆ†é …ï¼‰
    law_name = gold_info.get("law", "")
    law_match = is_law_match(chunk_content, law_name)
    
    return law_match


def extract_articles_from_text(text: str) -> List[str]:
    """å¾æ–‡æœ¬ä¸­æå–æ³•æ¢ä¿¡æ¯"""
    import re
    
    articles = []
    
    # åŒ¹é… "ç¬¬Xæ¢" æ¨¡å¼
    patterns = [
        r"ç¬¬(\d+)æ¢",
        r"ç¬¬(\d+)æ¢ä¹‹(\d+)",
        r"ç¬¬(\d+)-(\d+)æ¢"
    ]
    
    for pattern in patterns:
        matches = re.findall(pattern, text)
        for match in matches:
            if isinstance(match, tuple):
                if len(match) == 2:
                    if match[1]:  # æœ‰ä¹‹N
                        articles.append(f"ç¬¬{match[0]}æ¢ä¹‹{match[1]}")
                    else:  # ç¯„åœ
                        articles.append(f"ç¬¬{match[0]}-{match[1]}æ¢")
                else:
                    articles.append(f"ç¬¬{match[0]}æ¢")
            else:
                articles.append(f"ç¬¬{match}æ¢")
    
    return list(set(articles))  # å»é‡


def asyncio_run(coro):
    import asyncio
    try:
        loop = asyncio.get_running_loop()
    except RuntimeError:
        loop = None
    if loop and loop.is_running():
        # create task and wait
        return asyncio.run_coroutine_threadsafe(coro, loop).result()
    return asyncio.run(coro)


@app.post("/api/retrieve")
def retrieve(req: RetrieveRequest):
    if store.embeddings is None:
        return JSONResponse(status_code=400, content={"error": "run /embed first"})
    
    # è¨ˆç®—ç›¸ä¼¼åº¦ä¸¦æ’åºï¼ˆåªä½¿ç”¨å¯†é›†å‘é‡ï¼‰
    idxs, sims = rank_with_dense_vectors(req.query, req.k)

    # Use the same order as built in /embed
    chunks_flat = store.chunks_flat
    mapping_doc_ids = store.chunk_doc_ids

    results = []
    for rank, (i, score) in enumerate(zip(idxs, sims), start=1):
        if i < 0 or i >= len(chunks_flat):
            continue
        
        # ç²å–æ–‡æª”ä¿¡æ¯
        doc_id = mapping_doc_ids[i]
        doc = store.docs.get(doc_id)
        
        # åŸºæœ¬çµæœ
        result = {
            "rank": rank,
            "score": float(score),
            "doc_id": doc_id,
            "chunk_index": i,
            "content": chunks_flat[i][:2000],
        }
        
        # å¦‚æœæœ‰çµæ§‹åŒ–chunksï¼Œæ·»åŠ metadata
        if doc and hasattr(doc, 'structured_chunks') and doc.structured_chunks and i < len(doc.structured_chunks):
            structured_chunk = doc.structured_chunks[i]
            result["metadata"] = structured_chunk.get("metadata", {})
            result["chunk_id"] = structured_chunk.get("chunk_id", "")
            
            # æ·»åŠ æ³•å¾‹çµæ§‹ä¿¡æ¯
            metadata = structured_chunk.get("metadata", {})
            result["legal_structure"] = {
                "id": metadata.get("id", ""),
                "spans": metadata.get("spans", {}),
                "page_range": metadata.get("page_range", {})
            }
        
        results.append(result)
    
    # è¨ˆç®— P@K å’Œ R@Kï¼ˆå¦‚æœæœ‰ QA æ•¸æ“šï¼‰
    metrics = calculate_retrieval_metrics(req.query, results, req.k)
    
    # åˆ¤æ–· embedding provider å’Œ modelï¼ˆä¸å†æ”¯æŒ TF-IDFï¼‰
    if USE_GEMINI_EMBEDDING and GOOGLE_API_KEY:
        embedding_provider = "gemini"
        embedding_model = "gemini-embedding-001"
    elif USE_BGE_M3_EMBEDDING and SENTENCE_TRANSFORMERS_AVAILABLE:
        embedding_provider = "bge-m3"
        embedding_model = "BAAI/bge-m3"
    else:
        embedding_provider = "unknown"
        embedding_model = "unknown"

    return {
        "query": req.query, 
        "k": req.k, 
        "results": results,
        "metrics": metrics,
        "embedding_provider": embedding_provider,
        "embedding_model": embedding_model
    }


@app.post("/api/hierarchical-retrieve")
def hierarchical_retrieve(req: RetrieveRequest):
    """å¤šå±¤æ¬¡æª¢ç´¢ï¼šåŸºæ–¼è«–æ–‡çš„å¤šå±¤æ¬¡åµŒå…¥æª¢ç´¢æ–¹æ³•"""
    if store.embeddings is None:
        return JSONResponse(status_code=400, content={"error": "run /embed first"})
    
    # ç²å–æ‰€æœ‰ chunks å’Œ metadata
    chunks_flat = store.chunks_flat
    mapping_doc_ids = store.chunk_doc_ids
    
    if not chunks_flat:
        return JSONResponse(status_code=400, content={"error": "no chunks available"})
    
    # æ§‹å»ºå±¤æ¬¡åŒ–ç¯€é»
    hierarchical_nodes = []
    for i, (chunk, doc_id) in enumerate(zip(chunks_flat, mapping_doc_ids)):
        doc = store.docs.get(doc_id)
        metadata = {}
        
        # æå–å±¤æ¬¡åŒ–metadata
        if doc and hasattr(doc, 'structured_chunks') and doc.structured_chunks and i < len(doc.structured_chunks):
            structured_chunk = doc.structured_chunks[i]
            metadata = structured_chunk.get("metadata", {})
            
            # ç¢ºå®šå±¤æ¬¡ç´šåˆ¥
            hierarchy_level = "article"  # é»˜èª
            if metadata.get("article"):
                hierarchy_level = "article"
            elif metadata.get("section"):
                hierarchy_level = "section"
            elif metadata.get("chapter"):
                hierarchy_level = "chapter"
            
            metadata["hierarchy_level"] = hierarchy_level
        
        hierarchical_nodes.append({
            "content": chunk,
            "metadata": metadata,
            "doc_id": doc_id,
            "chunk_index": i
        })
    
    # å¤šå±¤æ¬¡æª¢ç´¢é‚è¼¯ - æ”¹é€²ç‰ˆ
    # 1. å…ˆæŒ‰å±¤æ¬¡åˆ†çµ„ï¼ˆåŸºæ–¼å…§å®¹åˆ†æï¼‰
    article_nodes = []
    section_nodes = []
    chapter_nodes = []
    general_nodes = []
    
    for node in hierarchical_nodes:
        content = node.get("content", "")
        hierarchy_level = detect_content_hierarchy(content)
        
        if hierarchy_level == "article":
            article_nodes.append(node)
        elif hierarchy_level == "section":
            section_nodes.append(node)
        elif hierarchy_level == "chapter":
            chapter_nodes.append(node)
        else:
            general_nodes.append(node)
    
    print(f"ğŸ” å±¤æ¬¡åˆ†çµ„: æ³•æ¢{len(article_nodes)}å€‹, ç¯€{len(section_nodes)}å€‹, ç« {len(chapter_nodes)}å€‹, ä¸€èˆ¬{len(general_nodes)}å€‹")
    
    # 2. å°æ¯å€‹å±¤æ¬¡é€²è¡Œæª¢ç´¢
    all_results = []
    
    # æ³•æ¢ç´šåˆ¥æª¢ç´¢ï¼ˆæœ€é«˜ç²¾åº¦ï¼‰
    if article_nodes:
        article_indices = [i for i, node in enumerate(hierarchical_nodes) if node in article_nodes]
        if article_indices:
            article_idxs, article_sims = rank_with_dense_vectors(req.query, k=min(len(article_indices), req.k * 2))
            for idx, sim in zip(article_idxs, article_sims):
                if idx in article_indices:
                    node = hierarchical_nodes[idx]
                    all_results.append({
                        "rank": len(all_results) + 1,
                        "score": float(sim),
                        "doc_id": node["doc_id"],
                        "chunk_index": idx,
                        "content": node["content"][:2000],
                        "metadata": node["metadata"],
                        "hierarchy_level": "article",
                        "hierarchy_weight": 1.0
                    })
    
    # ç¯€ç´šåˆ¥æª¢ç´¢ï¼ˆä¸­ç­‰ç²¾åº¦ï¼‰
    if section_nodes and len(all_results) < req.k:
        section_indices = [i for i, node in enumerate(hierarchical_nodes) if node in section_nodes]
        if section_indices:
            section_idxs, section_sims = rank_with_dense_vectors(req.query, k=min(len(section_indices), req.k))
            for idx, sim in zip(section_idxs, section_sims):
                if idx in section_indices and len(all_results) < req.k:
                    node = hierarchical_nodes[idx]
                    all_results.append({
                        "rank": len(all_results) + 1,
                        "score": float(sim) * 0.8,  # ç¯€ç´šåˆ¥æ¬Šé‡
                        "doc_id": node["doc_id"],
                        "chunk_index": idx,
                        "content": node["content"][:2000],
                        "metadata": node["metadata"],
                        "hierarchy_level": "section",
                        "hierarchy_weight": 0.8
                    })
    
    # ç« ç´šåˆ¥æª¢ç´¢ï¼ˆè¼ƒä½ç²¾åº¦ä½†å»£åº¦æ›´å¤§ï¼‰
    if chapter_nodes and len(all_results) < req.k:
        chapter_indices = [i for i, node in enumerate(hierarchical_nodes) if node in chapter_nodes]
        if chapter_indices:
            chapter_idxs, chapter_sims = rank_with_dense_vectors(req.query, k=min(len(chapter_indices), req.k))
            for idx, sim in zip(chapter_idxs, chapter_sims):
                if idx in chapter_indices and len(all_results) < req.k:
                    node = hierarchical_nodes[idx]
                    all_results.append({
                        "rank": len(all_results) + 1,
                        "score": float(sim) * 0.6,  # ç« ç´šåˆ¥æ¬Šé‡
                        "doc_id": node["doc_id"],
                        "chunk_index": idx,
                        "content": node["content"][:2000],
                        "metadata": node["metadata"],
                        "hierarchy_level": "chapter",
                        "hierarchy_weight": 0.6
                    })
    
    # å–å‰kå€‹çµæœ
    results = all_results[:req.k]
    
    # è¨ˆç®—å¤šå±¤æ¬¡æª¢ç´¢æŒ‡æ¨™
    metrics = calculate_retrieval_metrics(req.query, results, req.k)
    
    # æ·»åŠ å¤šå±¤æ¬¡æª¢ç´¢ç‰¹å®šä¿¡æ¯
    hierarchy_stats = {
        "article_results": len([r for r in results if r.get("hierarchy_level") == "article"]),
        "section_results": len([r for r in results if r.get("hierarchy_level") == "section"]),
        "chapter_results": len([r for r in results if r.get("hierarchy_level") == "chapter"])
    }
    
    metrics["hierarchical_analysis"] = hierarchy_stats
    metrics["note"] = f"å¤šå±¤æ¬¡æª¢ç´¢: æ³•æ¢{hierarchy_stats['article_results']}å€‹, ç¯€{hierarchy_stats['section_results']}å€‹, ç« {hierarchy_stats['chapter_results']}å€‹"
    
    # åˆ¤æ–· embedding provider å’Œ model
    embedding_provider = "gemini"
    embedding_model = "text-embedding-004"
    
    return {
        "results": results,
        "metrics": metrics,
        "embedding_provider": embedding_provider,
        "embedding_model": embedding_model
    }


@app.post("/api/multi-level-retrieve")
def multi_level_retrieve(req: RetrieveRequest):
    """å¤šå±¤æ¬¡æª¢ç´¢ï¼šåŸºæ–¼æŸ¥è©¢åˆ†é¡çš„æ™ºèƒ½å±¤æ¬¡é¸æ“‡æª¢ç´¢"""
    # æª¢æŸ¥æ˜¯å¦æœ‰å¯ç”¨çš„å¤šå±¤æ¬¡embedding
    if not store.has_multi_level_embeddings():
        return JSONResponse(
            status_code=400, 
            content={"error": "Multi-level embeddings not available. Please run /api/multi-level-embed first."}
        )
    
    # åˆ†ææŸ¥è©¢ä¸¦åˆ†é¡
    query_analysis = get_query_analysis(req.query)
    recommended_level = query_analysis['recommended_level']
    query_type = query_analysis['query_type']
    confidence = query_analysis['confidence']
    
    # ç²å–å¯ç”¨çš„embeddingå±¤æ¬¡
    available_levels = store.get_available_levels()
    print(f"ğŸ” æŸ¥è©¢åˆ†æï¼šé¡å‹={query_type}, ç½®ä¿¡åº¦={confidence:.3f}, æ¨è–¦å±¤æ¬¡={recommended_level}")
    print(f"ğŸ“Š å¯ç”¨å±¤æ¬¡: {available_levels}")
    
    # æª¢æŸ¥æ¨è–¦å±¤æ¬¡æ˜¯å¦å¯ç”¨ï¼Œå¦‚æœä¸å¯ç”¨å‰‡é¸æ“‡æœ€ä½³å¯ç”¨å±¤æ¬¡
    if recommended_level not in available_levels:
        # æŒ‰å„ªå…ˆç´šé¸æ“‡å¯ç”¨çš„å±¤æ¬¡
        fallback_levels = ['basic_unit', 'basic_unit_component', 'enumeration', 'basic_unit_hierarchy', 'document_component', 'document']
        for fallback_level in fallback_levels:
            if fallback_level in available_levels:
                recommended_level = fallback_level
                print(f"âš ï¸  æ¨è–¦å±¤æ¬¡ {query_analysis['recommended_level']} ä¸å¯ç”¨ï¼Œä½¿ç”¨å‚™é¸å±¤æ¬¡: {recommended_level}")
                break
    
    # ç²å–æ¨è–¦å±¤æ¬¡çš„embedding
    level_data = store.get_multi_level_embeddings(recommended_level)
    if not level_data:
        return JSONResponse(
            status_code=400, 
            content={"error": f"No embeddings available for level: {recommended_level}. Available levels: {available_levels}"}
        )
    
    vectors = level_data['embeddings']
    chunks = level_data['chunks']
    doc_ids = level_data['doc_ids']
    metadata = level_data.get('metadata', {})
    
    print(f"ğŸ“Š ä½¿ç”¨å±¤æ¬¡ '{recommended_level}' é€²è¡Œæª¢ç´¢ï¼Œå…± {len(chunks)} å€‹chunks")
    
    # åŸ·è¡Œæª¢ç´¢
    try:
        import numpy as np
        
        # æª¢æ¸¬å­˜å„²çš„embeddingæ¨¡å‹ä¿¡æ¯
        embedding_provider = metadata.get('provider')
        embedding_dimension = metadata.get('dimension')
        
        if embedding_provider:
            print(f"ğŸ” æª¢æ¸¬åˆ°å­˜å„²çš„embeddingæä¾›è€…: {embedding_provider}, ç¶­åº¦: {embedding_dimension}")
        
        # æ ¹æ“šå­˜å„²çš„embeddingæ¨¡å‹é¸æ“‡æŸ¥è©¢å‘é‡åŒ–æ–¹æ³•
        query_vector = None
        if embedding_provider == 'gemini' or (not embedding_provider and USE_GEMINI_EMBEDDING and GOOGLE_API_KEY):
            query_vector = asyncio.run(embed_gemini([req.query]))[0]
            print(f"âœ… ä½¿ç”¨Geminiç”ŸæˆæŸ¥è©¢å‘é‡ï¼Œç¶­åº¦: {len(query_vector)}")
        elif embedding_provider == 'bge-m3' or (not embedding_provider and USE_BGE_M3_EMBEDDING and SENTENCE_TRANSFORMERS_AVAILABLE):
            query_vector = embed_bge_m3([req.query])[0]
            print(f"âœ… ä½¿ç”¨BGE-M3ç”ŸæˆæŸ¥è©¢å‘é‡ï¼Œç¶­åº¦: {len(query_vector)}")
        else:
            return JSONResponse(
                status_code=500,
                content={"error": "No embedding method available for query"}
            )
        
        # é©—è­‰ç¶­åº¦åŒ¹é…
        if embedding_dimension and len(query_vector) != embedding_dimension:
            print(f"âš ï¸ è­¦å‘Šï¼šæŸ¥è©¢å‘é‡ç¶­åº¦({len(query_vector)})èˆ‡å­˜å„²å‘é‡ç¶­åº¦({embedding_dimension})ä¸åŒ¹é…")
            return JSONResponse(
                status_code=500,
                content={"error": f"Dimension mismatch: query vector has {len(query_vector)} dimensions but stored embeddings have {embedding_dimension} dimensions. Please re-run /api/multi-level-embed with the current embedding provider."}
            )
        
        # è¨ˆç®—ç›¸ä¼¼åº¦
        if isinstance(vectors, list):
            vectors = np.array(vectors)
        if isinstance(query_vector, list):
            query_vector = np.array(query_vector)
        
        similarities = cosine_similarity([query_vector], vectors)[0]
        
        # ç²å–top-kçµæœ
        top_indices = np.argsort(similarities)[::-1][:req.k]
        
        results = []
        for i, idx in enumerate(top_indices):
            doc_id = doc_ids[idx]
            doc = store.get_doc(doc_id)
            
            result = {
                "rank": i + 1,
                "content": chunks[idx],
                "similarity": float(similarities[idx]),
                "doc_id": doc_id,
                "doc_name": doc.filename if doc else "Unknown",
                "chunk_index": idx,
                "metadata": {
                    "level": recommended_level,
                    "query_type": query_type,
                    "confidence": confidence
                }
            }
            results.append(result)
        
        # è¨ˆç®—æª¢ç´¢æŒ‡æ¨™
        metrics = {
            "total_chunks_searched": len(chunks),
            "query_type": query_type,
            "recommended_level": recommended_level,
            "classification_confidence": confidence,
            "embedding_provider": "gemini" if USE_GEMINI_EMBEDDING else "bge-m3",
            "embedding_model": "text-embedding-004" if USE_GEMINI_EMBEDDING else "BAAI/bge-m3"
        }
        
        return {
            "results": results,
            "metrics": metrics,
            "query_analysis": query_analysis
        }
        
    except Exception as e:
        print(f"âŒ å¤šå±¤æ¬¡æª¢ç´¢éŒ¯èª¤: {e}")
        return JSONResponse(
            status_code=500,
            content={"error": f"Multi-level retrieval failed: {str(e)}"}
        )


@app.post("/api/query-analysis")
def analyze_query(req: RetrieveRequest):
    """æŸ¥è©¢åˆ†æç«¯é»ï¼šåˆ†ææŸ¥è©¢é¡å‹ä¸¦æ¨è–¦æª¢ç´¢ç­–ç•¥"""
    query_analysis = get_query_analysis(req.query)
    
    # æª¢æŸ¥å¯ç”¨çš„embeddingå±¤æ¬¡
    available_levels = store.get_available_levels()
    has_multi_level = store.has_multi_level_embeddings()
    
    # ç”Ÿæˆæª¢ç´¢å»ºè­°
    retrieval_suggestions = {
        "recommended_method": "multi-level" if has_multi_level else "standard",
        "recommended_level": query_analysis['recommended_level'],
        "available_levels": available_levels,
        "alternative_levels": [level for level in available_levels if level != query_analysis['recommended_level']]
    }
    
    return {
        "query_analysis": query_analysis,
        "retrieval_suggestions": retrieval_suggestions,
        "system_status": {
            "has_multi_level_embeddings": has_multi_level,
            "has_standard_embeddings": store.embeddings is not None
        }
    }


@app.post("/api/multi-level-fusion-retrieve")
def multi_level_fusion_retrieve(req: MultiLevelFusionRequest):
    """å¤šå±¤æ¬¡èåˆæª¢ç´¢ï¼šå¾æ‰€æœ‰å±¤æ¬¡æª¢ç´¢ä¸¦èåˆçµæœ"""
    # æª¢æŸ¥æ˜¯å¦æœ‰å¯ç”¨çš„å¤šå±¤æ¬¡embedding
    if not store.has_multi_level_embeddings():
        return JSONResponse(
            status_code=400, 
            content={"error": "Multi-level embeddings not available. Please run /api/multi-level-embed first."}
        )
    
    # åˆ†ææŸ¥è©¢
    query_analysis = get_query_analysis(req.query)
    available_levels = store.get_available_levels()
    
    print(f"ğŸ” å¤šå±¤æ¬¡èåˆæª¢ç´¢ï¼šæŸ¥è©¢é¡å‹={query_analysis['query_type']}, å¯ç”¨å±¤æ¬¡={available_levels}")
    
    # å¦‚æœæ²’æœ‰å¯ç”¨çš„å±¤æ¬¡ï¼Œè¿”å›éŒ¯èª¤
    if not available_levels:
        return JSONResponse(
            status_code=400,
            content={"error": "No multi-level embeddings available. Please run /api/multi-level-embed first."}
        )
    
    # å¾æ‰€æœ‰å¯ç”¨å±¤æ¬¡æª¢ç´¢
    level_results = {}
    total_chunks_searched = 0
    
    try:
        import numpy as np
        
        # æª¢æ¸¬ç¬¬ä¸€å€‹å¯ç”¨å±¤æ¬¡çš„æ¨¡å‹ä¿¡æ¯ï¼Œç¢ºä¿ä½¿ç”¨ç›¸åŒçš„æ¨¡å‹
        first_level = available_levels[0] if available_levels else None
        embedding_provider = None
        embedding_dimension = None
        
        if first_level:
            first_level_data = store.get_multi_level_embeddings(first_level)
            if first_level_data and 'metadata' in first_level_data:
                embedding_provider = first_level_data['metadata'].get('provider')
                embedding_dimension = first_level_data['metadata'].get('dimension')
                print(f"ğŸ” æª¢æ¸¬åˆ°å­˜å„²çš„embeddingæä¾›è€…: {embedding_provider}, ç¶­åº¦: {embedding_dimension}")
        
        # æ ¹æ“šå­˜å„²çš„embeddingæ¨¡å‹é¸æ“‡æŸ¥è©¢å‘é‡åŒ–æ–¹æ³•
        query_vector = None
        if embedding_provider == 'gemini' or (not embedding_provider and USE_GEMINI_EMBEDDING and GOOGLE_API_KEY):
            query_vector = asyncio.run(embed_gemini([req.query]))[0]
            print(f"âœ… ä½¿ç”¨Geminiç”ŸæˆæŸ¥è©¢å‘é‡ï¼Œç¶­åº¦: {len(query_vector)}")
        elif embedding_provider == 'bge-m3' or (not embedding_provider and USE_BGE_M3_EMBEDDING and SENTENCE_TRANSFORMERS_AVAILABLE):
            query_vector = embed_bge_m3([req.query])[0]
            print(f"âœ… ä½¿ç”¨BGE-M3ç”ŸæˆæŸ¥è©¢å‘é‡ï¼Œç¶­åº¦: {len(query_vector)}")
        else:
            return JSONResponse(
                status_code=500,
                content={"error": "No embedding method available for query"}
            )
        
        # é©—è­‰ç¶­åº¦åŒ¹é…
        if embedding_dimension and len(query_vector) != embedding_dimension:
            print(f"âš ï¸ è­¦å‘Šï¼šæŸ¥è©¢å‘é‡ç¶­åº¦({len(query_vector)})èˆ‡å­˜å„²å‘é‡ç¶­åº¦({embedding_dimension})ä¸åŒ¹é…")
            return JSONResponse(
                status_code=500,
                content={"error": f"Dimension mismatch: query vector has {len(query_vector)} dimensions but stored embeddings have {embedding_dimension} dimensions. Please re-run /api/multi-level-embed with the current embedding provider."}
            )
        
        # å°æ¯å€‹å±¤æ¬¡é€²è¡Œæª¢ç´¢
        for level_name in available_levels:
            level_data = store.get_multi_level_embeddings(level_name)
            if not level_data:
                continue
            
            vectors = level_data['embeddings']
            chunks = level_data['chunks']
            doc_ids = level_data['doc_ids']
            
            print(f"ğŸ“Š æª¢ç´¢å±¤æ¬¡ '{level_name}'ï¼š{len(chunks)} å€‹chunks")
            total_chunks_searched += len(chunks)
            
            # è¨ˆç®—ç›¸ä¼¼åº¦
            if isinstance(vectors, list):
                vectors = np.array(vectors)
            if isinstance(query_vector, list):
                query_vector = np.array(query_vector)
            
            similarities = cosine_similarity([query_vector], vectors)[0]
            
            # ç²å–top-kçµæœ
            top_indices = np.argsort(similarities)[::-1][:req.k]
            
            level_results[level_name] = []
            for i, idx in enumerate(top_indices):
                doc_id = doc_ids[idx]
                doc = store.get_doc(doc_id)
                
                result = {
                    "rank": int(i + 1),
                    "content": chunks[idx],
                    "similarity": float(similarities[idx]),
                    "doc_id": doc_id,
                    "doc_name": doc.filename if doc else "Unknown",
                    "chunk_index": int(idx),
                    "metadata": {
                        "level": level_name,
                        "query_type": query_analysis['query_type'],
                        "confidence": query_analysis['confidence']
                    }
                }
                level_results[level_name].append(result)
        
        if not level_results:
            return JSONResponse(
                status_code=400,
                content={"error": "No results found from any level"}
            )
        
        # å‰µå»ºèåˆé…ç½®
        fusion_config = FusionConfig(
            strategy=req.fusion_strategy,
            level_weights=req.level_weights,
            similarity_threshold=req.similarity_threshold,
            max_results=req.max_results,
            normalize_scores=req.normalize_scores
        )
        
        # åŸ·è¡Œçµæœèåˆ
        print(f"ğŸ”„ åŸ·è¡Œçµæœèåˆï¼šç­–ç•¥={req.fusion_strategy}")
        fused_results = fuse_multi_level_results(level_results, fusion_config)
        
        # è¨ˆç®—èåˆæŒ‡æ¨™
        fusion_metrics = {
            "total_chunks_searched": total_chunks_searched,
            "levels_searched": list(level_results.keys()),
            "fusion_strategy": req.fusion_strategy,
            "level_weights": req.level_weights or fusion_config.level_weights,
            "similarity_threshold": req.similarity_threshold,
            "max_results": req.max_results,
            "query_type": query_analysis['query_type'],
            "classification_confidence": query_analysis['confidence'],
            "embedding_provider": "gemini" if USE_GEMINI_EMBEDDING else "bge-m3",
            "embedding_model": "text-embedding-004" if USE_GEMINI_EMBEDDING else "BAAI/bge-m3"
        }
        
        # çµ±è¨ˆå„å±¤æ¬¡çš„è²¢ç»
        level_contributions = {}
        for level, results in level_results.items():
            level_contributions[level] = {
                "num_results": len(results),
                "avg_similarity": sum(r['similarity'] for r in results) / len(results) if results else 0,
                "max_similarity": max(r['similarity'] for r in results) if results else 0
            }
        
        fusion_metrics["level_contributions"] = level_contributions
        
        return {
            "results": fused_results,
            "metrics": fusion_metrics,
            "query_analysis": query_analysis,
            "level_results": level_results  # åŒ…å«åŸå§‹å„å±¤æ¬¡çµæœ
        }
        
    except Exception as e:
        print(f"âŒ å¤šå±¤æ¬¡èåˆæª¢ç´¢éŒ¯èª¤: {e}")
        return JSONResponse(
            status_code=500,
            content={"error": f"Multi-level fusion retrieval failed: {str(e)}"}
        )


@app.post("/api/hybrid-retrieve")
def hybrid_retrieve(req: RetrieveRequest):
    """HybridRAG æª¢ç´¢ï¼šçµåˆå‘é‡ç›¸ä¼¼åº¦å’Œæ³•å¾‹çµæ§‹è¦å‰‡"""
    if store.embeddings is None:
        return JSONResponse(status_code=400, content={"error": "run /embed first"})
    
    # ç²å–æ‰€æœ‰ chunks å’Œ metadata
    chunks_flat = store.chunks_flat
    mapping_doc_ids = store.chunk_doc_ids
    
    if not chunks_flat:
        return JSONResponse(status_code=400, content={"error": "no chunks available"})
    
    # æ§‹å»º nodes æ ¼å¼ä¾› hybrid_rank ä½¿ç”¨
    nodes = []
    for i, (chunk, doc_id) in enumerate(zip(chunks_flat, mapping_doc_ids)):
        doc = store.docs.get(doc_id)
        metadata = {}
        
        # å¦‚æœæœ‰çµæ§‹åŒ–chunksï¼Œæå–metadata
        if doc and hasattr(doc, 'structured_chunks') and doc.structured_chunks and i < len(doc.structured_chunks):
            structured_chunk = doc.structured_chunks[i]
            metadata = structured_chunk.get("metadata", {})
        
        nodes.append({
            "content": chunk,
            "metadata": metadata,
            "doc_id": doc_id,
            "chunk_index": i
        })
    
    # å…ˆç”¨å¯†é›†å‘é‡å¾—åˆ°æ¯å€‹ç¯€é»çš„å‘é‡åˆ†æ•¸
    # æˆ‘å€‘å°æ‰€æœ‰ç¯€é»é€²è¡Œç›¸ä¼¼åº¦è¨ˆç®—ï¼Œç„¶å¾Œåªå–å‰ k çš„çµæœåš Hybrid æ’åº
    dense_top_k = min(len(nodes), max(req.k * 4, req.k))
    all_vec_idxs, all_vec_sims = rank_with_dense_vectors(req.query, k=len(nodes))
    # æ˜ å°„å‡ºç¯€é»é †åºå°æ‡‰çš„åˆ†æ•¸ï¼Œåˆå§‹åŒ–ç‚º0
    node_vector_scores = [0.0] * len(nodes)
    for rank_idx, node_idx in enumerate(all_vec_idxs):
        node_vector_scores[node_idx] = float(all_vec_sims[rank_idx])

    # å–å‘é‡åˆ†æ•¸æœ€é«˜çš„å‰ dense_top_k ç¯€é»ä½œç‚º Hybrid å€™é¸
    top_vec_pairs = sorted(
        [(i, s) for i, s in enumerate(node_vector_scores)], key=lambda x: x[1], reverse=True
    )[:dense_top_k]
    candidate_nodes = [nodes[i] for i, _ in top_vec_pairs]
    candidate_scores = [s for _, s in top_vec_pairs]

    # ä½¿ç”¨ hybrid_rank é€²è¡Œæª¢ç´¢ï¼ˆå‘é‡åˆ†æ•¸ + metadata åŠ åˆ†ï¼‰
    config = HybridConfig(
        alpha=0.8,  # å‘é‡ç›¸ä¼¼åº¦æ¬Šé‡
        w_law_match=0.15,  # æ³•åå°é½Šæ¬Šé‡
        w_article_match=0.15,  # æ¢è™Ÿå°é½Šæ¬Šé‡
        w_keyword_hit=0.05,  # è¡“èªå‘½ä¸­æ¬Šé‡
        max_bonus=0.4  # æœ€å¤§åŠ åˆ†
    )

    hybrid_results = hybrid_rank(
        req.query, candidate_nodes, k=req.k, config=config, vector_scores=candidate_scores
    )
    
    # è½‰æ›ç‚ºæ¨™æº–æ ¼å¼
    results = []
    for rank, item in enumerate(hybrid_results, start=1):
        result = {
            "rank": rank,
            "score": item["score"],
            "vector_score": item["vector_score"],
            "bonus": item["bonus"],
            "doc_id": item["doc_id"],
            "chunk_index": item["chunk_index"],
            "content": item["content"][:2000],
            "metadata": item["metadata"]
        }
        
        # æ·»åŠ æ³•å¾‹çµæ§‹ä¿¡æ¯
        if item["metadata"]:
            result["legal_structure"] = {
                "id": item["metadata"].get("id", ""),
                "category": item["metadata"].get("category", ""),
                "article_label": item["metadata"].get("article_label", ""),
                "article_number": item["metadata"].get("article_number"),
                "article_suffix": item["metadata"].get("article_suffix"),
                "spans": item["metadata"].get("spans", {}),
                "page_range": item["metadata"].get("page_range", {})
            }
        
        results.append(result)
    
    # è¨ˆç®— P@K å’Œ R@Kï¼ˆå¦‚æœæœ‰ QA æ•¸æ“šï¼‰
    metrics = calculate_retrieval_metrics(req.query, results, req.k)
    
    # åˆ¤æ–· embedding provider å’Œ modelï¼ˆä¸å†æ”¯æŒ TF-IDFï¼‰
    if USE_GEMINI_EMBEDDING and GOOGLE_API_KEY:
        embedding_provider = "gemini"
        embedding_model = "gemini-embedding-001"
    elif USE_BGE_M3_EMBEDDING and SENTENCE_TRANSFORMERS_AVAILABLE:
        embedding_provider = "bge-m3"
        embedding_model = "BAAI/bge-m3"
    else:
        embedding_provider = "unknown"
        embedding_model = "unknown"
    
    return {
        "query": req.query, 
        "k": req.k, 
        "results": results,
        "method": "hybrid_rag",
        "metrics": metrics,
        "embedding_provider": embedding_provider,
        "embedding_model": embedding_model,
        "config": {
            "alpha": config.alpha,
            "w_law_match": config.w_law_match,
            "w_article_match": config.w_article_match,
            "w_keyword_hit": config.w_keyword_hit,
            "max_bonus": config.max_bonus
        }
    }


async def gemini_chat(messages: List[Dict[str, str]]) -> str:
    if not httpx:
        raise RuntimeError("httpx not available")
    
    # å„ªå…ˆä½¿ç”¨ GOOGLE_API_KEYï¼Œå¦‚æœæ²’æœ‰å‰‡ä½¿ç”¨ GEMINI_API_KEY
    api_key = GOOGLE_API_KEY or os.getenv("GEMINI_API_KEY")
    if not api_key:
        raise RuntimeError("GOOGLE_API_KEY or GEMINI_API_KEY not set")
    
    model = os.getenv("GOOGLE_CHAT_MODEL", "gemini-1.5-flash")
    # Use Generative Language API: models/{model}:generateContent
    url = f"https://generativelanguage.googleapis.com/v1beta/models/{model}:generateContent"
    # Convert messages to Gemini format
    contents = []
    for message in messages:
        contents.append({
            "parts": [{"text": message.get("content", "")}],
            "role": "user" if message.get("role") == "user" else "model"
        })
    
    payload = {
        "contents": contents,
        "generationConfig": {
            "temperature": 0.2,
            "maxOutputTokens": 2048
        }
    }
    
    headers = {
        "x-goog-api-key": api_key,
        "Content-Type": "application/json"
    }
    
    async with httpx.AsyncClient(timeout=60) as client:
        r = await client.post(url, headers=headers, json=payload)
        r.raise_for_status()
        data = r.json()
        # Extract response from new format
        if "candidates" in data and data["candidates"]:
            candidate = data["candidates"][0]
            if "content" in candidate and "parts" in candidate["content"]:
                return candidate["content"]["parts"][0].get("text", "").strip()
        return "No response generated"


def simple_extractive_answer(query: str, contexts: List[str]) -> str:
    """é‡å°ä¸­è‹±æ–‡æ”¹é€²çš„æ¥µç°¡æŠ½å–å¼å›ç­”ï¼š
    - æ”¯æ´ä¸­æ–‡æ–·å¥ï¼ˆã€‚ï¼ï¼Ÿï¼›ï¼‰èˆ‡æ›è¡Œ
    - åˆ†è©åŒæ™‚è€ƒæ…®è‹±æ–‡/æ•¸å­—è©èˆ‡ä¸­æ–‡å–®å­—
    - è‹¥ç„¡æ˜é¡¯é‡ç–Šï¼Œå›é€€è¼¸å‡ºå‰å¹¾å¥æœ€å‰é¢çš„å…§å®¹
    """
    import re
    from collections import Counter

    # 1) æ–·å¥ï¼ˆåŒæ™‚æ”¯æ´ä¸­è‹±æ¨™é»èˆ‡æ›è¡Œï¼‰
    def split_sentences(text: str) -> List[str]:
        # ä¿ç•™åŸæ–‡ç‰‡æ®µï¼Œé¿å…éåº¦åˆ‡ç¢
        # å…ˆæŒ‰æ›è¡Œæ‹†ï¼Œå†æŒ‰ä¸­æ–‡/è‹±æ–‡å¥æœ«æ¨™é»ç´°åˆ†
        parts: List[str] = []
        for seg in re.split(r"[\n\r]+", text):
            seg = seg.strip()
            if not seg:
                continue
            parts.extend([s.strip() for s in re.split(r"(?<=[ã€‚ï¼ï¼Ÿ!?ï¼›;])\s+", seg) if s.strip()])
        return parts

    # 2) ç°¡å–®åˆ†è©ï¼šè‹±æ–‡/æ•¸å­—è© + ä¸­æ–‡å–®å­—
    def tokenize(text: str) -> List[str]:
        text_norm = text.lower()
        en = re.findall(r"[a-z0-9_]+", text_norm)
        zh = re.findall(r"[\u4e00-\u9fff]", text_norm)
        return en + zh

    q_tokens = set(tokenize(query))
    if not q_tokens:
        q_tokens = set(query.lower())  # é€€åŒ–ç‚ºå­—ç¬¦é›†åˆ

    # 3) èšåˆæ‰€æœ‰ä¸Šä¸‹æ–‡çš„å¥å­
    sents: List[str] = []
    for ctx in contexts:
        sents.extend(split_sentences(ctx))

    # 4) è¨ˆåˆ†ï¼šé‡ç–Š token æ•¸é‡ + è¼•åº¦é•·åº¦å¹³è¡¡
    counts = Counter()
    for s in sents:
        t = tokenize(s)
        if not t:
            continue
        overlap = len(set(t) & q_tokens)
        if overlap > 0:
            # è¼•åº¦é¼“å‹µè¼ƒå®Œæ•´å¥å­
            counts[s] = overlap + min(len(s) / 200.0, 1.0)

    # 5) å›å‚³ï¼šæœ‰åŒ¹é…å‰‡å–å‰5å¥ï¼Œå¦å‰‡å›é€€å–æœ€å‰é¢å…§å®¹
    if counts:
        best = [s for s, _ in counts.most_common(5)]
        return " \n".join(best)

    # å›é€€ï¼šå–å‰å…©æ®µçš„å‰å…©å¥
    fallback: List[str] = []
    for ctx in contexts[:2]:
        ss = split_sentences(ctx)
        fallback.extend(ss[:2])
        if len(fallback) >= 4:
            break
    if fallback:
        return " \n".join(fallback[:4])
    return "No relevant answer found in context."


@app.post("/api/generate")
def generate(req: GenerateRequest):
    # ä½¿ç”¨ HybridRAGï¼ˆå‘é‡æª¢ç´¢ + metadata é—œéµå­—åŠ åˆ†ï¼‰å–å¾—ç”Ÿæˆä¸Šä¸‹æ–‡
    if store.embeddings is None:
        return JSONResponse(status_code=400, content={"error": "run /embed first"})

    # æ§‹å»º nodesï¼ˆèˆ‡ /api/hybrid-retrieve ä¿æŒä¸€è‡´ï¼‰
    chunks_flat = store.chunks_flat
    mapping_doc_ids = store.chunk_doc_ids
    if not chunks_flat:
        return JSONResponse(status_code=400, content={"error": "no chunks available"})

    nodes = []
    for i, (chunk, doc_id) in enumerate(zip(chunks_flat, mapping_doc_ids)):
        doc = store.docs.get(doc_id)
        metadata = {}
        if doc and hasattr(doc, 'structured_chunks') and doc.structured_chunks and i < len(doc.structured_chunks):
            structured_chunk = doc.structured_chunks[i]
            metadata = structured_chunk.get("metadata", {})
        nodes.append({
            "content": chunk,
            "metadata": metadata,
            "doc_id": doc_id,
            "chunk_index": i
        })

    # å…ˆç”¨å¯†é›†å‘é‡è¨ˆç®—æ‰€æœ‰ç¯€é»çš„ç›¸ä¼¼åº¦ï¼Œå–å‰ N åš Hybrid å€™é¸
    dense_top_k = min(len(nodes), max(req.top_k * 4, req.top_k))
    all_vec_idxs, all_vec_sims = rank_with_dense_vectors(req.query, k=len(nodes))
    node_vector_scores = [0.0] * len(nodes)
    for rank_idx, node_idx in enumerate(all_vec_idxs):
        node_vector_scores[node_idx] = float(all_vec_sims[rank_idx])
    top_vec_pairs = sorted(
        [(i, s) for i, s in enumerate(node_vector_scores)], key=lambda x: x[1], reverse=True
    )[:dense_top_k]
    candidate_nodes = [nodes[i] for i, _ in top_vec_pairs]
    candidate_scores = [s for _, s in top_vec_pairs]

    config = HybridConfig(
        alpha=0.8,
        w_law_match=0.15,
        w_article_match=0.15,
        w_keyword_hit=0.05,
        max_bonus=0.4,
    )
    hybrid_results = hybrid_rank(req.query, candidate_nodes, k=req.top_k, config=config, vector_scores=candidate_scores)

    # ç”Ÿæˆä½¿ç”¨çš„çµæœ
    results = []
    for rank, item in enumerate(hybrid_results, start=1):
        result = {
            "rank": rank,
            "score": item.get("score"),
            "vector_score": item.get("vector_score"),
            "bonus": item.get("bonus"),
            "doc_id": item.get("doc_id"),
            "chunk_index": item.get("chunk_index"),
            "content": item.get("content"),
        }
        md = (item.get("metadata") or {})
        if md:
            result["legal_structure"] = {
                "id": md.get("id", ""),
                "category": md.get("category", ""),
                "article_label": md.get("article_label", ""),
                "article_number": md.get("article_number"),
                "article_suffix": md.get("article_suffix"),
                "spans": md.get("spans", {}),
                "page_range": md.get("page_range", {}),
            }
        results.append(result)
    contexts = [item["content"] for item in results]

    # æ§‹å»ºçµæ§‹åŒ–ä¸Šä¸‹æ–‡ä¿¡æ¯
    structured_context = []
    legal_references = []
    
    for item in results:
        context_text = item["content"]
        
        # å¦‚æœæœ‰æ³•å¾‹çµæ§‹ä¿¡æ¯ï¼Œæ·»åŠ åˆ°ä¸Šä¸‹æ–‡ä¸­
        if "legal_structure" in item:
            legal_info = item["legal_structure"]
            law_name = legal_info.get("law_name", "")
            article = legal_info.get("article", "")
            item_ref = legal_info.get("item", "")
            sub_item = legal_info.get("sub_item", "")
            chunk_type = legal_info.get("chunk_type", "")
            
            # æ§‹å»ºæ³•å¾‹å¼•ç”¨
            legal_ref = f"{law_name}"
            if article:
                legal_ref += f" {article}"
            if item_ref:
                legal_ref += f" {item_ref}"
            if sub_item:
                legal_ref += f" {sub_item}"
            
            if legal_ref not in legal_references:
                legal_references.append(legal_ref)
            
            # æ·»åŠ çµæ§‹åŒ–ä¸Šä¸‹æ–‡
            structured_context.append(f"[{legal_ref}] {context_text}")
        else:
            structured_context.append(context_text)

    reasoning_steps = [
        {"type": "plan", "text": "Read query, identify entities and constraints."},
        {"type": "gather", "text": f"Collect top-{req.top_k} chunks as context."},
        {"type": "analyze", "text": f"Analyze legal structure: {', '.join(legal_references[:3])}."},
        {"type": "synthesize", "text": "Synthesize answer grounded in retrieved text with legal references."},
    ]

    if USE_GEMINI_COMPLETION:
        # æ§‹å»ºåŒ…å«æ³•å¾‹çµæ§‹ä¿¡æ¯çš„prompt
        system_prompt = """ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„æ³•å¾‹åŠ©æ‰‹ã€‚è«‹åŸºæ–¼æä¾›çš„æ³•å¾‹æ–‡æª”å…§å®¹å›ç­”å•é¡Œã€‚

é‡è¦è¦æ±‚ï¼š
1. åªä½¿ç”¨æä¾›çš„ä¸Šä¸‹æ–‡å…§å®¹å›ç­”å•é¡Œ
2. å¦‚æœç­”æ¡ˆæ¶‰åŠå…·é«”æ³•å¾‹æ¢æ–‡ï¼Œè«‹å¼•ç”¨ç›¸é—œçš„æ³•è¦åç¨±å’Œæ¢æ–‡è™Ÿç¢¼
3. å¦‚æœä¿¡æ¯ä¸è¶³ï¼Œè«‹æ˜ç¢ºèªªæ˜ä½ ä¸çŸ¥é“
4. å›ç­”è¦æº–ç¢ºã€å°ˆæ¥­ï¼Œç¬¦åˆæ³•å¾‹æ–‡æª”çš„è¡¨è¿°æ–¹å¼"""

        user_content = f"å•é¡Œ: {req.query}\n\n"
        
        if legal_references:
            user_content += f"ç›¸é—œæ³•è¦: {', '.join(legal_references)}\n\n"
        
        user_content += "æ³•å¾‹æ–‡æª”å…§å®¹:\n" + "\n---\n".join(structured_context)
        
        prompt = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_content},
        ]
        
        try:
            answer = asyncio_run(gemini_chat(prompt))
        except Exception as e:
            answer = f"Geminièª¿ç”¨å¤±æ•—: {e}. å›é€€åˆ°æå–å¼å›ç­”ã€‚\n" + simple_extractive_answer(req.query, contexts)
    else:
        answer = simple_extractive_answer(req.query, contexts)

    return {
        "query": req.query,
        "answer": answer,
        "contexts": results,
        "legal_references": legal_references,
        "steps": reasoning_steps,
    }


def merge_law_documents(law_documents: List[Dict[str, Any]]) -> Dict[str, Any]:
    """
    æ•´åˆå¤šå€‹æ³•å¾‹æ–‡æª”æˆä¸€å€‹çµ±ä¸€çš„JSONçµæ§‹
    
    åƒæ•¸:
    - law_documents: å¤šå€‹æ³•å¾‹æ–‡æª”çš„åˆ—è¡¨
    
    è¿”å›:
    - æ•´åˆå¾Œçš„æ³•å¾‹æ–‡æª”ï¼Œæ ¼å¼ç‚º {"laws": [...]}
    """
    if not law_documents:
        return {"laws": []}
    
    # ç¢ºä¿æ¯å€‹æ³•å¾‹æ–‡æª”éƒ½æœ‰å”¯ä¸€çš„IDå‰ç¶´
    merged_laws = []
    global_id_counter = 0
    
    for doc in law_documents:
        if not doc or "law_name" not in doc:
            continue
            
        law_name = doc["law_name"]
        law_prefix = f"{law_name}_{global_id_counter}"
        
        # å‰µå»ºæ–°çš„æ³•å¾‹æ–‡æª”çµæ§‹
        merged_law = {
            "law_name": law_name,
            "chapters": []
        }
        
        # è™•ç†ç« ç¯€
        chapters = doc.get("chapters", [])
        for chapter in chapters:
            chapter_name = chapter.get("chapter", "")
            merged_chapter = {
                "chapter": chapter_name,
                "sections": []
            }
            
            # è™•ç†ç¯€
            sections = chapter.get("sections", [])
            for section in sections:
                section_name = section.get("section", "")
                merged_section = {
                    "section": section_name,
                    "articles": []
                }
                
                # è™•ç†æ¢æ–‡
                articles = section.get("articles", [])
                for article in articles:
                    article_name = article.get("article", "")
                    merged_article = {
                        "article": article_name,
                        "content": article.get("content", ""),
                        "items": []
                    }
                    
                    # è™•ç†é …ç›® - æ”¯æ´æ–°çµæ§‹ (paragraphs) å’ŒèˆŠçµæ§‹ (items)
                    paragraphs = article.get("paragraphs", [])
                    items = article.get("items", [])  # ç›¸å®¹æ€§ï¼šitems å¯èƒ½æŒ‡å‘ paragraphs
                    
                    # ä½¿ç”¨ paragraphs å¦‚æœå­˜åœ¨ï¼Œå¦å‰‡ä½¿ç”¨ items
                    items_to_process = paragraphs if paragraphs else items
                    
                    for item in items_to_process:
                        # æ”¯æ´æ–°çµæ§‹çš„éµå
                        item_name = item.get("paragraph", item.get("item", ""))
                        item_content = item.get("content", "")
                        
                        merged_item = {
                            "item": item_name,  # ä¿æŒå‘å¾Œç›¸å®¹
                            "paragraph": item_name,  # æ–°çµæ§‹
                            "content": item_content,
                            "sub_items": [],
                            "subparagraphs": []  # æ–°çµæ§‹
                        }
                        
                        # è™•ç†å­é …ç›® - æ”¯æ´æ–°çµæ§‹ (subparagraphs) å’ŒèˆŠçµæ§‹ (sub_items)
                        subparagraphs = item.get("subparagraphs", [])
                        sub_items = item.get("sub_items", [])
                        
                        # ä½¿ç”¨ subparagraphs å¦‚æœå­˜åœ¨ï¼Œå¦å‰‡ä½¿ç”¨ sub_items
                        sub_items_to_process = subparagraphs if subparagraphs else sub_items
                        
                        for sub_item in sub_items_to_process:
                            # æ”¯æ´æ–°çµæ§‹çš„éµå
                            sub_item_name = sub_item.get("subparagraph", sub_item.get("sub_item", ""))
                            sub_item_content = sub_item.get("content", "")
                            
                            merged_sub_item = {
                                "sub_item": sub_item_name,  # ä¿æŒå‘å¾Œç›¸å®¹
                                "subparagraph": sub_item_name,  # æ–°çµæ§‹
                                "content": sub_item_content,
                                "items": [],  # æ–°çµæ§‹çš„ç¬¬ä¸‰å±¤
                                "metadata": {
                                    "id": f"{law_prefix}_{chapter_name}_{section_name}_{article_name}_{item_name}_{sub_item_name}".replace(" ", "_"),
                                    "spans": sub_item.get("metadata", {}).get("spans", {}),
                                    "page_range": sub_item.get("metadata", {}).get("page_range", {})
                                }
                            }
                            
                            # è™•ç†ç¬¬ä¸‰å±¤é …ç›® (items)
                            third_level_items = sub_item.get("items", [])
                            for third_item in third_level_items:
                                third_item_name = third_item.get("item", "")
                                merged_third_item = {
                                    "item": third_item_name,
                                    "content": third_item.get("content", ""),
                                    "metadata": {
                                        "id": f"{law_prefix}_{chapter_name}_{section_name}_{article_name}_{item_name}_{sub_item_name}_{third_item_name}".replace(" ", "_"),
                                        "spans": third_item.get("metadata", {}).get("spans", {}),
                                        "page_range": third_item.get("metadata", {}).get("page_range", {})
                                    }
                                }
                                merged_sub_item["items"].append(merged_third_item)
                            
                            merged_item["sub_items"].append(merged_sub_item)
                            merged_item["subparagraphs"].append(merged_sub_item)
                        
                        # ç‚ºé …ç›®æ·»åŠ metadata
                        merged_item["metadata"] = {
                            "id": f"{law_prefix}_{chapter_name}_{section_name}_{article_name}_{item_name}".replace(" ", "_"),
                            "spans": item.get("metadata", {}).get("spans", {}),
                            "page_range": item.get("metadata", {}).get("page_range", {})
                        }
                        merged_article["items"].append(merged_item)
                    
                    # ç‚ºæ¢æ–‡æ·»åŠ metadata
                    merged_article["metadata"] = {
                        "id": f"{law_prefix}_{chapter_name}_{section_name}_{article_name}".replace(" ", "_"),
                        "spans": article.get("metadata", {}).get("spans", {}),
                        "page_range": article.get("metadata", {}).get("page_range", {})
                    }
                    merged_section["articles"].append(merged_article)
                
                merged_chapter["sections"].append(merged_section)
            
            merged_law["chapters"].append(merged_chapter)
        
        merged_laws.append(merged_law)
        global_id_counter += 1
    
    return {"laws": merged_laws}


def convert_pdf_structured(file_content: bytes, filename: str, options: MetadataOptions) -> Dict[str, Any]:
    """å°‡PDFè½‰æ›ç‚ºçµæ§‹åŒ–æ ¼å¼"""
    import time
    start_time = time.time()
    
    try:
        # Read PDF content safely; skip pages with no text
        try:
            reader = PdfReader(io.BytesIO(file_content))
        except Exception as e:
            raise Exception(f"ç„¡æ³•è®€å–PDFæ–‡ä»¶: {str(e)}")
        
        # æ‰¹é‡æå–æ–‡æœ¬ï¼Œé¡¯ç¤ºé€²åº¦
        texts = []
        total_pages = len(reader.pages)
        print(f"ç¸½é æ•¸: {total_pages}")
        
        for i, page in enumerate(reader.pages):
            try:
                t = page.extract_text() or ""
                texts.append(t)
            except Exception:
                texts.append("")
            
            # æ¯è™•ç†10é é¡¯ç¤ºé€²åº¦
            if (i + 1) % 10 == 0 or (i + 1) == total_pages:
                print(f"å·²è™•ç† {i + 1}/{total_pages} é ")
        
        if not any(texts):  # No text extracted
            raise Exception("PDFæ–‡ä»¶ä¸­æ²¡æœ‰æ‰¾åˆ°å¯æå–çš„æ–‡æœ¬å†…å®¹")
            
        full_text = "\n".join(texts)
        print(f"æ–‡æœ¬æå–å®Œæˆï¼Œç¸½é•·åº¦: {len(full_text)} å­—ç¬¦")

        def normalize_digits(s: str) -> str:
            # Convert fullwidth digits to ASCII for simpler matching
            fw = "ï¼ï¼‘ï¼’ï¼“ï¼”ï¼•ï¼–ï¼—ï¼˜ï¼™"
            hw = "0123456789"
            return s.translate(str.maketrans(fw, hw))

        # Determine law name: first non-empty line containing a legal keyword, else filename
        lines = [normalize_digits((ln or "").strip()) for ln in full_text.splitlines()]
        law_name = None
        for ln in lines:
            if not ln:
                continue
            if any(key in ln for key in ["æ³•", "æ¢ä¾‹", "æ³•è¦", "æ³•å¾‹"]):
                law_name = ln
                break
        if not law_name:
            base = os.path.splitext(filename or "document")[0]
            law_name = base or "æœªå‘½åæ³•è¦"

        chapter_re = re.compile(r"^ç¬¬\s*([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒ0-9]+)\s*ç« [\u3000\s]*(.*)$")
        section_re = re.compile(r"^ç¬¬\s*([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒ0-9]+)\s*ç¯€[\u3000\s]*(.*)$")
        article_re = re.compile(r"^ç¬¬\s*([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒ0-9]+(?:ä¹‹[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å0-9]+)?)\s*æ¢[\u3000\s]*(.*)$")

        def parse_item_line(ln: str):
            # Match common item markers like ã€Œä¸€ã€ã€ã€Œ1.ã€ã€Œï¼ˆä¸€ï¼‰ã€ã€Œ(1)ã€ã€Œ1ï¼‰ã€ etc.
            ln = ln.lstrip()
            # ï¼ˆä¸€ï¼‰ or (1)
            m = re.match(r"^[ï¼ˆ(]([0-9ï¼-ï¼™ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+)[ï¼‰)]\s*(.*)$", ln)
            if m:
                return m.group(1), m.group(2), "parentheses"
            # ä¸€ã€ äºŒã€ åã€ style (Chinese numerals with punctuation)
            m = re.match(r"^([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+)[ã€ï¼.ï¼‰)]\s*(.*)$", ln)
            if m:
                return m.group(1), m.group(2), "chinese_with_punct"
            # 1. 1ã€ 1) styles (Arabic numbers with punctuation)
            m = re.match(r"^([0-9ï¼-ï¼™]+)[ã€ï¼.ï¼‰)]\s*(.*)$", ln)
            if m:
                return m.group(1), m.group(2), "arabic_with_punct"
            # 1 2 3 styles (Arabic numbers followed by space, common in ROC legal documents)
            m = re.match(r"^([0-9ï¼-ï¼™]+)\s+(.*)$", ln)
            if m:
                return m.group(1), m.group(2), "arabic_space"
            # ä¸€ äºŒ ä¸‰ styles (Chinese numerals followed by space, sub-items)
            m = re.match(r"^([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+)\s+(.*)$", ln)
            if m:
                return m.group(1), m.group(2), "chinese_space"
            return None, None, None

        structure: Dict[str, Any] = {"law_name": law_name, "chapters": []}
        current_chapter: Optional[Dict[str, Any]] = None
        current_section: Optional[Dict[str, Any]] = None
        current_article: Optional[Dict[str, Any]] = None
        # ä¾æ“šå°ç£æ³•å¾‹å±¤æ¬¡ï¼šæ¢ â†’ é …(Paragraph) â†’ æ¬¾(Subparagraph) â†’ ç›®(Item)
        current_paragraph: Optional[Dict[str, Any]] = None
        current_subparagraph: Optional[Dict[str, Any]] = None
        current_item_lvl3: Optional[Dict[str, Any]] = None

        def ensure_chapter():
            nonlocal current_chapter
            if current_chapter is None:
                current_chapter = {"chapter": "æœªåˆ†é¡ç« ", "sections": []}
                structure["chapters"].append(current_chapter)

        def ensure_section():
            nonlocal current_section
            ensure_chapter()
            if current_section is None:
                current_section = {"section": "æœªåˆ†é¡ç¯€", "articles": []}
                current_chapter["sections"].append(current_section)

        for raw in lines:
            ln = raw.strip()
            if not ln:
                continue

            # Headings
            m = chapter_re.match(ln)
            if m:
                num_raw = m.group(1)
                title = f"ç¬¬{num_raw}ç« " + (f" {m.group(2).strip()}" if m.group(2) else "")
                current_chapter = {"chapter": title, "chapter_no": normalize_digits(num_raw), "type_en": "Chapter", "sections": []}
                structure["chapters"].append(current_chapter)
                current_section = None
                current_article = None
                current_paragraph = None
                current_subparagraph = None
                current_item_lvl3 = None
                continue

            m = section_re.match(ln)
            if m:
                ensure_chapter()
                num_raw = m.group(1)
                title = f"ç¬¬{num_raw}ç¯€" + (f" {m.group(2).strip()}" if m.group(2) else "")
                current_section = {"section": title, "section_no": normalize_digits(num_raw), "type_en": "Section", "articles": []}
                current_chapter["sections"].append(current_section)
                current_article = None
                current_paragraph = None
                current_subparagraph = None
                current_item_lvl3 = None
                continue

            m = article_re.match(ln)
            if m:
                ensure_section()
                num_raw = m.group(1)
                title = f"ç¬¬{num_raw}æ¢"
                rest = m.group(2).strip() if m.group(2) else ""
                # å»ºç«‹æ¢æ–‡ï¼Œæ–°å¢ paragraphs æ¸…å–®ä¸¦ä¿ç•™ç›¸å®¹çš„ items æ¬„ä½
                current_article = {"article": title, "article_no": normalize_digits(num_raw), "type_en": "Article", "content": rest, "paragraphs": []}
                # ç›¸å®¹èˆŠæ¬„ä½ï¼ˆå°‡æŒ‡å‘åŒä¸€å€‹åˆ—è¡¨ï¼‰
                current_article["items"] = current_article["paragraphs"]
                current_section["articles"].append(current_article)
                current_paragraph = None
                current_subparagraph = None
                current_item_lvl3 = None
                continue

            # æ¢æ–‡å…§å±¤ç´šè§£æï¼šé …(é˜¿æ‹‰ä¼¯æ•¸å­—) â†’ æ¬¾(ä¸­æ–‡æ•¸å­—) â†’ ç›®ï¼ˆæ‹¬è™Ÿä¸­æ–‡æ•¸å­—ï¼‰
            if current_article is not None:
                num, content, item_type = parse_item_line(ln)
                if num is not None:
                    num = normalize_digits(num)
                    # 1) é … Paragraph: é˜¿æ‹‰ä¼¯æ•¸å­—ï¼ˆå« 1. 1ã€ 1) æˆ–æ•¸å­—+ç©ºç™½ï¼‰
                    if item_type in ("arabic_with_punct", "arabic_space"):
                        current_paragraph = {"paragraph": str(num), "paragraph_no": str(num), "type_en": "Paragraph", "content": content or "", "subparagraphs": []}
                        # ç›¸å®¹æ¬„ä½
                        current_paragraph["sub_items"] = current_paragraph["subparagraphs"]
                        current_article["paragraphs"].append(current_paragraph)
                        current_item_lvl3 = None
                        current_subparagraph = None
                    # 2) æ¬¾ Subparagraph: ä¸­æ–‡æ•¸å­—ï¼ˆå« ä¸€ã€ æˆ– ä¸­æ–‡æ•¸å­—+ç©ºç™½ï¼‰
                    elif item_type in ("chinese_with_punct", "chinese_space") and current_paragraph is not None:
                        if "subparagraphs" not in current_paragraph:
                            current_paragraph["subparagraphs"] = []
                            current_paragraph["sub_items"] = current_paragraph["subparagraphs"]
                        current_subparagraph = {"subparagraph": str(num), "subparagraph_no": str(num), "type_en": "Subparagraph", "content": content or "", "items": []}
                        # ç¬¬ä¸‰ç´šç›¸å®¹éµå
                        current_subparagraph["sub_sub_items"] = current_subparagraph["items"]
                        current_paragraph["subparagraphs"].append(current_subparagraph)
                        current_item_lvl3 = None
                    # 3) ç›® Item: æ‹¬è™Ÿä¸­æ–‡æˆ–æ•¸å­—ï¼ˆï¼ˆä¸€ï¼‰ã€(1)ï¼‰å‡ºç¾åœ¨æ¬¾å…§
                    elif item_type == "parentheses" and current_subparagraph is not None:
                        if "items" not in current_subparagraph:
                            current_subparagraph["items"] = []
                            current_subparagraph["sub_sub_items"] = current_subparagraph["items"]
                        current_item_lvl3 = {"item": str(num), "item_no": str(num), "type_en": "Item", "content": content or ""}
                        current_subparagraph["items"].append(current_item_lvl3)
                    else:
                        # è‹¥ç„¡æ³•åˆ¤åˆ¥å±¤ç´šï¼Œè¦–ç‚ºç•¶å‰æœ€æ·±å±¤çš„çºŒè¡Œæ–‡å­—
                        pass
                else:
                    # çºŒè¡Œæ–‡å­—ï¼šé™„åŠ åˆ°æœ€æ·±å±¤ï¼ˆç›® â†’ æ¬¾ â†’ é … â†’ æ¢ï¼‰
                    if current_item_lvl3 is not None:
                        sep = "\n" if current_item_lvl3.get("content") else ""
                        current_item_lvl3["content"] = f"{current_item_lvl3.get('content','')}{sep}{ln}"
                    elif current_subparagraph is not None:
                        sep = "\n" if current_subparagraph.get("content") else ""
                        current_subparagraph["content"] = f"{current_subparagraph.get('content','')}{sep}{ln}"
                    elif current_paragraph is not None:
                        sep = "\n" if current_paragraph.get("content") else ""
                        current_paragraph["content"] = f"{current_paragraph.get('content','')}{sep}{ln}"
                    else:
                        # accumulate into article content
                        if "content" not in current_article or current_article["content"] is None:
                            current_article["content"] = ln
                        else:
                            current_article["content"] = (current_article["content"] + "\n" + ln).strip()
                continue

            # If no article yet, but we have text, place it under a default article
            if current_section is not None and current_article is None:
                current_article = {"article": "æœªæ¨™ç¤ºæ¢æ–‡", "content": ln, "paragraphs": []}
                current_article["items"] = current_article["paragraphs"]
                current_section["articles"].append(current_article)
                current_paragraph = None
                current_subparagraph = None
                current_item_lvl3 = None
            elif current_article is None:
                ensure_section()
                current_article = {"article": "æœªæ¨™ç¤ºæ¢æ–‡", "content": ln, "paragraphs": []}
                current_article["items"] = current_article["paragraphs"]
                current_section["articles"].append(current_article)
                current_paragraph = None
                current_subparagraph = None
                current_item_lvl3 = None
            else:
                # fallback append
                current_article["content"] = (current_article.get("content", "") + "\n" + ln).strip()

        # å„ªåŒ–ç‰ˆæœ¬çš„metadataæ·»åŠ 
        def add_metadata_to_structure_optimized(structure, options, full_text):
            """å„ªåŒ–ç‰ˆæœ¬çš„metadataæ·»åŠ ï¼Œå¤§å¹…æå‡æ€§èƒ½"""
            print("é–‹å§‹æ·»åŠ metadata...")
            metadata_start = time.time()
            
            # é è¨ˆç®—æ‰€æœ‰æ¢æ–‡ï¼ˆé¿å…é‡è¤‡è¨ˆç®—ï¼‰
            all_articles = []
            for chapter in structure["chapters"]:
                for section in chapter["sections"]:
                    for article in section["articles"]:
                        all_articles.append({
                            "article": article["article"],
                            "content": article["content"],
                            "chapter": chapter["chapter"],
                            "section": section["section"]
                        })
            
            print(f"æ‰¾åˆ° {len(all_articles)} å€‹æ¢æ–‡")
            
            # æ‰¹é‡è™•ç†metadataï¼ˆå¦‚æœå•Ÿç”¨ï¼‰
            if options.include_id or options.include_page_range or options.include_spans:
                print("æ‰¹é‡è™•ç†metadata...")
            
            processed_count = 0
            for chapter in structure["chapters"]:
                chapter_name = chapter["chapter"]
                for section in chapter["sections"]:
                    section_name = section["section"]
                    for article in section["articles"]:
                        article_name = article["article"]
                        
                        # ç°¡åŒ–çš„metadataè™•ç†
                        article_metadata = {}
                        if options.include_id:
                            article_metadata["id"] = f"{structure['law_name']}_{chapter_name}_{section_name}_{article_name}".replace(" ", "_")
                        if options.include_page_range:
                            article_metadata["page_range"] = {"start": 1, "end": 1}  # ç°¡åŒ–çš„é é¢ç¯„åœ
                        if options.include_spans:
                            article_metadata["spans"] = {"start": 0, "end": len(article["content"])}
                        if options.include_page_range:
                            # ç°¡åŒ–çš„é ç¢¼ç¯„åœï¼ˆåŸºæ–¼æ–‡æœ¬ä½ç½®ä¼°ç®—ï¼‰
                            article_metadata["page_range"] = {"start": 1, "end": 1}  # ç°¡åŒ–ç‰ˆæœ¬
                        if options.include_spans:
                            # ç°¡åŒ–çš„æ–‡æœ¬å®šä½
                            start_pos = full_text.find(article["content"][:50])  # ä½¿ç”¨å‰50å­—ç¬¦å®šä½
                            if start_pos >= 0:
                                article_metadata["spans"] = [{
                                    "start_char": start_pos,
                                    "end_char": start_pos + len(article["content"]),
                                    "text": article["content"][:100] + "..." if len(article["content"]) > 100 else article["content"],
                                    "page": 1,
                                    "confidence": 0.8,
                                    "found": True
                                }]
                            else:
                                article_metadata["spans"] = []
                        
                        article["metadata"] = article_metadata
                        
                        # ç‚ºé …ç›®æ·»åŠ ç°¡åŒ–metadata - æ”¯æ´æ–°çµæ§‹ (paragraphs) å’ŒèˆŠçµæ§‹ (items)
                        paragraphs = article.get("paragraphs", [])
                        items = article.get("items", [])
                        items_to_process = paragraphs if paragraphs else items
                        
                        for item in items_to_process:
                            # æ”¯æ´æ–°çµæ§‹çš„éµå
                            item_name = item.get("paragraph", item.get("item", ""))
                            item_metadata = {}
                            if options.include_id:
                                item_metadata["id"] = f"{structure['law_name']}_{chapter_name}_{section_name}_{article_name}_{item_name}".replace(" ", "_")
                            if options.include_page_range:
                                item_metadata["page_range"] = {"start": 1, "end": 1}  # ç°¡åŒ–çš„é é¢ç¯„åœ
                            if options.include_spans:
                                item_metadata["spans"] = {"start": 0, "end": len(item["content"])}
                            
                            item["metadata"] = item_metadata
                            
                            # ç‚ºå­é …ç›®æ·»åŠ ç°¡åŒ–metadata - æ”¯æ´æ–°çµæ§‹ (subparagraphs) å’ŒèˆŠçµæ§‹ (sub_items)
                            subparagraphs = item.get("subparagraphs", [])
                            sub_items = item.get("sub_items", [])
                            sub_items_to_process = subparagraphs if subparagraphs else sub_items
                            
                            for sub_item in sub_items_to_process:
                                # æ”¯æ´æ–°çµæ§‹çš„éµå
                                sub_item_name = sub_item.get("subparagraph", sub_item.get("sub_item", ""))
                                sub_item_metadata = {}
                                if options.include_id:
                                    sub_item_metadata["id"] = f"{structure['law_name']}_{chapter_name}_{section_name}_{article_name}_{item_name}_{sub_item_name}".replace(" ", "_")
                                if options.include_page_range:
                                    sub_item_metadata["page_range"] = {"start": 1, "end": 1}  # ç°¡åŒ–çš„é é¢ç¯„åœ
                                if options.include_spans:
                                    sub_item_metadata["spans"] = {"start": 0, "end": len(sub_item["content"])}
                                
                                sub_item["metadata"] = sub_item_metadata
                                
                                # è™•ç†ç¬¬ä¸‰å±¤é …ç›® (items)
                                third_level_items = sub_item.get("items", [])
                                for third_item in third_level_items:
                                    third_item_name = third_item.get("item", "")
                                    third_item_metadata = {}
                                    if options.include_id:
                                        third_item_metadata["id"] = f"{structure['law_name']}_{chapter_name}_{section_name}_{article_name}_{item_name}_{sub_item_name}_{third_item_name}".replace(" ", "_")
                                    if options.include_page_range:
                                        third_item_metadata["page_range"] = {"start": 1, "end": 1}  # ç°¡åŒ–çš„é é¢ç¯„åœ
                                    if options.include_spans:
                                        third_item_metadata["spans"] = {"start": 0, "end": len(third_item["content"])}
                                    
                                    third_item["metadata"] = third_item_metadata
                        
                        processed_count += 1
                        if processed_count % 10 == 0:
                            print(f"å·²è™•ç† {processed_count} å€‹æ¢æ–‡")
            
            metadata_time = time.time() - metadata_start
            print(f"Metadataè™•ç†å®Œæˆï¼Œè€—æ™‚: {metadata_time:.2f}ç§’")
        
        # æ·»åŠ metadataï¼ˆä½¿ç”¨å„ªåŒ–ç‰ˆæœ¬ï¼‰
        if any([options.include_id, options.include_page_range, options.include_spans]):
            add_metadata_to_structure_optimized(structure, options, full_text)
        else:
            print("è·³émetadataè™•ç†ï¼ˆæœªå•Ÿç”¨ï¼‰")

        total_time = time.time() - start_time
        print(f"ç¸½è½‰æ›æ™‚é–“: {total_time:.2f}ç§’")
        
        return {
            "text": full_text,
            "metadata": structure,
            "processing_time": total_time,
            "success": True
        }
        
    except Exception as e:
        return {
            "text": "",
            "metadata": {"error": str(e)},
            "processing_time": time.time() - start_time,
            "success": False,
            "error": str(e)
        }


# ç•°æ­¥ä»»å‹™å­˜å„²
conversion_tasks = {}

# PDFç·©å­˜å­˜å„² (åŸºæ–¼æ–‡ä»¶å…§å®¹å“ˆå¸Œ)
pdf_cache = {}

# æ¸…ç†èˆŠä»»å‹™çš„å¾Œå°ä»»å‹™
async def cleanup_old_tasks():
    """æ¸…ç†è¶…é1å°æ™‚çš„èˆŠä»»å‹™"""
    while True:
        try:
            current_time = time.time()
            expired_tasks = []
            
            for task_id, task in conversion_tasks.items():
                if current_time - task["created_at"] > 3600:  # 1å°æ™‚
                    expired_tasks.append(task_id)
            
            for task_id in expired_tasks:
                del conversion_tasks[task_id]
                print(f"æ¸…ç†éæœŸä»»å‹™: {task_id}")
            
            # æ¯5åˆ†é˜æ¸…ç†ä¸€æ¬¡
            await asyncio.sleep(300)
        except Exception as e:
            print(f"æ¸…ç†ä»»å‹™æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            await asyncio.sleep(60)  # å‡ºéŒ¯æ™‚ç­‰å¾…1åˆ†é˜å†é‡è©¦

# æ¸…ç†ä»»å‹™å°‡åœ¨æ‡‰ç”¨å•Ÿå‹•æ™‚å•Ÿå‹•
@app.on_event("startup")
async def startup_event():
    """æ‡‰ç”¨å•Ÿå‹•æ™‚çš„äº‹ä»¶"""
    import asyncio
    asyncio.create_task(cleanup_old_tasks())

@app.post("/api/convert")
async def convert(file: UploadFile = File(...), metadata_options: str = Form("{}")):
    """å•Ÿå‹•PDFè½‰æ›ä»»å‹™"""
    try:
        # Parse metadata options
        try:
            metadata_config = json.loads(metadata_options)
            options = MetadataOptions(**metadata_config)
        except:
            options = MetadataOptions()  # ä½¿ç”¨é»˜èªé¸é …
        
        # Validate file type
        if not file.filename or not file.filename.lower().endswith('.pdf'):
            return JSONResponse(
                status_code=400, 
                content={"error": "åªæ”¯æŒPDFæ–‡ä»¶æ ¼å¼", "detail": "Invalid file type"}
            )
        
        # Reset file pointer to beginning
        await file.seek(0)
        
        # ç”Ÿæˆä»»å‹™ID
        task_id = f"convert_{int(time.time() * 1000)}_{hash(file.filename) % 10000}"
        
        # è®€å–æ–‡ä»¶å…§å®¹
        file_content = await file.read()
        
        # æª¢æŸ¥ç·©å­˜ï¼ˆåŸºæ–¼æ–‡ä»¶å…§å®¹å“ˆå¸Œï¼‰
        import hashlib
        file_hash = hashlib.md5(file_content).hexdigest()
        
        # æª¢æŸ¥æ˜¯å¦å·²ç·©å­˜
        cache_key = f"{file_hash}_{json.dumps(options.__dict__, sort_keys=True)}"
        if cache_key in pdf_cache:
            cached_result = pdf_cache[cache_key]
            print(f"ä½¿ç”¨ç·©å­˜çš„PDFè½‰æ›çµæœ: {file.filename}")
            
            # ç”Ÿæˆæ–°çš„doc_id
            doc_id = f"doc_{int(time.time() * 1000)}_{hash(file.filename) % 10000}"
            
            # å°‡æ–‡æª”å­˜å„²åˆ°storeä¸­
            store.docs[doc_id] = DocRecord(
                id=doc_id,
                filename=file.filename,
                text=cached_result["text"],
                json_data=cached_result["metadata"],
                chunks=[],
                chunk_size=0,
                overlap=0,
            )
            
            return {
                "doc_id": doc_id,
                "filename": file.filename,
                "text_length": cached_result["text_length"],
                "metadata": cached_result["metadata"],
                "processing_time": 0.1,  # ç·©å­˜å‘½ä¸­ï¼Œå¹¾ä¹ç¬é–“å®Œæˆ
                "cached": True
            }
        
        # å‰µå»ºä»»å‹™
        conversion_tasks[task_id] = {
            "status": "pending",
            "progress": 0,
            "filename": file.filename,
            "created_at": time.time(),
            "result": None,
            "error": None,
            "file_hash": file_hash,
            "cache_key": cache_key
        }
        
        # å•Ÿå‹•å¾Œå°ä»»å‹™
        import asyncio
        asyncio.create_task(process_pdf_conversion(task_id, file_content, options))
        
        return {
            "task_id": task_id,
            "status": "pending",
            "message": "PDFè½‰æ›ä»»å‹™å·²å•Ÿå‹•ï¼Œè«‹ä½¿ç”¨task_idæŸ¥è©¢é€²åº¦"
        }
        
    except Exception as e:
        print(f"Convert endpoint error: {e}")
        return JSONResponse(
            status_code=500, 
            content={"error": "å•Ÿå‹•PDFè½‰æ›ä»»å‹™å¤±æ•—", "detail": str(e)}
        )


async def process_pdf_conversion(task_id: str, file_content: bytes, options: MetadataOptions):
    """å¾Œå°è™•ç†PDFè½‰æ›"""
    import time
    start_time = time.time()
    
    try:
        # æ›´æ–°ä»»å‹™ç‹€æ…‹
        conversion_tasks[task_id]["status"] = "processing"
        conversion_tasks[task_id]["progress"] = 10
        
        print(f"é–‹å§‹è½‰æ›PDF: {conversion_tasks[task_id]['filename']}")
        
        # ç›´æ¥èª¿ç”¨convert_pdf_structuredå‡½æ•¸
        conversion_tasks[task_id]["progress"] = 20
        result = convert_pdf_structured(file_content, conversion_tasks[task_id]['filename'], options)
        
        if not result["success"]:
            conversion_tasks[task_id]["status"] = "failed"
            conversion_tasks[task_id]["error"] = result.get("error", "PDFè½‰æ›å¤±æ•—")
            return
        
        # æå–çµæœ
        full_text = result["text"]
        structure = result["metadata"]
        total_time = result["processing_time"]
        
        conversion_tasks[task_id]["progress"] = 80
        print(f"PDFè½‰æ›å®Œæˆï¼Œæ–‡æœ¬é•·åº¦: {len(full_text)} å­—ç¬¦")
        
        # ç”Ÿæˆæ–‡æª”ID
        doc_id = f"doc_{int(time.time() * 1000)}_{hash(conversion_tasks[task_id]['filename']) % 10000}"
        
        # å°‡æ–‡æª”å­˜å„²åˆ°storeä¸­
        store.docs[doc_id] = DocRecord(
            id=doc_id,
            filename=conversion_tasks[task_id]['filename'],
            text=full_text,
            json_data=structure,
            chunks=[],
            chunk_size=0,
            overlap=0,
        )
        
        # é‡ç½®åµŒå…¥ç‹€æ…‹
        store.reset_embeddings()
        
        # ä¿å­˜åˆ°ç·©å­˜
        cache_data = {
            "text": full_text,
            "metadata": structure,
            "text_length": len(full_text),
            "processing_time": total_time
        }
        pdf_cache[conversion_tasks[task_id]["cache_key"]] = cache_data
        
        # é™åˆ¶ç·©å­˜å¤§å°ï¼ˆæœ€å¤šä¿å­˜100å€‹è½‰æ›çµæœï¼‰
        if len(pdf_cache) > 100:
            # åˆªé™¤æœ€èˆŠçš„ç·©å­˜é …
            oldest_key = next(iter(pdf_cache))
            del pdf_cache[oldest_key]
        
        # æ›´æ–°ä»»å‹™ç‹€æ…‹ç‚ºå®Œæˆ
        conversion_tasks[task_id]["status"] = "completed"
        conversion_tasks[task_id]["progress"] = 100
        conversion_tasks[task_id]["result"] = {
            "doc_id": doc_id,
            "filename": conversion_tasks[task_id]['filename'],
            "text_length": len(full_text),
            "metadata": structure,
            "processing_time": total_time
        }
        
    except Exception as e:
        # æ›´æ–°ä»»å‹™ç‹€æ…‹ç‚ºå¤±æ•—
        conversion_tasks[task_id]["status"] = "failed"
        conversion_tasks[task_id]["error"] = str(e)
        print(f"PDFè½‰æ›å¤±æ•—: {str(e)}")


@app.get("/api/convert/status/{task_id}")
async def get_convert_status(task_id: str):
    """æŸ¥è©¢PDFè½‰æ›ä»»å‹™ç‹€æ…‹"""
    if task_id not in conversion_tasks:
        raise HTTPException(status_code=404, detail="ä»»å‹™ä¸å­˜åœ¨")
    
    task = conversion_tasks[task_id]
    
    # æ¸…ç†è¶…é1å°æ™‚çš„èˆŠä»»å‹™
    if time.time() - task["created_at"] > 3600:
        del conversion_tasks[task_id]
        raise HTTPException(status_code=404, detail="ä»»å‹™å·²éæœŸ")
    
    return {
        "task_id": task_id,
        "status": task["status"],
        "progress": task["progress"],
        "filename": task["filename"],
        "result": task.get("result"),
        "error": task.get("error")
    }


def run_evaluation_task(task_id: str):
    """
    åœ¨å¾Œå°é‹è¡Œè©•æ¸¬ä»»å‹™
    """
    task = eval_store.get_task(task_id)
    if not task:
        return
    
    try:
        eval_store.update_task_status(task_id, "running")
        
        doc = store.docs.get(task.doc_id)
        if not doc:
            eval_store.update_task_status(task_id, "failed", error_message="Document not found")
            return
        
        results = []
        total_configs = len(task.configs)
        
        for i, config in enumerate(task.configs):
            result = evaluate_chunk_config(doc, config, task.test_queries, task.k_values, task.strategy)
            results.append(result)
            
            # æ›´æ–°é€²åº¦
            progress = (i + 1) / total_configs
            eval_store.update_task_status(task_id, "running", progress=progress)
        
        eval_store.update_task_status(task_id, "completed", results=results)
        
    except Exception as e:
        eval_store.update_task_status(task_id, "failed", error_message=str(e))


@app.post("/api/evaluate/fixed-size")
def start_fixed_size_evaluation(req: FixedSizeEvaluationRequest, background_tasks: BackgroundTasks):
    """
    é–‹å§‹å›ºå®šå¤§å°åˆ†å‰²ç­–ç•¥è©•æ¸¬
    """
    doc = store.docs.get(req.doc_id)
    if not doc:
        return JSONResponse(status_code=404, content={"error": "Document not found"})
    
    # æª¢æŸ¥æ˜¯å¦å·²æœ‰ç”Ÿæˆçš„å•é¡Œ
    if not hasattr(doc, 'generated_questions') or not doc.generated_questions:
        return JSONResponse(
            status_code=400, 
            content={"error": "è«‹å…ˆä½¿ç”¨ã€Œç”Ÿæˆå•é¡Œã€åŠŸèƒ½ç‚ºæ–‡æª”ç”Ÿæˆæ¸¬è©¦å•é¡Œï¼Œç„¶å¾Œå†é€²è¡Œè©•æ¸¬"}
        )
    
    # ä½¿ç”¨æ–‡æª”ä¸­å­˜å„²çš„å•é¡Œè€Œä¸æ˜¯é è¨­å•é¡Œ
    req.test_queries = doc.generated_questions
    
    # ç”Ÿæˆæ‰€æœ‰é…ç½®çµ„åˆï¼ŒåŒ…æ‹¬ç­–ç•¥ç‰¹å®šåƒæ•¸
    configs = []
    for chunk_size in req.chunk_sizes:
        for overlap_ratio in req.overlap_ratios:
            overlap = int(chunk_size * overlap_ratio)
            
            # æ ¹æ“šç­–ç•¥ç”Ÿæˆä¸åŒçš„åƒæ•¸çµ„åˆ
            if req.strategy == "structured_hierarchical":
                for chunk_by in req.chunk_by_options:
                    config = ChunkConfig(
                        chunk_size=chunk_size,
                        overlap=overlap,
                        overlap_ratio=overlap_ratio,
                        chunk_by=chunk_by
                    )
                    configs.append(config)
            elif req.strategy == "rcts_hierarchical":
                for preserve_structure in req.preserve_structure_options:
                    config = ChunkConfig(
                        chunk_size=chunk_size,
                        overlap=overlap,
                        overlap_ratio=overlap_ratio,
                        preserve_structure=preserve_structure,
                        chunk_by="article"  # é»˜èªå€¼
                    )
                    configs.append(config)
            elif req.strategy == "hierarchical":
                for level_depth in req.level_depth_options:
                    for min_chunk_size in req.min_chunk_size_options:
                        config = ChunkConfig(
                            chunk_size=chunk_size,
                            overlap=overlap,
                            overlap_ratio=overlap_ratio,
                            level_depth=level_depth,
                            min_chunk_size=min_chunk_size,
                            chunk_by="article"  # é»˜èªå€¼
                        )
                        configs.append(config)
            elif req.strategy == "semantic":
                for similarity_threshold in req.similarity_threshold_options:
                    for context_window in req.context_window_options:
                        config = ChunkConfig(
                            chunk_size=chunk_size,
                            overlap=overlap,
                            overlap_ratio=overlap_ratio,
                            similarity_threshold=similarity_threshold,
                            context_window=context_window,
                            chunk_by="article"  # é»˜èªå€¼
                        )
                        configs.append(config)
            elif req.strategy == "llm_semantic":
                for semantic_threshold in req.semantic_threshold_options:
                    for context_window in req.context_window_options:
                        config = ChunkConfig(
                            chunk_size=chunk_size,
                            overlap=overlap,
                            overlap_ratio=overlap_ratio,
                            semantic_threshold=semantic_threshold,
                            context_window=context_window,
                            chunk_by="article"  # é»˜èªå€¼
                        )
                        configs.append(config)
            elif req.strategy == "sliding_window":
                for window_size in req.window_size_options:
                    for step_size in req.step_size_options:
                        for boundary_aware in req.boundary_aware_options:
                            for preserve_sentences in req.preserve_sentences_options:
                                for min_chunk_size_sw in req.min_chunk_size_options_sw:
                                    for max_chunk_size_sw in req.max_chunk_size_options_sw:
                                        config = ChunkConfig(
                                            chunk_size=window_size,  # ä½¿ç”¨window_sizeä½œç‚ºchunk_size
                                            overlap=overlap,
                                            overlap_ratio=overlap_ratio,
                                            strategy="sliding_window",
                                            step_size=step_size,
                                            window_size=window_size,
                                            boundary_aware=boundary_aware,
                                            preserve_sentences=preserve_sentences,
                                            min_chunk_size_sw=min_chunk_size_sw,
                                            max_chunk_size_sw=max_chunk_size_sw,
                                            chunk_by="article"  # é»˜èªå€¼
                                        )
                                        configs.append(config)
            elif req.strategy == "hybrid":
                for switch_threshold in req.switch_threshold_options:
                    for secondary_size in req.secondary_size_options:
                        config = ChunkConfig(
                            chunk_size=chunk_size,
                            overlap=overlap,
                            overlap_ratio=overlap_ratio,
                            switch_threshold=switch_threshold,
                            secondary_size=secondary_size,
                            chunk_by="article"  # é»˜èªå€¼
                        )
                        configs.append(config)
            else:
                # é»˜èªé…ç½®ï¼ˆfixed_sizeç­‰ï¼‰
                config = ChunkConfig(
                    chunk_size=chunk_size,
                    overlap=overlap,
                    overlap_ratio=overlap_ratio,
                    chunk_by="article"  # é»˜èªå€¼
                )
                configs.append(config)
    
    # ç²å–åˆ†å‰²ç­–ç•¥ï¼ˆå¾è«‹æ±‚ä¸­ç²å–ï¼Œé»˜èªç‚ºfixed_sizeï¼‰
    strategy = getattr(req, 'strategy', 'fixed_size')
    
    # å‰µå»ºè©•æ¸¬ä»»å‹™
    task_id = eval_store.create_task(
        doc_id=req.doc_id,
        configs=configs,
        test_queries=req.test_queries,
        k_values=req.k_values,
        strategy=strategy
    )
    
    # åœ¨å¾Œå°é‹è¡Œè©•æ¸¬
    background_tasks.add_task(run_evaluation_task, task_id)
    
    return {
        "task_id": task_id,
        "status": "started",
        "total_configs": len(configs),
        "message": "è©•æ¸¬ä»»å‹™å·²é–‹å§‹ï¼Œè«‹ä½¿ç”¨task_idæŸ¥è©¢é€²åº¦"
    }


@app.get("/api/evaluate/status/{task_id}")
def get_evaluation_status(task_id: str):
    """
    ç²å–è©•æ¸¬ä»»å‹™ç‹€æ…‹
    """
    task = eval_store.get_task(task_id)
    if not task:
        return JSONResponse(status_code=404, content={"error": "Task not found"})
    
    total_configs = len(task.configs)
    completed_configs = int(task.progress * total_configs) if task.progress > 0 else 0
    
    return {
        "task_id": task_id,
        "status": task.status,
        "created_at": task.created_at.isoformat(),
        "completed_at": task.completed_at.isoformat() if task.completed_at else None,
        "error_message": task.error_message,
        "total_configs": total_configs,
        "completed_configs": completed_configs,
        "progress": task.progress
    }


@app.get("/api/evaluate/results/{task_id}")
def get_evaluation_results(task_id: str):
    """
    ç²å–è©•æ¸¬çµæœ
    """
    task = eval_store.get_task(task_id)
    if not task:
        return JSONResponse(status_code=404, content={"error": "Task not found"})
    
    if task.status != "completed":
        return JSONResponse(status_code=400, content={"error": "Task not completed yet"})
    
    # è½‰æ›çµæœç‚ºå¯åºåˆ—åŒ–çš„æ ¼å¼
    results = []
    for result in task.results:
        result_dict = {
            "config": result.config,  # ç¾åœ¨ config å·²ç¶“æ˜¯å­—å…¸äº†
            "metrics": {
                "precision_omega": result.metrics.precision_omega,
                "precision_at_k": result.metrics.precision_at_k,
                "recall_at_k": result.metrics.recall_at_k,
                "chunk_count": result.metrics.chunk_count,
                "avg_chunk_length": result.metrics.avg_chunk_length,
                "length_variance": result.metrics.length_variance
            },
            "test_queries": result.test_queries,
            "retrieval_results": result.retrieval_results,
            "timestamp": result.timestamp.isoformat()
        }
        results.append(result_dict)
    
    return {
        "task_id": task_id,
        "status": task.status,
        "results": results,
        "summary": {
            "total_configs": len(results),
            "best_precision_omega": max(r["metrics"]["precision_omega"] for r in results),
            "best_precision_at_5": max(r["metrics"]["precision_at_k"].get(5, 0) for r in results),
            "best_recall_at_5": max(r["metrics"]["recall_at_k"].get(5, 0) for r in results),
            "avg_chunk_count": sum(r["metrics"]["chunk_count"] for r in results) / len(results),
            "avg_chunk_length": sum(r["metrics"]["avg_chunk_length"] for r in results) / len(results)
        }
    }


@app.get("/api/evaluate/comparison/{task_id}")
def get_evaluation_comparison(task_id: str):
    """
    ç²å–è©•æ¸¬çµæœå°æ¯”åˆ†æ
    """
    task = eval_store.get_task(task_id)
    if not task:
        return JSONResponse(status_code=404, content={"error": "Task not found"})
    
    if task.status != "completed":
        return JSONResponse(status_code=400, content={"error": "Task not completed yet"})
    
    # ç”Ÿæˆå°æ¯”åˆ†æ
    comparison = {
        "chunk_size_analysis": {},
        "overlap_analysis": {},
        "strategy_specific_analysis": {},
        "recommendations": []
    }
    
    # æŒ‰chunk sizeåˆ†çµ„åˆ†æ
    chunk_size_groups = {}
    for result in task.results:
        size = result.config["chunk_size"]
        if size not in chunk_size_groups:
            chunk_size_groups[size] = []
        chunk_size_groups[size].append(result)
    
    for size, results in chunk_size_groups.items():
        avg_metrics = {
            "precision_omega": sum(r.metrics.precision_omega for r in results) / len(results),
            "precision_at_k": {
                "1": sum(r.metrics.precision_at_k.get(1, 0) for r in results) / len(results),
                "3": sum(r.metrics.precision_at_k.get(3, 0) for r in results) / len(results),
                "5": sum(r.metrics.precision_at_k.get(5, 0) for r in results) / len(results),
                "10": sum(r.metrics.precision_at_k.get(10, 0) for r in results) / len(results)
            },
            "recall_at_k": {
                "1": sum(r.metrics.recall_at_k.get(1, 0) for r in results) / len(results),
                "3": sum(r.metrics.recall_at_k.get(3, 0) for r in results) / len(results),
                "5": sum(r.metrics.recall_at_k.get(5, 0) for r in results) / len(results),
                "10": sum(r.metrics.recall_at_k.get(10, 0) for r in results) / len(results)
            },
            "avg_chunk_count": sum(r.metrics.chunk_count for r in results) / len(results),
            "avg_chunk_length": sum(r.metrics.avg_chunk_length for r in results) / len(results),
            "length_variance": sum(r.metrics.length_variance for r in results) / len(results)
        }
        comparison["chunk_size_analysis"][size] = avg_metrics
    
    # æŒ‰overlap ratioåˆ†çµ„åˆ†æ
    overlap_groups = {}
    for result in task.results:
        ratio = result.config["overlap_ratio"]
        if ratio not in overlap_groups:
            overlap_groups[ratio] = []
        overlap_groups[ratio].append(result)
    
    for ratio, results in overlap_groups.items():
        avg_metrics = {
            "precision_omega": sum(r.metrics.precision_omega for r in results) / len(results),
            "precision_at_k": {
                "1": sum(r.metrics.precision_at_k.get(1, 0) for r in results) / len(results),
                "3": sum(r.metrics.precision_at_k.get(3, 0) for r in results) / len(results),
                "5": sum(r.metrics.precision_at_k.get(5, 0) for r in results) / len(results),
                "10": sum(r.metrics.precision_at_k.get(10, 0) for r in results) / len(results)
            },
            "recall_at_k": {
                "1": sum(r.metrics.recall_at_k.get(1, 0) for r in results) / len(results),
                "3": sum(r.metrics.recall_at_k.get(3, 0) for r in results) / len(results),
                "5": sum(r.metrics.recall_at_k.get(5, 0) for r in results) / len(results),
                "10": sum(r.metrics.recall_at_k.get(10, 0) for r in results) / len(results)
            },
            "avg_chunk_count": sum(r.metrics.chunk_count for r in results) / len(results),
            "avg_chunk_length": sum(r.metrics.avg_chunk_length for r in results) / len(results),
            "length_variance": sum(r.metrics.length_variance for r in results) / len(results)
        }
        comparison["overlap_analysis"][ratio] = avg_metrics
    
    # æŒ‰ç­–ç•¥ç‰¹å®šåƒæ•¸åˆ†çµ„åˆ†æ
    if task.results:
        strategy = task.results[0].config.get("strategy", "fixed_size")
        
        if strategy == "structured_hierarchical":
            # æŒ‰åˆ†å‰²å–®ä½åˆ†çµ„
            chunk_by_groups = {}
            for result in task.results:
                chunk_by = result.config.get("chunk_by", "article")
                if chunk_by not in chunk_by_groups:
                    chunk_by_groups[chunk_by] = []
                chunk_by_groups[chunk_by].append(result)
            
            for chunk_by, results in chunk_by_groups.items():
                avg_metrics = {
                    "precision_omega": sum(r.metrics.precision_omega for r in results) / len(results),
                    "precision_at_k": {
                        "1": sum(r.metrics.precision_at_k.get(1, 0) for r in results) / len(results),
                        "3": sum(r.metrics.precision_at_k.get(3, 0) for r in results) / len(results),
                        "5": sum(r.metrics.precision_at_k.get(5, 0) for r in results) / len(results),
                        "10": sum(r.metrics.precision_at_k.get(10, 0) for r in results) / len(results)
                    },
                    "recall_at_k": {
                        "1": sum(r.metrics.recall_at_k.get(1, 0) for r in results) / len(results),
                        "3": sum(r.metrics.recall_at_k.get(3, 0) for r in results) / len(results),
                        "5": sum(r.metrics.recall_at_k.get(5, 0) for r in results) / len(results),
                        "10": sum(r.metrics.recall_at_k.get(10, 0) for r in results) / len(results)
                    },
                    "avg_chunk_count": sum(r.metrics.chunk_count for r in results) / len(results),
                    "avg_chunk_length": sum(r.metrics.avg_chunk_length for r in results) / len(results),
                    "length_variance": sum(r.metrics.length_variance for r in results) / len(results)
                }
                comparison["strategy_specific_analysis"][f"chunk_by_{chunk_by}"] = avg_metrics
        
        elif strategy == "rcts_hierarchical":
            # æŒ‰ä¿æŒçµæ§‹åˆ†çµ„
            preserve_groups = {}
            for result in task.results:
                preserve = result.config.get("preserve_structure", True)
                key = "preserve_structure" if preserve else "no_preserve_structure"
                if key not in preserve_groups:
                    preserve_groups[key] = []
                preserve_groups[key].append(result)
            
            for key, results in preserve_groups.items():
                avg_metrics = {
                    "precision_omega": sum(r.metrics.precision_omega for r in results) / len(results),
                    "precision_at_k": {
                        "1": sum(r.metrics.precision_at_k.get(1, 0) for r in results) / len(results),
                        "3": sum(r.metrics.precision_at_k.get(3, 0) for r in results) / len(results),
                        "5": sum(r.metrics.precision_at_k.get(5, 0) for r in results) / len(results),
                        "10": sum(r.metrics.precision_at_k.get(10, 0) for r in results) / len(results)
                    },
                    "recall_at_k": {
                        "1": sum(r.metrics.recall_at_k.get(1, 0) for r in results) / len(results),
                        "3": sum(r.metrics.recall_at_k.get(3, 0) for r in results) / len(results),
                        "5": sum(r.metrics.recall_at_k.get(5, 0) for r in results) / len(results),
                        "10": sum(r.metrics.recall_at_k.get(10, 0) for r in results) / len(results)
                    },
                    "avg_chunk_count": sum(r.metrics.chunk_count for r in results) / len(results),
                    "avg_chunk_length": sum(r.metrics.avg_chunk_length for r in results) / len(results),
                    "length_variance": sum(r.metrics.length_variance for r in results) / len(results)
                }
                comparison["strategy_specific_analysis"][key] = avg_metrics
        
        elif strategy == "hierarchical":
            # æŒ‰å±¤æ¬¡æ·±åº¦åˆ†çµ„
            level_groups = {}
            for result in task.results:
                level = result.config.get("level_depth", 3)
                if level not in level_groups:
                    level_groups[level] = []
                level_groups[level].append(result)
            
            for level, results in level_groups.items():
                avg_metrics = {
                    "precision_omega": sum(r.metrics.precision_omega for r in results) / len(results),
                    "precision_at_k": {
                        "1": sum(r.metrics.precision_at_k.get(1, 0) for r in results) / len(results),
                        "3": sum(r.metrics.precision_at_k.get(3, 0) for r in results) / len(results),
                        "5": sum(r.metrics.precision_at_k.get(5, 0) for r in results) / len(results),
                        "10": sum(r.metrics.precision_at_k.get(10, 0) for r in results) / len(results)
                    },
                    "recall_at_k": {
                        "1": sum(r.metrics.recall_at_k.get(1, 0) for r in results) / len(results),
                        "3": sum(r.metrics.recall_at_k.get(3, 0) for r in results) / len(results),
                        "5": sum(r.metrics.recall_at_k.get(5, 0) for r in results) / len(results),
                        "10": sum(r.metrics.recall_at_k.get(10, 0) for r in results) / len(results)
                    },
                    "avg_chunk_count": sum(r.metrics.chunk_count for r in results) / len(results),
                    "avg_chunk_length": sum(r.metrics.avg_chunk_length for r in results) / len(results),
                    "length_variance": sum(r.metrics.length_variance for r in results) / len(results)
                }
                comparison["strategy_specific_analysis"][f"level_depth_{level}"] = avg_metrics
    
    # ç”Ÿæˆæ¨è–¦
    best_overall = max(task.results, key=lambda r: (
        r.metrics.precision_omega * 0.4 + 
        r.metrics.precision_at_k.get(5, 0) * 0.3 + 
        r.metrics.recall_at_k.get(5, 0) * 0.3
    ))
    
    # ç”Ÿæˆè©³ç´°çš„æ¨è–¦é…ç½®
    config_parts = []
    config_parts.append(f"chunk_size={best_overall.config['chunk_size']}")
    config_parts.append(f"overlap_ratio={best_overall.config['overlap_ratio']}")
    
    # æ·»åŠ ç­–ç•¥ç‰¹å®šåƒæ•¸
    strategy = best_overall.config.get("strategy", "fixed_size")
    if strategy == "structured_hierarchical":
        chunk_by = best_overall.config.get("chunk_by", "article")
        chunk_by_label = {"article": "æŒ‰æ¢æ–‡åˆ†å‰²", "item": "æŒ‰é …åˆ†å‰²", "section": "æŒ‰ç¯€åˆ†å‰²", "chapter": "æŒ‰ç« åˆ†å‰²"}.get(chunk_by, chunk_by)
        config_parts.append(f"chunk_by={chunk_by}({chunk_by_label})")
    elif strategy == "rcts_hierarchical":
        preserve = best_overall.config.get("preserve_structure", True)
        config_parts.append(f"preserve_structure={preserve}({'ä¿æŒçµæ§‹' if preserve else 'ä¸ä¿æŒçµæ§‹'})")
    elif strategy == "hierarchical":
        level = best_overall.config.get("level_depth", 3)
        min_size = best_overall.config.get("min_chunk_size", 200)
        config_parts.append(f"level_depth={level}")
        config_parts.append(f"min_chunk_size={min_size}")
    elif strategy == "semantic":
        threshold = best_overall.config.get("similarity_threshold", 0.6)
        context = best_overall.config.get("context_window", 100)
        config_parts.append(f"similarity_threshold={threshold}")
        config_parts.append(f"context_window={context}")
    elif strategy == "llm_semantic":
        threshold = best_overall.config.get("semantic_threshold", 0.7)
        context = best_overall.config.get("context_window", 100)
        config_parts.append(f"semantic_threshold={threshold}")
        config_parts.append(f"context_window={context}")
    elif strategy == "sliding_window":
        step = best_overall.config.get("step_size", 250)
        config_parts.append(f"step_size={step}")
    elif strategy == "hybrid":
        switch = best_overall.config.get("switch_threshold", 0.5)
        secondary = best_overall.config.get("secondary_size", 400)
        config_parts.append(f"switch_threshold={switch}")
        config_parts.append(f"secondary_size={secondary}")
    
    comparison["recommendations"] = [
        f"æœ€ä½³é…ç½®ï¼š{', '.join(config_parts)}",
        f"è©²é…ç½®çš„precision omega: {best_overall.metrics.precision_omega:.3f}",
        f"è©²é…ç½®çš„precision@5: {best_overall.metrics.precision_at_k.get(5, 0):.3f}",
        f"è©²é…ç½®çš„recall@5: {best_overall.metrics.recall_at_k.get(5, 0):.3f}",
        f"è©²é…ç½®çš„chunk count: {best_overall.metrics.chunk_count}",
        f"è©²é…ç½®çš„å¹³å‡chunké•·åº¦: {best_overall.metrics.avg_chunk_length:.1f}"
    ]
    
    return comparison


@app.post("/api/generate-questions")
def generate_questions(req: GenerateQuestionsRequest):
    """
    ç”Ÿæˆç¹é«”ä¸­æ–‡æ³•å¾‹è€ƒå¤é¡Œå¾æ³•å¾‹æ–‡æœ¬ä¸­ç”Ÿæˆå•é¡Œ
    """
    doc = store.docs.get(req.doc_id)
    if not doc:
        return JSONResponse(status_code=404, content={"error": "Document not found"})
    
    start_time = time.time()
    
    try:
        # ä½¿ç”¨Geminiç”Ÿæˆå•é¡Œ
        questions = generate_questions_with_gemini(
            doc.text, 
            req.num_questions, 
            req.question_types, 
            req.difficulty_levels
        )
        
        generation_time = time.time() - start_time
        
        # å°‡ç”Ÿæˆçš„å•é¡Œå­˜å„²åˆ°æ–‡æª”è¨˜éŒ„ä¸­
        question_texts = [q.question for q in questions]
        doc.generated_questions = question_texts
        store.docs[req.doc_id] = doc  # æ›´æ–°æ–‡æª”è¨˜éŒ„
        
        # æª¢æŸ¥æ˜¯å¦ç”Ÿæˆäº†å•é¡Œ
        if not questions:
            print("è­¦å‘Šï¼šæ²’æœ‰ç”Ÿæˆä»»ä½•å•é¡Œ")
            return JSONResponse(
                status_code=400,
                content={"error": "ç„¡æ³•å¾æ–‡æª”ä¸­ç”Ÿæˆå•é¡Œï¼Œè«‹æª¢æŸ¥æ–‡æª”å…§å®¹æ˜¯å¦åŒ…å«æ³•å¾‹æ¢æ–‡"}
            )
        
        result = QuestionGenerationResult(
            doc_id=req.doc_id,
            total_questions=len(questions),
            questions=questions,
            generation_time=generation_time,
            timestamp=datetime.now()
        )
        
        response_data = {
            "success": True,
            "result": {
                "doc_id": result.doc_id,
                "total_questions": result.total_questions,
                "generation_time": result.generation_time,
                "timestamp": result.timestamp.isoformat(),
                "questions": [
                    {
                        "question": q.question,
                        "references": q.references,
                        "question_type": q.question_type,
                        "difficulty": q.difficulty,
                        "keywords": q.keywords,
                        "estimated_tokens": q.estimated_tokens
                    }
                    for q in result.questions
                ]
            }
        }
        
        print(f"è¿”å›éŸ¿æ‡‰æ•¸æ“š: success={response_data['success']}, questions_count={len(response_data['result']['questions'])}")
        return response_data
        
    except Exception as e:
        print(f"å•é¡Œç”Ÿæˆç•°å¸¸: {str(e)}")  # æ·»åŠ æ—¥èªŒ
        return JSONResponse(
            status_code=500, 
            content={"error": f"å•é¡Œç”Ÿæˆå¤±æ•—: {str(e)}"}
        )


@app.get("/docs/schema")
def schema():
    # Minimal shape for frontend wiring/testing
    return {
        "upload": {"POST": {"multipart": True}},
        "chunk": {"POST": {"json": {"doc_id": "str", "chunk_size": "int", "overlap": "int"}}},
        "embed": {"POST": {"json": {"doc_ids": "List[str]|None"}}},
        "retrieve": {"POST": {"json": {"query": "str", "k": "int"}}},
        "generate": {"POST": {"json": {"query": "str", "top_k": "int"}}},
        "evaluate/fixed-size": {"POST": {"json": "FixedSizeEvaluationRequest"}},
        "evaluate/status/{task_id}": {"GET": {}},
        "evaluate/results/{task_id}": {"GET": {}},
        "evaluate/comparison/{task_id}": {"GET": {}},
        "generate-questions": {"POST": {"json": "GenerateQuestionsRequest"}},
        # æ–°å¢çš„å¢å¼·ç‰ˆAPIç«¯é»
        "legal-semantic-chunk": {"POST": {"json": "ChunkConfig"}},
        "multi-level-semantic-chunk": {"POST": {"json": "ChunkConfig"}},
        "build-concept-graph": {"POST": {}},
        "concept-graph-retrieve": {"POST": {"json": "RetrieveRequest"}},
        "adaptive-retrieve": {"POST": {"json": "RetrieveRequest"}},
        "strategy-performance": {"GET": {}},
        "concept-graph-info": {"GET": {}},
    }


# ============================================================================
# æ–°å¢çš„å¢å¼·ç‰ˆåŠŸèƒ½ - æ³•å¾‹èªç¾©æª¢ç´¢æ”¹é€²
# ============================================================================

# å°å…¥æ–°çš„æ¨¡çµ„
try:
    from .legal_semantic_chunking import LegalSemanticIntegrityChunking, MultiLevelSemanticChunking
    from .legal_concept_graph import LegalConceptGraph, LegalConceptGraphRetrieval
    from .adaptive_legal_rag import AdaptiveLegalRAG, QueryAnalyzer
    from .legal_reasoning_engine import legal_reasoning_engine
    from .intelligent_legal_concept_extractor import intelligent_extractor
    from .dynamic_concept_learning import dynamic_learning_system
    
    # åˆå§‹åŒ–å¢å¼·ç‰ˆçµ„ä»¶
    legal_semantic_chunker = LegalSemanticIntegrityChunking()
    multi_level_chunker = MultiLevelSemanticChunking()
    concept_graph = LegalConceptGraph()
    concept_graph_retrieval = None
    adaptive_rag = AdaptiveLegalRAG()
    
    print("âœ… å¢å¼·ç‰ˆåŠŸèƒ½æ¨¡çµ„è¼‰å…¥æˆåŠŸ")
    
except ImportError as e:
    print(f"âš ï¸  å¢å¼·ç‰ˆåŠŸèƒ½æ¨¡çµ„è¼‰å…¥å¤±æ•—: {e}")
    print("   è«‹ç¢ºä¿æ‰€æœ‰æ–°å¢æ–‡ä»¶éƒ½å­˜åœ¨")
    legal_semantic_chunker = None
    multi_level_chunker = None
    concept_graph = None
    concept_graph_retrieval = None
    adaptive_rag = None
    legal_reasoning_engine = None
    intelligent_extractor = None
    dynamic_learning_system = None


@app.post("/api/legal-semantic-chunk")
def legal_semantic_chunk(req: ChunkConfig):
    """æ³•å¾‹èªç¾©å®Œæ•´æ€§åˆ†å¡Š"""
    if not legal_semantic_chunker:
        return JSONResponse(status_code=503, content={"error": "æ³•å¾‹èªç¾©åˆ†å¡ŠåŠŸèƒ½æœªå•Ÿç”¨"})
    
    try:
        doc = store.get_doc(req.doc_id)
        if not doc:
            return JSONResponse(status_code=404, content={"error": f"æ–‡æª” {req.doc_id} ä¸å­˜åœ¨"})
        
        print(f"ğŸ” é–‹å§‹æ³•å¾‹èªç¾©å®Œæ•´æ€§åˆ†å¡Šï¼Œæ–‡æª”: {doc.filename}")
        
        # ä½¿ç”¨æ³•å¾‹èªç¾©å®Œæ•´æ€§åˆ†å¡Š
        chunks_with_span = legal_semantic_chunker.chunk(
            doc.text,
            max_chunk_size=req.chunk_size,
            overlap_ratio=req.overlap_ratio,
            preserve_concepts=True
        )
        
        # æå–ç´”æ–‡æœ¬chunks
        chunks = [chunk["content"] for chunk in chunks_with_span]
        
        # æ›´æ–°æ–‡æª”è¨˜éŒ„
        doc.chunks = chunks
        doc.chunk_size = req.chunk_size
        doc.overlap = int(req.chunk_size * req.overlap_ratio)
        doc.structured_chunks = chunks_with_span
        doc.chunking_strategy = "legal_semantic_integrity"
        store.add_doc(doc)
        
        store.reset_embeddings()
        
        # è¨ˆç®—çµ±è¨ˆä¿¡æ¯
        chunk_lengths = [len(chunk) for chunk in chunks] if chunks else []
        avg_chunk_length = sum(chunk_lengths) / len(chunk_lengths) if chunk_lengths else 0
        min_length = min(chunk_lengths) if chunk_lengths else 0
        max_length = max(chunk_lengths) if chunk_lengths else 0
        
        if chunk_lengths:
            variance = sum((length - avg_chunk_length) ** 2 for length in chunk_lengths) / len(chunk_lengths)
        else:
            variance = 0
        
        # è¨ˆç®—æ¦‚å¿µå®Œæ•´æ€§çµ±è¨ˆ
        concept_stats = _calculate_concept_statistics(chunks_with_span)
        
        return {
            "doc_id": req.doc_id,
            "chunk_count": len(chunks),
            "avg_chunk_length": avg_chunk_length,
            "min_chunk_length": min_length,
            "max_chunk_length": max_length,
            "length_variance": variance,
            "strategy": "legal_semantic_integrity",
            "config": req.dict(),
            "chunks_with_span": chunks_with_span,
            "concept_statistics": concept_stats
        }
        
    except Exception as e:
        print(f"âŒ æ³•å¾‹èªç¾©åˆ†å¡ŠéŒ¯èª¤: {e}")
        return JSONResponse(status_code=500, content={"error": f"åˆ†å¡ŠéŒ¯èª¤: {str(e)}"})


@app.post("/api/multi-level-semantic-chunk")
def multi_level_semantic_chunk(req: ChunkConfig):
    """å¤šå±¤æ¬¡èªç¾©åˆ†å¡Š"""
    if not multi_level_chunker:
        return JSONResponse(status_code=503, content={"error": "å¤šå±¤æ¬¡èªç¾©åˆ†å¡ŠåŠŸèƒ½æœªå•Ÿç”¨"})
    
    try:
        doc = store.get_doc(req.doc_id)
        if not doc:
            return JSONResponse(status_code=404, content={"error": f"æ–‡æª” {req.doc_id} ä¸å­˜åœ¨"})
        
        print(f"ğŸ” é–‹å§‹å¤šå±¤æ¬¡èªç¾©åˆ†å¡Šï¼Œæ–‡æª”: {doc.filename}")
        
        # ä½¿ç”¨å¤šå±¤æ¬¡èªç¾©åˆ†å¡Š
        multi_level_chunks = multi_level_chunker.chunk(
            doc.text,
            max_chunk_size=req.chunk_size,
            overlap_ratio=req.overlap_ratio
        )
        
        # ä¿å­˜å¤šå±¤æ¬¡åˆ†å¡Šçµæœ
        doc.multi_level_chunks = multi_level_chunks
        doc.chunking_strategy = "multi_level_semantic"
        store.add_doc(doc)
        
        store.reset_embeddings()
        
        # è¨ˆç®—å„å±¤æ¬¡çµ±è¨ˆ
        level_statistics = {}
        for level_name, level_chunks in multi_level_chunks.items():
            chunk_lengths = [len(chunk["content"]) for chunk in level_chunks]
            level_statistics[level_name] = {
                "chunk_count": len(level_chunks),
                "avg_length": sum(chunk_lengths) / len(chunk_lengths) if chunk_lengths else 0,
                "min_length": min(chunk_lengths) if chunk_lengths else 0,
                "max_length": max(chunk_lengths) if chunk_lengths else 0
            }
        
        return {
            "doc_id": req.doc_id,
            "strategy": "multi_level_semantic",
            "config": req.dict(),
            "multi_level_chunks": multi_level_chunks,
            "level_statistics": level_statistics
        }
        
    except Exception as e:
        print(f"âŒ å¤šå±¤æ¬¡èªç¾©åˆ†å¡ŠéŒ¯èª¤: {e}")
        return JSONResponse(status_code=500, content={"error": f"åˆ†å¡ŠéŒ¯èª¤: {str(e)}"})


@app.post("/api/build-concept-graph")
def build_concept_graph():
    """æ§‹å»ºæ³•å¾‹æ¦‚å¿µåœ–"""
    if not concept_graph:
        return JSONResponse(status_code=503, content={"error": "æ¦‚å¿µåœ–åŠŸèƒ½æœªå•Ÿç”¨"})
    
    try:
        print("ğŸ”¨ é–‹å§‹æ§‹å»ºæ³•å¾‹æ¦‚å¿µåœ–...")
        
        # ç²å–æ‰€æœ‰æ–‡æª”
        docs = store.list_docs()
        if not docs:
            return JSONResponse(status_code=400, content={"error": "æ²’æœ‰æ–‡æª”å¯ç”¨"})
        
        # æº–å‚™æ–‡æª”æ•¸æ“š
        documents = []
        for doc in docs:
            if doc.chunks:
                for i, chunk in enumerate(doc.chunks):
                    documents.append({
                        'content': chunk,
                        'doc_id': doc.id,
                        'chunk_index': i,
                        'filename': doc.filename
                    })
        
        if not documents:
            return JSONResponse(status_code=400, content={"error": "æ²’æœ‰å¯ç”¨çš„æ–‡æª”å…§å®¹"})
        
        # æ§‹å»ºæ¦‚å¿µåœ–
        concept_graph.build_graph(documents)
        
        # åˆå§‹åŒ–æ¦‚å¿µåœ–æª¢ç´¢
        global concept_graph_retrieval
        concept_graph_retrieval = LegalConceptGraphRetrieval(concept_graph)
        
        # è¨»å†Šåˆ°è‡ªé©æ‡‰RAG
        if adaptive_rag:
            adaptive_rag.register_strategy('concept_graph', concept_graph_retrieval)
        
        # ç²å–æ¦‚å¿µåœ–çµ±è¨ˆ
        graph_stats = {
            'node_count': concept_graph.graph.number_of_nodes(),
            'edge_count': concept_graph.graph.number_of_edges(),
            'concept_count': len(concept_graph.concepts),
            'relation_count': len(concept_graph.relations)
        }
        
        print(f"âœ… æ¦‚å¿µåœ–æ§‹å»ºå®Œæˆ: {graph_stats}")
        
        return {
            "status": "success",
            "message": "æ¦‚å¿µåœ–æ§‹å»ºå®Œæˆ",
            "statistics": graph_stats
        }
        
    except Exception as e:
        print(f"âŒ æ¦‚å¿µåœ–æ§‹å»ºéŒ¯èª¤: {e}")
        return JSONResponse(status_code=500, content={"error": f"æ§‹å»ºéŒ¯èª¤: {str(e)}"})


@app.post("/api/concept-graph-retrieve")
def concept_graph_retrieve(req: RetrieveRequest):
    """æ¦‚å¿µåœ–æª¢ç´¢"""
    if not concept_graph_retrieval:
        return JSONResponse(status_code=400, content={"error": "æ¦‚å¿µåœ–æœªæ§‹å»ºï¼Œè«‹å…ˆèª¿ç”¨ /api/build-concept-graph"})
    
    try:
        print(f"ğŸ” é–‹å§‹æ¦‚å¿µåœ–æª¢ç´¢ï¼ŒæŸ¥è©¢: '{req.query}'")
        
        # åŸ·è¡Œæ¦‚å¿µåœ–æª¢ç´¢
        results = concept_graph_retrieval.retrieve(req.query, req.k)
        
        # è¨ˆç®—æª¢ç´¢æŒ‡æ¨™
        metrics = calculate_retrieval_metrics(req.query, results, req.k)
        
        # æ·»åŠ æ¦‚å¿µåœ–ç‰¹å®šä¿¡æ¯
        metrics["concept_graph_analysis"] = {
            "reasoning_paths_used": len(set(r.get('reasoning_path', []) for r in results)),
            "concept_matches": len([r for r in results if r.get('concept_based', False)]),
            "avg_reasoning_score": sum(r.get('reasoning_score', 0) for r in results) / len(results) if results else 0
        }
        
        metrics["note"] = f"æ¦‚å¿µåœ–æª¢ç´¢: ä½¿ç”¨{metrics['concept_graph_analysis']['reasoning_paths_used']}æ¢æ¨ç†è·¯å¾‘"
        
        return {
            "results": results,
            "metrics": metrics,
            "embedding_provider": "concept_graph",
            "embedding_model": "legal_concept_reasoning"
        }
        
    except Exception as e:
        print(f"âŒ æ¦‚å¿µåœ–æª¢ç´¢éŒ¯èª¤: {e}")
        return JSONResponse(status_code=500, content={"error": f"æª¢ç´¢éŒ¯èª¤: {str(e)}"})


@app.post("/api/adaptive-retrieve")
def adaptive_retrieve(req: RetrieveRequest):
    """è‡ªé©æ‡‰æª¢ç´¢"""
    if not adaptive_rag:
        return JSONResponse(status_code=503, content={"error": "è‡ªé©æ‡‰æª¢ç´¢åŠŸèƒ½æœªå•Ÿç”¨"})
    
    try:
        print(f"ğŸš€ é–‹å§‹è‡ªé©æ‡‰æª¢ç´¢ï¼ŒæŸ¥è©¢: '{req.query}'")
        
        # ç¢ºä¿æª¢ç´¢ç­–ç•¥å·²è¨»å†Š
        if not adaptive_rag.retrieval_strategies:
            _register_default_strategies()
        
        # åŸ·è¡Œè‡ªé©æ‡‰æª¢ç´¢
        results = adaptive_rag.retrieve(req.query, req.k)
        
        # è¨ˆç®—æª¢ç´¢æŒ‡æ¨™
        metrics = calculate_retrieval_metrics(req.query, results, req.k)
        
        # æ·»åŠ è‡ªé©æ‡‰æª¢ç´¢ç‰¹å®šä¿¡æ¯
        if results:
            first_result = results[0]
            contributing_strategies = first_result.get('contributing_strategies', [])
            strategy_count = first_result.get('strategy_count', 0)
            
            metrics["adaptive_analysis"] = {
                "strategies_used": list(set(contributing_strategies)),
                "strategy_count": strategy_count,
                "fusion_performed": first_result.get('metadata', {}).get('adaptive_fusion', False),
                "avg_fused_score": sum(r.get('fused_score', 0) for r in results) / len(results)
            }
            
            metrics["note"] = f"è‡ªé©æ‡‰æª¢ç´¢: èåˆ{strategy_count}å€‹ç­–ç•¥"
        
        return {
            "results": results,
            "metrics": metrics,
            "embedding_provider": "adaptive_rag",
            "embedding_model": "multi_strategy_fusion"
        }
        
    except Exception as e:
        print(f"âŒ è‡ªé©æ‡‰æª¢ç´¢éŒ¯èª¤: {e}")
        return JSONResponse(status_code=500, content={"error": f"æª¢ç´¢éŒ¯èª¤: {str(e)}"})


@app.get("/api/strategy-performance")
def get_strategy_performance():
    """ç²å–ç­–ç•¥æ€§èƒ½çµ±è¨ˆ"""
    if not adaptive_rag:
        return JSONResponse(status_code=503, content={"error": "è‡ªé©æ‡‰æª¢ç´¢åŠŸèƒ½æœªå•Ÿç”¨"})
    
    try:
        performance = adaptive_rag.performance_monitor.get_strategy_performance()
        
        return {
            "strategy_performance": performance,
            "total_retrievals": len(adaptive_rag.performance_monitor.retrieval_history)
        }
        
    except Exception as e:
        print(f"âŒ ç²å–ç­–ç•¥æ€§èƒ½éŒ¯èª¤: {e}")
        return JSONResponse(status_code=500, content={"error": f"ç²å–æ€§èƒ½éŒ¯èª¤: {str(e)}"})


@app.get("/api/concept-graph-info")
def get_concept_graph_info():
    """ç²å–æ¦‚å¿µåœ–ä¿¡æ¯"""
    if not concept_graph:
        return JSONResponse(status_code=503, content={"error": "æ¦‚å¿µåœ–åŠŸèƒ½æœªå•Ÿç”¨"})
    
    try:
        # ç²å–æ¦‚å¿µåˆ—è¡¨
        concepts_info = []
        for concept_id, concept in concept_graph.concepts.items():
            concepts_info.append({
                "concept_id": concept_id,
                "concept_name": concept.concept_name,
                "content": concept.content[:200] + "..." if len(concept.content) > 200 else concept.content,
                "importance_score": concept.importance_score,
                "frequency": concept.frequency
            })
        
        # ç²å–é—œä¿‚åˆ—è¡¨
        relations_info = []
        for relation in concept_graph.relations:
            relations_info.append({
                "source": relation.source_concept,
                "target": relation.target_concept,
                "relation_type": relation.relation_type,
                "confidence": relation.confidence
            })
        
        # ç²å–åœ–çµ±è¨ˆ
        graph_stats = {
            "node_count": concept_graph.graph.number_of_nodes(),
            "edge_count": concept_graph.graph.number_of_edges(),
            "concept_count": len(concept_graph.concepts),
            "relation_count": len(concept_graph.relations)
        }
        
        # ç²å–åº¦ä¸­å¿ƒæ€§æœ€é«˜çš„æ¦‚å¿µï¼ˆå‰10å€‹ï¼‰
        if concept_graph.graph.number_of_nodes() > 0:
            import networkx as nx
            centrality = nx.degree_centrality(concept_graph.graph)
            top_concepts = sorted(centrality.items(), key=lambda x: x[1], reverse=True)[:10]
            top_concepts_info = [
                {
                    "concept_id": concept_id,
                    "concept_name": concept_graph.concepts[concept_id].concept_name,
                    "centrality": centrality_score
                }
                for concept_id, centrality_score in top_concepts
            ]
        else:
            top_concepts_info = []
        
        return {
            "graph_statistics": graph_stats,
            "top_concepts": top_concepts_info,
            "concepts": concepts_info[:20],  # åªè¿”å›å‰20å€‹æ¦‚å¿µ
            "relations": relations_info[:20],  # åªè¿”å›å‰20å€‹é—œä¿‚
            "total_concepts": len(concepts_info),
            "total_relations": len(relations_info)
        }
        
    except Exception as e:
        print(f"âŒ ç²å–æ¦‚å¿µåœ–ä¿¡æ¯éŒ¯èª¤: {e}")
        return JSONResponse(status_code=500, content={"error": f"ç²å–æ¦‚å¿µåœ–ä¿¡æ¯éŒ¯èª¤: {str(e)}"})


@app.post("/api/legal-reasoning")
def analyze_legal_query(request: Dict[str, str]):
    """æ³•å¾‹æ¨ç†åˆ†æ"""
    if not legal_reasoning_engine:
        return JSONResponse(status_code=503, content={"error": "æ³•å¾‹æ¨ç†å¼•æ“æœªå•Ÿç”¨"})
    
    try:
        query = request.get("query", "")
        if not query:
            return JSONResponse(status_code=400, content={"error": "æŸ¥è©¢ä¸èƒ½ç‚ºç©º"})
        
        print(f"ğŸ” é–‹å§‹æ³•å¾‹æ¨ç†åˆ†æï¼ŒæŸ¥è©¢: '{query}'")
        
        # åŸ·è¡Œæ¨ç†åˆ†æ
        analysis = legal_reasoning_engine.analyze_query(query)
        
        return {
            "analysis_result": analysis,
            "status": "success"
        }
        
    except Exception as e:
        print(f"âŒ æ³•å¾‹æ¨ç†åˆ†æéŒ¯èª¤: {e}")
        return JSONResponse(status_code=500, content={"error": f"æ¨ç†åˆ†æéŒ¯èª¤: {str(e)}"})


@app.post("/api/extract-legal-concepts")
def extract_legal_concepts():
    """æ™ºèƒ½æå–æ³•å¾‹æ¦‚å¿µ"""
    if not intelligent_extractor:
        return JSONResponse(status_code=503, content={"error": "æ™ºèƒ½æ¦‚å¿µæå–å™¨æœªå•Ÿç”¨"})
    
    try:
        print("ğŸ” é–‹å§‹æ™ºèƒ½æ³•å¾‹æ¦‚å¿µæå–...")
        
        # ç²å–æ‰€æœ‰æ–‡æª”
        docs = store.list_docs()
        if not docs:
            return JSONResponse(status_code=400, content={"error": "æ²’æœ‰æ–‡æª”å¯ç”¨"})
        
        # æº–å‚™æ–‡æª”æ•¸æ“š
        documents = []
        for doc in docs:
            if hasattr(doc, 'structured_chunks') and doc.structured_chunks:
                documents.append({
                    'filename': doc.filename,
                    'structured_chunks': doc.structured_chunks
                })
        
        if not documents:
            return JSONResponse(status_code=400, content={"error": "æ²’æœ‰çµæ§‹åŒ–åˆ†å¡Šæ•¸æ“š"})
        
        # åŸ·è¡Œæ¦‚å¿µæå–
        extraction_result = intelligent_extractor.extract_concepts_from_documents(documents)
        
        # ä¿å­˜æå–çµæœåˆ°å…¨å±€è®Šé‡
        global extracted_legal_concepts
        extracted_legal_concepts = extraction_result
        
        return {
            "extraction_result": extraction_result,
            "status": "success"
        }
        
    except Exception as e:
        print(f"âŒ æ¦‚å¿µæå–éŒ¯èª¤: {e}")
        return JSONResponse(status_code=500, content={"error": f"æ¦‚å¿µæå–éŒ¯èª¤: {str(e)}"})


@app.post("/api/learn-from-feedback")
def learn_from_feedback(request: Dict[str, Any]):
    """å¾ç”¨æˆ¶åé¥‹ä¸­å­¸ç¿’"""
    if not dynamic_learning_system:
        return JSONResponse(status_code=503, content={"error": "å‹•æ…‹å­¸ç¿’ç³»çµ±æœªå•Ÿç”¨"})
    
    try:
        query = request.get("query", "")
        retrieved_results = request.get("retrieved_results", [])
        user_feedback = request.get("user_feedback", {})
        
        if not query:
            return JSONResponse(status_code=400, content={"error": "æŸ¥è©¢ä¸èƒ½ç‚ºç©º"})
        
        print(f"ğŸ§  é–‹å§‹å¾åé¥‹ä¸­å­¸ç¿’: '{query}'")
        
        # åŸ·è¡Œå­¸ç¿’
        learning_result = dynamic_learning_system.learn_from_query_feedback(
            query, retrieved_results, user_feedback
        )
        
        return {
            "learning_result": learning_result,
            "status": "success"
        }
        
    except Exception as e:
        print(f"âŒ å­¸ç¿’éŒ¯èª¤: {e}")
        return JSONResponse(status_code=500, content={"error": f"å­¸ç¿’éŒ¯èª¤: {str(e)}"})


@app.get("/api/learning-statistics")
def get_learning_statistics():
    """ç²å–å­¸ç¿’çµ±è¨ˆ"""
    if not dynamic_learning_system:
        return JSONResponse(status_code=503, content={"error": "å‹•æ…‹å­¸ç¿’ç³»çµ±æœªå•Ÿç”¨"})
    
    try:
        statistics = dynamic_learning_system.get_learning_statistics()
        
        return {
            "statistics": statistics,
            "status": "success"
        }
        
    except Exception as e:
        print(f"âŒ ç²å–å­¸ç¿’çµ±è¨ˆéŒ¯èª¤: {e}")
        return JSONResponse(status_code=500, content={"error": f"ç²å–å­¸ç¿’çµ±è¨ˆéŒ¯èª¤: {str(e)}"})


@app.post("/api/enhanced-query-expansion")
def enhanced_query_expansion(request: Dict[str, str]):
    """å¢å¼·æŸ¥è©¢æ“´å±•"""
    if not dynamic_learning_system:
        return JSONResponse(status_code=503, content={"error": "å‹•æ…‹å­¸ç¿’ç³»çµ±æœªå•Ÿç”¨"})
    
    try:
        query = request.get("query", "")
        if not query:
            return JSONResponse(status_code=400, content={"error": "æŸ¥è©¢ä¸èƒ½ç‚ºç©º"})
        
        print(f"ğŸ” é–‹å§‹å¢å¼·æŸ¥è©¢æ“´å±•: '{query}'")
        
        # åŸ·è¡Œå¢å¼·æŸ¥è©¢æ“´å±•
        expansion_result = dynamic_learning_system.generate_enhanced_query_expansion(query)
        
        return {
            "expansion_result": expansion_result,
            "status": "success"
        }
        
    except Exception as e:
        print(f"âŒ å¢å¼·æŸ¥è©¢æ“´å±•éŒ¯èª¤: {e}")
        return JSONResponse(status_code=500, content={"error": f"å¢å¼·æŸ¥è©¢æ“´å±•éŒ¯èª¤: {str(e)}"})


def _calculate_concept_statistics(chunks_with_span: List[Dict[str, Any]]) -> Dict[str, Any]:
    """è¨ˆç®—æ¦‚å¿µçµ±è¨ˆä¿¡æ¯"""
    stats = {
        "total_chunks": len(chunks_with_span),
        "concept_chunks": 0,
        "definition_chunks": 0,
        "exception_chunks": 0,
        "condition_chunks": 0,
        "avg_importance_score": 0.0,
        "concept_density": 0.0
    }
    
    total_importance = 0.0
    total_concept_count = 0
    
    for chunk in chunks_with_span:
        metadata = chunk.get("metadata", {})
        semantic_features = metadata.get("semantic_features", {})
        
        concept_count = semantic_features.get("concept_count", 0)
        importance_score = semantic_features.get("importance_score", 0.0)
        
        if concept_count > 0:
            stats["concept_chunks"] += 1
            total_importance += importance_score
            total_concept_count += concept_count
        
        if semantic_features.get("has_definition", False):
            stats["definition_chunks"] += 1
        
        if semantic_features.get("has_exception", False):
            stats["exception_chunks"] += 1
        
        if semantic_features.get("has_condition", False):
            stats["condition_chunks"] += 1
    
    if stats["concept_chunks"] > 0:
        stats["avg_importance_score"] = total_importance / stats["concept_chunks"]
    
    if len(chunks_with_span) > 0:
        stats["concept_density"] = total_concept_count / len(chunks_with_span)
    
    return stats


def _register_default_strategies():
    """è¨»å†Šé»˜èªæª¢ç´¢ç­–ç•¥"""
    if not adaptive_rag:
        return
        
    # è¨»å†Šå‘é‡æª¢ç´¢
    adaptive_rag.register_strategy('vector_search', {
        'retrieve': lambda query, **kwargs: retrieve_original(query, kwargs.get('k', 5))
    })
    
    # è¨»å†ŠHybridRAG
    adaptive_rag.register_strategy('hybrid_rag', {
        'retrieve': lambda query, **kwargs: hybrid_retrieve_original(query, kwargs.get('k', 5))
    })
    
    # è¨»å†Šå¤šå±¤æ¬¡æª¢ç´¢
    adaptive_rag.register_strategy('hierarchical', {
        'retrieve': lambda query, **kwargs: hierarchical_retrieve_original(query, kwargs.get('k', 5))
    })


def retrieve_original(query: str, k: int):
    """åŸå§‹å‘é‡æª¢ç´¢"""
    # é€™è£¡èª¿ç”¨åŸæœ‰çš„æª¢ç´¢é‚è¼¯
    pass


def hybrid_retrieve_original(query: str, k: int):
    """åŸå§‹HybridRAGæª¢ç´¢"""
    # é€™è£¡èª¿ç”¨åŸæœ‰çš„HybridRAGé‚è¼¯
    pass


def hierarchical_retrieve_original(query: str, k: int):
    """åŸå§‹å¤šå±¤æ¬¡æª¢ç´¢"""
    # é€™è£¡èª¿ç”¨åŸæœ‰çš„å¤šå±¤æ¬¡æª¢ç´¢é‚è¼¯
    pass


# ==================== HopRAG API ç«¯é» ====================

@app.post("/api/build-hoprag-graph")
async def build_hoprag_graph():
    """æ§‹å»ºHopRAGåœ–è­œ"""
    try:
        print("ğŸ—ï¸ é–‹å§‹HopRAGåœ–è­œæ§‹å»º...")
        
        # æª¢æŸ¥å¤šå±¤æ¬¡embeddingæ˜¯å¦å¯ç”¨
        if not store.has_multi_level_embeddings():
            return JSONResponse(
                status_code=400,
                content={"error": "Multi-level embeddings not available. Please run /api/multi-level-embed first."}
            )
        
        # å¾ç¾æœ‰çš„å¤šå±¤æ¬¡chunksæ§‹å»ºHopRAGåœ–
        multi_level_chunks = {}
        doc_status = {}
        for doc_id, doc in store.docs.items():
            doc_status[doc_id] = {
                "has_multi_level_chunks": hasattr(doc, 'multi_level_chunks') and doc.multi_level_chunks is not None,
                "chunking_strategy": getattr(doc, 'chunking_strategy', 'unknown'),
                "multi_level_chunks_count": len(doc.multi_level_chunks) if hasattr(doc, 'multi_level_chunks') and doc.multi_level_chunks else 0
            }
            if hasattr(doc, 'multi_level_chunks') and doc.multi_level_chunks:
                multi_level_chunks[doc_id] = doc.multi_level_chunks
        
        if not multi_level_chunks:
            print(f"âŒ æ²’æœ‰æ‰¾åˆ°å¤šå±¤æ¬¡chunksï¼Œæ–‡æª”ç‹€æ…‹: {doc_status}")
            return JSONResponse(
                status_code=400,
                content={
                    "error": "No multi-level chunks available. Please run multi-level chunking first.",
                    "doc_status": doc_status,
                    "available_levels": store.get_available_levels()
                }
            )
        
        print(f"ğŸ“Š æ‰¾åˆ° {len(multi_level_chunks)} å€‹æ–‡æª”çš„å¤šå±¤æ¬¡chunks")
        
        # æ§‹å»ºHopRAGåœ–
        await hoprag_system.build_graph_from_multi_level_chunks(multi_level_chunks)
        
        # ç²å–çµ±è¨ˆä¿¡æ¯
        stats = hoprag_system.get_graph_statistics()
        
        print(f"âœ… HopRAGåœ–è­œæ§‹å»ºæˆåŠŸï¼ç¯€é»: {stats.get('total_nodes', 0)}, é‚Š: {stats.get('total_edges', 0)}")
        
        return {
            "message": "HopRAG graph built successfully",
            "statistics": stats,
            "timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        print(f"âŒ HopRAGåœ–æ§‹å»ºå¤±æ•—: {e}")
        import traceback
        traceback.print_exc()
        return JSONResponse(
            status_code=500,
            content={"error": f"Failed to build HopRAG graph: {str(e)}"}
        )


@app.get("/api/hoprag-progress")
def get_hoprag_progress():
    """ç²å–HopRAGåœ–è­œæ§‹å»ºé€²åº¦"""
    try:
        # é€™è£¡å¯ä»¥è¿”å›ç•¶å‰çš„æ§‹å»ºé€²åº¦
        # ç”±æ–¼HopRAGç³»çµ±æ²’æœ‰å…§å»ºé€²åº¦è¿½è¹¤ï¼Œæˆ‘å€‘è¿”å›åŸºæœ¬ç‹€æ…‹
        return {
            "status": "building" if not hoprag_system.is_graph_built else "completed",
            "message": "HopRAGåœ–è­œæ§‹å»ºä¸­ï¼Œè«‹æŸ¥çœ‹æœå‹™å™¨æ—¥èªŒäº†è§£è©³ç´°é€²åº¦",
            "timestamp": datetime.now().isoformat()
        }
    except Exception as e:
        return JSONResponse(
            status_code=500,
            content={"error": f"Failed to get progress: {str(e)}"}
        )

@app.get("/api/hoprag-status")
def get_hoprag_status():
    """ç²å–HopRAGç³»çµ±ç‹€æ…‹"""
    try:
        client_status = hoprag_client_manager.get_client_status()
        graph_stats = hoprag_system.get_graph_statistics()
        module_status = hoprag_system.get_module_status()
        
        return {
            "client_status": client_status,
            "graph_statistics": graph_stats,
            "module_status": module_status,
            "system_ready": hoprag_system.is_graph_built,
            "timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        return JSONResponse(
            status_code=500,
            content={"error": f"Failed to get HopRAG status: {str(e)}"}
        )


@app.post("/api/hoprag-config")
def update_hoprag_config(config_data: dict):
    """æ›´æ–°HopRAGé…ç½®"""
    try:
        new_config = HopRAGConfig.from_dict(config_data)
        new_config.validate()
        
        hoprag_system.update_config(new_config)
        
        return {
            "message": "HopRAG configuration updated successfully",
            "config": new_config.to_dict(),
            "timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        return JSONResponse(
            status_code=400,
            content={"error": f"Invalid configuration: {str(e)}"}
        )


@app.get("/api/hoprag-config")
def get_hoprag_config():
    """ç²å–ç•¶å‰HopRAGé…ç½®"""
    try:
        return {
            "config": hoprag_system.config.to_dict(),
            "timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        return JSONResponse(
            status_code=500,
            content={"error": f"Failed to get configuration: {str(e)}"}
        )


@app.post("/api/hoprag-export")
def export_hoprag_graph():
    """å°å‡ºHopRAGåœ–æ•¸æ“š"""
    try:
        if not hoprag_system.is_graph_built:
            return JSONResponse(
                status_code=400,
                content={"error": "HopRAG graph not built. Please run /api/build-hoprag-graph first."}
            )
        
        graph_data = hoprag_system.export_graph_data()
        
        return {
            "message": "HopRAG graph data exported successfully",
            "data": graph_data,
            "timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        return JSONResponse(
            status_code=500,
            content={"error": f"Failed to export graph data: {str(e)}"}
        )


@app.post("/api/hoprag-import")
def import_hoprag_graph(graph_data: dict):
    """å°å…¥HopRAGåœ–æ•¸æ“š"""
    try:
        hoprag_system.import_graph_data(graph_data)
        
        return {
            "message": "HopRAG graph data imported successfully",
            "statistics": hoprag_system.get_graph_statistics(),
            "timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        return JSONResponse(
            status_code=500,
            content={"error": f"Failed to import graph data: {str(e)}"}
        )


@app.post("/api/hoprag-reset")
def reset_hoprag_system():
    """é‡ç½®HopRAGç³»çµ±"""
    try:
        hoprag_system.reset_system()
        
        return {
            "message": "HopRAG system reset successfully",
            "timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        return JSONResponse(
            status_code=500,
            content={"error": f"Failed to reset system: {str(e)}"}
        )


@app.post("/api/hoprag-enhanced-retrieve")
async def hoprag_enhanced_retrieve(req: RetrieveRequest):
    """HopRAGå¢å¼·æª¢ç´¢"""
    try:
        # æª¢æŸ¥HopRAGåœ–æ˜¯å¦å·²æ§‹å»º
        if not hoprag_system.is_graph_built:
            return JSONResponse(
                status_code=400,
                content={"error": "HopRAG graph not built. Please run /api/build-hoprag-graph first."}
            )
        
        # æª¢æŸ¥å¤šå±¤æ¬¡embeddingæ˜¯å¦å¯ç”¨
        if not store.has_multi_level_embeddings():
            return JSONResponse(
                status_code=400,
                content={"error": "Multi-level embeddings not available. Please run /api/multi-level-embed first."}
            )
        
        # åŸ·è¡ŒåŸºç¤æª¢ç´¢ï¼ˆä½¿ç”¨ç¾æœ‰çš„å¤šå±¤æ¬¡æª¢ç´¢ï¼‰
        base_strategy = getattr(req, 'base_strategy', 'multi_level')
        use_hoprag = getattr(req, 'use_hoprag', True)
        
        if base_strategy == 'multi_level':
            base_results = multi_level_retrieve_original(req.query, k=20)
        elif base_strategy == 'single_level':
            base_results = hierarchical_retrieve_original(req.query, k=20)
        else:
            base_results = hybrid_retrieve_original(req.query, k=20)
        
        # HopRAGå¢å¼·è™•ç†
        if use_hoprag:
            enhanced_results = await hoprag_system.enhanced_retrieve(
                query=req.query,
                base_results=base_results,
                k=req.k
            )
        else:
            enhanced_results = base_results[:req.k]
        
        return {
            "query": req.query,
            "results": enhanced_results,
            "strategy": "hoprag_enhanced",
            "base_strategy": base_strategy,
            "hoprag_enabled": use_hoprag,
            "num_results": len(enhanced_results),
            "timestamp": datetime.now().isoformat()
        }
        
    except Exception as e:
        print(f"âŒ HopRAGå¢å¼·æª¢ç´¢å¤±æ•—: {e}")
        return JSONResponse(
            status_code=500,
            content={"error": f"HopRAG enhanced retrieval failed: {str(e)}"}
        )


def multi_level_retrieve_original(query: str, k: int):
    """åŸå§‹å¤šå±¤æ¬¡æª¢ç´¢ï¼ˆç”¨æ–¼HopRAGåŸºç¤æª¢ç´¢ï¼‰"""
    try:
        # æª¢æŸ¥æ˜¯å¦æœ‰å¯ç”¨çš„å¤šå±¤æ¬¡embedding
        if not store.has_multi_level_embeddings():
            return []
        
        # åˆ†ææŸ¥è©¢ä¸¦åˆ†é¡
        query_analysis = get_query_analysis(query)
        recommended_level = query_analysis['recommended_level']
        
        # ç²å–å¯ç”¨çš„embeddingå±¤æ¬¡
        available_levels = store.get_available_levels()
        
        # æª¢æŸ¥æ¨è–¦å±¤æ¬¡æ˜¯å¦å¯ç”¨ï¼Œå¦‚æœä¸å¯ç”¨å‰‡é¸æ“‡æœ€ä½³å¯ç”¨å±¤æ¬¡
        if recommended_level not in available_levels:
            fallback_levels = ['basic_unit', 'basic_unit_component', 'enumeration', 'basic_unit_hierarchy', 'document_component', 'document']
            for fallback_level in fallback_levels:
                if fallback_level in available_levels:
                    recommended_level = fallback_level
                    break
        
        # ç²å–æ¨è–¦å±¤æ¬¡çš„embedding
        level_data = store.get_multi_level_embeddings(recommended_level)
        if not level_data:
            return []
        
        vectors = level_data['embeddings']
        chunks = level_data['chunks']
        doc_ids = level_data['doc_ids']
        
        # è¨ˆç®—æŸ¥è©¢embedding
        if USE_GEMINI_EMBEDDING and GOOGLE_API_KEY:
            query_vector = asyncio.run(embed_gemini([query]))[0]
        elif USE_BGE_M3_EMBEDDING and SENTENCE_TRANSFORMERS_AVAILABLE:
            query_vector = embed_bge_m3([query])[0]
        else:
            return []
        
        # è¨ˆç®—ç›¸ä¼¼åº¦
        import numpy as np
        if isinstance(vectors, list):
            vectors = np.array(vectors)
        if isinstance(query_vector, list):
            query_vector = np.array(query_vector)
        
        similarities = cosine_similarity([query_vector], vectors)[0]
        
        # ç²å–top-kçµæœ
        top_indices = np.argsort(similarities)[::-1][:k]
        
        results = []
        for i, idx in enumerate(top_indices):
            chunk_content = chunks[idx]
            doc_id = doc_ids[idx]
            similarity_score = similarities[idx]
            
            results.append({
                'node_id': f"{doc_id}_{idx}",
                'content': chunk_content,
                'similarity_score': float(similarity_score),
                'doc_id': doc_id,
                'chunk_index': idx,
                'rank': i + 1
            })
        
        return results
        
    except Exception as e:
        print(f"âŒ å¤šå±¤æ¬¡æª¢ç´¢å¤±æ•—: {e}")
        return []


