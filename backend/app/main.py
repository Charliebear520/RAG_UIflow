from __future__ import annotations

import io
import os
import uuid
from dataclasses import dataclass
import re
from typing import List, Optional, Dict, Any, Tuple
import json
from datetime import datetime
import asyncio
import time
from concurrent.futures import ThreadPoolExecutor

from fastapi import FastAPI, UploadFile, File, Form, HTTPException, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from pydantic import BaseModel
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

from .models import ChunkConfig, MetadataOptions
from .hybrid_search import hybrid_rank, HybridConfig
from .store import InMemoryStore
try:
    from rank_bm25 import BM25Okapi  # type: ignore
    BM25_AVAILABLE = True
except ImportError:
    BM25Okapi = None  # type: ignore
    BM25_AVAILABLE = False
from dotenv import load_dotenv
try:
    from PyPDF2 import PdfReader
    PYPDF2_AVAILABLE = True
except ImportError:
    PdfReader = None
    PYPDF2_AVAILABLE = False
import pdfplumber
try:
    import jieba  # type: ignore
    import jieba.analyse  # type: ignore
    jieba.initialize()
except ImportError:
    jieba = None  # type: ignore

try:
    import google.generativeai as genai  # type: ignore
    GEMINI_AVAILABLE = True
except ImportError:  # pragma: no cover - optional dependency
    genai = None  # type: ignore
    GEMINI_AVAILABLE = False

try:
    from sentence_transformers import SentenceTransformer  # type: ignore
    SENTENCE_TRANSFORMERS_AVAILABLE = True
except ImportError:  # pragma: no cover - optional dependency
    SentenceTransformer = None  # type: ignore
    SENTENCE_TRANSFORMERS_AVAILABLE = False

load_dotenv()


def get_env_bool(name: str, default: bool = False) -> bool:
    v = os.getenv(name)
    if v is None:
        return default
    return v.lower() in {"1", "true", "yes", "on"}


GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")
USE_GEMINI_EMBEDDING = get_env_bool("USE_GEMINI_EMBEDDING", True)  # é»˜èªä½¿ç”¨ Gemini
USE_GEMINI_COMPLETION = get_env_bool("USE_GEMINI_COMPLETION", False)
USE_BGE_M3_EMBEDDING = get_env_bool("USE_BGE_M3_EMBEDDING", False)  # BGE-M3 å‚™ç”¨é¸é …

# èª¿è©¦ä¿¡æ¯
print(f"ğŸ”§ Embedding é…ç½®:")
print(f"   USE_GEMINI_EMBEDDING: {USE_GEMINI_EMBEDDING}")
print(f"   GOOGLE_API_KEY: {'å·²è¨­ç½®' if GOOGLE_API_KEY else 'æœªè¨­ç½®'}")
print(f"   GEMINI_API_KEY: {'å·²è¨­ç½®' if os.getenv('GEMINI_API_KEY') else 'æœªè¨­ç½®'}")
print(f"   USE_BGE_M3_EMBEDDING: {USE_BGE_M3_EMBEDDING}")
print(f"   GOOGLE_EMBEDDING_MODEL: {os.getenv('GOOGLE_EMBEDDING_MODEL', 'gemini-embedding-001')}")
print(f"   USE_GEMINI_COMPLETION: {USE_GEMINI_COMPLETION}")

try:
    import httpx
except Exception:  # pragma: no cover
    httpx = None


# ---- Simple in-memory store (demo only) ----
@dataclass
class DocRecord:
    id: str
    filename: str
    text: str
    chunks: List[str]
    chunk_size: int
    overlap: int
    json_data: Optional[Dict[str, Any]] = None  # å­˜å„²çµæ§‹åŒ–JSONæ•¸æ“š
    structured_chunks: Optional[List[Dict[str, Any]]] = None  # å­˜å„²çµæ§‹åŒ–chunks
    generated_questions: Optional[List[str]] = None  # å­˜å„²ç”Ÿæˆçš„å•é¡Œ








store = InMemoryStore()


app = FastAPI(title="RAG Visualizer API", version="0.1.0")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # During dev; restrict in prod
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# æš«æ™‚åœç”¨routes.pyçš„åŒ…å«ï¼Œé¿å…å¾ªç’°å°å…¥å•é¡Œ
from .routes import router
app.include_router(router, prefix="/api")


class ChunkRequest(BaseModel):
    doc_id: str
    chunk_size: int = 500
    overlap: int = 50
    strategy: str = "fixed_size"
    use_json_structure: bool = False
    
    # ç­–ç•¥ç‰¹å®šåƒæ•¸
    hierarchical_params: Optional[Dict[str, Any]] = None
    adaptive_params: Optional[Dict[str, Any]] = None
    hybrid_params: Optional[Dict[str, Any]] = None
    semantic_params: Optional[Dict[str, Any]] = None
    rcts_hierarchical_params: Optional[Dict[str, Any]] = None
    structured_hierarchical_params: Optional[Dict[str, Any]] = None


class EmbedRequest(BaseModel):
    doc_ids: Optional[List[str]] = None  # if None, embed all


class RetrieveRequest(BaseModel):
    query: str
    k: int = 5


class GenerateRequest(BaseModel):
    query: str
    top_k: int = 5


# MetadataOptions å·²ç§»è‡³ models.py


# è©•æ¸¬ç›¸é—œçš„æ•¸æ“šæ¨¡å‹
# ChunkConfig å·²ç§»è‡³ models.py


class EvaluationMetrics(BaseModel):
    precision_omega: float  # PrecisionÎ© - æœ€å¤§æº–ç¢ºç‡
    precision_at_k: Dict[int, float]  # k -> precision score
    recall_at_k: Dict[int, float]  # k -> recall score
    chunk_count: int
    avg_chunk_length: float
    length_variance: float


class EvaluationResult(BaseModel):
    config: Dict[str, Any]  # æ”¹ç‚ºå­—å…¸ä»¥æ”¯æ´å‹•æ…‹åƒæ•¸
    metrics: EvaluationMetrics
    test_queries: List[str]
    retrieval_results: Dict[str, List[Dict]]  # query -> results
    timestamp: datetime


@dataclass
class EvaluationTask:
    id: str
    doc_id: str
    configs: List[ChunkConfig]
    test_queries: List[str]
    k_values: List[int]
    status: str  # "pending", "running", "completed", "failed"
    results: List[EvaluationResult]
    created_at: datetime
    progress: float = 0.0  # æ–°å¢ï¼šé€²åº¦ 0.0 to 1.0
    completed_at: Optional[datetime] = None
    error_message: Optional[str] = None
    strategy: str = "fixed_size"  # æ–°å¢ï¼šåˆ†å‰²ç­–ç•¥


class EvaluationStore:
    def __init__(self) -> None:
        self.tasks: Dict[str, EvaluationTask] = {}
        self.executor = ThreadPoolExecutor(max_workers=2)

    def create_task(self, doc_id: str, configs: List[ChunkConfig], 
                   test_queries: List[str], k_values: List[int], 
                   strategy: str = "fixed_size") -> str:
        task_id = str(uuid.uuid4())
        task = EvaluationTask(
            id=task_id,
            doc_id=doc_id,
            configs=configs,
            test_queries=test_queries,
            k_values=k_values,
            strategy=strategy,
            status="pending",
            results=[],
            created_at=datetime.now()
        )
        self.tasks[task_id] = task
        return task_id

    def get_task(self, task_id: str) -> Optional[EvaluationTask]:
        return self.tasks.get(task_id)

    def update_task_status(self, task_id: str, status: str, 
                          results: Optional[List[EvaluationResult]] = None,
                          error_message: Optional[str] = None,
                          progress: Optional[float] = None):
        if task_id in self.tasks:
            self.tasks[task_id].status = status
            if results is not None:
                self.tasks[task_id].results = results
            if error_message is not None:
                self.tasks[task_id].error_message = error_message
            if progress is not None:
                self.tasks[task_id].progress = progress
            if status == "completed":
                self.tasks[task_id].completed_at = datetime.now()
                self.tasks[task_id].progress = 1.0


# å‰µå»ºè©•ä¼°å­˜å„²å¯¦ä¾‹
eval_store = EvaluationStore()


class FixedSizeEvaluationRequest(BaseModel):
    doc_id: str
    chunk_sizes: List[int] = [300, 500, 800]
    overlap_ratios: List[float] = [0.0, 0.1, 0.2]
    strategy: str = "fixed_size"  # æ–°å¢ï¼šåˆ†å‰²ç­–ç•¥
    test_queries: List[str] = [
        "è‘—ä½œæ¬Šçš„å®šç¾©æ˜¯ä»€éº¼ï¼Ÿ",
        "ä»€éº¼æƒ…æ³ä¸‹å¯ä»¥åˆç†ä½¿ç”¨ä»–äººä½œå“ï¼Ÿ",
        "ä¾µçŠ¯è‘—ä½œæ¬Šçš„æ³•å¾‹å¾Œæœæ˜¯ä»€éº¼ï¼Ÿ",
        "è‘—ä½œæ¬Šçš„ä¿è­·æœŸé™æ˜¯å¤šä¹…ï¼Ÿ",
        "å¦‚ä½•ç”³è«‹è‘—ä½œæ¬Šç™»è¨˜ï¼Ÿ"
    ]
    k_values: List[int] = [1, 3, 5, 10]
    
    # ç­–ç•¥ç‰¹å®šåƒæ•¸é¸é … - é è¨­åŒ…å«æ‰€æœ‰æ’åˆ—çµ„åˆ
    chunk_by_options: List[str] = ["article", "item", "section", "chapter"]  # çµæ§‹åŒ–å±¤æ¬¡åˆ†å‰²é¸é …
    preserve_structure_options: List[bool] = [True, False]  # RCTSå±¤æ¬¡åˆ†å‰²é¸é …
    level_depth_options: List[int] = [2, 3, 4]  # å±¤æ¬¡åˆ†å‰²é¸é …
    similarity_threshold_options: List[float] = [0.5, 0.6, 0.7]  # èªç¾©åˆ†å‰²é¸é …
    semantic_threshold_options: List[float] = [0.6, 0.7, 0.8]  # LLMèªç¾©åˆ†å‰²é¸é …
    switch_threshold_options: List[float] = [0.3, 0.5, 0.7]  # æ··åˆåˆ†å‰²é¸é …
    min_chunk_size_options: List[int] = [100, 200, 300]  # å±¤æ¬¡åˆ†å‰²é¸é …
    context_window_options: List[int] = [50, 100, 150]  # èªç¾©åˆ†å‰²é¸é …
    step_size_options: List[int] = [200, 250, 300]  # æ»‘å‹•è¦–çª—é¸é …
    window_size_options: List[int] = [400, 500, 600, 800]  # æ»‘å‹•è¦–çª—é¸é …
    boundary_aware_options: List[bool] = [True, False]  # æ»‘å‹•è¦–çª—é¸é …
    preserve_sentences_options: List[bool] = [True, False]  # æ»‘å‹•è¦–çª—é¸é …
    min_chunk_size_options_sw: List[int] = [50, 100, 150]  # æ»‘å‹•è¦–çª—å°ˆç”¨é¸é …
    max_chunk_size_options_sw: List[int] = [800, 1000, 1200]  # æ»‘å‹•è¦–çª—å°ˆç”¨é¸é …
    secondary_size_options: List[int] = [300, 400, 500]  # æ··åˆåˆ†å‰²é¸é …  # ç”¨æ–¼è¨ˆç®—recall@K


class GenerateQuestionsRequest(BaseModel):
    doc_id: str
    num_questions: int = 10
    question_types: List[str] = ["æ¡ˆä¾‹æ‡‰ç”¨", "æƒ…å¢ƒåˆ†æ", "å¯¦å‹™è™•ç†", "æ³•å¾‹å¾Œæœ", "åˆè¦åˆ¤æ–·"]  # å•é¡Œé¡å‹
    difficulty_levels: List[str] = ["åŸºç¤", "é€²éš", "æ‡‰ç”¨"]  # é›£åº¦ç­‰ç´š


class GeneratedQuestion(BaseModel):
    question: str
    references: List[str]  # ç›¸é—œæ³•è¦æ¢æ–‡
    question_type: str
    difficulty: str
    keywords: List[str]
    estimated_tokens: int


class QuestionGenerationResult(BaseModel):
    doc_id: str
    total_questions: int
    questions: List[GeneratedQuestion]
    generation_time: float
    timestamp: datetime


def generate_unique_id(law_name: str, chapter: str, section: str, article: str, item: Optional[str] = None) -> str:
    """ç”Ÿæˆid"""
    # æ¸…ç†æ³•è¦åç¨±
    law_clean = re.sub(r'[^\w]', '', law_name.lower())
    law_clean = re.sub(r'æ³•è¦åç¨±|æ³•|æ¢ä¾‹', '', law_clean)
    
    # æå–ç« ç¯€
    chapter_num = re.search(r'ç¬¬([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒ0-9]+)ç« ', chapter)
    chapter_num = chapter_num.group(1) if chapter_num else "0"
    
    # æå–ç¯€
    section_num = re.search(r'ç¬¬([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒ0-9]+)ç¯€', section)
    section_num = section_num.group(1) if section_num else "0"
    
    # æå–æ¢æ–‡
    article_num = re.search(r'ç¬¬([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒ0-9]+)æ¢', article)
    article_num = article_num.group(1) if article_num else "0"
    
    # çµ„åˆID
    parts = [law_clean, f"ch{chapter_num}", f"sec{section_num}", f"art{article_num}"]
    if item:
        parts.append(f"item{item}")
    
    return "-".join(parts)


def extract_keywords_with_gemini(text: str, top_k: int = 5) -> List[str]:
    """ä½¿ç”¨Geminiæ¨¡å‹æå–é—œéµè©"""
    if not GEMINI_AVAILABLE:
        return extract_keywords_fallback(text, top_k)
    
    try:
        # å„ªå…ˆä½¿ç”¨ GOOGLE_API_KEYï¼Œå¦‚æœæ²’æœ‰å‰‡ä½¿ç”¨ GEMINI_API_KEY
        api_key = GOOGLE_API_KEY or os.getenv('GEMINI_API_KEY')
        if not api_key:
            return extract_keywords_fallback(text, top_k)
        
        # Configure API key using getattr to avoid static export issues
        cfg = getattr(genai, "configure", None)
        if callable(cfg):
            cfg(api_key=api_key)  # type: ignore[misc]
        ModelCls = getattr(genai, "GenerativeModel", None)
        if ModelCls is None:
            return extract_keywords_fallback(text, top_k)
        model = ModelCls('gemini-2.0-flash-exp')
        
        prompt = f"""
        è«‹å¾ä»¥ä¸‹æ³•å¾‹æ¢æ–‡å…§å®¹ä¸­æå–{top_k}å€‹æœ€é‡è¦çš„é—œéµè©ã€‚
        é—œéµè©æ‡‰è©²æ˜¯æ³•å¾‹è¡“èªã€é‡è¦æ¦‚å¿µæˆ–æ ¸å¿ƒå…§å®¹ã€‚
        è«‹åªè¿”å›é—œéµè©ï¼Œç”¨é€—è™Ÿåˆ†éš”ï¼Œä¸è¦å…¶ä»–è§£é‡‹ã€‚
        
        æ¢æ–‡å…§å®¹ï¼š
        {text}
        """
        
        response = model.generate_content(prompt)
        keywords_text = response.text.strip()
        
        # è§£æé—œéµè©
        keywords = [kw.strip() for kw in keywords_text.split(',') if kw.strip()]
        return keywords[:top_k]
        
    except Exception as e:
        print(f"Geminié—œéµè©æå–å¤±æ•—: {e}")
        return extract_keywords_fallback(text, top_k)


def extract_keywords_fallback(text: str, top_k: int = 5) -> List[str]:
    """å‚™ç”¨é—œéµè©æå–æ–¹æ³•"""
    if jieba is None:
        # å¦‚æœjiebaä¸å¯ç”¨ï¼Œä½¿ç”¨ç°¡å–®çš„æ­£å‰‡è¡¨é”å¼
        words = re.findall(r'[\u4e00-\u9fff]+', text)
        return list(set(words))[:top_k]
    
    try:
        # ä½¿ç”¨jiebaæå–é—œéµè©ï¼›éƒ¨åˆ†ç‰ˆæœ¬å‹åˆ¥ç‚º List[Tuple[str, float]] | List[str]
        from typing import cast, List as _List
        kws = jieba.analyse.extract_tags(text, topK=top_k, withWeight=False)  # type: ignore[call-arg]
        keywords = cast(_List[str], list(kws))
        return keywords[:top_k] if keywords else []
    except:
        # å¦‚æœjiebaå¤±æ•—ï¼Œä½¿ç”¨ç°¡å–®çš„æ­£å‰‡è¡¨é”å¼
        words = re.findall(r'[\u4e00-\u9fff]+', text)
        return list(set(words))[:top_k]


def extract_keywords(text: str, top_k: int = 5) -> List[str]:
    """æå–é—œéµè© - å„ªå…ˆä½¿ç”¨Geminiï¼Œå‚™ç”¨jieba"""
    return extract_keywords_with_gemini(text, top_k)


def extract_cross_references(text: str) -> List[str]:
    """æå–äº¤å‰å¼•ç”¨"""
    references = []
    
    # åŒ¹é…ã€Œç¬¬Xæ¢ã€
    article_refs = re.findall(r'ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒ0-9]+æ¢', text)
    references.extend(article_refs)
    
    # åŒ¹é…ã€Œç¬¬Xé …ã€
    item_refs = re.findall(r'ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒ0-9]+é …', text)
    references.extend(item_refs)
    
    # åŒ¹é…ã€Œç¬¬Xæ¬¾ã€
    clause_refs = re.findall(r'ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒ0-9]+æ¬¾', text)
    references.extend(clause_refs)
    
    # åŒ¹é…ã€Œå‰é …ã€ã€Œå‰æ¢ã€ã€Œæº–ç”¨ä¹‹ã€ç­‰
    if re.search(r'å‰é …|å‰æ¢|æº–ç”¨ä¹‹|ä¾.*è¦å®š|æ¯”ç…§.*è¾¦ç†|é©ç”¨.*è¦å®š', text):
        references.append("internal_reference")
    
    # åŒ¹é…ã€Œæœ¬æ³•ã€ã€Œæœ¬æ¢ä¾‹ã€ç­‰è‡ªå¼•ç”¨
    if re.search(r'æœ¬æ³•|æœ¬æ¢ä¾‹|æœ¬è¦å‰‡|æœ¬è¾¦æ³•', text):
        references.append("self_reference")
    
    # åŒ¹é…ã€Œå…¶ä»–æ³•å¾‹ã€ã€Œç›¸é—œæ³•è¦ã€ç­‰å¤–éƒ¨å¼•ç”¨
    if re.search(r'å…¶ä»–æ³•å¾‹|ç›¸é—œæ³•è¦|å…¶ä»–æ³•è¦|å…¶ä»–æ¢ä¾‹', text):
        references.append("external_reference")
    
    return list(set(references))


def preprocess_text(text: str) -> List[str]:
    """
    æ–‡æœ¬é è™•ç†ï¼šåˆ†è©ã€å»åœç”¨è©ã€æ¸…ç†
    """
    if not text:
        return []
    
    # ä½¿ç”¨jiebaåˆ†è©
    if jieba:
        words = jieba.lcut(text)
    else:
        # ç°¡å–®çš„å­—ç¬¦ç´šåˆ†è©ä½œç‚ºå‚™é¸
        words = list(text)
    
    # ä¸­æ–‡åœç”¨è©åˆ—è¡¨ï¼ˆæ³•å¾‹æ–‡æª”å°ˆç”¨ï¼Œè¼ƒå°‘éæ¿¾ï¼‰
    stop_words = {
        'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'éƒ½', 'ä¸€', 'ä¸Š', 'ä¹Ÿ', 'å¾ˆ', 'åˆ°', 'è¦', 'å»', 'æœƒ', 'è‘—', 'æ²’æœ‰', 'çœ‹', 'å¥½', 'è‡ªå·±', 'é€™', 'é‚£', 'å®ƒ', 'ä»–', 'å¥¹', 'æˆ‘å€‘', 'ä½ å€‘', 'ä»–å€‘', 'å¥¹å€‘', 'å®ƒå€‘', 'ä»€éº¼', 'æ€éº¼', 'ç‚ºä»€éº¼', 'å“ªè£¡', 'ä»€éº¼æ™‚å€™', 'å¤šå°‘', 'å¹¾å€‹', 'ä¸€äº›', 'æ‰€æœ‰', 'æ¯å€‹', 'ä»»ä½•', 'å¦‚æœ', 'å› ç‚º', 'æ‰€ä»¥', 'ä½†æ˜¯', 'ç„¶å¾Œ', 'æˆ–è€…', 'è€Œä¸”', 'é›–ç„¶', 'ä¸é', 'åªæ˜¯', 'å°±æ˜¯', 'é‚„æ˜¯', 'å·²ç¶“', 'æ­£åœ¨', 'å°‡è¦', 'å¯ä»¥', 'æ‡‰è©²', 'å¿…é ˆ', 'éœ€è¦', 'æƒ³è¦', 'å¸Œæœ›', 'å–œæ­¡', 'ä¸å–œæ­¡', 'çŸ¥é“', 'ä¸çŸ¥é“', 'æ˜ç™½', 'ä¸æ˜ç™½', 'è¨˜å¾—', 'å¿˜è¨˜', 'é–‹å§‹', 'çµæŸ', 'ç¹¼çºŒ', 'åœæ­¢', 'å®Œæˆ', 'åš', 'åšé', 'æ­£åœ¨åš', 'å°‡è¦åš', 'è¢«', 'æŠŠ', 'çµ¦', 'å°', 'å‘', 'å¾', 'åˆ°', 'åœ¨', 'æ–¼', 'ç‚º', 'ä»¥', 'ç”¨', 'é€šé', 'æ ¹æ“š', 'æŒ‰ç…§', 'ä¾ç…§', 'é—œæ–¼', 'å°æ–¼', 'è‡³æ–¼', 'é™¤äº†', 'åŒ…æ‹¬', 'ä»¥åŠ', 'èˆ‡', 'æˆ–', 'ä½†', 'ç„¶è€Œ', 'å› æ­¤', 'æ–¼æ˜¯', 'ç„¶å¾Œ', 'æ¥è‘—', 'æœ€å¾Œ', 'é¦–å…ˆ', 'å…¶æ¬¡', 'å†æ¬¡', 'å¦å¤–', 'æ­¤å¤–', 'ä¸¦ä¸”', 'åŒæ™‚', 'ä¸€èµ·', 'åˆ†åˆ¥', 'å„è‡ª', 'å…±åŒ', 'å–®ç¨', 'ç¨ç«‹', 'ç›¸é—œ', 'ç„¡é—œ', 'é‡è¦', 'ä¸é‡è¦', 'ä¸»è¦', 'æ¬¡è¦', 'åŸºæœ¬', 'æ ¹æœ¬', 'æ ¸å¿ƒ', 'é—œéµ', 'å¿…è¦', 'ä¸å¿…è¦', 'å¯èƒ½', 'ä¸å¯èƒ½', 'ä¸€å®š', 'ä¸ä¸€å®š', 'è‚¯å®š', 'ä¸è‚¯å®š', 'ç¢ºå®š', 'ä¸ç¢ºå®š', 'æ¸…æ¥š', 'ä¸æ¸…æ¥š', 'æ˜ç¢º', 'ä¸æ˜ç¢º', 'å…·é«”', 'ä¸å…·é«”', 'è©³ç´°', 'ä¸è©³ç´°', 'ç°¡å–®', 'è¤‡é›œ', 'å®¹æ˜“', 'å›°é›£', 'æ–¹ä¾¿', 'ä¸æ–¹ä¾¿', 'å¿«é€Ÿ', 'æ…¢é€Ÿ', 'é«˜æ•ˆ', 'ä½æ•ˆ', 'æœ‰æ•ˆ', 'ç„¡æ•ˆ', 'æˆåŠŸ', 'å¤±æ•—', 'æ­£ç¢º', 'éŒ¯èª¤', 'å°', 'éŒ¯', 'å¥½', 'å£', 'å„ª', 'åŠ£', 'é«˜', 'ä½', 'å¤§', 'å°', 'å¤š', 'å°‘', 'é•·', 'çŸ­', 'å¯¬', 'çª„', 'åš', 'è–„', 'æ·±', 'æ·º', 'æ–°', 'èˆŠ', 'å¹´è¼•', 'è€', 'æ—©', 'æ™š', 'å¿«', 'æ…¢', 'ç†±', 'å†·', 'æš–', 'æ¶¼', 'ä¹¾', 'æ¿•', 'äº®', 'æš—', 'æ˜', 'æ¸…', 'æ¿', 'éœ', 'å‹•', 'å®‰', 'å±', 'å¹³', 'é™¡', 'ç›´', 'å½', 'åœ“', 'æ–¹', 'å°–', 'éˆ', 'è»Ÿ', 'ç¡¬', 'è¼•', 'é‡', 'å¼·', 'å¼±', 'ç·Š', 'é¬†', 'å¯†', 'ç–', 'æ»¿', 'ç©º', 'å¯¦', 'è™›', 'çœŸ', 'å‡', 'æ­£', 'è² ', 'åŠ ', 'æ¸›', 'ä¹˜', 'é™¤', 'ç­‰æ–¼', 'ä¸ç­‰æ–¼', 'å¤§æ–¼', 'å°æ–¼', 'å¤§æ–¼ç­‰æ–¼', 'å°æ–¼ç­‰æ–¼', 'å’Œ', 'å·®', 'ç©', 'å•†', 'é¤˜', 'å€', 'åˆ†', 'æ¯”', 'ç‡', 'æ¯”ä¾‹', 'ç™¾åˆ†', 'åƒåˆ†', 'è¬åˆ†', 'å„„åˆ†', 'å…†åˆ†', 'äº¬åˆ†', 'å“åˆ†', 'ç§­åˆ†', 'ç©°åˆ†', 'æºåˆ†', 'æ¾—åˆ†', 'æ­£åˆ†', 'è¼‰åˆ†', 'æ¥µåˆ†', 'æ†æ²³æ²™åˆ†', 'é˜¿åƒ§ç¥‡åˆ†', 'é‚£ç”±ä»–åˆ†', 'ä¸å¯æ€è­°åˆ†', 'ç„¡é‡å¤§æ•¸åˆ†'
    }
    
    # éæ¿¾åœç”¨è©å’ŒçŸ­è©
    filtered_words = []
    for word in words:
        word = word.strip()
        if len(word) > 1 and word not in stop_words and not word.isdigit():
            filtered_words.append(word)
    
    return filtered_words


def calculate_tfidf_importance(texts: List[str], target_text: str) -> float:
    """
    ä½¿ç”¨TF-IDFè¨ˆç®—æ–‡æœ¬é‡è¦æ€§
    """
    if not texts or not target_text:
        return 1.0
    
    try:
        # é è™•ç†æ‰€æœ‰æ–‡æœ¬
        processed_texts = [' '.join(preprocess_text(text)) for text in texts]
        processed_target = ' '.join(preprocess_text(target_text))
        
        if not processed_target:
            return 1.0
        
        # è¨ˆç®—TF-IDF
        vectorizer = TfidfVectorizer(
            max_features=1000,
            ngram_range=(1, 2),
            min_df=1,
            max_df=0.95
        )
        
        # æ“¬åˆæ‰€æœ‰æ–‡æœ¬
        all_texts = processed_texts + [processed_target]
        tfidf_matrix = vectorizer.fit_transform(all_texts)
        
        # ç²å–ç›®æ¨™æ–‡æœ¬çš„TF-IDFå‘é‡ï¼ˆé¿å…ç¨€ç–çŸ©é™£çš„åˆ‡ç‰‡è­¦å‘Šï¼‰
        target_vector = tfidf_matrix.getrow(tfidf_matrix.shape[0] - 1)
        
        # è¨ˆç®—èˆ‡å…¶ä»–æ–‡æœ¬çš„å¹³å‡ç›¸ä¼¼åº¦
        similarities = cosine_similarity(target_vector, tfidf_matrix[:-1])
        avg_similarity = similarities.mean()
        
        # è¨ˆç®—TF-IDFåˆ†æ•¸ï¼ˆè©é »-é€†æ–‡æª”é »ç‡ï¼‰
        tfidf_scores = target_vector.toarray()[0]
        tfidf_sum = tfidf_scores.sum()
        
        # ç¶œåˆè©•åˆ†ï¼šTF-IDFåˆ†æ•¸ + ç›¸ä¼¼åº¦
        importance = (tfidf_sum * 0.7 + avg_similarity * 0.3) * 10
        
        # æ¨™æº–åŒ–åˆ°1-5ç¯„åœ
        importance = max(0.1, min(5.0, importance))
        
        return round(importance, 2)
        
    except Exception as e:
        print(f"TF-IDFè¨ˆç®—éŒ¯èª¤: {e}")
        return 1.0


def calculate_bm25_importance(texts: List[str], target_text: str) -> float:
    """
    ä½¿ç”¨BM25è¨ˆç®—æ–‡æœ¬é‡è¦æ€§
    """
    if not texts or not target_text or not BM25_AVAILABLE:
        return 1.0
    
    try:
        # é è™•ç†æ‰€æœ‰æ–‡æœ¬
        processed_texts = [preprocess_text(text) for text in texts]
        processed_target = preprocess_text(target_text)
        
        if not processed_target:
            return 1.0
        
        # åˆå§‹åŒ–BM25
        bm25 = BM25Okapi(processed_texts)
        
        # è¨ˆç®—BM25åˆ†æ•¸
        scores = bm25.get_scores(processed_target)
        
        if len(scores) == 0:
            return 1.0
        
        # è¨ˆç®—å¹³å‡åˆ†æ•¸
        avg_score = scores.mean()
        
        # è¨ˆç®—æœ€é«˜åˆ†æ•¸
        max_score = scores.max()
        
        # ç¶œåˆè©•åˆ†ï¼šå¹³å‡åˆ†æ•¸ + æœ€é«˜åˆ†æ•¸
        importance = (avg_score * 0.6 + max_score * 0.4) * 2
        
        # æ¨™æº–åŒ–åˆ°1-5ç¯„åœ
        importance = max(0.1, min(5.0, importance))
        
        return round(importance, 2)
        
    except Exception as e:
        print(f"BM25è¨ˆç®—éŒ¯èª¤: {e}")
        return 1.0


def calculate_importance(chapter: str, section: str, article: str, content: str = "", all_articles: List[Dict] = None) -> float:
    """
    è¨ˆç®—é‡è¦æ€§æ¬Šé‡ - ä½¿ç”¨TF-IDFå’ŒBM25å‹•æ…‹è¨ˆç®—
    
    åƒæ•¸:
    - chapter: ç« ç¯€åç¨±
    - section: ç¯€åç¨±  
    - article: æ¢æ–‡åç¨±
    - content: æ¢æ–‡å…§å®¹
    - all_articles: æ‰€æœ‰æ¢æ–‡åˆ—è¡¨ï¼Œç”¨æ–¼è¨ˆç®—ç›¸å°é‡è¦æ€§
    """
    # åŸºç¤æ¬Šé‡
    base_weight = 1.0
    
    # å¦‚æœæ²’æœ‰å…§å®¹æˆ–æ‰€æœ‰æ¢æ–‡ï¼Œä½¿ç”¨éœæ…‹æ¬Šé‡
    if not content or not all_articles:
        return calculate_static_importance(chapter, section, article)
    
    try:
        # æº–å‚™æ‰€æœ‰æ¢æ–‡çš„æ–‡æœ¬
        all_texts = []
        for art in all_articles:
            text = f"{art.get('article', '')} {art.get('content', '')}"
            if text.strip():
                all_texts.append(text)
        
        if len(all_texts) < 2:
            return calculate_static_importance(chapter, section, article)
        
        # ç›®æ¨™æ–‡æœ¬
        target_text = f"{article} {content}"
        
        # è¨ˆç®—TF-IDFé‡è¦æ€§
        tfidf_importance = calculate_tfidf_importance(all_texts, target_text)
        
        # è¨ˆç®—BM25é‡è¦æ€§
        bm25_importance = calculate_bm25_importance(all_texts, target_text)
        
        # ç¶œåˆè©•åˆ†ï¼šTF-IDF 60% + BM25 40%
        dynamic_weight = tfidf_importance * 0.6 + bm25_importance * 0.4
        
        # çµåˆéœæ…‹æ¬Šé‡ï¼ˆ30%ï¼‰å’Œå‹•æ…‹æ¬Šé‡ï¼ˆ70%ï¼‰
        final_weight = base_weight * 0.3 + dynamic_weight * 0.7
        
        return round(final_weight, 2)
        
    except Exception as e:
        print(f"å‹•æ…‹é‡è¦æ€§è¨ˆç®—éŒ¯èª¤: {e}")
        return calculate_static_importance(chapter, section, article)


def calculate_static_importance(chapter: str, section: str, article: str) -> float:
    """
    éœæ…‹é‡è¦æ€§æ¬Šé‡è¨ˆç®—ï¼ˆå‚™ç”¨æ–¹æ³•ï¼‰
    """
    weight = 1.0
    
    # ç¸½å‰‡ç« ç¯€æ¬Šé‡æ›´é«˜ (åŸºç¤æ€§æ¢æ–‡)
    if "ç¸½å‰‡" in chapter or "ç¬¬ä¸€ç« " in chapter or "é€šå‰‡" in chapter:
        weight *= 1.5
    
    # å®šç¾©æ€§æ¢æ–‡æ¬Šé‡æ›´é«˜ (æ ¸å¿ƒæ¦‚å¿µ)
    if "å®šç¾©" in article or "ç”¨è©" in article or "é‡‹ç¾©" in article:
        weight *= 1.3
    
    # ç½°å‰‡ç« ç¯€æ¬Šé‡è¼ƒé«˜ (æ³•å¾‹å¾Œæœ)
    if "ç½°å‰‡" in chapter or "ç½°" in chapter or "è™•ç½°" in chapter:
        weight *= 1.2
    
    # æ–½è¡Œç´°å‰‡æ¬Šé‡è¼ƒä½ (ç¨‹åºæ€§æ¢æ–‡)
    if "æ–½è¡Œ" in chapter or "ç¨‹åº" in chapter or "æµç¨‹" in chapter:
        weight *= 0.8
    
    # é™„å‰‡æ¬Šé‡æœ€ä½ (è£œå……æ€§æ¢æ–‡)
    if "é™„å‰‡" in chapter or "é™„" in chapter:
        weight *= 0.7
    
    # é€šå‰‡ç¯€æ¬Šé‡è¼ƒé«˜
    if "é€šå‰‡" in section:
        weight *= 1.2
    
    return round(weight, 2)




def extract_spans_with_pdfplumber(pdf_file, text_content: str, full_text: str = "") -> List[Dict[str, Any]]:
    """ä½¿ç”¨pdfplumberæå–æ–‡å­—ç‰‡æ®µç¯„åœ"""
    spans = []
    
    try:
        # é‡ç½®æ–‡ä»¶æŒ‡é‡
        pdf_file.seek(0)
        
        with pdfplumber.open(pdf_file) as pdf:
            # é¦–å…ˆåœ¨æ•´å€‹æ–‡æª”ä¸­æŸ¥æ‰¾å…§å®¹
            all_text = ""
            page_texts = []
            
            for page_num, page in enumerate(pdf.pages, 1):
                page_text = page.extract_text() or ""
                page_texts.append(page_text)
                all_text += page_text + "\n"
            
            # åœ¨å®Œæ•´æ–‡æœ¬ä¸­æŸ¥æ‰¾å…§å®¹
            if text_content.strip():
                # æ¸…ç†æ–‡æœ¬å…§å®¹ï¼Œå»é™¤å¤šé¤˜ç©ºç™½
                clean_content = re.sub(r'\s+', ' ', text_content.strip())
                clean_all_text = re.sub(r'\s+', ' ', all_text)
                
                start_idx = clean_all_text.find(clean_content)
                if start_idx != -1:
                    end_idx = start_idx + len(clean_content)
                    
                    # è¨ˆç®—åœ¨å“ªå€‹é é¢
                    page_num = 1
                    current_pos = 0
                    for i, page_text in enumerate(page_texts):
                        clean_page_text = re.sub(r'\s+', ' ', page_text)
                        page_len = len(clean_page_text)
                        
                        if current_pos <= start_idx < current_pos + page_len:
                            page_num = i + 1
                            # è¨ˆç®—åœ¨è©²é é¢å…§çš„ç›¸å°ä½ç½®
                            page_start = start_idx - current_pos
                            page_end = page_start + len(clean_content)
                            
                            spans.append({
                                "start_char": start_idx,
                                "end_char": end_idx,
                                "page_start_char": page_start,
                                "page_end_char": page_end,
                                "text": clean_content[:100] + "..." if len(clean_content) > 100 else clean_content,
                                "page": page_num,
                                "confidence": 1.0
                            })
                            break
                        current_pos += page_len + 1  # +1 for newline
                
                # å¦‚æœæ²’æ‰¾åˆ°å®Œæ•´åŒ¹é…ï¼Œå˜—è©¦éƒ¨åˆ†åŒ¹é…
                if not spans and len(clean_content) > 10:
                    # å˜—è©¦åŒ¹é…å‰20å€‹å­—ç¬¦
                    partial_content = clean_content[:20]
                    start_idx = clean_all_text.find(partial_content)
                    if start_idx != -1:
                        end_idx = start_idx + len(clean_content)
                        
                        # è¨ˆç®—é é¢ä½ç½®
                        page_num = 1
                        current_pos = 0
                        for i, page_text in enumerate(page_texts):
                            clean_page_text = re.sub(r'\s+', ' ', page_text)
                            page_len = len(clean_page_text)
                            
                            if current_pos <= start_idx < current_pos + page_len:
                                page_num = i + 1
                                page_start = start_idx - current_pos
                                page_end = page_start + len(clean_content)
                                
                                spans.append({
                                    "start_char": start_idx,
                                    "end_char": end_idx,
                                    "page_start_char": page_start,
                                    "page_end_char": page_end,
                                    "text": clean_content[:100] + "..." if len(clean_content) > 100 else clean_content,
                                    "page": page_num,
                                    "confidence": 0.8,
                                    "note": "partial_match"
                                })
                                break
                            current_pos += page_len + 1
            
            # å¦‚æœé‚„æ˜¯æ²’æ‰¾åˆ°ï¼Œä½¿ç”¨é—œéµè©åŒ¹é…
            if not spans and text_content.strip():
                keywords = re.findall(r'[\u4e00-\u9fff]+', text_content)
                if keywords:
                    # æ‰¾åˆ°åŒ…å«æœ€å¤šé—œéµè©çš„é é¢
                    best_page = 1
                    best_score = 0
                    
                    for page_num, page_text in enumerate(page_texts, 1):
                        clean_page_text = re.sub(r'\s+', ' ', page_text)
                        score = sum(1 for keyword in keywords if keyword in clean_page_text)
                        if score > best_score:
                            best_score = score
                            best_page = page_num
                    
                    if best_score > 0:
                        spans.append({
                            "start_char": 0,
                            "end_char": len(text_content),
                            "page_start_char": 0,
                            "page_end_char": len(text_content),
                            "text": text_content[:100] + "..." if len(text_content) > 100 else text_content,
                            "page": best_page,
                            "confidence": 0.5,
                            "note": "keyword_match",
                            "matched_keywords": [kw for kw in keywords if kw in page_texts[best_page-1]]
                        })
                        
    except Exception as e:
        print(f"Error extracting spans: {e}")
    
    return spans


def get_text_position_in_document(full_text: str, target_text: str) -> Dict[str, Any]:
    """ç²å–æ–‡æœ¬åœ¨æ–‡æª”ä¸­çš„ä½ç½®ä¿¡æ¯"""
    if not target_text.strip():
        return {"start": 0, "end": 0, "found": False}
    
    # æ¸…ç†æ–‡æœ¬å…§å®¹
    clean_target = re.sub(r'\s+', ' ', target_text.strip())
    clean_full = re.sub(r'\s+', ' ', full_text)
    
    start_idx = clean_full.find(clean_target)
    if start_idx != -1:
        return {
            "start": start_idx,
            "end": start_idx + len(clean_target),
            "found": True,
            "confidence": 1.0
        }
    
    # å˜—è©¦éƒ¨åˆ†åŒ¹é…
    if len(clean_target) > 10:
        partial = clean_target[:15]
        start_idx = clean_full.find(partial)
        if start_idx != -1:
            return {
                "start": start_idx,
                "end": start_idx + len(clean_target),
                "found": True,
                "confidence": 0.7,
                "note": "partial_match"
            }
    
    return {"start": 0, "end": 0, "found": False, "confidence": 0.0}


def get_page_range_for_text(pdf_file, target_text: str) -> Dict[str, int]:
    """ç²å–æ–‡æœ¬åœ¨PDFä¸­çš„é ç¢¼ç¯„åœ"""
    try:
        # é‡ç½®æ–‡ä»¶æŒ‡é’ˆ
        pdf_file.seek(0)
        
        with pdfplumber.open(pdf_file) as pdf:
            start_page = None
            end_page = None
            
            for page_num, page in enumerate(pdf.pages, 1):
                page_text = page.extract_text() or ""
                clean_page_text = re.sub(r'\s+', ' ', page_text)
                clean_target = re.sub(r'\s+', ' ', target_text.strip())
                
                if clean_target in clean_page_text:
                    if start_page is None:
                        start_page = page_num
                    end_page = page_num
                elif len(clean_target) > 10:
                    # å˜—è©¦éƒ¨åˆ†åŒ¹é…
                    partial = clean_target[:20]
                    if partial in clean_page_text:
                        if start_page is None:
                            start_page = page_num
                        end_page = page_num
            
            if start_page is not None:
                return {"start": start_page, "end": end_page or start_page}
            else:
                return {"start": 1, "end": 1}  # é»˜èªå€¼
                
    except Exception as e:
        print(f"Error getting page range: {e}")
        return {"start": 1, "end": 1}  # é»˜è®¤å€¼




@app.get("/health")
def health():
    return {"ok": True}


@app.post("/upload")
async def upload(file: UploadFile = File(...)):
    content = await file.read()
    doc_id = str(uuid.uuid4())
    
    # æª¢æŸ¥æ–‡ä»¶é¡å‹ä¸¦ç›¸æ‡‰è™•ç†
    if file.filename and file.filename.lower().endswith('.pdf'):
        # è™•ç†PDFæ–‡ä»¶
        try:
            import io
            if pdfplumber:
                # ä½¿ç”¨pdfplumberè§£æPDF
                pdf_file = io.BytesIO(content)
                text = ""
                with pdfplumber.open(pdf_file) as pdf:
                    for page in pdf.pages:
                        page_text = page.extract_text()
                        if page_text:
                            text += page_text + "\n"
            else:
                # å‚™ç”¨æ–¹æ¡ˆï¼šä½¿ç”¨PyPDF2
                pdf_file = io.BytesIO(content)
                if PYPDF2_AVAILABLE:
                    pdf_reader = PdfReader(pdf_file)
                    text = ""
                    for page in pdf_reader.pages:
                        text += page.extract_text() + "\n"
                else:
                    # å¦‚æœæ²’æœ‰PDFè§£æåº«ï¼Œè¿”å›éŒ¯èª¤
                    return JSONResponse(
                        status_code=400, 
                        content={"error": "PDF parsing libraries not available. Please install pdfplumber or PyPDF2."}
                    )
        except Exception as e:
            return JSONResponse(
                status_code=400, 
                content={"error": f"Failed to parse PDF: {str(e)}"}
            )
    else:
        # è™•ç†æ–‡æœ¬æ–‡ä»¶
        try:
            text = content.decode("utf-8", errors="ignore")
        except Exception:
            text = str(content)
    
    # æ¸…ç†æ–‡æœ¬
    text = text.strip()
    if not text:
        return JSONResponse(
            status_code=400, 
            content={"error": "No text content found in the file"}
        )
    
    store.docs[doc_id] = DocRecord(
        id=doc_id,
        filename=file.filename,
        text=text,
        json_data=None,  # åˆå§‹ç‚ºNoneï¼Œå¾ŒçºŒé€šé/update-jsonç«¯é»æ›´æ–°
        chunks=[],
        chunk_size=0,
        overlap=0,
    )
    # When uploading new docs, prior embeddings are invalid
    store.reset_embeddings()
    return {"doc_id": doc_id, "filename": file.filename, "num_chars": len(text)}


@app.post("/api/update-json")
async def update_json(request: dict):
    """æ›´æ–°æ–‡æª”çš„JSONçµæ§‹åŒ–æ•¸æ“š"""
    doc_id = request.get("doc_id")
    json_data = request.get("json_data")
    
    if not doc_id or not json_data:
        return JSONResponse(
            status_code=400,
            content={"error": "doc_id and json_data are required"}
        )
    
    if doc_id not in store.docs:
        return JSONResponse(
            status_code=404,
            content={"error": "Document not found"}
        )
    
    # æ›´æ–°æ–‡æª”çš„JSONæ•¸æ“š
    store.docs[doc_id].json_data = json_data
    
    # é‡ç½®ç›¸é—œç‹€æ…‹ï¼Œå› ç‚ºJSONæ•¸æ“šæ”¹è®Šå¯èƒ½å½±éŸ¿chunking
    store.docs[doc_id].chunks = []
    store.docs[doc_id].chunk_size = 0
    store.docs[doc_id].overlap = 0
    store.reset_embeddings()
    
    return {"success": True, "message": "JSON data updated successfully"}


def sliding_window_chunks(text: str, chunk_size: int, overlap: int) -> List[str]:
    """å›ºå®šå¤§å°æ»‘å‹•çª—å£åˆ†å‰²"""
    if chunk_size <= 0:
        return [text]
    if overlap >= chunk_size:
        overlap = max(0, chunk_size - 1)
    chunks: List[str] = []
    start = 0
    n = len(text)
    while start < n:
        end = min(n, start + chunk_size)
        chunks.append(text[start:end])
        if end == n:
            break
        start = end - overlap
    return chunks


def hierarchical_chunks(text: str, max_chunk_size: int, min_chunk_size: int, overlap: int, level_depth: int) -> List[str]:
    """å±¤æ¬¡åŒ–åˆ†å‰²ç­–ç•¥"""
    if max_chunk_size <= 0:
        return [text]
    
    # é¦–å…ˆæŒ‰æ®µè½åˆ†å‰²
    paragraphs = text.split('\n\n')
    chunks = []
    
    for para in paragraphs:
        if len(para) <= max_chunk_size:
            chunks.append(para)
        else:
            # å¦‚æœæ®µè½å¤ªé•·ï¼ŒæŒ‰å¥å­åˆ†å‰²
            sentences = para.split('ã€‚')
            current_chunk = ""
            
            for sentence in sentences:
                if len(current_chunk + sentence) <= max_chunk_size:
                    current_chunk += sentence + "ã€‚"
                else:
                    if current_chunk and len(current_chunk) >= min_chunk_size:
                        chunks.append(current_chunk.strip())
                    current_chunk = sentence + "ã€‚"
            
            if current_chunk and len(current_chunk) >= min_chunk_size:
                chunks.append(current_chunk.strip())
    
    # æ‡‰ç”¨é‡ç–Š
    if overlap > 0:
        overlapped_chunks = []
        for i, chunk in enumerate(chunks):
            overlapped_chunks.append(chunk)
            if i < len(chunks) - 1 and len(chunk) > overlap:
                # æ·»åŠ é‡ç–Šéƒ¨åˆ†
                overlap_text = chunk[-overlap:]
                next_chunk = chunks[i + 1]
                if len(next_chunk) > overlap:
                    overlapped_chunks.append(overlap_text + next_chunk[overlap:])
        return overlapped_chunks
    
    return chunks


def adaptive_chunks(text: str, target_size: int, tolerance: int, overlap: int, semantic_threshold: float) -> List[str]:
    """è‡ªé©æ‡‰åˆ†å‰²ç­–ç•¥"""
    if target_size <= 0:
        return [text]
    
    chunks = []
    start = 0
    n = len(text)
    
    while start < n:
        # å˜—è©¦æ‰¾åˆ°æœ€ä½³åˆ†å‰²é»
        end = min(n, start + target_size)
        
        # å¦‚æœæ¥è¿‘ç›®æ¨™å¤§å°ï¼Œå°‹æ‰¾èªç¾©é‚Šç•Œ
        if end - start >= target_size - tolerance:
            # å°‹æ‰¾å¥è™Ÿã€æ®µè½ç­‰èªç¾©é‚Šç•Œ
            for i in range(end, max(start + target_size - tolerance, start), -1):
                if i < n and text[i] in ['ã€‚', '\n', 'ï¼', 'ï¼Ÿ']:
                    end = i + 1
                    break
        
        chunk = text[start:end].strip()
        if chunk:
            chunks.append(chunk)
        
        if end == n:
            break
        
        # è¨ˆç®—ä¸‹ä¸€å€‹chunkçš„èµ·å§‹ä½ç½®ï¼ˆè€ƒæ…®é‡ç–Šï¼‰
        start = max(start + 1, end - overlap)
    
    return chunks


def hybrid_chunks(text: str, primary_size: int, secondary_size: int, overlap: int, switch_threshold: float) -> List[str]:
    """æ··åˆåˆ†å‰²ç­–ç•¥"""
    if primary_size <= 0:
        return [text]
    
    chunks = []
    start = 0
    n = len(text)
    
    while start < n:
        # æ±ºå®šä½¿ç”¨ä¸»è¦å¤§å°é‚„æ˜¯æ¬¡è¦å¤§å°
        remaining_text = text[start:]
        avg_sentence_length = len(remaining_text) / max(1, remaining_text.count('ã€‚'))
        
        if avg_sentence_length > primary_size * switch_threshold:
            chunk_size = secondary_size
        else:
            chunk_size = primary_size
        
        end = min(n, start + chunk_size)
        chunk = text[start:end].strip()
        
        if chunk:
            chunks.append(chunk)
        
        if end == n:
            break
        
        start = max(start + 1, end - overlap)
    
    return chunks


def semantic_chunks(text: str, target_size: int, similarity_threshold: float, overlap: int, context_window: int) -> List[str]:
    """èªç¾©åˆ†å‰²ç­–ç•¥"""
    if target_size <= 0:
        return [text]
    
    # ç°¡åŒ–å¯¦ç¾ï¼šæŒ‰å¥å­åˆ†å‰²ï¼Œç„¶å¾Œåˆä½µç›¸ä¼¼çš„å¥å­
    sentences = text.split('ã€‚')
    chunks = []
    current_chunk = ""
    
    for sentence in sentences:
        if not sentence.strip():
            continue
            
        sentence = sentence.strip() + "ã€‚"
        
        # å¦‚æœç•¶å‰chunkåŠ ä¸Šæ–°å¥å­ä¸è¶…éç›®æ¨™å¤§å°
        if len(current_chunk + sentence) <= target_size:
            current_chunk += sentence
        else:
            # ä¿å­˜ç•¶å‰chunk
            if current_chunk:
                chunks.append(current_chunk.strip())
            
            # é–‹å§‹æ–°chunk
            current_chunk = sentence
    
    # æ·»åŠ æœ€å¾Œä¸€å€‹chunk
    if current_chunk:
        chunks.append(current_chunk.strip())
    
    return chunks


def json_structured_chunks(json_data: Dict[str, Any], chunk_size: int, overlap: int) -> List[Dict[str, Any]]:
    """
    åŸºæ–¼JSONçµæ§‹çš„æ™ºèƒ½åˆ†å‰²
    ä¿ç•™æ³•å¾‹æ–‡æª”çš„çµæ§‹åŒ–ä¿¡æ¯
    æ”¯æŒå–®ä¸€æ³•å¾‹æ–‡æª”å’Œå¤šæ³•å¾‹æ–‡æª”æ ¼å¼
    """
    if not json_data or chunk_size <= 0:
        return []
    
    chunks = []
    
    # æª¢æŸ¥æ˜¯å¦ç‚ºå¤šæ³•å¾‹æ–‡æª”æ ¼å¼
    if "laws" in json_data:
        # å¤šæ³•å¾‹æ–‡æª”æ ¼å¼
        laws = json_data.get("laws", [])
        for law in laws:
            law_chunks = process_single_law(law, chunk_size, overlap)
            chunks.extend(law_chunks)
    else:
        # å–®ä¸€æ³•å¾‹æ–‡æª”æ ¼å¼
        law_chunks = process_single_law(json_data, chunk_size, overlap)
        chunks.extend(law_chunks)
    
    return chunks


def process_single_law(law_data: Dict[str, Any], chunk_size: int, overlap: int) -> List[Dict[str, Any]]:
    """
    è™•ç†å–®ä¸€æ³•å¾‹æ–‡æª”
    """
    chunks = []
    law_name = law_data.get("law_name", "æœªå‘½åæ³•è¦")
    
    def create_chunk(content: str, metadata: Dict[str, Any], chunk_id: str) -> Dict[str, Any]:
        """å‰µå»ºåŒ…å«metadataçš„chunk"""
        return {
            "chunk_id": chunk_id,
            "content": content,
            "metadata": {
                "id": metadata.get("id", ""),
                "spans": metadata.get("spans", {}),
                "page_range": metadata.get("page_range", {})
            }
        }
    
    def process_article(article: Dict[str, Any], chapter: str, section: str) -> List[Dict[str, Any]]:
        """è™•ç†å–®å€‹æ¢æ–‡"""
        article_chunks = []
        article_title = article.get("article", "")
        article_content = article.get("content", "")
        items = article.get("items", [])
        
        # è™•ç†æ¢æ–‡ä¸»é«”
        if article_content:
            # å¦‚æœæ¢æ–‡å…§å®¹è¼ƒçŸ­ï¼Œç›´æ¥ä½œç‚ºä¸€å€‹chunk
            if len(article_content) <= chunk_size:
                metadata = {
                    "id": article.get("metadata", {}).get("id", ""),
                    "spans": article.get("metadata", {}).get("spans", {}),
                    "page_range": article.get("metadata", {}).get("page_range", {})
                }
                chunk_id = f"{article_title}_main"
                article_chunks.append(create_chunk(article_content, metadata, chunk_id))
            else:
                # æ¢æ–‡å…§å®¹è¼ƒé•·ï¼Œéœ€è¦åˆ†å‰²
                text_chunks = sliding_window_chunks(article_content, chunk_size, overlap)
                for i, chunk_text in enumerate(text_chunks):
                    metadata = {
                        "id": article.get("metadata", {}).get("id", ""),
                        "spans": article.get("metadata", {}).get("spans", {}),
                        "page_range": article.get("metadata", {}).get("page_range", {})
                    }
                    chunk_id = f"{article_title}_part_{i+1}"
                    article_chunks.append(create_chunk(chunk_text, metadata, chunk_id))
        
        # è™•ç†æ¢æ–‡é …ç›®
        for item in items:
            item_title = item.get("item", "")
            item_content = item.get("content", "")
            sub_items = item.get("sub_items", [])
            
            # è™•ç†é …ç›®ä¸»é«”
            if item_content:
                if len(item_content) <= chunk_size:
                    metadata = {
                        "id": item.get("metadata", {}).get("id", ""),
                        "spans": item.get("metadata", {}).get("spans", {}),
                        "page_range": item.get("metadata", {}).get("page_range", {})
                    }
                    chunk_id = f"{article_title}_{item_title}_main"
                    article_chunks.append(create_chunk(item_content, metadata, chunk_id))
                else:
                    # é …ç›®å…§å®¹è¼ƒé•·ï¼Œéœ€è¦åˆ†å‰²
                    text_chunks = sliding_window_chunks(item_content, chunk_size, overlap)
                    for i, chunk_text in enumerate(text_chunks):
                        metadata = {
                            "id": item.get("metadata", {}).get("id", ""),
                            "spans": item.get("metadata", {}).get("spans", {}),
                            "page_range": item.get("metadata", {}).get("page_range", {})
                        }
                        chunk_id = f"{article_title}_{item_title}_part_{i+1}"
                        article_chunks.append(create_chunk(chunk_text, metadata, chunk_id))
            
            # è™•ç†å­é …ç›®
            for sub_item in sub_items:
                sub_item_title = sub_item.get("sub_item", "")
                sub_item_content = sub_item.get("content", "")
                
                if sub_item_content:
                    if len(sub_item_content) <= chunk_size:
                        metadata = {
                            "id": sub_item.get("metadata", {}).get("id", ""),
                            "spans": sub_item.get("metadata", {}).get("spans", {}),
                            "page_range": sub_item.get("metadata", {}).get("page_range", {})
                        }
                        chunk_id = f"{article_title}_{item_title}_{sub_item_title}"
                        article_chunks.append(create_chunk(sub_item_content, metadata, chunk_id))
                    else:
                        # å­é …ç›®å…§å®¹è¼ƒé•·ï¼Œéœ€è¦åˆ†å‰²
                        text_chunks = sliding_window_chunks(sub_item_content, chunk_size, overlap)
                        for i, chunk_text in enumerate(text_chunks):
                            metadata = {
                                "id": sub_item.get("metadata", {}).get("id", ""),
                                "spans": sub_item.get("metadata", {}).get("spans", {}),
                                "page_range": sub_item.get("metadata", {}).get("page_range", {})
                            }
                            chunk_id = f"{article_title}_{item_title}_{sub_item_title}_part_{i+1}"
                            article_chunks.append(create_chunk(chunk_text, metadata, chunk_id))
        
        return article_chunks
    
    # éæ­·æ‰€æœ‰ç« ç¯€
    chapters = law_data.get("chapters", [])
    for chapter in chapters:
        chapter_title = chapter.get("chapter", "")
        sections = chapter.get("sections", [])
        
        for section in sections:
            section_title = section.get("section", "")
            articles = section.get("articles", [])
            
            for article in articles:
                article_chunks = process_article(article, chapter_title, section_title)
                chunks.extend(article_chunks)
    
    return chunks


# è©•æ¸¬ç›¸é—œå‡½æ•¸
def calculate_precision_at_k(retrieved_chunks: List[str], query: str, k: int) -> float:
    """
    è¨ˆç®—Precision@K - æª¢ç´¢å‡ºä¾†çš„tokensä¸­ï¼Œæœ‰å¤šå°‘æ˜¯çœŸæ­£ç›¸é—œçš„
    """
    if not retrieved_chunks or k <= 0:
        return 0.0
    
    # å–å‰kå€‹çµæœ
    top_k_chunks = retrieved_chunks[:k]
    
    # æ”¹é€²çš„é—œéµè©åŒ¹é…æ–¹æ³• - ä½¿ç”¨å­—ç¬¦ç´šåŒ¹é…
    query_chars = set(query.replace(' ', '').replace('ï¼Ÿ', '').replace('ï¼', '').replace('ï¼Œ', '').replace('ã€‚', ''))
    if not query_chars:
        return 0.0
    
    relevant_count = 0
    for chunk in top_k_chunks:
        chunk_chars = set(chunk.replace(' ', '').replace('ï¼Œ', '').replace('ã€‚', '').replace('ï¼›', '').replace('ï¼š', ''))
        # å¦‚æœæŸ¥è©¢ä¸­çš„å­—ç¬¦æœ‰50%ä»¥ä¸Šå‡ºç¾åœ¨chunkä¸­ï¼Œèªç‚ºç›¸é—œ
        overlap_chars = query_chars & chunk_chars
        if len(overlap_chars) >= len(query_chars) * 0.5:
            relevant_count += 1
    
    return relevant_count / len(top_k_chunks)


def calculate_precision_omega(retrieved_chunks: List[str], query: str) -> float:
    """
    è¨ˆç®—PrecisionÎ© - å‡è¨­Recallæ˜¯æ»¿åˆ†ï¼Œæœ€å¤§çš„æº–ç¢ºç‡æ˜¯å¤šå°‘
    """
    if not retrieved_chunks:
        return 0.0
    
    # æ”¹é€²çš„é—œéµè©åŒ¹é…æ–¹æ³• - ä½¿ç”¨å­—ç¬¦ç´šåŒ¹é…
    query_chars = set(query.replace(' ', '').replace('ï¼Ÿ', '').replace('ï¼', '').replace('ï¼Œ', '').replace('ã€‚', ''))
    if not query_chars:
        return 0.0
    
    relevant_count = 0
    for chunk in retrieved_chunks:
        chunk_chars = set(chunk.replace(' ', '').replace('ï¼Œ', '').replace('ã€‚', '').replace('ï¼›', '').replace('ï¼š', ''))
        # å¦‚æœæŸ¥è©¢ä¸­çš„å­—ç¬¦æœ‰30%ä»¥ä¸Šå‡ºç¾åœ¨chunkä¸­ï¼Œèªç‚ºç›¸é—œ
        overlap_chars = query_chars & chunk_chars
        if len(overlap_chars) >= len(query_chars) * 0.3:
            relevant_count += 1
    
    return relevant_count / len(retrieved_chunks)


def calculate_recall_at_k(retrieved_chunks: List[str], query: str, k: int, 
                         ground_truth_chunks: List[str] = None) -> float:
    """
    è¨ˆç®—Recall@K - åœ¨å‰Kå€‹æª¢ç´¢çµæœä¸­å‘½ä¸­ç›¸é—œchunkçš„æ¯”ä¾‹
    """
    if not retrieved_chunks or k <= 0:
        return 0.0
    
    # å–å‰kå€‹çµæœ
    top_k_chunks = retrieved_chunks[:k]
    
    # å¦‚æœæ²’æœ‰ground truthï¼Œä½¿ç”¨é—œéµè©åŒ¹é…ä½œç‚ºè¿‘ä¼¼
    if ground_truth_chunks is None:
        # æ”¹é€²çš„é—œéµè©åŒ¹é…æ–¹æ³• - ä½¿ç”¨å­—ç¬¦ç´šåŒ¹é…
        query_chars = set(query.replace(' ', '').replace('ï¼Ÿ', '').replace('ï¼', '').replace('ï¼Œ', '').replace('ã€‚', ''))
        if not query_chars:
            return 0.0
        
        # é¦–å…ˆè¨ˆç®—ç¸½ç›¸é—œæ–‡æª”æ•¸é‡ï¼ˆéœ€è¦å¾æ‰€æœ‰chunksä¸­è¨ˆç®—ï¼Œä¸åªæ˜¯top_kï¼‰
        # ä½†ç”±æ–¼æˆ‘å€‘æ²’æœ‰è¨ªå•æ‰€æœ‰chunksï¼Œæˆ‘å€‘éœ€è¦ä¸€å€‹è¿‘ä¼¼æ–¹æ³•
        # é€™è£¡æˆ‘å€‘å‡è¨­ç¸½ç›¸é—œæ–‡æª”æ•¸é‡ç­‰æ–¼æª¢ç´¢åˆ°çš„ç›¸é—œæ–‡æª”æ•¸é‡ï¼ˆé€™æ˜¯ä¸€å€‹è¿‘ä¼¼ï¼‰
        retrieved_relevant_count = 0
        for chunk in top_k_chunks:
            chunk_chars = set(chunk.replace(' ', '').replace('ï¼Œ', '').replace('ã€‚', '').replace('ï¼›', '').replace('ï¼š', ''))
            # å¦‚æœæŸ¥è©¢ä¸­çš„å­—ç¬¦æœ‰50%ä»¥ä¸Šå‡ºç¾åœ¨chunkä¸­ï¼Œèªç‚ºç›¸é—œ
            overlap_chars = query_chars & chunk_chars
            if len(overlap_chars) >= len(query_chars) * 0.5:
                retrieved_relevant_count += 1
        
        # ç”±æ–¼ç„¡æ³•æº–ç¢ºè¨ˆç®—ç¸½ç›¸é—œæ–‡æª”æ•¸é‡ï¼Œæˆ‘å€‘ä½¿ç”¨ä¸€å€‹ä¿å®ˆçš„ä¼°è¨ˆ
        # å‡è¨­ç¸½ç›¸é—œæ–‡æª”æ•¸é‡è‡³å°‘ç­‰æ–¼æª¢ç´¢åˆ°çš„ç›¸é—œæ–‡æª”æ•¸é‡
        total_relevant_estimate = max(retrieved_relevant_count, 1)
        
        return retrieved_relevant_count / total_relevant_estimate
    
    # ä½¿ç”¨ground truthè¨ˆç®— - é€™è£¡ground_truth_chunkså¯¦éš›ä¸Šæ˜¯æ‰€æœ‰chunks
    # é¦–å…ˆè¨ˆç®—æ‰€æœ‰chunksä¸­ç›¸é—œçš„æ•¸é‡
    query_chars = set(query.replace(' ', '').replace('ï¼Ÿ', '').replace('ï¼', '').replace('ï¼Œ', '').replace('ã€‚', ''))
    if not query_chars:
        return 0.0
    
    total_relevant_count = 0
    for chunk in ground_truth_chunks:
        chunk_chars = set(chunk.replace(' ', '').replace('ï¼Œ', '').replace('ã€‚', '').replace('ï¼›', '').replace('ï¼š', ''))
        overlap_chars = query_chars & chunk_chars
        if len(overlap_chars) >= len(query_chars) * 0.3:
            total_relevant_count += 1
    
    # è¨ˆç®—æª¢ç´¢åˆ°çš„ç›¸é—œchunksæ•¸é‡
    retrieved_relevant_count = 0
    for chunk in top_k_chunks:
        chunk_chars = set(chunk.replace(' ', '').replace('ï¼Œ', '').replace('ã€‚', '').replace('ï¼›', '').replace('ï¼š', ''))
        overlap_chars = query_chars & chunk_chars
        if len(overlap_chars) >= len(query_chars) * 0.3:
            retrieved_relevant_count += 1
    
    return retrieved_relevant_count / total_relevant_count if total_relevant_count > 0 else 0


def calculate_faithfulness(chunks: List[str]) -> float:
    """
    è¨ˆç®—å¿ å¯¦åº¦ - è©•ä¼°chunkæ˜¯å¦ä¿æŒå®Œæ•´èªç¾©
    åŸºæ–¼å¥å­å®Œæ•´æ€§ã€æ®µè½é‚Šç•Œç­‰
    """
    if not chunks:
        return 0.0
    
    total_score = 0.0
    
    for chunk in chunks:
        score = 1.0
        
        # æª¢æŸ¥å¥å­å®Œæ•´æ€§
        sentences = re.split(r'[ã€‚ï¼ï¼Ÿ]', chunk)
        incomplete_sentences = sum(1 for s in sentences if s.strip() and not s.endswith(('ã€‚', 'ï¼', 'ï¼Ÿ')))
        if len(sentences) > 1:
            score *= (1.0 - incomplete_sentences / len(sentences))
        
        # æª¢æŸ¥æ®µè½å®Œæ•´æ€§
        if chunk.startswith(('ç¬¬', 'æ¢', 'é …', 'æ¬¾')) and not chunk.endswith(('ã€‚', 'ï¼', 'ï¼Ÿ')):
            score *= 0.8
        
        total_score += score
    
    return total_score / len(chunks)


def calculate_fragmentation_score(chunks: List[str], original_text: str) -> float:
    """
    è¨ˆç®—ç¢ç‰‡åŒ–ç¨‹åº¦ - è©•ä¼°æ–‡æœ¬è¢«åˆ†å‰²çš„ç´°ç¢ç¨‹åº¦
    è¿”å›å€¼è¶Šé«˜è¡¨ç¤ºç¢ç‰‡åŒ–è¶Šåš´é‡
    """
    if not chunks or not original_text:
        return 0.0
    
    # è¨ˆç®—å¹³å‡chunké•·åº¦ç›¸å°æ–¼åŸæ–‡çš„æ¯”ä¾‹
    avg_chunk_length = sum(len(chunk) for chunk in chunks) / len(chunks)
    length_ratio = avg_chunk_length / len(original_text)
    
    # è¨ˆç®—chunkæ•¸é‡
    chunk_count_ratio = len(chunks) / (len(original_text) / 500)  # ä»¥500å­—ç¬¦ç‚ºåŸºæº–
    
    # ç¶œåˆè©•åˆ†
    fragmentation = (1.0 - length_ratio) * 0.6 + chunk_count_ratio * 0.4
    
    return min(1.0, max(0.0, fragmentation))


def generate_questions_with_gemini(text_content: str, num_questions: int, 
                                 question_types: List[str], difficulty_levels: List[str]) -> List[GeneratedQuestion]:
    """
    ä½¿ç”¨Geminiç”Ÿæˆç¹é«”ä¸­æ–‡æ³•å¾‹è€ƒå¤é¡Œ
    åƒè€ƒihoweræ–‡ç« çš„åšæ³•ï¼Œå¾æ–‡æœ¬ä¸­éš¨æ©Ÿé¸æ“‡å…§å®¹ç”Ÿæˆå•é¡Œ
    """
    if not GEMINI_AVAILABLE:
        return generate_questions_fallback(text_content, num_questions)
    
    try:
        # å„ªå…ˆä½¿ç”¨ GOOGLE_API_KEYï¼Œå¦‚æœæ²’æœ‰å‰‡ä½¿ç”¨ GEMINI_API_KEY
        api_key = GOOGLE_API_KEY or os.getenv('GEMINI_API_KEY')
        if not api_key:
            print("è­¦å‘Šï¼šGOOGLE_API_KEY å’Œ GEMINI_API_KEY éƒ½æœªè¨­ç½®ï¼Œä½¿ç”¨å‚™ç”¨æ–¹æ³•")
            return generate_questions_fallback(text_content, num_questions)
        
        cfg = getattr(genai, "configure", None)
        if callable(cfg):
            cfg(api_key=api_key)  # type: ignore[misc]
        ModelCls = getattr(genai, "GenerativeModel", None)
        if ModelCls is None:
            print("è­¦å‘Šï¼šç„¡æ³•ç²å– GenerativeModel é¡ï¼Œä½¿ç”¨å‚™ç”¨æ–¹æ³•")
            return generate_questions_fallback(text_content, num_questions)
        model = ModelCls('gemini-2.0-flash-exp')
        
        # å¾æ–‡æœ¬ä¸­éš¨æ©Ÿé¸æ“‡4000 tokensçš„å…§å®¹ï¼ˆæ¨¡æ“¬ihowerçš„åšæ³•ï¼‰
        import random
        text_chunks = text_content.split('\n')
        random.shuffle(text_chunks)
        
        # é¸æ“‡è¶³å¤ çš„å…§å®¹ä¾†ç”Ÿæˆå•é¡Œ
        selected_content = ""
        current_tokens = 0
        max_tokens = 4000
        
        for chunk in text_chunks:
            if current_tokens + len(chunk) > max_tokens:
                break
            selected_content += chunk + "\n"
            current_tokens += len(chunk)
        
        if not selected_content.strip():
            selected_content = text_content[:2000]  # å‚™ç”¨æ–¹æ¡ˆ
        
        prompt = f"""
ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„æ³•å¾‹æ•™è‚²å°ˆå®¶ï¼Œè«‹æ ¹æ“šä»¥ä¸‹æ³•å¾‹æ–‡æœ¬å…§å®¹ï¼Œç”Ÿæˆ{num_questions}é“ç¹é«”ä¸­æ–‡è€ƒå¤é¡Œã€‚

é‡è¦è¦æ±‚ï¼š
1. æ‰€æœ‰å•é¡Œå¿…é ˆä½¿ç”¨ç¹é«”ä¸­æ–‡ï¼ˆå°ç£ç”¨æ³•ï¼‰
2. å•é¡Œé¡å‹æ‡‰åŒ…å«ï¼š{', '.join(question_types)}ï¼Œéš¨æ©Ÿåˆ†é…ä½†ç¢ºä¿å¤šæ¨£æ€§
3. é›£åº¦ç­‰ç´šæ‡‰åŒ…å«ï¼š{', '.join(difficulty_levels)}ï¼Œéš¨æ©Ÿåˆ†é…ï¼ŒåŸºç¤å•é¡Œèšç„¦å–®ä¸€æ¦‚å¿µï¼Œé€²éšå•é¡Œæ¶‰åŠå¤šæ¦‚å¿µï¼Œæ‡‰ç”¨å•é¡Œæ¨¡æ“¬å¯¦å‹™å ´æ™¯
4. æ¯é“é¡Œç›®éƒ½è¦æ¨™æ˜ç›¸é—œçš„æ³•è¦æ¢æ–‡
5. å•é¡Œæ‡‰è©²åŸºæ–¼æ–‡æœ¬ä¸­çš„å…·é«”å…§å®¹ï¼Œä¸æ˜¯æ³›æ³›è€Œè«‡
6. å•é¡Œæ‡‰è©²æœ‰æ˜ç¢ºçš„ç­”æ¡ˆï¼Œå¯ä»¥åœ¨æ–‡æœ¬ä¸­æ‰¾åˆ°ä¾æ“š

æ ¸å¿ƒè¨­è¨ˆåŸå‰‡ï¼š
7. é‡é»ï¼šé¿å…ç´”ç²¹çš„æ¢æ–‡èƒŒèª¦é¡Œç›®ï¼Œæ”¹ç‚ºå¯¦éš›ç”Ÿæ´»æ¡ˆä¾‹æ‡‰ç”¨é¡Œ
8. å•é¡Œæ‡‰è©²è¨­è¨ˆæˆæƒ…å¢ƒå¼æ¡ˆä¾‹ï¼Œè®“å­¸ç”Ÿæ€è€ƒå¦‚ä½•åœ¨å¯¦éš›ç”Ÿæ´»ä¸­æ‡‰ç”¨æ³•å¾‹æ¦‚å¿µ
9. ä½¿ç”¨ã€Œå¦‚æœ...é‚£éº¼...ã€æˆ–ã€Œç•¶...æ™‚...ã€çš„æƒ…å¢ƒè¨­å®š
10. æä¾›å…·é«”çš„ç”Ÿæ´»å ´æ™¯ï¼ˆå¦‚ï¼šç¶²è·¯ä½¿ç”¨ã€å‰µä½œåˆ†äº«ã€å•†æ¥­æ´»å‹•ç­‰ï¼‰
11. è©¢å•ã€Œæ‡‰è©²å¦‚ä½•è™•ç†ã€ã€ã€Œæ˜¯å¦ç¬¦åˆæ³•å¾‹è¦å®šã€ã€ã€Œæœƒç”¢ç”Ÿä»€éº¼å¾Œæœã€ç­‰
12. é¿å…ç›´æ¥å•ã€Œç¬¬Xæ¢è¦å®šä»€éº¼ã€é€™é¡èƒŒèª¦é¡Œ

æ–‡æœ¬å…§å®¹ï¼š
{selected_content}

è«‹ä»¥JSONæ ¼å¼è¿”å›çµæœï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
{{
  "questions": [
    {{
      "question": "å•é¡Œå…§å®¹",
      "references": ["ç¬¬Xæ¢", "ç¬¬Yæ¢ç¬¬Zé …"],
      "question_type": "æ¡ˆä¾‹æ‡‰ç”¨/æƒ…å¢ƒåˆ†æ/å¯¦å‹™è™•ç†/æ³•å¾‹å¾Œæœ/åˆè¦åˆ¤æ–·",
      "difficulty": "åŸºç¤/é€²éš/æ‡‰ç”¨",
      "keywords": ["é—œéµè©1", "é—œéµè©2"],
      "estimated_tokens": ä¼°ç®—çš„tokenæ•¸é‡
    }}
  ]
}}

è«‹ç¢ºä¿ç”Ÿæˆçš„å•é¡Œéƒ½æ˜¯å¯¦éš›ç”Ÿæ´»æ¡ˆä¾‹æ‡‰ç”¨é¡Œï¼Œé¿å…æ¢æ–‡èƒŒèª¦ï¼Œè®“å­¸ç”Ÿèƒ½å¤ æ€è€ƒå¦‚ä½•åœ¨çœŸå¯¦æƒ…å¢ƒä¸­æ‡‰ç”¨æ³•å¾‹çŸ¥è­˜ã€‚
"""
        
        response = model.generate_content(prompt)
        response_text = response.text.strip()
        
        # è§£æJSONéŸ¿æ‡‰
        try:
            # æ¸…ç†éŸ¿æ‡‰æ–‡æœ¬ï¼Œç§»é™¤å¯èƒ½çš„markdownæ ¼å¼
            if response_text.startswith('```json'):
                response_text = response_text[7:]
            if response_text.endswith('```'):
                response_text = response_text[:-3]
            
            import json
            result = json.loads(response_text)
            
            questions = []
            for q_data in result.get('questions', []):
                question = GeneratedQuestion(
                    question=q_data.get('question', ''),
                    references=q_data.get('references', []),
                    question_type=q_data.get('question_type', ''),
                    difficulty=q_data.get('difficulty', ''),
                    keywords=q_data.get('keywords', []),
                    estimated_tokens=q_data.get('estimated_tokens', 0)
                )
                questions.append(question)
            
            return questions[:num_questions]  # ç¢ºä¿ä¸è¶…éè«‹æ±‚æ•¸é‡
            
        except json.JSONDecodeError as e:
            print(f"JSONè§£æéŒ¯èª¤: {e}")
            print(f"éŸ¿æ‡‰å…§å®¹: {response_text[:500]}...")  # åªé¡¯ç¤ºå‰500å­—ç¬¦
            return generate_questions_fallback(text_content, num_questions)
        
    except Exception as e:
        print(f"Geminiå•é¡Œç”Ÿæˆå¤±æ•—: {e}")
        return generate_questions_fallback(text_content, num_questions)


def generate_questions_fallback(text_content: str, num_questions: int) -> List[GeneratedQuestion]:
    """
    å‚™ç”¨å•é¡Œç”Ÿæˆæ–¹æ³•
    """
    questions = []
    
    # ç°¡å–®çš„æ­£å‰‡è¡¨é”å¼æå–æ³•æ¢
    import re
    articles = re.findall(r'ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒ0-9]+æ¢[^ã€‚]*ã€‚', text_content)
    
    print(f"å‚™ç”¨æ–¹æ³•ï¼šå¾æ–‡æœ¬ä¸­æ‰¾åˆ° {len(articles)} å€‹æ³•æ¢")
    
    # ç”ŸæˆåŸºç¤å•é¡Œ
    question_templates = [
        ("{article}çš„å®šç¾©æ˜¯ä»€éº¼ï¼Ÿ", "å®šç¾©", "åŸºç¤"),
        ("{article}çš„é©ç”¨æ¢ä»¶ç‚ºä½•ï¼Ÿ", "æ¢ä»¶", "åŸºç¤"),
        ("é•å{article}çš„æ³•å¾‹å¾Œæœæ˜¯ä»€éº¼ï¼Ÿ", "å¾Œæœ", "é€²éš"),
        ("{article}çš„ç”³è«‹ç¨‹åºç‚ºä½•ï¼Ÿ", "ç¨‹åº", "é€²éš"),
        ("{article}çš„ä¿è­·æœŸé™æ˜¯å¤šä¹…ï¼Ÿ", "æœŸé™", "åŸºç¤"),
    ]
    
    if articles:
        # å¦‚æœæœ‰æ³•æ¢ï¼ŒåŸºæ–¼æ³•æ¢ç”Ÿæˆå•é¡Œ
        for i in range(min(num_questions, len(articles))):
            article = articles[i % len(articles)]
            template, q_type, difficulty = question_templates[i % len(question_templates)]
            
            # æå–æ¢æ–‡è™Ÿç¢¼
            article_match = re.search(r'ç¬¬([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒ0-9]+)æ¢', article)
            article_num = article_match.group(1) if article_match else str(i+1)
            
            question = GeneratedQuestion(
                question=template.format(article=f"ç¬¬{article_num}æ¢"),
                references=[f"ç¬¬{article_num}æ¢"],
                question_type=q_type,
                difficulty=difficulty,
                keywords=extract_keywords(article, 3),
                estimated_tokens=len(article) + 50
            )
            questions.append(question)
    else:
        # å¦‚æœæ²’æœ‰æ‰¾åˆ°æ³•æ¢ï¼Œç”Ÿæˆé€šç”¨å•é¡Œ
        print("è­¦å‘Šï¼šæ²’æœ‰æ‰¾åˆ°æ³•æ¢ï¼Œç”Ÿæˆé€šç”¨å•é¡Œ")
        generic_questions = [
            "è«‹èªªæ˜æœ¬æ³•å¾‹æ–‡æª”çš„ä¸»è¦å…§å®¹å’Œç›®çš„ï¼Ÿ",
            "æœ¬æ³•å¾‹æ–‡æª”é©ç”¨æ–¼å“ªäº›æƒ…æ³ï¼Ÿ",
            "é•åæœ¬æ³•å¾‹è¦å®šæœƒç”¢ç”Ÿä»€éº¼å¾Œæœï¼Ÿ",
            "å¦‚ä½•ç”³è«‹æœ¬æ³•å¾‹è¦å®šçš„ç›¸é—œæ¬Šåˆ©ï¼Ÿ",
            "æœ¬æ³•å¾‹è¦å®šçš„ä¿è­·æœŸé™æ˜¯å¤šä¹…ï¼Ÿ"
        ]
        
        for i in range(min(num_questions, len(generic_questions))):
            question = GeneratedQuestion(
                question=generic_questions[i],
                references=["ç›¸é—œæ³•æ¢"],
                question_type="åŸºç¤æ¦‚å¿µ",
                difficulty="åŸºç¤",
                keywords=extract_keywords(text_content[:200], 3),
                estimated_tokens=100
            )
            questions.append(question)
    
    print(f"å‚™ç”¨æ–¹æ³•ç”Ÿæˆäº† {len(questions)} å€‹å•é¡Œ")
    return questions


def evaluate_chunk_config(doc: DocRecord, config: ChunkConfig, 
                         test_queries: List[str], k_values: List[int], 
                         strategy: str = "fixed_size") -> EvaluationResult:
    """
    è©•ä¼°å–®å€‹chunké…ç½®
    """
    # æ ¹æ“šç­–ç•¥ç”Ÿæˆchunksï¼Œå‚³éç­–ç•¥ç‰¹å®šåƒæ•¸
    if strategy == "fixed_size":
        chunks = sliding_window_chunks(doc.text, config.chunk_size, config.overlap)
    elif strategy == "hierarchical":
        from .chunking import chunk_text
        chunks = chunk_text(doc.text, strategy="hierarchical", 
                           max_chunk_size=config.chunk_size, 
                           overlap_ratio=config.overlap_ratio,
                           min_chunk_size=config.min_chunk_size,
                           level_depth=config.level_depth)
    elif strategy == "rcts_hierarchical":
        from .chunking import chunk_text
        chunks = chunk_text(doc.text, strategy="rcts_hierarchical", 
                           max_chunk_size=config.chunk_size, 
                           overlap_ratio=config.overlap_ratio,
                           preserve_structure=config.preserve_structure)
    elif strategy == "structured_hierarchical":
        from .chunking import chunk_text
        chunks = chunk_text(doc.text, strategy="structured_hierarchical", 
                           json_data=doc.json_data, 
                           max_chunk_size=config.chunk_size, 
                           overlap_ratio=config.overlap_ratio,
                           chunk_by=config.chunk_by)
    elif strategy == "semantic":
        from .chunking import chunk_text
        chunks = chunk_text(doc.text, strategy="semantic", 
                           max_chunk_size=config.chunk_size, 
                           similarity_threshold=config.similarity_threshold,
                           context_window=config.context_window,
                           overlap_ratio=config.overlap_ratio)
    elif strategy == "sliding_window":
        from .chunking import chunk_text
        chunks = chunk_text(doc.text, strategy="sliding_window", 
                           window_size=config.window_size, 
                           step_size=config.step_size,
                           overlap_ratio=config.overlap_ratio,
                           boundary_aware=config.boundary_aware,
                           min_chunk_size_sw=config.min_chunk_size_sw,
                           max_chunk_size_sw=config.max_chunk_size_sw,
                           preserve_sentences=config.preserve_sentences)
    elif strategy == "llm_semantic":
        from .chunking import chunk_text
        chunks = chunk_text(doc.text, strategy="llm_semantic", 
                           max_chunk_size=config.chunk_size, 
                           semantic_threshold=config.semantic_threshold,
                           context_window=config.context_window,
                           overlap_ratio=config.overlap_ratio)
    elif strategy == "hybrid":
        from .chunking import chunk_text
        chunks = chunk_text(doc.text, strategy="hybrid", 
                           primary_size=config.chunk_size, 
                           secondary_size=config.secondary_size,
                           switch_threshold=config.switch_threshold,
                           overlap_ratio=config.overlap_ratio)
    else:
        # é»˜èªä½¿ç”¨å›ºå®šå¤§å°åˆ†å¡Š
        chunks = sliding_window_chunks(doc.text, config.chunk_size, config.overlap)

    # è¨ˆç®—åŸºæœ¬çµ±è¨ˆ
    chunk_count = len(chunks)
    avg_chunk_length = sum(len(c) for c in chunks) / chunk_count if chunk_count else 0.0
    lengths = [len(c) for c in chunks]
    length_variance = (
        sum((l - avg_chunk_length) ** 2 for l in lengths) / chunk_count if chunk_count else 0.0
    )

    # ä½¿ç”¨TF-IDFç‚ºæ¯å€‹æŸ¥è©¢åšæª¢ç´¢æ‰“åˆ†ï¼ˆä¸­æ–‡ç”¨è‡ªå®šç¾©åˆ†è©ï¼‰
    def to_tokens(s: str) -> str:
        toks = preprocess_text(s)
        return " ".join(toks) if toks else s

    processed_chunks = [to_tokens(c) for c in chunks]
    # è‹¥æ–‡æª”éçŸ­ï¼Œé¿å…vectorizerå ±éŒ¯
    if not processed_chunks:
        processed_chunks = [""]

    vectorizer = TfidfVectorizer(
        analyzer="word",
        token_pattern=r"[^\s]+",
        max_features=5000,
        min_df=1,
        max_df=0.98,
        ngram_range=(1, 2),
    )
    X = vectorizer.fit_transform(processed_chunks)

    retrieval_results: Dict[str, List[Dict]] = {}
    precision_at_k_scores: Dict[int, List[float]] = {k: [] for k in k_values}
    recall_at_k_scores: Dict[int, List[float]] = {k: [] for k in k_values}
    precision_omega_scores: List[float] = []

    def compute_pr(retrieved_indices: List[int], relevant_set: set[int], k: int) -> Tuple[float, float]:
        if k <= 0:
            return 0.0, 0.0
        topk = retrieved_indices[:k]
        hit = sum(1 for i in topk if i in relevant_set)
        precision = hit / k
        recall = hit / max(1, len(relevant_set))
        return precision, recall

    max_k = max(k_values) if k_values else 10

    for query in test_queries:
        q = to_tokens(query)
        if not q.strip():
            q = query or ""

        q_vec = vectorizer.transform([q])
        # ä½™å¼¦ç›¸ä¼¼åº¦
        sims = cosine_similarity(q_vec, X).ravel()
        ranked_idx = sims.argsort()[::-1].tolist()

        # å®šç¾©ç›¸é—œé›†ï¼šåˆ†æ•¸é”åˆ°æœ€ä½³åˆ†æ•¸çš„æŸä¸€æ¯”ä¾‹é–¾å€¼ï¼ˆä¾‹å¦‚0.7ï¼‰ä¸”>0
        best = float(sims[ranked_idx[0]]) if ranked_idx else 0.0
        threshold = best * 0.7 if best > 0 else 0.0
        relevant_set = {i for i, s in enumerate(sims) if s >= threshold and s > 0}
        # é˜²æ­¢ç©ºé›†åˆå°è‡´recallç„¡æ„ç¾©ï¼Œè‹¥å…¨éƒ¨ç‚º0åˆ†ï¼Œå‰‡èªç‚ºæ²’æœ‰ç›¸é—œæ–‡æª”
        # è‹¥åªæœ‰æ¥µå°‘æ•¸éé›¶ï¼Œè‡³å°‘ä¿ç•™top1ç‚ºç›¸é—œ
        if best > 0 and not relevant_set:
            relevant_set = {ranked_idx[0]}

        # ä¿å­˜å‰max_kå€‹æª¢ç´¢çµæœä¾›å¯©æŸ¥
        retrieval_results[query] = [
            {
                "chunk_index": i,
                "score": float(sims[i]),
                "content": (chunks[i][:200] + "...") if len(chunks[i]) > 200 else chunks[i],
            }
            for i in ranked_idx[:max_k]
        ]

        # æŒ‡æ¨™è¨ˆç®—
        for k in k_values:
            p, r = compute_pr(ranked_idx, relevant_set, k)
            precision_at_k_scores[k].append(p)
            recall_at_k_scores[k].append(r)

        # PrecisionÎ©: ç†æƒ³æƒ…æ³ä¸‹ï¼ˆæœ€å„ªæ’åºï¼‰åœ¨k=max_kæ™‚å¯é”åˆ°çš„ç²¾åº¦
        # = min(|R|, max_k) / max_k
        precision_omega_scores.append(
            min(len(relevant_set), max_k) / max_k if max_k > 0 else 0.0
        )

    # èšåˆå¹³å‡
    avg_precision_omega = sum(precision_omega_scores) / len(precision_omega_scores) if precision_omega_scores else 0.0
    avg_precision_at_k = {k: (sum(v) / len(v) if v else 0.0) for k, v in precision_at_k_scores.items()}
    avg_recall_at_k = {k: (sum(v) / len(v) if v else 0.0) for k, v in recall_at_k_scores.items()}

    metrics = EvaluationMetrics(
        precision_omega=avg_precision_omega,
        precision_at_k=avg_precision_at_k,
        recall_at_k=avg_recall_at_k,
        chunk_count=chunk_count,
        avg_chunk_length=avg_chunk_length,
        length_variance=length_variance,
    )

    # å‰µå»ºè©³ç´°çš„é…ç½®ä¿¡æ¯ï¼ŒåŒ…å«æ‰€æœ‰ç­–ç•¥ç‰¹å®šåƒæ•¸
    detailed_config = {
        "chunk_size": config.chunk_size,
        "overlap": config.overlap,
        "overlap_ratio": config.overlap_ratio,
        "strategy": strategy,
    }
    
    # æ ¹æ“šç­–ç•¥æ·»åŠ ç‰¹å®šåƒæ•¸
    if strategy == "structured_hierarchical":
        detailed_config["chunk_by"] = config.chunk_by
    elif strategy == "rcts_hierarchical":
        detailed_config["preserve_structure"] = config.preserve_structure
    elif strategy == "hierarchical":
        detailed_config["level_depth"] = config.level_depth
        detailed_config["min_chunk_size"] = config.min_chunk_size
    elif strategy == "semantic":
        detailed_config["similarity_threshold"] = config.similarity_threshold
        detailed_config["context_window"] = config.context_window
    elif strategy == "llm_semantic":
        detailed_config["semantic_threshold"] = config.semantic_threshold
        detailed_config["context_window"] = config.context_window
    elif strategy == "sliding_window":
        detailed_config["window_size"] = config.window_size
        detailed_config["step_size"] = config.step_size
        detailed_config["boundary_aware"] = config.boundary_aware
        detailed_config["preserve_sentences"] = config.preserve_sentences
        detailed_config["min_chunk_size_sw"] = config.min_chunk_size_sw
        detailed_config["max_chunk_size_sw"] = config.max_chunk_size_sw
    elif strategy == "hybrid":
        detailed_config["switch_threshold"] = config.switch_threshold
        detailed_config["secondary_size"] = config.secondary_size

    return EvaluationResult(
        config=detailed_config,
        metrics=metrics,
        test_queries=test_queries,
        retrieval_results=retrieval_results,
        timestamp=datetime.now(),
    )


@app.post("/api/chunk")
def chunk(req: ChunkRequest):
    doc = store.docs.get(req.doc_id)
    if not doc:
        return JSONResponse(status_code=404, content={"error": "doc not found"})
    
    # å°å…¥æ–°çš„chunkingæ¨¡çµ„
    from .chunking import chunk_text
    
    # æ ¹æ“šä¸åŒç­–ç•¥é€²è¡Œåˆ†å¡Š
    strategy = req.strategy
    use_json_structure = req.use_json_structure
    
    # å¦‚æœå•Ÿç”¨JSONçµæ§‹åŒ–åˆ†å‰²ä¸”æœ‰JSONæ•¸æ“šï¼Œå„ªå…ˆä½¿ç”¨JSONçµæ§‹åŒ–åˆ†å‰²
    if use_json_structure and doc.json_data:
        structured_chunks = json_structured_chunks(doc.json_data, req.chunk_size, req.overlap)
        # æå–ç´”æ–‡æœ¬chunksç”¨æ–¼å¾ŒçºŒè™•ç†
        chunks = [chunk["content"] for chunk in structured_chunks]
        # å­˜å„²çµæ§‹åŒ–chunksåˆ°æ–‡æª”ä¸­
        doc.structured_chunks = structured_chunks
    else:
        # ä½¿ç”¨æ–°çš„chunkingæ¨¡çµ„
        chunk_kwargs = {
            "chunk_size": req.chunk_size,
            "overlap": req.overlap,
        }
        
        # æ ¹æ“šç­–ç•¥æ·»åŠ ç‰¹å®šåƒæ•¸
        if strategy == 'hierarchical' and req.hierarchical_params:
            chunk_kwargs.update({
                "max_chunk_size": req.chunk_size,
                "min_chunk_size": req.hierarchical_params.get('min_chunk_size', req.chunk_size // 2),
                "overlap_ratio": req.overlap / req.chunk_size if req.chunk_size > 0 else 0.1,
                "level_depth": req.hierarchical_params.get('level_depth', 2)
            })
        elif strategy == 'rcts_hierarchical' and req.rcts_hierarchical_params:
            chunk_kwargs.update({
                "max_chunk_size": req.chunk_size,
                "overlap_ratio": req.rcts_hierarchical_params.get('overlap_ratio', 0.1),
                "preserve_structure": req.rcts_hierarchical_params.get('preserve_structure', True)
            })
        elif strategy == 'structured_hierarchical' and req.structured_hierarchical_params:
            chunk_kwargs.update({
                "max_chunk_size": req.chunk_size,
                "overlap_ratio": req.structured_hierarchical_params.get('overlap_ratio', 0.1),
                "chunk_by": req.structured_hierarchical_params.get('chunk_by', 'article')
            })
        elif strategy == 'adaptive' and req.adaptive_params:
            chunk_kwargs.update({
                "target_size": req.chunk_size,
                "tolerance": req.adaptive_params.get('tolerance', req.chunk_size // 10),
                "semantic_threshold": req.adaptive_params.get('semantic_threshold', 0.7)
            })
        elif strategy == 'hybrid' and req.hybrid_params:
            chunk_kwargs.update({
                "primary_size": req.chunk_size,
                "secondary_size": req.hybrid_params.get('secondary_size', req.chunk_size // 2),
                "switch_threshold": req.hybrid_params.get('switch_threshold', 0.8)
            })
        elif strategy == 'semantic' and req.semantic_params:
            chunk_kwargs.update({
                "target_size": req.chunk_size,
                "similarity_threshold": req.semantic_params.get('similarity_threshold', 0.6),
                "context_window": req.semantic_params.get('context_window', 100)
            })
        
        # ä½¿ç”¨æ–°çš„chunkingæ¨¡çµ„
        chunks = chunk_text(doc.text, strategy=strategy, json_data=doc.json_data, **chunk_kwargs)
        
        # æ¸…ç©ºçµæ§‹åŒ–chunks
        doc.structured_chunks = []
    
    # è¨ˆç®—è©³ç´°æŒ‡æ¨™
    chunk_lengths = [len(chunk) for chunk in chunks]
    avg_length = sum(chunk_lengths) / len(chunk_lengths) if chunks else 0
    length_variance = 0
    if len(chunk_lengths) > 1:
        variance = sum((length - avg_length) ** 2 for length in chunk_lengths) / len(chunk_lengths)
        length_variance = variance / avg_length if avg_length > 0 else 0
    
    doc.chunks = chunks
    doc.chunk_size = req.chunk_size
    doc.overlap = req.overlap
    # invalidates embeddings for safety
    store.reset_embeddings()
    
    return {
        "doc_id": doc.id, 
        "num_chunks": len(chunks), 
        "chunk_size": req.chunk_size, 
        "overlap": req.overlap,
        "strategy": strategy,
        "sample": chunks[:3],  # å‰3å€‹chunksä½œç‚ºé è¦½
        "all_chunks": chunks,  # æ‰€æœ‰chunks
        "metrics": {
            "avg_length": round(avg_length, 2),
            "length_variance": round(length_variance, 3),
            "min_length": min(chunk_lengths) if chunks else 0,
            "max_length": max(chunk_lengths) if chunks else 0,
            "overlap_rate": req.overlap / req.chunk_size if req.chunk_size > 0 else 0
        }
    }


async def embed_gemini(texts: List[str]) -> List[List[float]]:
    """Call Google Generative API (Gemini) embeddings endpoint using API key.

    Note: This uses the REST endpoint pattern with the API key in query params.
    """
    if not httpx:
        raise RuntimeError("httpx not available")
    if not GOOGLE_API_KEY:
        raise RuntimeError("GOOGLE_API_KEY not set")
    model = os.getenv("GOOGLE_EMBEDDING_MODEL", "text-embedding-004")
    # ä½¿ç”¨æ­£ç¢ºçš„ API ç«¯é»æ ¼å¼
    url = f"https://generativelanguage.googleapis.com/v1beta/models/{model}:embedContent"
    headers = {
        "x-goog-api-key": GOOGLE_API_KEY,
        "Content-Type": "application/json"
    }
    out: List[List[float]] = []
    async with httpx.AsyncClient(timeout=60) as client:
        # é€å€‹è™•ç†æ–‡æœ¬ï¼ˆGemini API éœ€è¦å–®å€‹è«‹æ±‚ï¼‰
        for text in texts:
            payload = {
                "model": f"models/{model}",
                "content": {"parts": [{"text": text}]}
            }
            r = await client.post(url, headers=headers, json=payload)
            r.raise_for_status()
            data = r.json()
            # æ ¹æ“šå®˜æ–¹æ–‡æª”ï¼ŒéŸ¿æ‡‰æ ¼å¼æ˜¯ {"embedding": {"values": [...]}}
            embedding_values = data.get("embedding", {}).get("values", [])
            out.append(embedding_values)
    return out


def embed_bge_m3(texts: List[str]) -> List[List[float]]:
    """ä½¿ç”¨ BGE-M3 æ¨¡å‹é€²è¡Œ embedding"""
    if not SENTENCE_TRANSFORMERS_AVAILABLE:
        raise RuntimeError("sentence-transformers not available")
    
    try:
        # è¼‰å…¥ BGE-M3 æ¨¡å‹
        model = SentenceTransformer('BAAI/bge-m3')
        
        # æ‰¹é‡è™•ç†æ–‡æœ¬
        embeddings = model.encode(texts, batch_size=32, show_progress_bar=True)
        
        # è½‰æ›ç‚ºåˆ—è¡¨æ ¼å¼
        return embeddings.tolist()
        
    except Exception as e:
        raise RuntimeError(f"BGE-M3 embedding failed: {e}")


@app.post("/api/embed")
async def embed(req: EmbedRequest):
    # gather chunks across selected docs
    selected = req.doc_ids or list(store.docs.keys())
    all_chunks: List[str] = []
    chunk_doc_ids: List[str] = []
    for d in selected:
        doc = store.docs.get(d)
        if doc and doc.chunks:
            all_chunks.extend(doc.chunks)
            chunk_doc_ids.extend([doc.id] * len(doc.chunks))

    if not all_chunks:
        return JSONResponse(status_code=400, content={"error": "no chunks to embed"})

    # å˜—è©¦ä½¿ç”¨ Gemini embeddingï¼ˆä¸»è¦é¸é …ï¼‰
    if USE_GEMINI_EMBEDDING and GOOGLE_API_KEY:
        try:
            vectors = await embed_gemini(all_chunks)
            store.embeddings = vectors
            store.chunk_doc_ids = chunk_doc_ids
            store.chunks_flat = all_chunks
            return {
                "provider": "gemini", 
                "model": "text-embedding-004",
                "num_vectors": len(vectors),
                "dimension": len(vectors[0]) if vectors else 0
            }
        except Exception as e:
            print(f"Gemini embedding failed: {e}")
            # å¦‚æœ Gemini å¤±æ•—ï¼Œå˜—è©¦ BGE-M3
    
    # å˜—è©¦ä½¿ç”¨ BGE-M3 embeddingï¼ˆå‚™ç”¨é¸é …ï¼‰
    if USE_BGE_M3_EMBEDDING and SENTENCE_TRANSFORMERS_AVAILABLE:
        try:
            vectors = embed_bge_m3(all_chunks)
            store.embeddings = vectors
            store.chunk_doc_ids = chunk_doc_ids
            store.chunks_flat = all_chunks
            return {
                "provider": "bge-m3", 
                "model": "BAAI/bge-m3",
                "num_vectors": len(vectors),
                "dimension": len(vectors[0]) if vectors else 0
            }
        except Exception as e:
            print(f"BGE-M3 embedding failed: {e}")
    
    # æ²’æœ‰å¯ç”¨çš„ embedding æ–¹æ³•
    return JSONResponse(
        status_code=500, 
        content={
            "error": "No embedding method available. Please configure Gemini API key or BGE-M3 model."
        }
    )


def rank_with_dense_vectors(query: str, k: int):
    """ä½¿ç”¨å¯†é›†å‘é‡é€²è¡Œç›¸ä¼¼åº¦è¨ˆç®—ï¼ˆæ”¯æŒ Gemini å’Œ BGE-M3ï¼‰"""
    import numpy as np
    vecs = np.array(store.embeddings, dtype=float)  # type: ignore[assignment]
    
    # æ ¹æ“šç•¶å‰é…ç½®é¸æ“‡æŸ¥è©¢å‘é‡åŒ–æ–¹æ³•
    if USE_GEMINI_EMBEDDING and GOOGLE_API_KEY:
        try:
            qvec = np.array(asyncio_run(embed_gemini([query]))[0], dtype=float)
        except Exception as e:
            print(f"Gemini query embedding failed: {e}")
            # å¦‚æœ Gemini å¤±æ•—ï¼Œå˜—è©¦ BGE-M3
            if USE_BGE_M3_EMBEDDING and SENTENCE_TRANSFORMERS_AVAILABLE:
                try:
                    qvec = np.array(embed_bge_m3([query])[0], dtype=float)
                except Exception as e2:
                    print(f"BGE-M3 query embedding failed: {e2}")
                    raise RuntimeError("Both Gemini and BGE-M3 query embedding failed")
            else:
                raise RuntimeError("Gemini query embedding failed and BGE-M3 not available")
    elif USE_BGE_M3_EMBEDDING and SENTENCE_TRANSFORMERS_AVAILABLE:
        try:
            qvec = np.array(embed_bge_m3([query])[0], dtype=float)
        except Exception as e:
            print(f"BGE-M3 query embedding failed: {e}")
            raise RuntimeError("BGE-M3 query embedding failed")
    else:
        raise RuntimeError("No dense embedding method available")
    
    # normalize
    vecs_norm = vecs / (np.linalg.norm(vecs, axis=1, keepdims=True) + 1e-8)
    q_norm = qvec / (np.linalg.norm(qvec) + 1e-8)
    sims = vecs_norm @ q_norm
    idxs = np.argsort(-sims)[:k]
    return idxs.tolist(), sims[idxs].tolist()


def calculate_retrieval_metrics(query: str, results: List[Dict], k: int) -> Dict[str, float]:
    """è¨ˆç®—æª¢ç´¢æŒ‡æ¨™ P@K å’Œ R@K"""
    try:
        # å˜—è©¦å¾ QA æ•¸æ“šä¸­ç²å–ç›¸é—œæ–‡æª”
        qa_data = load_qa_data()
        if not qa_data:
            return {"p_at_k": 0.0, "r_at_k": 0.0, "note": "No QA data available"}
        
        # æ‰¾åˆ°èˆ‡æŸ¥è©¢æœ€åŒ¹é…çš„ QA é …ç›®
        best_match = None
        best_similarity = 0.0
        
        for qa_item in qa_data:
            # æ”¹é€²çš„æ–‡æœ¬ç›¸ä¼¼åº¦åŒ¹é…
            qa_query = qa_item.get("query", "").lower()
            query_lower = query.lower()
            
            # æ–¹æ³•1: ç›´æ¥åŒ…å«åŒ¹é…
            if query_lower in qa_query or qa_query in query_lower:
                similarity = 1.0
            else:
                # æ–¹æ³•2: æå–æ³•æ¢è™Ÿç¢¼é€²è¡ŒåŒ¹é…
                import re
                query_article = re.search(r'ç¬¬(\d+(?:ä¹‹\d+)?)æ¢', query_lower)
                qa_article = re.search(r'ç¬¬(\d+(?:ä¹‹\d+)?)æ¢', qa_query)
                
                if query_article and qa_article:
                    if query_article.group(1) == qa_article.group(1):
                        similarity = 0.8
                    else:
                        similarity = 0.0
                else:
                    # æ–¹æ³•3: è©å½™é‡ç–Šåº¦
                    query_words = set(query_lower.split())
                    qa_words = set(qa_query.split())
                    overlap = len(query_words.intersection(qa_words))
                    similarity = overlap / max(len(query_words), len(qa_words), 1)
            
            if similarity > best_similarity:
                best_similarity = similarity
                best_match = qa_item
        
        if not best_match or best_similarity < 0.3:  # ç›¸ä¼¼åº¦é–¾å€¼
            return {"p_at_k": 0.0, "r_at_k": 0.0, "note": "No matching QA found"}
        
        # å¾ gold å­—æ®µä¸­æå–ç›¸é—œçš„æ³•æ¢ä¿¡æ¯
        gold = best_match.get("gold", {})
        if gold:
            # å¾ gold å­—æ®µæ§‹å»ºæ³•æ¢ä¿¡æ¯
            law = gold.get("law", "")
            article_number = gold.get("article_number")
            article_suffix = gold.get("article_suffix")
            
            if law and article_number:
                article_text = f"ç¬¬{article_number}æ¢"
                if article_suffix:
                    article_text += f"ä¹‹{article_suffix}"
                relevant_articles = [article_text]
            else:
                # å¦‚æœæ²’æœ‰ gold ä¿¡æ¯ï¼Œå˜—è©¦å¾æŸ¥è©¢ä¸­æå–
                relevant_articles = extract_articles_from_text(best_match.get("query", ""))
        else:
            # å¦‚æœæ²’æœ‰ gold å­—æ®µï¼Œå˜—è©¦å¾æŸ¥è©¢ä¸­æå–
            relevant_articles = extract_articles_from_text(best_match.get("query", ""))
        
        if not relevant_articles:
            return {"p_at_k": 0.0, "r_at_k": 0.0, "note": "No relevant articles found"}
        
        # è¨ˆç®— P@K å’Œ R@K
        relevant_count = 0
        for result in results[:k]:
            content = result.get("content", "").lower()
            # æª¢æŸ¥æ˜¯å¦åŒ…å«ç›¸é—œæ³•æ¢
            for article in relevant_articles:
                # æ¨™æº–åŒ–æ³•æ¢æ ¼å¼é€²è¡ŒåŒ¹é…
                article_normalized = article.lower().replace(" ", "")
                content_normalized = content.replace(" ", "")
                if article_normalized in content_normalized:
                    relevant_count += 1
                    break
        
        p_at_k = relevant_count / k if k > 0 else 0.0
        r_at_k = relevant_count / len(relevant_articles) if relevant_articles else 0.0
        
        return {
            "p_at_k": p_at_k,
            "r_at_k": r_at_k,
            "relevant_articles": relevant_articles,
            "qa_similarity": best_similarity,
            "matched_qa": best_match.get("query", "")[:100] + "..."
        }
        
    except Exception as e:
        return {"p_at_k": 0.0, "r_at_k": 0.0, "error": str(e)}


def load_qa_data() -> List[Dict]:
    """è¼‰å…¥ QA æ•¸æ“š"""
    try:
        import json
        import os
        
        # å˜—è©¦è¼‰å…¥ä¸åŒçš„ QA æ–‡ä»¶
        qa_files = [
            "QA/copyright_p.json",
            "QA/copyright_n.json", 
            "QA/copyright.json",
            "QA/qa_gold.json"
        ]
        
        for qa_file in qa_files:
            qa_path = os.path.join(os.path.dirname(__file__), "..", "..", qa_file)
            if os.path.exists(qa_path):
                with open(qa_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    if isinstance(data, list) and len(data) > 0:
                        return data
        
        return []
    except Exception as e:
        print(f"è¼‰å…¥ QA æ•¸æ“šå¤±æ•—: {e}")
        return []


def extract_articles_from_text(text: str) -> List[str]:
    """å¾æ–‡æœ¬ä¸­æå–æ³•æ¢ä¿¡æ¯"""
    import re
    
    articles = []
    
    # åŒ¹é… "ç¬¬Xæ¢" æ¨¡å¼
    patterns = [
        r"ç¬¬(\d+)æ¢",
        r"ç¬¬(\d+)æ¢ä¹‹(\d+)",
        r"ç¬¬(\d+)-(\d+)æ¢"
    ]
    
    for pattern in patterns:
        matches = re.findall(pattern, text)
        for match in matches:
            if isinstance(match, tuple):
                if len(match) == 2:
                    if match[1]:  # æœ‰ä¹‹N
                        articles.append(f"ç¬¬{match[0]}æ¢ä¹‹{match[1]}")
                    else:  # ç¯„åœ
                        articles.append(f"ç¬¬{match[0]}-{match[1]}æ¢")
                else:
                    articles.append(f"ç¬¬{match[0]}æ¢")
            else:
                articles.append(f"ç¬¬{match}æ¢")
    
    return list(set(articles))  # å»é‡


def asyncio_run(coro):
    import asyncio
    try:
        loop = asyncio.get_running_loop()
    except RuntimeError:
        loop = None
    if loop and loop.is_running():
        # create task and wait
        return asyncio.run_coroutine_threadsafe(coro, loop).result()
    return asyncio.run(coro)


@app.post("/api/retrieve")
def retrieve(req: RetrieveRequest):
    if store.embeddings is None:
        return JSONResponse(status_code=400, content={"error": "run /embed first"})
    
    # è¨ˆç®—ç›¸ä¼¼åº¦ä¸¦æ’åºï¼ˆåªä½¿ç”¨å¯†é›†å‘é‡ï¼‰
    idxs, sims = rank_with_dense_vectors(req.query, req.k)

    # Use the same order as built in /embed
    chunks_flat = store.chunks_flat
    mapping_doc_ids = store.chunk_doc_ids

    results = []
    for rank, (i, score) in enumerate(zip(idxs, sims), start=1):
        if i < 0 or i >= len(chunks_flat):
            continue
        
        # ç²å–æ–‡æª”ä¿¡æ¯
        doc_id = mapping_doc_ids[i]
        doc = store.docs.get(doc_id)
        
        # åŸºæœ¬çµæœ
        result = {
            "rank": rank,
            "score": float(score),
            "doc_id": doc_id,
            "chunk_index": i,
            "content": chunks_flat[i][:2000],
        }
        
        # å¦‚æœæœ‰çµæ§‹åŒ–chunksï¼Œæ·»åŠ metadata
        if doc and hasattr(doc, 'structured_chunks') and doc.structured_chunks and i < len(doc.structured_chunks):
            structured_chunk = doc.structured_chunks[i]
            result["metadata"] = structured_chunk.get("metadata", {})
            result["chunk_id"] = structured_chunk.get("chunk_id", "")
            
            # æ·»åŠ æ³•å¾‹çµæ§‹ä¿¡æ¯
            metadata = structured_chunk.get("metadata", {})
            result["legal_structure"] = {
                "id": metadata.get("id", ""),
                "spans": metadata.get("spans", {}),
                "page_range": metadata.get("page_range", {})
            }
        
        results.append(result)
    
    # è¨ˆç®— P@K å’Œ R@Kï¼ˆå¦‚æœæœ‰ QA æ•¸æ“šï¼‰
    metrics = calculate_retrieval_metrics(req.query, results, req.k)
    
    # åˆ¤æ–· embedding provider å’Œ modelï¼ˆä¸å†æ”¯æŒ TF-IDFï¼‰
    if USE_GEMINI_EMBEDDING and GOOGLE_API_KEY:
        embedding_provider = "gemini"
        embedding_model = "text-embedding-004"
    elif USE_BGE_M3_EMBEDDING and SENTENCE_TRANSFORMERS_AVAILABLE:
        embedding_provider = "bge-m3"
        embedding_model = "BAAI/bge-m3"
    else:
        embedding_provider = "unknown"
        embedding_model = "unknown"

    return {
        "query": req.query, 
        "k": req.k, 
        "results": results,
        "metrics": metrics,
        "embedding_provider": embedding_provider,
        "embedding_model": embedding_model
    }


@app.post("/api/hybrid-retrieve")
def hybrid_retrieve(req: RetrieveRequest):
    """HybridRAG æª¢ç´¢ï¼šçµåˆå‘é‡ç›¸ä¼¼åº¦å’Œæ³•å¾‹çµæ§‹è¦å‰‡"""
    if store.embeddings is None:
        return JSONResponse(status_code=400, content={"error": "run /embed first"})
    
    # ç²å–æ‰€æœ‰ chunks å’Œ metadata
    chunks_flat = store.chunks_flat
    mapping_doc_ids = store.chunk_doc_ids
    
    if not chunks_flat:
        return JSONResponse(status_code=400, content={"error": "no chunks available"})
    
    # æ§‹å»º nodes æ ¼å¼ä¾› hybrid_rank ä½¿ç”¨
    nodes = []
    for i, (chunk, doc_id) in enumerate(zip(chunks_flat, mapping_doc_ids)):
        doc = store.docs.get(doc_id)
        metadata = {}
        
        # å¦‚æœæœ‰çµæ§‹åŒ–chunksï¼Œæå–metadata
        if doc and hasattr(doc, 'structured_chunks') and doc.structured_chunks and i < len(doc.structured_chunks):
            structured_chunk = doc.structured_chunks[i]
            metadata = structured_chunk.get("metadata", {})
        
        nodes.append({
            "content": chunk,
            "metadata": metadata,
            "doc_id": doc_id,
            "chunk_index": i
        })
    
    # å…ˆç”¨å¯†é›†å‘é‡å¾—åˆ°æ¯å€‹ç¯€é»çš„å‘é‡åˆ†æ•¸
    # æˆ‘å€‘å°æ‰€æœ‰ç¯€é»é€²è¡Œç›¸ä¼¼åº¦è¨ˆç®—ï¼Œç„¶å¾Œåªå–å‰ k çš„çµæœåš Hybrid æ’åº
    dense_top_k = min(len(nodes), max(req.k * 4, req.k))
    all_vec_idxs, all_vec_sims = rank_with_dense_vectors(req.query, k=len(nodes))
    # æ˜ å°„å‡ºç¯€é»é †åºå°æ‡‰çš„åˆ†æ•¸ï¼Œåˆå§‹åŒ–ç‚º0
    node_vector_scores = [0.0] * len(nodes)
    for rank_idx, node_idx in enumerate(all_vec_idxs):
        node_vector_scores[node_idx] = float(all_vec_sims[rank_idx])

    # å–å‘é‡åˆ†æ•¸æœ€é«˜çš„å‰ dense_top_k ç¯€é»ä½œç‚º Hybrid å€™é¸
    top_vec_pairs = sorted(
        [(i, s) for i, s in enumerate(node_vector_scores)], key=lambda x: x[1], reverse=True
    )[:dense_top_k]
    candidate_nodes = [nodes[i] for i, _ in top_vec_pairs]
    candidate_scores = [s for _, s in top_vec_pairs]

    # ä½¿ç”¨ hybrid_rank é€²è¡Œæª¢ç´¢ï¼ˆå‘é‡åˆ†æ•¸ + metadata åŠ åˆ†ï¼‰
    config = HybridConfig(
        alpha=0.8,  # å‘é‡ç›¸ä¼¼åº¦æ¬Šé‡
        w_law_match=0.15,  # æ³•åå°é½Šæ¬Šé‡
        w_article_match=0.15,  # æ¢è™Ÿå°é½Šæ¬Šé‡
        w_keyword_hit=0.05,  # è¡“èªå‘½ä¸­æ¬Šé‡
        max_bonus=0.4  # æœ€å¤§åŠ åˆ†
    )

    hybrid_results = hybrid_rank(
        req.query, candidate_nodes, k=req.k, config=config, vector_scores=candidate_scores
    )
    
    # è½‰æ›ç‚ºæ¨™æº–æ ¼å¼
    results = []
    for rank, item in enumerate(hybrid_results, start=1):
        result = {
            "rank": rank,
            "score": item["score"],
            "vector_score": item["vector_score"],
            "bonus": item["bonus"],
            "doc_id": item["doc_id"],
            "chunk_index": item["chunk_index"],
            "content": item["content"][:2000],
            "metadata": item["metadata"]
        }
        
        # æ·»åŠ æ³•å¾‹çµæ§‹ä¿¡æ¯
        if item["metadata"]:
            result["legal_structure"] = {
                "id": item["metadata"].get("id", ""),
                "category": item["metadata"].get("category", ""),
                "article_label": item["metadata"].get("article_label", ""),
                "article_number": item["metadata"].get("article_number"),
                "article_suffix": item["metadata"].get("article_suffix"),
                "spans": item["metadata"].get("spans", {}),
                "page_range": item["metadata"].get("page_range", {})
            }
        
        results.append(result)
    
    # è¨ˆç®— P@K å’Œ R@Kï¼ˆå¦‚æœæœ‰ QA æ•¸æ“šï¼‰
    metrics = calculate_retrieval_metrics(req.query, results, req.k)
    
    # åˆ¤æ–· embedding provider å’Œ modelï¼ˆä¸å†æ”¯æŒ TF-IDFï¼‰
    if USE_GEMINI_EMBEDDING and GOOGLE_API_KEY:
        embedding_provider = "gemini"
        embedding_model = "text-embedding-004"
    elif USE_BGE_M3_EMBEDDING and SENTENCE_TRANSFORMERS_AVAILABLE:
        embedding_provider = "bge-m3"
        embedding_model = "BAAI/bge-m3"
    else:
        embedding_provider = "unknown"
        embedding_model = "unknown"
    
    return {
        "query": req.query, 
        "k": req.k, 
        "results": results,
        "method": "hybrid_rag",
        "metrics": metrics,
        "embedding_provider": embedding_provider,
        "embedding_model": embedding_model,
        "config": {
            "alpha": config.alpha,
            "w_law_match": config.w_law_match,
            "w_article_match": config.w_article_match,
            "w_keyword_hit": config.w_keyword_hit,
            "max_bonus": config.max_bonus
        }
    }


async def gemini_chat(messages: List[Dict[str, str]]) -> str:
    if not httpx:
        raise RuntimeError("httpx not available")
    
    # å„ªå…ˆä½¿ç”¨ GOOGLE_API_KEYï¼Œå¦‚æœæ²’æœ‰å‰‡ä½¿ç”¨ GEMINI_API_KEY
    api_key = GOOGLE_API_KEY or os.getenv("GEMINI_API_KEY")
    if not api_key:
        raise RuntimeError("GOOGLE_API_KEY or GEMINI_API_KEY not set")
    
    model = os.getenv("GOOGLE_CHAT_MODEL", "gemini-1.5-flash")
    # Use Generative Language API: models/{model}:generateContent
    url = f"https://generativelanguage.googleapis.com/v1beta/models/{model}:generateContent"
    # Convert messages to Gemini format
    contents = []
    for message in messages:
        contents.append({
            "parts": [{"text": message.get("content", "")}],
            "role": "user" if message.get("role") == "user" else "model"
        })
    
    payload = {
        "contents": contents,
        "generationConfig": {
            "temperature": 0.2,
            "maxOutputTokens": 2048
        }
    }
    
    headers = {
        "x-goog-api-key": api_key,
        "Content-Type": "application/json"
    }
    
    async with httpx.AsyncClient(timeout=60) as client:
        r = await client.post(url, headers=headers, json=payload)
        r.raise_for_status()
        data = r.json()
        # Extract response from new format
        if "candidates" in data and data["candidates"]:
            candidate = data["candidates"][0]
            if "content" in candidate and "parts" in candidate["content"]:
                return candidate["content"]["parts"][0].get("text", "").strip()
        return "No response generated"


def simple_extractive_answer(query: str, contexts: List[str]) -> str:
    """é‡å°ä¸­è‹±æ–‡æ”¹é€²çš„æ¥µç°¡æŠ½å–å¼å›ç­”ï¼š
    - æ”¯æ´ä¸­æ–‡æ–·å¥ï¼ˆã€‚ï¼ï¼Ÿï¼›ï¼‰èˆ‡æ›è¡Œ
    - åˆ†è©åŒæ™‚è€ƒæ…®è‹±æ–‡/æ•¸å­—è©èˆ‡ä¸­æ–‡å–®å­—
    - è‹¥ç„¡æ˜é¡¯é‡ç–Šï¼Œå›é€€è¼¸å‡ºå‰å¹¾å¥æœ€å‰é¢çš„å…§å®¹
    """
    import re
    from collections import Counter

    # 1) æ–·å¥ï¼ˆåŒæ™‚æ”¯æ´ä¸­è‹±æ¨™é»èˆ‡æ›è¡Œï¼‰
    def split_sentences(text: str) -> List[str]:
        # ä¿ç•™åŸæ–‡ç‰‡æ®µï¼Œé¿å…éåº¦åˆ‡ç¢
        # å…ˆæŒ‰æ›è¡Œæ‹†ï¼Œå†æŒ‰ä¸­æ–‡/è‹±æ–‡å¥æœ«æ¨™é»ç´°åˆ†
        parts: List[str] = []
        for seg in re.split(r"[\n\r]+", text):
            seg = seg.strip()
            if not seg:
                continue
            parts.extend([s.strip() for s in re.split(r"(?<=[ã€‚ï¼ï¼Ÿ!?ï¼›;])\s+", seg) if s.strip()])
        return parts

    # 2) ç°¡å–®åˆ†è©ï¼šè‹±æ–‡/æ•¸å­—è© + ä¸­æ–‡å–®å­—
    def tokenize(text: str) -> List[str]:
        text_norm = text.lower()
        en = re.findall(r"[a-z0-9_]+", text_norm)
        zh = re.findall(r"[\u4e00-\u9fff]", text_norm)
        return en + zh

    q_tokens = set(tokenize(query))
    if not q_tokens:
        q_tokens = set(query.lower())  # é€€åŒ–ç‚ºå­—ç¬¦é›†åˆ

    # 3) èšåˆæ‰€æœ‰ä¸Šä¸‹æ–‡çš„å¥å­
    sents: List[str] = []
    for ctx in contexts:
        sents.extend(split_sentences(ctx))

    # 4) è¨ˆåˆ†ï¼šé‡ç–Š token æ•¸é‡ + è¼•åº¦é•·åº¦å¹³è¡¡
    counts = Counter()
    for s in sents:
        t = tokenize(s)
        if not t:
            continue
        overlap = len(set(t) & q_tokens)
        if overlap > 0:
            # è¼•åº¦é¼“å‹µè¼ƒå®Œæ•´å¥å­
            counts[s] = overlap + min(len(s) / 200.0, 1.0)

    # 5) å›å‚³ï¼šæœ‰åŒ¹é…å‰‡å–å‰5å¥ï¼Œå¦å‰‡å›é€€å–æœ€å‰é¢å…§å®¹
    if counts:
        best = [s for s, _ in counts.most_common(5)]
        return " \n".join(best)

    # å›é€€ï¼šå–å‰å…©æ®µçš„å‰å…©å¥
    fallback: List[str] = []
    for ctx in contexts[:2]:
        ss = split_sentences(ctx)
        fallback.extend(ss[:2])
        if len(fallback) >= 4:
            break
    if fallback:
        return " \n".join(fallback[:4])
    return "No relevant answer found in context."


@app.post("/api/generate")
def generate(req: GenerateRequest):
    # ä½¿ç”¨ HybridRAGï¼ˆå‘é‡æª¢ç´¢ + metadata é—œéµå­—åŠ åˆ†ï¼‰å–å¾—ç”Ÿæˆä¸Šä¸‹æ–‡
    if store.embeddings is None:
        return JSONResponse(status_code=400, content={"error": "run /embed first"})

    # æ§‹å»º nodesï¼ˆèˆ‡ /api/hybrid-retrieve ä¿æŒä¸€è‡´ï¼‰
    chunks_flat = store.chunks_flat
    mapping_doc_ids = store.chunk_doc_ids
    if not chunks_flat:
        return JSONResponse(status_code=400, content={"error": "no chunks available"})

    nodes = []
    for i, (chunk, doc_id) in enumerate(zip(chunks_flat, mapping_doc_ids)):
        doc = store.docs.get(doc_id)
        metadata = {}
        if doc and hasattr(doc, 'structured_chunks') and doc.structured_chunks and i < len(doc.structured_chunks):
            structured_chunk = doc.structured_chunks[i]
            metadata = structured_chunk.get("metadata", {})
        nodes.append({
            "content": chunk,
            "metadata": metadata,
            "doc_id": doc_id,
            "chunk_index": i
        })

    # å…ˆç”¨å¯†é›†å‘é‡è¨ˆç®—æ‰€æœ‰ç¯€é»çš„ç›¸ä¼¼åº¦ï¼Œå–å‰ N åš Hybrid å€™é¸
    dense_top_k = min(len(nodes), max(req.top_k * 4, req.top_k))
    all_vec_idxs, all_vec_sims = rank_with_dense_vectors(req.query, k=len(nodes))
    node_vector_scores = [0.0] * len(nodes)
    for rank_idx, node_idx in enumerate(all_vec_idxs):
        node_vector_scores[node_idx] = float(all_vec_sims[rank_idx])
    top_vec_pairs = sorted(
        [(i, s) for i, s in enumerate(node_vector_scores)], key=lambda x: x[1], reverse=True
    )[:dense_top_k]
    candidate_nodes = [nodes[i] for i, _ in top_vec_pairs]
    candidate_scores = [s for _, s in top_vec_pairs]

    config = HybridConfig(
        alpha=0.8,
        w_law_match=0.15,
        w_article_match=0.15,
        w_keyword_hit=0.05,
        max_bonus=0.4,
    )
    hybrid_results = hybrid_rank(req.query, candidate_nodes, k=req.top_k, config=config, vector_scores=candidate_scores)

    # ç”Ÿæˆä½¿ç”¨çš„çµæœ
    results = []
    for rank, item in enumerate(hybrid_results, start=1):
        result = {
            "rank": rank,
            "score": item.get("score"),
            "vector_score": item.get("vector_score"),
            "bonus": item.get("bonus"),
            "doc_id": item.get("doc_id"),
            "chunk_index": item.get("chunk_index"),
            "content": item.get("content"),
        }
        md = (item.get("metadata") or {})
        if md:
            result["legal_structure"] = {
                "id": md.get("id", ""),
                "category": md.get("category", ""),
                "article_label": md.get("article_label", ""),
                "article_number": md.get("article_number"),
                "article_suffix": md.get("article_suffix"),
                "spans": md.get("spans", {}),
                "page_range": md.get("page_range", {}),
            }
        results.append(result)
    contexts = [item["content"] for item in results]

    # æ§‹å»ºçµæ§‹åŒ–ä¸Šä¸‹æ–‡ä¿¡æ¯
    structured_context = []
    legal_references = []
    
    for item in results:
        context_text = item["content"]
        
        # å¦‚æœæœ‰æ³•å¾‹çµæ§‹ä¿¡æ¯ï¼Œæ·»åŠ åˆ°ä¸Šä¸‹æ–‡ä¸­
        if "legal_structure" in item:
            legal_info = item["legal_structure"]
            law_name = legal_info.get("law_name", "")
            article = legal_info.get("article", "")
            item_ref = legal_info.get("item", "")
            sub_item = legal_info.get("sub_item", "")
            chunk_type = legal_info.get("chunk_type", "")
            
            # æ§‹å»ºæ³•å¾‹å¼•ç”¨
            legal_ref = f"{law_name}"
            if article:
                legal_ref += f" {article}"
            if item_ref:
                legal_ref += f" {item_ref}"
            if sub_item:
                legal_ref += f" {sub_item}"
            
            if legal_ref not in legal_references:
                legal_references.append(legal_ref)
            
            # æ·»åŠ çµæ§‹åŒ–ä¸Šä¸‹æ–‡
            structured_context.append(f"[{legal_ref}] {context_text}")
        else:
            structured_context.append(context_text)

    reasoning_steps = [
        {"type": "plan", "text": "Read query, identify entities and constraints."},
        {"type": "gather", "text": f"Collect top-{req.top_k} chunks as context."},
        {"type": "analyze", "text": f"Analyze legal structure: {', '.join(legal_references[:3])}."},
        {"type": "synthesize", "text": "Synthesize answer grounded in retrieved text with legal references."},
    ]

    if USE_GEMINI_COMPLETION:
        # æ§‹å»ºåŒ…å«æ³•å¾‹çµæ§‹ä¿¡æ¯çš„prompt
        system_prompt = """ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„æ³•å¾‹åŠ©æ‰‹ã€‚è«‹åŸºæ–¼æä¾›çš„æ³•å¾‹æ–‡æª”å…§å®¹å›ç­”å•é¡Œã€‚

é‡è¦è¦æ±‚ï¼š
1. åªä½¿ç”¨æä¾›çš„ä¸Šä¸‹æ–‡å…§å®¹å›ç­”å•é¡Œ
2. å¦‚æœç­”æ¡ˆæ¶‰åŠå…·é«”æ³•å¾‹æ¢æ–‡ï¼Œè«‹å¼•ç”¨ç›¸é—œçš„æ³•è¦åç¨±å’Œæ¢æ–‡è™Ÿç¢¼
3. å¦‚æœä¿¡æ¯ä¸è¶³ï¼Œè«‹æ˜ç¢ºèªªæ˜ä½ ä¸çŸ¥é“
4. å›ç­”è¦æº–ç¢ºã€å°ˆæ¥­ï¼Œç¬¦åˆæ³•å¾‹æ–‡æª”çš„è¡¨è¿°æ–¹å¼"""

        user_content = f"å•é¡Œ: {req.query}\n\n"
        
        if legal_references:
            user_content += f"ç›¸é—œæ³•è¦: {', '.join(legal_references)}\n\n"
        
        user_content += "æ³•å¾‹æ–‡æª”å…§å®¹:\n" + "\n---\n".join(structured_context)
        
        prompt = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_content},
        ]
        
        try:
            answer = asyncio_run(gemini_chat(prompt))
        except Exception as e:
            answer = f"Geminièª¿ç”¨å¤±æ•—: {e}. å›é€€åˆ°æå–å¼å›ç­”ã€‚\n" + simple_extractive_answer(req.query, contexts)
    else:
        answer = simple_extractive_answer(req.query, contexts)

    return {
        "query": req.query,
        "answer": answer,
        "contexts": results,
        "legal_references": legal_references,
        "steps": reasoning_steps,
    }


def merge_law_documents(law_documents: List[Dict[str, Any]]) -> Dict[str, Any]:
    """
    æ•´åˆå¤šå€‹æ³•å¾‹æ–‡æª”æˆä¸€å€‹çµ±ä¸€çš„JSONçµæ§‹
    
    åƒæ•¸:
    - law_documents: å¤šå€‹æ³•å¾‹æ–‡æª”çš„åˆ—è¡¨
    
    è¿”å›:
    - æ•´åˆå¾Œçš„æ³•å¾‹æ–‡æª”ï¼Œæ ¼å¼ç‚º {"laws": [...]}
    """
    if not law_documents:
        return {"laws": []}
    
    # ç¢ºä¿æ¯å€‹æ³•å¾‹æ–‡æª”éƒ½æœ‰å”¯ä¸€çš„IDå‰ç¶´
    merged_laws = []
    global_id_counter = 0
    
    for doc in law_documents:
        if not doc or "law_name" not in doc:
            continue
            
        law_name = doc["law_name"]
        law_prefix = f"{law_name}_{global_id_counter}"
        
        # å‰µå»ºæ–°çš„æ³•å¾‹æ–‡æª”çµæ§‹
        merged_law = {
            "law_name": law_name,
            "chapters": []
        }
        
        # è™•ç†ç« ç¯€
        chapters = doc.get("chapters", [])
        for chapter in chapters:
            chapter_name = chapter.get("chapter", "")
            merged_chapter = {
                "chapter": chapter_name,
                "sections": []
            }
            
            # è™•ç†ç¯€
            sections = chapter.get("sections", [])
            for section in sections:
                section_name = section.get("section", "")
                merged_section = {
                    "section": section_name,
                    "articles": []
                }
                
                # è™•ç†æ¢æ–‡
                articles = section.get("articles", [])
                for article in articles:
                    article_name = article.get("article", "")
                    merged_article = {
                        "article": article_name,
                        "content": article.get("content", ""),
                        "items": []
                    }
                    
                    # è™•ç†é …ç›®
                    items = article.get("items", [])
                    for item in items:
                        item_name = item.get("item", "")
                        merged_item = {
                            "item": item_name,
                            "content": item.get("content", ""),
                            "sub_items": []
                        }
                        
                        # è™•ç†å­é …ç›®
                        sub_items = item.get("sub_items", [])
                        for sub_item in sub_items:
                            sub_item_name = sub_item.get("sub_item", "")
                            merged_sub_item = {
                                "sub_item": sub_item_name,
                                "content": sub_item.get("content", ""),
                                "metadata": {
                                    "id": f"{law_prefix}_{chapter_name}_{section_name}_{article_name}_{item_name}_{sub_item_name}".replace(" ", "_"),
                                    "spans": sub_item.get("metadata", {}).get("spans", {}),
                                    "page_range": sub_item.get("metadata", {}).get("page_range", {})
                                }
                            }
                            merged_item["sub_items"].append(merged_sub_item)
                        
                        # ç‚ºé …ç›®æ·»åŠ metadata
                        merged_item["metadata"] = {
                            "id": f"{law_prefix}_{chapter_name}_{section_name}_{article_name}_{item_name}".replace(" ", "_"),
                            "spans": item.get("metadata", {}).get("spans", {}),
                            "page_range": item.get("metadata", {}).get("page_range", {})
                        }
                        merged_article["items"].append(merged_item)
                    
                    # ç‚ºæ¢æ–‡æ·»åŠ metadata
                    merged_article["metadata"] = {
                        "id": f"{law_prefix}_{chapter_name}_{section_name}_{article_name}".replace(" ", "_"),
                        "spans": article.get("metadata", {}).get("spans", {}),
                        "page_range": article.get("metadata", {}).get("page_range", {})
                    }
                    merged_section["articles"].append(merged_article)
                
                merged_chapter["sections"].append(merged_section)
            
            merged_law["chapters"].append(merged_chapter)
        
        merged_laws.append(merged_law)
        global_id_counter += 1
    
    return {"laws": merged_laws}


def convert_pdf_structured(file_content: bytes, filename: str, options: MetadataOptions) -> Dict[str, Any]:
    """å°‡PDFè½‰æ›ç‚ºçµæ§‹åŒ–æ ¼å¼"""
    import time
    start_time = time.time()
    
    try:
        # Read PDF content safely; skip pages with no text
        try:
            reader = PdfReader(io.BytesIO(file_content))
        except Exception as e:
            raise Exception(f"ç„¡æ³•è®€å–PDFæ–‡ä»¶: {str(e)}")
        
        # æ‰¹é‡æå–æ–‡æœ¬ï¼Œé¡¯ç¤ºé€²åº¦
        texts = []
        total_pages = len(reader.pages)
        print(f"ç¸½é æ•¸: {total_pages}")
        
        for i, page in enumerate(reader.pages):
            try:
                t = page.extract_text() or ""
                texts.append(t)
            except Exception:
                texts.append("")
            
            # æ¯è™•ç†10é é¡¯ç¤ºé€²åº¦
            if (i + 1) % 10 == 0 or (i + 1) == total_pages:
                print(f"å·²è™•ç† {i + 1}/{total_pages} é ")
        
        if not any(texts):  # No text extracted
            raise Exception("PDFæ–‡ä»¶ä¸­æ²¡æœ‰æ‰¾åˆ°å¯æå–çš„æ–‡æœ¬å†…å®¹")
            
        full_text = "\n".join(texts)
        print(f"æ–‡æœ¬æå–å®Œæˆï¼Œç¸½é•·åº¦: {len(full_text)} å­—ç¬¦")

        def normalize_digits(s: str) -> str:
            # Convert fullwidth digits to ASCII for simpler matching
            fw = "ï¼ï¼‘ï¼’ï¼“ï¼”ï¼•ï¼–ï¼—ï¼˜ï¼™"
            hw = "0123456789"
            return s.translate(str.maketrans(fw, hw))

        # Determine law name: first non-empty line containing a legal keyword, else filename
        lines = [normalize_digits((ln or "").strip()) for ln in full_text.splitlines()]
        law_name = None
        for ln in lines:
            if not ln:
                continue
            if any(key in ln for key in ["æ³•", "æ¢ä¾‹", "æ³•è¦", "æ³•å¾‹"]):
                law_name = ln
                break
        if not law_name:
            base = os.path.splitext(filename or "document")[0]
            law_name = base or "æœªå‘½åæ³•è¦"

        chapter_re = re.compile(r"^ç¬¬\s*([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒ0-9]+)\s*ç« [\u3000\s]*(.*)$")
        section_re = re.compile(r"^ç¬¬\s*([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒ0-9]+)\s*ç¯€[\u3000\s]*(.*)$")
        article_re = re.compile(r"^ç¬¬\s*([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒ0-9]+(?:ä¹‹[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å0-9]+)?)\s*æ¢[\u3000\s]*(.*)$")

        def parse_item_line(ln: str):
            # Match common item markers like ã€Œä¸€ã€ã€ã€Œ1.ã€ã€Œï¼ˆä¸€ï¼‰ã€ã€Œ(1)ã€ã€Œ1ï¼‰ã€ etc.
            ln = ln.lstrip()
            # ï¼ˆä¸€ï¼‰ or (1)
            m = re.match(r"^[ï¼ˆ(]([0-9ï¼-ï¼™ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+)[ï¼‰)]\s*(.*)$", ln)
            if m:
                return m.group(1), m.group(2), "parentheses"
            # ä¸€ã€ äºŒã€ åã€ style (Chinese numerals with punctuation)
            m = re.match(r"^([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+)[ã€ï¼.ï¼‰)]\s*(.*)$", ln)
            if m:
                return m.group(1), m.group(2), "chinese_with_punct"
            # 1. 1ã€ 1) styles (Arabic numbers with punctuation)
            m = re.match(r"^([0-9ï¼-ï¼™]+)[ã€ï¼.ï¼‰)]\s*(.*)$", ln)
            if m:
                return m.group(1), m.group(2), "arabic_with_punct"
            # 1 2 3 styles (Arabic numbers followed by space, common in ROC legal documents)
            m = re.match(r"^([0-9ï¼-ï¼™]+)\s+(.*)$", ln)
            if m:
                return m.group(1), m.group(2), "arabic_space"
            # ä¸€ äºŒ ä¸‰ styles (Chinese numerals followed by space, sub-items)
            m = re.match(r"^([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+)\s+(.*)$", ln)
            if m:
                return m.group(1), m.group(2), "chinese_space"
            return None, None, None

        structure: Dict[str, Any] = {"law_name": law_name, "chapters": []}
        current_chapter: Optional[Dict[str, Any]] = None
        current_section: Optional[Dict[str, Any]] = None
        current_article: Optional[Dict[str, Any]] = None
        current_item: Optional[Dict[str, Any]] = None
        current_sub_item: Optional[Dict[str, Any]] = None

        def ensure_chapter():
            nonlocal current_chapter
            if current_chapter is None:
                current_chapter = {"chapter": "æœªåˆ†é¡ç« ", "sections": []}
                structure["chapters"].append(current_chapter)

        def ensure_section():
            nonlocal current_section
            ensure_chapter()
            if current_section is None:
                current_section = {"section": "æœªåˆ†é¡ç¯€", "articles": []}
                current_chapter["sections"].append(current_section)

        for raw in lines:
            ln = raw.strip()
            if not ln:
                continue

            # Headings
            m = chapter_re.match(ln)
            if m:
                title = f"ç¬¬{m.group(1)}ç« " + (f" {m.group(2).strip()}" if m.group(2) else "")
                current_chapter = {"chapter": title, "sections": []}
                structure["chapters"].append(current_chapter)
                current_section = None
                current_article = None
                current_item = None
                current_sub_item = None
                continue

            m = section_re.match(ln)
            if m:
                ensure_chapter()
                title = f"ç¬¬{m.group(1)}ç¯€" + (f" {m.group(2).strip()}" if m.group(2) else "")
                current_section = {"section": title, "articles": []}
                current_chapter["sections"].append(current_section)
                current_article = None
                current_item = None
                current_sub_item = None
                current_sub_item = None
                continue

            m = article_re.match(ln)
            if m:
                ensure_section()
                title = f"ç¬¬{m.group(1)}æ¢"
                rest = m.group(2).strip() if m.group(2) else ""
                current_article = {"article": title, "content": rest, "items": []}
                current_section["articles"].append(current_article)
                current_item = None
                current_sub_item = None
                current_sub_item = None
                continue

            # Items within article
            if current_article is not None:
                num, content, item_type = parse_item_line(ln)
                if num is not None:
                    num = normalize_digits(num)
                    
                    # Determine if this is a sub-item (Chinese numerals after Arabic numerals)
                    if (item_type == "chinese_space" or item_type == "chinese_with_punct") and current_item is not None:
                        # This is a sub-item of the current item
                        if "sub_items" not in current_item:
                            current_item["sub_items"] = []
                        sub_item = {"item": str(num), "content": content or "", "sub_items": []}
                        current_item["sub_items"].append(sub_item)
                        current_sub_item = sub_item
                    else:
                        # This is a main item
                        current_item = {"item": str(num), "content": content or "", "sub_items": []}
                        current_article["items"].append(current_item)
                        current_sub_item = None
                else:
                    # continuation line
                    if current_sub_item is not None:
                        # Append to current sub-item
                        sep = "\n" if current_sub_item["content"] else ""
                        current_sub_item["content"] = f"{current_sub_item['content']}{sep}{ln}"
                    elif current_item is not None:
                        # Check if we have sub-items and append to the last sub-item
                        if "sub_items" in current_item and current_item["sub_items"]:
                            last_sub_item = current_item["sub_items"][-1]
                            sep = "\n" if last_sub_item["content"] else ""
                            last_sub_item["content"] = f"{last_sub_item['content']}{sep}{ln}"
                        else:
                            sep = "\n" if current_item["content"] else ""
                            current_item["content"] = f"{current_item['content']}{sep}{ln}"
                    else:
                        # accumulate into article content
                        if "content" not in current_article or current_article["content"] is None:
                            current_article["content"] = ln
                        else:
                            current_article["content"] = (current_article["content"] + "\n" + ln).strip()
                continue

            # If no article yet, but we have text, place it under a default article
            if current_section is not None and current_article is None:
                current_article = {"article": "æœªæ¨™ç¤ºæ¢æ–‡", "content": ln, "items": []}
                current_section["articles"].append(current_article)
                current_item = None
                current_sub_item = None
            elif current_article is None:
                ensure_section()
                current_article = {"article": "æœªæ¨™ç¤ºæ¢æ–‡", "content": ln, "items": []}
                current_section["articles"].append(current_article)
                current_item = None
                current_sub_item = None
            else:
                # fallback append
                current_article["content"] = (current_article.get("content", "") + "\n" + ln).strip()

        # å„ªåŒ–ç‰ˆæœ¬çš„metadataæ·»åŠ 
        def add_metadata_to_structure_optimized(structure, options, full_text):
            """å„ªåŒ–ç‰ˆæœ¬çš„metadataæ·»åŠ ï¼Œå¤§å¹…æå‡æ€§èƒ½"""
            print("é–‹å§‹æ·»åŠ metadata...")
            metadata_start = time.time()
            
            # é è¨ˆç®—æ‰€æœ‰æ¢æ–‡ï¼ˆé¿å…é‡è¤‡è¨ˆç®—ï¼‰
            all_articles = []
            for chapter in structure["chapters"]:
                for section in chapter["sections"]:
                    for article in section["articles"]:
                        all_articles.append({
                            "article": article["article"],
                            "content": article["content"],
                            "chapter": chapter["chapter"],
                            "section": section["section"]
                        })
            
            print(f"æ‰¾åˆ° {len(all_articles)} å€‹æ¢æ–‡")
            
            # æ‰¹é‡è™•ç†metadataï¼ˆå¦‚æœå•Ÿç”¨ï¼‰
            if options.include_id or options.include_page_range or options.include_spans:
                print("æ‰¹é‡è™•ç†metadata...")
            
            processed_count = 0
            for chapter in structure["chapters"]:
                chapter_name = chapter["chapter"]
                for section in chapter["sections"]:
                    section_name = section["section"]
                    for article in section["articles"]:
                        article_name = article["article"]
                        
                        # ç°¡åŒ–çš„metadataè™•ç†
                        article_metadata = {}
                        if options.include_id:
                            article_metadata["id"] = f"{structure['law_name']}_{chapter_name}_{section_name}_{article_name}".replace(" ", "_")
                        if options.include_page_range:
                            article_metadata["page_range"] = {"start": 1, "end": 1}  # ç°¡åŒ–çš„é é¢ç¯„åœ
                        if options.include_spans:
                            article_metadata["spans"] = {"start": 0, "end": len(article["content"])}
                        if options.include_page_range:
                            # ç°¡åŒ–çš„é ç¢¼ç¯„åœï¼ˆåŸºæ–¼æ–‡æœ¬ä½ç½®ä¼°ç®—ï¼‰
                            article_metadata["page_range"] = {"start": 1, "end": 1}  # ç°¡åŒ–ç‰ˆæœ¬
                        if options.include_spans:
                            # ç°¡åŒ–çš„æ–‡æœ¬å®šä½
                            start_pos = full_text.find(article["content"][:50])  # ä½¿ç”¨å‰50å­—ç¬¦å®šä½
                            if start_pos >= 0:
                                article_metadata["spans"] = [{
                                    "start_char": start_pos,
                                    "end_char": start_pos + len(article["content"]),
                                    "text": article["content"][:100] + "..." if len(article["content"]) > 100 else article["content"],
                                    "page": 1,
                                    "confidence": 0.8,
                                    "found": True
                                }]
                            else:
                                article_metadata["spans"] = []
                        
                        article["metadata"] = article_metadata
                        
                        # ç‚ºé …ç›®æ·»åŠ ç°¡åŒ–metadata
                        for item in article["items"]:
                            item_metadata = {}
                            if options.include_id:
                                item_metadata["id"] = f"{structure['law_name']}_{chapter_name}_{section_name}_{article_name}_{item['item']}".replace(" ", "_")
                            if options.include_page_range:
                                item_metadata["page_range"] = {"start": 1, "end": 1}  # ç°¡åŒ–çš„é é¢ç¯„åœ
                            if options.include_spans:
                                item_metadata["spans"] = {"start": 0, "end": len(item["content"])}
                            
                            item["metadata"] = item_metadata
                            
                            # ç‚ºå­é …ç›®æ·»åŠ ç°¡åŒ–metadata
                            for sub_item in item["sub_items"]:
                                sub_item_metadata = {}
                                if options.include_id:
                                    sub_item_metadata["id"] = f"{structure['law_name']}_{chapter_name}_{section_name}_{article_name}_{item['item']}_{sub_item['item']}".replace(" ", "_")
                                if options.include_page_range:
                                    sub_item_metadata["page_range"] = {"start": 1, "end": 1}  # ç°¡åŒ–çš„é é¢ç¯„åœ
                                if options.include_spans:
                                    sub_item_metadata["spans"] = {"start": 0, "end": len(sub_item["content"])}
                                
                                sub_item["metadata"] = sub_item_metadata
                        
                        processed_count += 1
                        if processed_count % 10 == 0:
                            print(f"å·²è™•ç† {processed_count} å€‹æ¢æ–‡")
            
            metadata_time = time.time() - metadata_start
            print(f"Metadataè™•ç†å®Œæˆï¼Œè€—æ™‚: {metadata_time:.2f}ç§’")
        
        # æ·»åŠ metadataï¼ˆä½¿ç”¨å„ªåŒ–ç‰ˆæœ¬ï¼‰
        if any([options.include_id, options.include_page_range, options.include_spans]):
            add_metadata_to_structure_optimized(structure, options, full_text)
        else:
            print("è·³émetadataè™•ç†ï¼ˆæœªå•Ÿç”¨ï¼‰")

        total_time = time.time() - start_time
        print(f"ç¸½è½‰æ›æ™‚é–“: {total_time:.2f}ç§’")
        
        return {
            "text": full_text,
            "metadata": structure,
            "processing_time": total_time,
            "success": True
        }
        
    except Exception as e:
        return {
            "text": "",
            "metadata": {"error": str(e)},
            "processing_time": time.time() - start_time,
            "success": False,
            "error": str(e)
        }


# ç•°æ­¥ä»»å‹™å­˜å„²
conversion_tasks = {}

# PDFç·©å­˜å­˜å„² (åŸºæ–¼æ–‡ä»¶å…§å®¹å“ˆå¸Œ)
pdf_cache = {}

# æ¸…ç†èˆŠä»»å‹™çš„å¾Œå°ä»»å‹™
async def cleanup_old_tasks():
    """æ¸…ç†è¶…é1å°æ™‚çš„èˆŠä»»å‹™"""
    while True:
        try:
            current_time = time.time()
            expired_tasks = []
            
            for task_id, task in conversion_tasks.items():
                if current_time - task["created_at"] > 3600:  # 1å°æ™‚
                    expired_tasks.append(task_id)
            
            for task_id in expired_tasks:
                del conversion_tasks[task_id]
                print(f"æ¸…ç†éæœŸä»»å‹™: {task_id}")
            
            # æ¯5åˆ†é˜æ¸…ç†ä¸€æ¬¡
            await asyncio.sleep(300)
        except Exception as e:
            print(f"æ¸…ç†ä»»å‹™æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            await asyncio.sleep(60)  # å‡ºéŒ¯æ™‚ç­‰å¾…1åˆ†é˜å†é‡è©¦

# æ¸…ç†ä»»å‹™å°‡åœ¨æ‡‰ç”¨å•Ÿå‹•æ™‚å•Ÿå‹•
@app.on_event("startup")
async def startup_event():
    """æ‡‰ç”¨å•Ÿå‹•æ™‚çš„äº‹ä»¶"""
    import asyncio
    asyncio.create_task(cleanup_old_tasks())

@app.post("/api/convert")
async def convert(file: UploadFile = File(...), metadata_options: str = Form("{}")):
    """å•Ÿå‹•PDFè½‰æ›ä»»å‹™"""
    try:
        # Parse metadata options
        try:
            metadata_config = json.loads(metadata_options)
            options = MetadataOptions(**metadata_config)
        except:
            options = MetadataOptions()  # ä½¿ç”¨é»˜èªé¸é …
        
        # Validate file type
        if not file.filename or not file.filename.lower().endswith('.pdf'):
            return JSONResponse(
                status_code=400, 
                content={"error": "åªæ”¯æŒPDFæ–‡ä»¶æ ¼å¼", "detail": "Invalid file type"}
            )
        
        # Reset file pointer to beginning
        await file.seek(0)
        
        # ç”Ÿæˆä»»å‹™ID
        task_id = f"convert_{int(time.time() * 1000)}_{hash(file.filename) % 10000}"
        
        # è®€å–æ–‡ä»¶å…§å®¹
        file_content = await file.read()
        
        # æª¢æŸ¥ç·©å­˜ï¼ˆåŸºæ–¼æ–‡ä»¶å…§å®¹å“ˆå¸Œï¼‰
        import hashlib
        file_hash = hashlib.md5(file_content).hexdigest()
        
        # æª¢æŸ¥æ˜¯å¦å·²ç·©å­˜
        cache_key = f"{file_hash}_{json.dumps(options.__dict__, sort_keys=True)}"
        if cache_key in pdf_cache:
            cached_result = pdf_cache[cache_key]
            print(f"ä½¿ç”¨ç·©å­˜çš„PDFè½‰æ›çµæœ: {file.filename}")
            
            # ç”Ÿæˆæ–°çš„doc_id
            doc_id = f"doc_{int(time.time() * 1000)}_{hash(file.filename) % 10000}"
            
            # å°‡æ–‡æª”å­˜å„²åˆ°storeä¸­
            store.docs[doc_id] = DocRecord(
                id=doc_id,
                filename=file.filename,
                text=cached_result["text"],
                json_data=cached_result["metadata"],
                chunks=[],
                chunk_size=0,
                overlap=0,
            )
            
            return {
                "doc_id": doc_id,
                "filename": file.filename,
                "text_length": cached_result["text_length"],
                "metadata": cached_result["metadata"],
                "processing_time": 0.1,  # ç·©å­˜å‘½ä¸­ï¼Œå¹¾ä¹ç¬é–“å®Œæˆ
                "cached": True
            }
        
        # å‰µå»ºä»»å‹™
        conversion_tasks[task_id] = {
            "status": "pending",
            "progress": 0,
            "filename": file.filename,
            "created_at": time.time(),
            "result": None,
            "error": None,
            "file_hash": file_hash,
            "cache_key": cache_key
        }
        
        # å•Ÿå‹•å¾Œå°ä»»å‹™
        import asyncio
        asyncio.create_task(process_pdf_conversion(task_id, file_content, options))
        
        return {
            "task_id": task_id,
            "status": "pending",
            "message": "PDFè½‰æ›ä»»å‹™å·²å•Ÿå‹•ï¼Œè«‹ä½¿ç”¨task_idæŸ¥è©¢é€²åº¦"
        }
        
    except Exception as e:
        print(f"Convert endpoint error: {e}")
        return JSONResponse(
            status_code=500, 
            content={"error": "å•Ÿå‹•PDFè½‰æ›ä»»å‹™å¤±æ•—", "detail": str(e)}
        )


async def process_pdf_conversion(task_id: str, file_content: bytes, options: MetadataOptions):
    """å¾Œå°è™•ç†PDFè½‰æ›"""
    import time
    start_time = time.time()
    
    try:
        # æ›´æ–°ä»»å‹™ç‹€æ…‹
        conversion_tasks[task_id]["status"] = "processing"
        conversion_tasks[task_id]["progress"] = 10
        
        print(f"é–‹å§‹è½‰æ›PDF: {conversion_tasks[task_id]['filename']}")
        
        # ç›´æ¥èª¿ç”¨convert_pdf_structuredå‡½æ•¸
        conversion_tasks[task_id]["progress"] = 20
        result = convert_pdf_structured(file_content, conversion_tasks[task_id]['filename'], options)
        
        if not result["success"]:
            conversion_tasks[task_id]["status"] = "failed"
            conversion_tasks[task_id]["error"] = result.get("error", "PDFè½‰æ›å¤±æ•—")
            return
        
        # æå–çµæœ
        full_text = result["text"]
        structure = result["metadata"]
        total_time = result["processing_time"]
        
        conversion_tasks[task_id]["progress"] = 80
        print(f"PDFè½‰æ›å®Œæˆï¼Œæ–‡æœ¬é•·åº¦: {len(full_text)} å­—ç¬¦")
        
        # ç”Ÿæˆæ–‡æª”ID
        doc_id = f"doc_{int(time.time() * 1000)}_{hash(conversion_tasks[task_id]['filename']) % 10000}"
        
        # å°‡æ–‡æª”å­˜å„²åˆ°storeä¸­
        store.docs[doc_id] = DocRecord(
            id=doc_id,
            filename=conversion_tasks[task_id]['filename'],
            text=full_text,
            json_data=structure,
            chunks=[],
            chunk_size=0,
            overlap=0,
        )
        
        # é‡ç½®åµŒå…¥ç‹€æ…‹
        store.reset_embeddings()
        
        # ä¿å­˜åˆ°ç·©å­˜
        cache_data = {
            "text": full_text,
            "metadata": structure,
            "text_length": len(full_text),
            "processing_time": total_time
        }
        pdf_cache[conversion_tasks[task_id]["cache_key"]] = cache_data
        
        # é™åˆ¶ç·©å­˜å¤§å°ï¼ˆæœ€å¤šä¿å­˜100å€‹è½‰æ›çµæœï¼‰
        if len(pdf_cache) > 100:
            # åˆªé™¤æœ€èˆŠçš„ç·©å­˜é …
            oldest_key = next(iter(pdf_cache))
            del pdf_cache[oldest_key]
        
        # æ›´æ–°ä»»å‹™ç‹€æ…‹ç‚ºå®Œæˆ
        conversion_tasks[task_id]["status"] = "completed"
        conversion_tasks[task_id]["progress"] = 100
        conversion_tasks[task_id]["result"] = {
            "doc_id": doc_id,
            "filename": conversion_tasks[task_id]['filename'],
            "text_length": len(full_text),
            "metadata": structure,
            "processing_time": total_time
        }
        
    except Exception as e:
        # æ›´æ–°ä»»å‹™ç‹€æ…‹ç‚ºå¤±æ•—
        conversion_tasks[task_id]["status"] = "failed"
        conversion_tasks[task_id]["error"] = str(e)
        print(f"PDFè½‰æ›å¤±æ•—: {str(e)}")


@app.get("/api/convert/status/{task_id}")
async def get_convert_status(task_id: str):
    """æŸ¥è©¢PDFè½‰æ›ä»»å‹™ç‹€æ…‹"""
    if task_id not in conversion_tasks:
        raise HTTPException(status_code=404, detail="ä»»å‹™ä¸å­˜åœ¨")
    
    task = conversion_tasks[task_id]
    
    # æ¸…ç†è¶…é1å°æ™‚çš„èˆŠä»»å‹™
    if time.time() - task["created_at"] > 3600:
        del conversion_tasks[task_id]
        raise HTTPException(status_code=404, detail="ä»»å‹™å·²éæœŸ")
    
    return {
        "task_id": task_id,
        "status": task["status"],
        "progress": task["progress"],
        "filename": task["filename"],
        "result": task.get("result"),
        "error": task.get("error")
    }


def run_evaluation_task(task_id: str):
    """
    åœ¨å¾Œå°é‹è¡Œè©•æ¸¬ä»»å‹™
    """
    task = eval_store.get_task(task_id)
    if not task:
        return
    
    try:
        eval_store.update_task_status(task_id, "running")
        
        doc = store.docs.get(task.doc_id)
        if not doc:
            eval_store.update_task_status(task_id, "failed", error_message="Document not found")
            return
        
        results = []
        total_configs = len(task.configs)
        
        for i, config in enumerate(task.configs):
            result = evaluate_chunk_config(doc, config, task.test_queries, task.k_values, task.strategy)
            results.append(result)
            
            # æ›´æ–°é€²åº¦
            progress = (i + 1) / total_configs
            eval_store.update_task_status(task_id, "running", progress=progress)
        
        eval_store.update_task_status(task_id, "completed", results=results)
        
    except Exception as e:
        eval_store.update_task_status(task_id, "failed", error_message=str(e))


@app.post("/api/evaluate/fixed-size")
def start_fixed_size_evaluation(req: FixedSizeEvaluationRequest, background_tasks: BackgroundTasks):
    """
    é–‹å§‹å›ºå®šå¤§å°åˆ†å‰²ç­–ç•¥è©•æ¸¬
    """
    doc = store.docs.get(req.doc_id)
    if not doc:
        return JSONResponse(status_code=404, content={"error": "Document not found"})
    
    # æª¢æŸ¥æ˜¯å¦å·²æœ‰ç”Ÿæˆçš„å•é¡Œ
    if not hasattr(doc, 'generated_questions') or not doc.generated_questions:
        return JSONResponse(
            status_code=400, 
            content={"error": "è«‹å…ˆä½¿ç”¨ã€Œç”Ÿæˆå•é¡Œã€åŠŸèƒ½ç‚ºæ–‡æª”ç”Ÿæˆæ¸¬è©¦å•é¡Œï¼Œç„¶å¾Œå†é€²è¡Œè©•æ¸¬"}
        )
    
    # ä½¿ç”¨æ–‡æª”ä¸­å­˜å„²çš„å•é¡Œè€Œä¸æ˜¯é è¨­å•é¡Œ
    req.test_queries = doc.generated_questions
    
    # ç”Ÿæˆæ‰€æœ‰é…ç½®çµ„åˆï¼ŒåŒ…æ‹¬ç­–ç•¥ç‰¹å®šåƒæ•¸
    configs = []
    for chunk_size in req.chunk_sizes:
        for overlap_ratio in req.overlap_ratios:
            overlap = int(chunk_size * overlap_ratio)
            
            # æ ¹æ“šç­–ç•¥ç”Ÿæˆä¸åŒçš„åƒæ•¸çµ„åˆ
            if req.strategy == "structured_hierarchical":
                for chunk_by in req.chunk_by_options:
                    config = ChunkConfig(
                        chunk_size=chunk_size,
                        overlap=overlap,
                        overlap_ratio=overlap_ratio,
                        chunk_by=chunk_by
                    )
                    configs.append(config)
            elif req.strategy == "rcts_hierarchical":
                for preserve_structure in req.preserve_structure_options:
                    config = ChunkConfig(
                        chunk_size=chunk_size,
                        overlap=overlap,
                        overlap_ratio=overlap_ratio,
                        preserve_structure=preserve_structure,
                        chunk_by="article"  # é»˜èªå€¼
                    )
                    configs.append(config)
            elif req.strategy == "hierarchical":
                for level_depth in req.level_depth_options:
                    for min_chunk_size in req.min_chunk_size_options:
                        config = ChunkConfig(
                            chunk_size=chunk_size,
                            overlap=overlap,
                            overlap_ratio=overlap_ratio,
                            level_depth=level_depth,
                            min_chunk_size=min_chunk_size,
                            chunk_by="article"  # é»˜èªå€¼
                        )
                        configs.append(config)
            elif req.strategy == "semantic":
                for similarity_threshold in req.similarity_threshold_options:
                    for context_window in req.context_window_options:
                        config = ChunkConfig(
                            chunk_size=chunk_size,
                            overlap=overlap,
                            overlap_ratio=overlap_ratio,
                            similarity_threshold=similarity_threshold,
                            context_window=context_window,
                            chunk_by="article"  # é»˜èªå€¼
                        )
                        configs.append(config)
            elif req.strategy == "llm_semantic":
                for semantic_threshold in req.semantic_threshold_options:
                    for context_window in req.context_window_options:
                        config = ChunkConfig(
                            chunk_size=chunk_size,
                            overlap=overlap,
                            overlap_ratio=overlap_ratio,
                            semantic_threshold=semantic_threshold,
                            context_window=context_window,
                            chunk_by="article"  # é»˜èªå€¼
                        )
                        configs.append(config)
            elif req.strategy == "sliding_window":
                for window_size in req.window_size_options:
                    for step_size in req.step_size_options:
                        for boundary_aware in req.boundary_aware_options:
                            for preserve_sentences in req.preserve_sentences_options:
                                for min_chunk_size_sw in req.min_chunk_size_options_sw:
                                    for max_chunk_size_sw in req.max_chunk_size_options_sw:
                                        config = ChunkConfig(
                                            chunk_size=window_size,  # ä½¿ç”¨window_sizeä½œç‚ºchunk_size
                                            overlap=overlap,
                                            overlap_ratio=overlap_ratio,
                                            strategy="sliding_window",
                                            step_size=step_size,
                                            window_size=window_size,
                                            boundary_aware=boundary_aware,
                                            preserve_sentences=preserve_sentences,
                                            min_chunk_size_sw=min_chunk_size_sw,
                                            max_chunk_size_sw=max_chunk_size_sw,
                                            chunk_by="article"  # é»˜èªå€¼
                                        )
                                        configs.append(config)
            elif req.strategy == "hybrid":
                for switch_threshold in req.switch_threshold_options:
                    for secondary_size in req.secondary_size_options:
                        config = ChunkConfig(
                            chunk_size=chunk_size,
                            overlap=overlap,
                            overlap_ratio=overlap_ratio,
                            switch_threshold=switch_threshold,
                            secondary_size=secondary_size,
                            chunk_by="article"  # é»˜èªå€¼
                        )
                        configs.append(config)
            else:
                # é»˜èªé…ç½®ï¼ˆfixed_sizeç­‰ï¼‰
                config = ChunkConfig(
                    chunk_size=chunk_size,
                    overlap=overlap,
                    overlap_ratio=overlap_ratio,
                    chunk_by="article"  # é»˜èªå€¼
                )
                configs.append(config)
    
    # ç²å–åˆ†å‰²ç­–ç•¥ï¼ˆå¾è«‹æ±‚ä¸­ç²å–ï¼Œé»˜èªç‚ºfixed_sizeï¼‰
    strategy = getattr(req, 'strategy', 'fixed_size')
    
    # å‰µå»ºè©•æ¸¬ä»»å‹™
    task_id = eval_store.create_task(
        doc_id=req.doc_id,
        configs=configs,
        test_queries=req.test_queries,
        k_values=req.k_values,
        strategy=strategy
    )
    
    # åœ¨å¾Œå°é‹è¡Œè©•æ¸¬
    background_tasks.add_task(run_evaluation_task, task_id)
    
    return {
        "task_id": task_id,
        "status": "started",
        "total_configs": len(configs),
        "message": "è©•æ¸¬ä»»å‹™å·²é–‹å§‹ï¼Œè«‹ä½¿ç”¨task_idæŸ¥è©¢é€²åº¦"
    }


@app.get("/api/evaluate/status/{task_id}")
def get_evaluation_status(task_id: str):
    """
    ç²å–è©•æ¸¬ä»»å‹™ç‹€æ…‹
    """
    task = eval_store.get_task(task_id)
    if not task:
        return JSONResponse(status_code=404, content={"error": "Task not found"})
    
    total_configs = len(task.configs)
    completed_configs = int(task.progress * total_configs) if task.progress > 0 else 0
    
    return {
        "task_id": task_id,
        "status": task.status,
        "created_at": task.created_at.isoformat(),
        "completed_at": task.completed_at.isoformat() if task.completed_at else None,
        "error_message": task.error_message,
        "total_configs": total_configs,
        "completed_configs": completed_configs,
        "progress": task.progress
    }


@app.get("/api/evaluate/results/{task_id}")
def get_evaluation_results(task_id: str):
    """
    ç²å–è©•æ¸¬çµæœ
    """
    task = eval_store.get_task(task_id)
    if not task:
        return JSONResponse(status_code=404, content={"error": "Task not found"})
    
    if task.status != "completed":
        return JSONResponse(status_code=400, content={"error": "Task not completed yet"})
    
    # è½‰æ›çµæœç‚ºå¯åºåˆ—åŒ–çš„æ ¼å¼
    results = []
    for result in task.results:
        result_dict = {
            "config": result.config,  # ç¾åœ¨ config å·²ç¶“æ˜¯å­—å…¸äº†
            "metrics": {
                "precision_omega": result.metrics.precision_omega,
                "precision_at_k": result.metrics.precision_at_k,
                "recall_at_k": result.metrics.recall_at_k,
                "chunk_count": result.metrics.chunk_count,
                "avg_chunk_length": result.metrics.avg_chunk_length,
                "length_variance": result.metrics.length_variance
            },
            "test_queries": result.test_queries,
            "retrieval_results": result.retrieval_results,
            "timestamp": result.timestamp.isoformat()
        }
        results.append(result_dict)
    
    return {
        "task_id": task_id,
        "status": task.status,
        "results": results,
        "summary": {
            "total_configs": len(results),
            "best_precision_omega": max(r["metrics"]["precision_omega"] for r in results),
            "best_precision_at_5": max(r["metrics"]["precision_at_k"].get(5, 0) for r in results),
            "best_recall_at_5": max(r["metrics"]["recall_at_k"].get(5, 0) for r in results),
            "avg_chunk_count": sum(r["metrics"]["chunk_count"] for r in results) / len(results),
            "avg_chunk_length": sum(r["metrics"]["avg_chunk_length"] for r in results) / len(results)
        }
    }


@app.get("/api/evaluate/comparison/{task_id}")
def get_evaluation_comparison(task_id: str):
    """
    ç²å–è©•æ¸¬çµæœå°æ¯”åˆ†æ
    """
    task = eval_store.get_task(task_id)
    if not task:
        return JSONResponse(status_code=404, content={"error": "Task not found"})
    
    if task.status != "completed":
        return JSONResponse(status_code=400, content={"error": "Task not completed yet"})
    
    # ç”Ÿæˆå°æ¯”åˆ†æ
    comparison = {
        "chunk_size_analysis": {},
        "overlap_analysis": {},
        "strategy_specific_analysis": {},
        "recommendations": []
    }
    
    # æŒ‰chunk sizeåˆ†çµ„åˆ†æ
    chunk_size_groups = {}
    for result in task.results:
        size = result.config["chunk_size"]
        if size not in chunk_size_groups:
            chunk_size_groups[size] = []
        chunk_size_groups[size].append(result)
    
    for size, results in chunk_size_groups.items():
        avg_metrics = {
            "precision_omega": sum(r.metrics.precision_omega for r in results) / len(results),
            "precision_at_k": {
                "1": sum(r.metrics.precision_at_k.get(1, 0) for r in results) / len(results),
                "3": sum(r.metrics.precision_at_k.get(3, 0) for r in results) / len(results),
                "5": sum(r.metrics.precision_at_k.get(5, 0) for r in results) / len(results),
                "10": sum(r.metrics.precision_at_k.get(10, 0) for r in results) / len(results)
            },
            "recall_at_k": {
                "1": sum(r.metrics.recall_at_k.get(1, 0) for r in results) / len(results),
                "3": sum(r.metrics.recall_at_k.get(3, 0) for r in results) / len(results),
                "5": sum(r.metrics.recall_at_k.get(5, 0) for r in results) / len(results),
                "10": sum(r.metrics.recall_at_k.get(10, 0) for r in results) / len(results)
            },
            "avg_chunk_count": sum(r.metrics.chunk_count for r in results) / len(results),
            "avg_chunk_length": sum(r.metrics.avg_chunk_length for r in results) / len(results),
            "length_variance": sum(r.metrics.length_variance for r in results) / len(results)
        }
        comparison["chunk_size_analysis"][size] = avg_metrics
    
    # æŒ‰overlap ratioåˆ†çµ„åˆ†æ
    overlap_groups = {}
    for result in task.results:
        ratio = result.config["overlap_ratio"]
        if ratio not in overlap_groups:
            overlap_groups[ratio] = []
        overlap_groups[ratio].append(result)
    
    for ratio, results in overlap_groups.items():
        avg_metrics = {
            "precision_omega": sum(r.metrics.precision_omega for r in results) / len(results),
            "precision_at_k": {
                "1": sum(r.metrics.precision_at_k.get(1, 0) for r in results) / len(results),
                "3": sum(r.metrics.precision_at_k.get(3, 0) for r in results) / len(results),
                "5": sum(r.metrics.precision_at_k.get(5, 0) for r in results) / len(results),
                "10": sum(r.metrics.precision_at_k.get(10, 0) for r in results) / len(results)
            },
            "recall_at_k": {
                "1": sum(r.metrics.recall_at_k.get(1, 0) for r in results) / len(results),
                "3": sum(r.metrics.recall_at_k.get(3, 0) for r in results) / len(results),
                "5": sum(r.metrics.recall_at_k.get(5, 0) for r in results) / len(results),
                "10": sum(r.metrics.recall_at_k.get(10, 0) for r in results) / len(results)
            },
            "avg_chunk_count": sum(r.metrics.chunk_count for r in results) / len(results),
            "avg_chunk_length": sum(r.metrics.avg_chunk_length for r in results) / len(results),
            "length_variance": sum(r.metrics.length_variance for r in results) / len(results)
        }
        comparison["overlap_analysis"][ratio] = avg_metrics
    
    # æŒ‰ç­–ç•¥ç‰¹å®šåƒæ•¸åˆ†çµ„åˆ†æ
    if task.results:
        strategy = task.results[0].config.get("strategy", "fixed_size")
        
        if strategy == "structured_hierarchical":
            # æŒ‰åˆ†å‰²å–®ä½åˆ†çµ„
            chunk_by_groups = {}
            for result in task.results:
                chunk_by = result.config.get("chunk_by", "article")
                if chunk_by not in chunk_by_groups:
                    chunk_by_groups[chunk_by] = []
                chunk_by_groups[chunk_by].append(result)
            
            for chunk_by, results in chunk_by_groups.items():
                avg_metrics = {
                    "precision_omega": sum(r.metrics.precision_omega for r in results) / len(results),
                    "precision_at_k": {
                        "1": sum(r.metrics.precision_at_k.get(1, 0) for r in results) / len(results),
                        "3": sum(r.metrics.precision_at_k.get(3, 0) for r in results) / len(results),
                        "5": sum(r.metrics.precision_at_k.get(5, 0) for r in results) / len(results),
                        "10": sum(r.metrics.precision_at_k.get(10, 0) for r in results) / len(results)
                    },
                    "recall_at_k": {
                        "1": sum(r.metrics.recall_at_k.get(1, 0) for r in results) / len(results),
                        "3": sum(r.metrics.recall_at_k.get(3, 0) for r in results) / len(results),
                        "5": sum(r.metrics.recall_at_k.get(5, 0) for r in results) / len(results),
                        "10": sum(r.metrics.recall_at_k.get(10, 0) for r in results) / len(results)
                    },
                    "avg_chunk_count": sum(r.metrics.chunk_count for r in results) / len(results),
                    "avg_chunk_length": sum(r.metrics.avg_chunk_length for r in results) / len(results),
                    "length_variance": sum(r.metrics.length_variance for r in results) / len(results)
                }
                comparison["strategy_specific_analysis"][f"chunk_by_{chunk_by}"] = avg_metrics
        
        elif strategy == "rcts_hierarchical":
            # æŒ‰ä¿æŒçµæ§‹åˆ†çµ„
            preserve_groups = {}
            for result in task.results:
                preserve = result.config.get("preserve_structure", True)
                key = "preserve_structure" if preserve else "no_preserve_structure"
                if key not in preserve_groups:
                    preserve_groups[key] = []
                preserve_groups[key].append(result)
            
            for key, results in preserve_groups.items():
                avg_metrics = {
                    "precision_omega": sum(r.metrics.precision_omega for r in results) / len(results),
                    "precision_at_k": {
                        "1": sum(r.metrics.precision_at_k.get(1, 0) for r in results) / len(results),
                        "3": sum(r.metrics.precision_at_k.get(3, 0) for r in results) / len(results),
                        "5": sum(r.metrics.precision_at_k.get(5, 0) for r in results) / len(results),
                        "10": sum(r.metrics.precision_at_k.get(10, 0) for r in results) / len(results)
                    },
                    "recall_at_k": {
                        "1": sum(r.metrics.recall_at_k.get(1, 0) for r in results) / len(results),
                        "3": sum(r.metrics.recall_at_k.get(3, 0) for r in results) / len(results),
                        "5": sum(r.metrics.recall_at_k.get(5, 0) for r in results) / len(results),
                        "10": sum(r.metrics.recall_at_k.get(10, 0) for r in results) / len(results)
                    },
                    "avg_chunk_count": sum(r.metrics.chunk_count for r in results) / len(results),
                    "avg_chunk_length": sum(r.metrics.avg_chunk_length for r in results) / len(results),
                    "length_variance": sum(r.metrics.length_variance for r in results) / len(results)
                }
                comparison["strategy_specific_analysis"][key] = avg_metrics
        
        elif strategy == "hierarchical":
            # æŒ‰å±¤æ¬¡æ·±åº¦åˆ†çµ„
            level_groups = {}
            for result in task.results:
                level = result.config.get("level_depth", 3)
                if level not in level_groups:
                    level_groups[level] = []
                level_groups[level].append(result)
            
            for level, results in level_groups.items():
                avg_metrics = {
                    "precision_omega": sum(r.metrics.precision_omega for r in results) / len(results),
                    "precision_at_k": {
                        "1": sum(r.metrics.precision_at_k.get(1, 0) for r in results) / len(results),
                        "3": sum(r.metrics.precision_at_k.get(3, 0) for r in results) / len(results),
                        "5": sum(r.metrics.precision_at_k.get(5, 0) for r in results) / len(results),
                        "10": sum(r.metrics.precision_at_k.get(10, 0) for r in results) / len(results)
                    },
                    "recall_at_k": {
                        "1": sum(r.metrics.recall_at_k.get(1, 0) for r in results) / len(results),
                        "3": sum(r.metrics.recall_at_k.get(3, 0) for r in results) / len(results),
                        "5": sum(r.metrics.recall_at_k.get(5, 0) for r in results) / len(results),
                        "10": sum(r.metrics.recall_at_k.get(10, 0) for r in results) / len(results)
                    },
                    "avg_chunk_count": sum(r.metrics.chunk_count for r in results) / len(results),
                    "avg_chunk_length": sum(r.metrics.avg_chunk_length for r in results) / len(results),
                    "length_variance": sum(r.metrics.length_variance for r in results) / len(results)
                }
                comparison["strategy_specific_analysis"][f"level_depth_{level}"] = avg_metrics
    
    # ç”Ÿæˆæ¨è–¦
    best_overall = max(task.results, key=lambda r: (
        r.metrics.precision_omega * 0.4 + 
        r.metrics.precision_at_k.get(5, 0) * 0.3 + 
        r.metrics.recall_at_k.get(5, 0) * 0.3
    ))
    
    # ç”Ÿæˆè©³ç´°çš„æ¨è–¦é…ç½®
    config_parts = []
    config_parts.append(f"chunk_size={best_overall.config['chunk_size']}")
    config_parts.append(f"overlap_ratio={best_overall.config['overlap_ratio']}")
    
    # æ·»åŠ ç­–ç•¥ç‰¹å®šåƒæ•¸
    strategy = best_overall.config.get("strategy", "fixed_size")
    if strategy == "structured_hierarchical":
        chunk_by = best_overall.config.get("chunk_by", "article")
        chunk_by_label = {"article": "æŒ‰æ¢æ–‡åˆ†å‰²", "item": "æŒ‰é …åˆ†å‰²", "section": "æŒ‰ç¯€åˆ†å‰²", "chapter": "æŒ‰ç« åˆ†å‰²"}.get(chunk_by, chunk_by)
        config_parts.append(f"chunk_by={chunk_by}({chunk_by_label})")
    elif strategy == "rcts_hierarchical":
        preserve = best_overall.config.get("preserve_structure", True)
        config_parts.append(f"preserve_structure={preserve}({'ä¿æŒçµæ§‹' if preserve else 'ä¸ä¿æŒçµæ§‹'})")
    elif strategy == "hierarchical":
        level = best_overall.config.get("level_depth", 3)
        min_size = best_overall.config.get("min_chunk_size", 200)
        config_parts.append(f"level_depth={level}")
        config_parts.append(f"min_chunk_size={min_size}")
    elif strategy == "semantic":
        threshold = best_overall.config.get("similarity_threshold", 0.6)
        context = best_overall.config.get("context_window", 100)
        config_parts.append(f"similarity_threshold={threshold}")
        config_parts.append(f"context_window={context}")
    elif strategy == "llm_semantic":
        threshold = best_overall.config.get("semantic_threshold", 0.7)
        context = best_overall.config.get("context_window", 100)
        config_parts.append(f"semantic_threshold={threshold}")
        config_parts.append(f"context_window={context}")
    elif strategy == "sliding_window":
        step = best_overall.config.get("step_size", 250)
        config_parts.append(f"step_size={step}")
    elif strategy == "hybrid":
        switch = best_overall.config.get("switch_threshold", 0.5)
        secondary = best_overall.config.get("secondary_size", 400)
        config_parts.append(f"switch_threshold={switch}")
        config_parts.append(f"secondary_size={secondary}")
    
    comparison["recommendations"] = [
        f"æœ€ä½³é…ç½®ï¼š{', '.join(config_parts)}",
        f"è©²é…ç½®çš„precision omega: {best_overall.metrics.precision_omega:.3f}",
        f"è©²é…ç½®çš„precision@5: {best_overall.metrics.precision_at_k.get(5, 0):.3f}",
        f"è©²é…ç½®çš„recall@5: {best_overall.metrics.recall_at_k.get(5, 0):.3f}",
        f"è©²é…ç½®çš„chunk count: {best_overall.metrics.chunk_count}",
        f"è©²é…ç½®çš„å¹³å‡chunké•·åº¦: {best_overall.metrics.avg_chunk_length:.1f}"
    ]
    
    return comparison


@app.post("/api/generate-questions")
def generate_questions(req: GenerateQuestionsRequest):
    """
    ç”Ÿæˆç¹é«”ä¸­æ–‡æ³•å¾‹è€ƒå¤é¡Œå¾æ³•å¾‹æ–‡æœ¬ä¸­ç”Ÿæˆå•é¡Œ
    """
    doc = store.docs.get(req.doc_id)
    if not doc:
        return JSONResponse(status_code=404, content={"error": "Document not found"})
    
    start_time = time.time()
    
    try:
        # ä½¿ç”¨Geminiç”Ÿæˆå•é¡Œ
        questions = generate_questions_with_gemini(
            doc.text, 
            req.num_questions, 
            req.question_types, 
            req.difficulty_levels
        )
        
        generation_time = time.time() - start_time
        
        # å°‡ç”Ÿæˆçš„å•é¡Œå­˜å„²åˆ°æ–‡æª”è¨˜éŒ„ä¸­
        question_texts = [q.question for q in questions]
        doc.generated_questions = question_texts
        store.docs[req.doc_id] = doc  # æ›´æ–°æ–‡æª”è¨˜éŒ„
        
        # æª¢æŸ¥æ˜¯å¦ç”Ÿæˆäº†å•é¡Œ
        if not questions:
            print("è­¦å‘Šï¼šæ²’æœ‰ç”Ÿæˆä»»ä½•å•é¡Œ")
            return JSONResponse(
                status_code=400,
                content={"error": "ç„¡æ³•å¾æ–‡æª”ä¸­ç”Ÿæˆå•é¡Œï¼Œè«‹æª¢æŸ¥æ–‡æª”å…§å®¹æ˜¯å¦åŒ…å«æ³•å¾‹æ¢æ–‡"}
            )
        
        result = QuestionGenerationResult(
            doc_id=req.doc_id,
            total_questions=len(questions),
            questions=questions,
            generation_time=generation_time,
            timestamp=datetime.now()
        )
        
        response_data = {
            "success": True,
            "result": {
                "doc_id": result.doc_id,
                "total_questions": result.total_questions,
                "generation_time": result.generation_time,
                "timestamp": result.timestamp.isoformat(),
                "questions": [
                    {
                        "question": q.question,
                        "references": q.references,
                        "question_type": q.question_type,
                        "difficulty": q.difficulty,
                        "keywords": q.keywords,
                        "estimated_tokens": q.estimated_tokens
                    }
                    for q in result.questions
                ]
            }
        }
        
        print(f"è¿”å›éŸ¿æ‡‰æ•¸æ“š: success={response_data['success']}, questions_count={len(response_data['result']['questions'])}")
        return response_data
        
    except Exception as e:
        print(f"å•é¡Œç”Ÿæˆç•°å¸¸: {str(e)}")  # æ·»åŠ æ—¥èªŒ
        return JSONResponse(
            status_code=500, 
            content={"error": f"å•é¡Œç”Ÿæˆå¤±æ•—: {str(e)}"}
        )


@app.get("/docs/schema")
def schema():
    # Minimal shape for frontend wiring/testing
    return {
        "upload": {"POST": {"multipart": True}},
        "chunk": {"POST": {"json": {"doc_id": "str", "chunk_size": "int", "overlap": "int"}}},
        "embed": {"POST": {"json": {"doc_ids": "List[str]|None"}}},
        "retrieve": {"POST": {"json": {"query": "str", "k": "int"}}},
        "generate": {"POST": {"json": {"query": "str", "top_k": "int"}}},
        "evaluate/fixed-size": {"POST": {"json": "FixedSizeEvaluationRequest"}},
        "evaluate/status/{task_id}": {"GET": {}},
        "evaluate/results/{task_id}": {"GET": {}},
        "evaluate/comparison/{task_id}": {"GET": {}},
        "generate-questions": {"POST": {"json": "GenerateQuestionsRequest"}},
    }
