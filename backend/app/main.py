from __future__ import annotations

import io
import os
import uuid
from dataclasses import dataclass
import re
from typing import List, Optional, Dict, Any, Tuple
import json
from datetime import datetime
import asyncio
import time
from concurrent.futures import ThreadPoolExecutor

from fastapi import FastAPI, UploadFile, File, Form, HTTPException, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import JSONResponse
from pydantic import BaseModel
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

from .models import DocRecord, ChunkConfig, MetadataOptions, MultiLevelFusionRequest, ECUAnnotation, GranularityComparisonRequest, AnnotationBatchRequest
from .hybrid_search import hybrid_rank, HybridConfig
from .store import InMemoryStore
from .faiss_store import FAISSVectorStore
from .bm25_index import BM25KeywordIndex
from .metadata_enhancer import MetadataEnhancer
from .enhanced_hybrid_rag import EnhancedHybridRAG, EnhancedHybridConfig
from .query_classifier import query_classifier, get_query_analysis
from .result_fusion import MultiLevelResultFusion, FusionConfig, fuse_multi_level_results
try:
    from rank_bm25 import BM25Okapi  # type: ignore
    BM25_AVAILABLE = True
except ImportError:
    BM25Okapi = None  # type: ignore
    BM25_AVAILABLE = False
from dotenv import load_dotenv
try:
    from PyPDF2 import PdfReader
    PYPDF2_AVAILABLE = True
except ImportError:
    PdfReader = None
    PYPDF2_AVAILABLE = False
import pdfplumber
try:
    import jieba  # type: ignore
    import jieba.analyse  # type: ignore
    jieba.initialize()
except ImportError:
    jieba = None  # type: ignore

try:
    import google.generativeai as genai  # type: ignore
    GEMINI_AVAILABLE = True
except ImportError:  # pragma: no cover - optional dependency
    genai = None  # type: ignore
    GEMINI_AVAILABLE = False

try:
    from sentence_transformers import SentenceTransformer  # type: ignore
    SENTENCE_TRANSFORMERS_AVAILABLE = True
except ImportError:  # pragma: no cover - optional dependency
    SentenceTransformer = None  # type: ignore
    SENTENCE_TRANSFORMERS_AVAILABLE = False

# Embedding ç¶­åº¦é…ç½®
# Gemini: æ”¯æ´ 128-3072ï¼Œå»ºè­° 768/1536/3072
# BGE-M3: å›ºå®š 1024 æˆ– 3072ï¼ˆå–æ±ºæ–¼é…ç½®ï¼‰
EMBEDDING_DIMENSION = 3072  # ğŸ¯ çµ±ä¸€é…ç½®ï¼šæ”¹é€™è£¡å°±èƒ½æ”¹å…¨éƒ¨

load_dotenv()


def get_env_bool(name: str, default: bool = False) -> bool:
    v = os.getenv(name)
    if v is None:
        return default
    return v.lower() in {"1", "true", "yes", "on"}


GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY") or "AIzaSyC3hF9d-BWVQRjTd_uzo4grF9upIDsZhEI"
USE_GEMINI_EMBEDDING = True  # âœ… ä½¿ç”¨ Gemini Embeddingï¼ˆå·²å„ªåŒ–é€Ÿç‡é™åˆ¶ï¼‰
USE_GEMINI_COMPLETION = True  # LLMæ¨ç†ä½¿ç”¨Gemini
USE_BGE_M3_EMBEDDING = False  # âŒ BGE-M3åœ¨Macä¸Šå¤ªæ…¢ï¼Œå·²ç¦ç”¨

# èª¿è©¦ä¿¡æ¯
print(f"ğŸ”§ Embedding é…ç½®:")
print(f"   USE_GEMINI_EMBEDDING: {USE_GEMINI_EMBEDDING}")
print(f"   GOOGLE_API_KEY: {'å·²è¨­ç½®' if GOOGLE_API_KEY else 'æœªè¨­ç½®'}")
print(f"   GEMINI_API_KEY: {'å·²è¨­ç½®' if os.getenv('GEMINI_API_KEY') else 'æœªè¨­ç½®'}")
print(f"   USE_BGE_M3_EMBEDDING: {USE_BGE_M3_EMBEDDING}")
print(f"   GOOGLE_EMBEDDING_MODEL: {os.getenv('GOOGLE_EMBEDDING_MODEL', 'gemini-embedding-001')}")
print(f"   USE_GEMINI_COMPLETION: {USE_GEMINI_COMPLETION}")

try:
    import httpx
except Exception:  # pragma: no cover
    httpx = None


# DocRecord å·²å¾ models å°å…¥








from .store import InMemoryStore
store = InMemoryStore()

# æ–°å¢ï¼šFAISSå‘é‡å­˜å„²
faiss_store = FAISSVectorStore()

# æ–°å¢ï¼šBM25é—œéµå­—ç´¢å¼•
bm25_index = BM25KeywordIndex()

# æ–°å¢ï¼šMetadataå¢å¼·å™¨
metadata_enhancer = MetadataEnhancer()

# æ–°å¢ï¼šå¢å¼·ç‰ˆHybridRAG
enhanced_hybrid_rag = EnhancedHybridRAG(faiss_store, bm25_index, metadata_enhancer)

# åˆå§‹åŒ–æ™‚è¼‰å…¥å·²ä¿å­˜çš„æ•¸æ“š
try:
    faiss_store.load_data()
    bm25_index.load_data()
    print("âœ… å·²è¼‰å…¥FAISSå’ŒBM25æ•¸æ“š")
except Exception as e:
    print(f"âš ï¸ è¼‰å…¥FAISSå’ŒBM25æ•¸æ“šå¤±æ•—: {e}")





app = FastAPI(title="RAG Visualizer API", version="0.1.0")

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # During dev; restrict in prod
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# æš«æ™‚åœç”¨routes.pyçš„åŒ…å«ï¼Œé¿å…å¾ªç’°å°å…¥å•é¡Œ
from .routes import router
app.include_router(router, prefix="/api")


class ChunkRequest(BaseModel):
    doc_id: str
    chunk_size: int = 500
    overlap: int = 50
    strategy: str = "fixed_size"
    use_json_structure: bool = False
    
    # ç­–ç•¥ç‰¹å®šåƒæ•¸
    hierarchical_params: Optional[Dict[str, Any]] = None
    adaptive_params: Optional[Dict[str, Any]] = None
    hybrid_params: Optional[Dict[str, Any]] = None
    semantic_params: Optional[Dict[str, Any]] = None
    rcts_hierarchical_params: Optional[Dict[str, Any]] = None
    structured_hierarchical_params: Optional[Dict[str, Any]] = None


class EmbedRequest(BaseModel):
    doc_ids: Optional[List[str]] = None  # if None, embed all
    enable_metadata_enhancement: bool = True  # æ˜¯å¦å•Ÿç”¨metadataå¢å¼·


class RetrieveRequest(BaseModel):
    query: str
    k: int = 5


class GenerateRequest(BaseModel):
    query: str
    top_k: int = 5


# MetadataOptions å·²ç§»è‡³ models.py


# è©•æ¸¬ç›¸é—œçš„æ•¸æ“šæ¨¡å‹
# ChunkConfig å·²ç§»è‡³ models.py


class EvaluationMetrics(BaseModel):
    precision_omega: float  # PrecisionÎ© - æœ€å¤§æº–ç¢ºç‡
    precision_at_k: Dict[int, float]  # k -> precision score
    recall_at_k: Dict[int, float]  # k -> recall score
    chunk_count: int
    avg_chunk_length: float
    length_variance: float


class EvaluationResult(BaseModel):
    config: Dict[str, Any]  # æ”¹ç‚ºå­—å…¸ä»¥æ”¯æ´å‹•æ…‹åƒæ•¸
    metrics: EvaluationMetrics
    test_queries: List[str]
    retrieval_results: Dict[str, List[Dict]]  # query -> results
    timestamp: datetime


@dataclass
class EvaluationTask:
    id: str
    doc_id: str
    configs: List[ChunkConfig]
    test_queries: List[str]
    k_values: List[int]
    status: str  # "pending", "running", "completed", "failed"
    results: List[EvaluationResult]
    created_at: datetime
    progress: float = 0.0  # æ–°å¢ï¼šé€²åº¦ 0.0 to 1.0
    completed_at: Optional[datetime] = None
    error_message: Optional[str] = None
    strategy: str = "fixed_size"  # æ–°å¢ï¼šåˆ†å‰²ç­–ç•¥


class EvaluationStore:
    def __init__(self) -> None:
        self.tasks: Dict[str, EvaluationTask] = {}
        self.executor = ThreadPoolExecutor(max_workers=2)

    def create_task(self, doc_id: str, configs: List[ChunkConfig], 
                   test_queries: List[str], k_values: List[int], 
                   strategy: str = "fixed_size") -> str:
        task_id = str(uuid.uuid4())
        task = EvaluationTask(
            id=task_id,
            doc_id=doc_id,
            configs=configs,
            test_queries=test_queries,
            k_values=k_values,
            strategy=strategy,
            status="pending",
            results=[],
            created_at=datetime.now()
        )
        self.tasks[task_id] = task
        return task_id

    def get_task(self, task_id: str) -> Optional[EvaluationTask]:
        return self.tasks.get(task_id)

    def update_task_status(self, task_id: str, status: str, 
                          results: Optional[List[EvaluationResult]] = None,
                          error_message: Optional[str] = None,
                          progress: Optional[float] = None):
        if task_id in self.tasks:
            self.tasks[task_id].status = status
            if results is not None:
                self.tasks[task_id].results = results
            if error_message is not None:
                self.tasks[task_id].error_message = error_message
            if progress is not None:
                self.tasks[task_id].progress = progress
            if status == "completed":
                self.tasks[task_id].completed_at = datetime.now()
                self.tasks[task_id].progress = 1.0


# å‰µå»ºè©•ä¼°å­˜å„²å¯¦ä¾‹
eval_store = EvaluationStore()


class FixedSizeEvaluationRequest(BaseModel):
    doc_id: str
    chunk_sizes: List[int] = [300, 500, 800]
    overlap_ratios: List[float] = [0.0, 0.1, 0.2]
    strategy: str = "fixed_size"  # æ–°å¢ï¼šåˆ†å‰²ç­–ç•¥
    test_queries: List[str] = [
        "è‘—ä½œæ¬Šçš„å®šç¾©æ˜¯ä»€éº¼ï¼Ÿ",
        "ä»€éº¼æƒ…æ³ä¸‹å¯ä»¥åˆç†ä½¿ç”¨ä»–äººä½œå“ï¼Ÿ",
        "ä¾µçŠ¯è‘—ä½œæ¬Šçš„æ³•å¾‹å¾Œæœæ˜¯ä»€éº¼ï¼Ÿ",
        "è‘—ä½œæ¬Šçš„ä¿è­·æœŸé™æ˜¯å¤šä¹…ï¼Ÿ",
        "å¦‚ä½•ç”³è«‹è‘—ä½œæ¬Šç™»è¨˜ï¼Ÿ"
    ]
    k_values: List[int] = [1, 3, 5, 10]
    
    # ç­–ç•¥ç‰¹å®šåƒæ•¸é¸é … - é è¨­åŒ…å«æ‰€æœ‰æ’åˆ—çµ„åˆ
    chunk_by_options: List[str] = ["article", "item", "section", "chapter"]  # çµæ§‹åŒ–å±¤æ¬¡åˆ†å‰²é¸é …
    preserve_structure_options: List[bool] = [True, False]  # RCTSå±¤æ¬¡åˆ†å‰²é¸é …
    level_depth_options: List[int] = [2, 3, 4]  # å±¤æ¬¡åˆ†å‰²é¸é …
    similarity_threshold_options: List[float] = [0.5, 0.6, 0.7]  # èªç¾©åˆ†å‰²é¸é …
    semantic_threshold_options: List[float] = [0.6, 0.7, 0.8]  # LLMèªç¾©åˆ†å‰²é¸é …
    switch_threshold_options: List[float] = [0.3, 0.5, 0.7]  # æ··åˆåˆ†å‰²é¸é …
    min_chunk_size_options: List[int] = [100, 200, 300]  # å±¤æ¬¡åˆ†å‰²é¸é …
    context_window_options: List[int] = [50, 100, 150]  # èªç¾©åˆ†å‰²é¸é …
    step_size_options: List[int] = [200, 250, 300]  # æ»‘å‹•è¦–çª—é¸é …
    window_size_options: List[int] = [400, 500, 600, 800]  # æ»‘å‹•è¦–çª—é¸é …
    boundary_aware_options: List[bool] = [True, False]  # æ»‘å‹•è¦–çª—é¸é …
    preserve_sentences_options: List[bool] = [True, False]  # æ»‘å‹•è¦–çª—é¸é …
    min_chunk_size_options_sw: List[int] = [50, 100, 150]  # æ»‘å‹•è¦–çª—å°ˆç”¨é¸é …
    max_chunk_size_options_sw: List[int] = [800, 1000, 1200]  # æ»‘å‹•è¦–çª—å°ˆç”¨é¸é …
    secondary_size_options: List[int] = [300, 400, 500]  # æ··åˆåˆ†å‰²é¸é …  # ç”¨æ–¼è¨ˆç®—recall@K


class GenerateQuestionsRequest(BaseModel):
    doc_id: str
    num_questions: int = 10
    question_types: List[str] = ["æ¡ˆä¾‹æ‡‰ç”¨", "æƒ…å¢ƒåˆ†æ", "å¯¦å‹™è™•ç†", "æ³•å¾‹å¾Œæœ", "åˆè¦åˆ¤æ–·"]  # å•é¡Œé¡å‹
    difficulty_levels: List[str] = ["åŸºç¤", "é€²éš", "æ‡‰ç”¨"]  # é›£åº¦ç­‰ç´š


class GeneratedQuestion(BaseModel):
    question: str
    references: List[str]  # ç›¸é—œæ³•è¦æ¢æ–‡
    question_type: str
    difficulty: str
    keywords: List[str]
    estimated_tokens: int


class QuestionGenerationResult(BaseModel):
    doc_id: str
    total_questions: int
    questions: List[GeneratedQuestion]
    generation_time: float
    timestamp: datetime


def generate_unique_id(law_name: str, chapter: str, section: str, article: str, item: Optional[str] = None) -> str:
    """ç”Ÿæˆid"""
    # æ¸…ç†æ³•è¦åç¨±
    law_clean = re.sub(r'[^\w]', '', law_name.lower())
    law_clean = re.sub(r'æ³•è¦åç¨±|æ³•|æ¢ä¾‹', '', law_clean)
    
    # æå–ç« ç¯€
    chapter_num = re.search(r'ç¬¬([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒ0-9]+)ç« ', chapter)
    chapter_num = chapter_num.group(1) if chapter_num else "0"
    
    # æå–ç¯€
    section_num = re.search(r'ç¬¬([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒ0-9]+)ç¯€', section)
    section_num = section_num.group(1) if section_num else "0"
    
    # æå–æ¢æ–‡
    article_num = re.search(r'ç¬¬([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒ0-9]+)æ¢', article)
    article_num = article_num.group(1) if article_num else "0"
    
    # çµ„åˆID
    parts = [law_clean, f"ch{chapter_num}", f"sec{section_num}", f"art{article_num}"]
    if item:
        parts.append(f"item{item}")
    
    return "-".join(parts)


def extract_keywords_with_gemini(text: str, top_k: int = 5) -> List[str]:
    """ä½¿ç”¨Geminiæ¨¡å‹æå–é—œéµè©"""
    if not GEMINI_AVAILABLE:
        return extract_keywords_fallback(text, top_k)
    
    try:
        # å„ªå…ˆä½¿ç”¨ GOOGLE_API_KEYï¼Œå¦‚æœæ²’æœ‰å‰‡ä½¿ç”¨ GEMINI_API_KEY
        api_key = GOOGLE_API_KEY or os.getenv('GEMINI_API_KEY')
        if not api_key:
            return extract_keywords_fallback(text, top_k)
        
        # Configure API key using getattr to avoid static export issues
        cfg = getattr(genai, "configure", None)
        if callable(cfg):
            cfg(api_key=api_key)  # type: ignore[misc]
        ModelCls = getattr(genai, "GenerativeModel", None)
        if ModelCls is None:
            return extract_keywords_fallback(text, top_k)
        model = ModelCls('gemini-2.0-flash-exp')
        
        prompt = f"""
        è«‹å¾ä»¥ä¸‹æ³•å¾‹æ¢æ–‡å…§å®¹ä¸­æå–{top_k}å€‹æœ€é‡è¦çš„é—œéµè©ã€‚
        é—œéµè©æ‡‰è©²æ˜¯æ³•å¾‹è¡“èªã€é‡è¦æ¦‚å¿µæˆ–æ ¸å¿ƒå…§å®¹ã€‚
        è«‹åªè¿”å›é—œéµè©ï¼Œç”¨é€—è™Ÿåˆ†éš”ï¼Œä¸è¦å…¶ä»–è§£é‡‹ã€‚
        
        æ¢æ–‡å…§å®¹ï¼š
        {text}
        """
        
        response = model.generate_content(prompt)
        keywords_text = response.text.strip()
        
        # è§£æé—œéµè©
        keywords = [kw.strip() for kw in keywords_text.split(',') if kw.strip()]
        return keywords[:top_k]
        
    except Exception as e:
        print(f"Geminié—œéµè©æå–å¤±æ•—: {e}")
        return extract_keywords_fallback(text, top_k)


def extract_keywords_fallback(text: str, top_k: int = 5) -> List[str]:
    """å‚™ç”¨é—œéµè©æå–æ–¹æ³•"""
    if jieba is None:
        # å¦‚æœjiebaä¸å¯ç”¨ï¼Œä½¿ç”¨ç°¡å–®çš„æ­£å‰‡è¡¨é”å¼
        words = re.findall(r'[\u4e00-\u9fff]+', text)
        return list(set(words))[:top_k]
    
    try:
        # ä½¿ç”¨jiebaæå–é—œéµè©ï¼›éƒ¨åˆ†ç‰ˆæœ¬å‹åˆ¥ç‚º List[Tuple[str, float]] | List[str]
        from typing import cast, List as _List
        kws = jieba.analyse.extract_tags(text, topK=top_k, withWeight=False)  # type: ignore[call-arg]
        keywords = cast(_List[str], list(kws))
        return keywords[:top_k] if keywords else []
    except:
        # å¦‚æœjiebaå¤±æ•—ï¼Œä½¿ç”¨ç°¡å–®çš„æ­£å‰‡è¡¨é”å¼
        words = re.findall(r'[\u4e00-\u9fff]+', text)
        return list(set(words))[:top_k]


def extract_keywords(text: str, top_k: int = 5) -> List[str]:
    """æå–é—œéµè© - å„ªå…ˆä½¿ç”¨Geminiï¼Œå‚™ç”¨jieba"""
    return extract_keywords_with_gemini(text, top_k)


def extract_cross_references(text: str) -> List[str]:
    """æå–äº¤å‰å¼•ç”¨"""
    references = []
    
    # åŒ¹é…ã€Œç¬¬Xæ¢ã€
    article_refs = re.findall(r'ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒ0-9]+æ¢', text)
    references.extend(article_refs)
    
    # åŒ¹é…ã€Œç¬¬Xé …ã€
    item_refs = re.findall(r'ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒ0-9]+é …', text)
    references.extend(item_refs)
    
    # åŒ¹é…ã€Œç¬¬Xæ¬¾ã€
    clause_refs = re.findall(r'ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒ0-9]+æ¬¾', text)
    references.extend(clause_refs)
    
    # åŒ¹é…ã€Œå‰é …ã€ã€Œå‰æ¢ã€ã€Œæº–ç”¨ä¹‹ã€ç­‰
    if re.search(r'å‰é …|å‰æ¢|æº–ç”¨ä¹‹|ä¾.*è¦å®š|æ¯”ç…§.*è¾¦ç†|é©ç”¨.*è¦å®š', text):
        references.append("internal_reference")
    
    # åŒ¹é…ã€Œæœ¬æ³•ã€ã€Œæœ¬æ¢ä¾‹ã€ç­‰è‡ªå¼•ç”¨
    if re.search(r'æœ¬æ³•|æœ¬æ¢ä¾‹|æœ¬è¦å‰‡|æœ¬è¾¦æ³•', text):
        references.append("self_reference")
    
    # åŒ¹é…ã€Œå…¶ä»–æ³•å¾‹ã€ã€Œç›¸é—œæ³•è¦ã€ç­‰å¤–éƒ¨å¼•ç”¨
    if re.search(r'å…¶ä»–æ³•å¾‹|ç›¸é—œæ³•è¦|å…¶ä»–æ³•è¦|å…¶ä»–æ¢ä¾‹', text):
        references.append("external_reference")
    
    return list(set(references))


def preprocess_text(text: str) -> List[str]:
    """
    æ–‡æœ¬é è™•ç†ï¼šåˆ†è©ã€å»åœç”¨è©ã€æ¸…ç†
    """
    if not text:
        return []
    
    # ä½¿ç”¨jiebaåˆ†è©
    if jieba:
        words = jieba.lcut(text)
    else:
        # ç°¡å–®çš„å­—ç¬¦ç´šåˆ†è©ä½œç‚ºå‚™é¸
        words = list(text)
    
    # ä¸­æ–‡åœç”¨è©åˆ—è¡¨ï¼ˆæ³•å¾‹æ–‡æª”å°ˆç”¨ï¼Œè¼ƒå°‘éæ¿¾ï¼‰
    stop_words = {
        'çš„', 'äº†', 'åœ¨', 'æ˜¯', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'éƒ½', 'ä¸€', 'ä¸Š', 'ä¹Ÿ', 'å¾ˆ', 'åˆ°', 'è¦', 'å»', 'æœƒ', 'è‘—', 'æ²’æœ‰', 'çœ‹', 'å¥½', 'è‡ªå·±', 'é€™', 'é‚£', 'å®ƒ', 'ä»–', 'å¥¹', 'æˆ‘å€‘', 'ä½ å€‘', 'ä»–å€‘', 'å¥¹å€‘', 'å®ƒå€‘', 'ä»€éº¼', 'æ€éº¼', 'ç‚ºä»€éº¼', 'å“ªè£¡', 'ä»€éº¼æ™‚å€™', 'å¤šå°‘', 'å¹¾å€‹', 'ä¸€äº›', 'æ‰€æœ‰', 'æ¯å€‹', 'ä»»ä½•', 'å¦‚æœ', 'å› ç‚º', 'æ‰€ä»¥', 'ä½†æ˜¯', 'ç„¶å¾Œ', 'æˆ–è€…', 'è€Œä¸”', 'é›–ç„¶', 'ä¸é', 'åªæ˜¯', 'å°±æ˜¯', 'é‚„æ˜¯', 'å·²ç¶“', 'æ­£åœ¨', 'å°‡è¦', 'å¯ä»¥', 'æ‡‰è©²', 'å¿…é ˆ', 'éœ€è¦', 'æƒ³è¦', 'å¸Œæœ›', 'å–œæ­¡', 'ä¸å–œæ­¡', 'çŸ¥é“', 'ä¸çŸ¥é“', 'æ˜ç™½', 'ä¸æ˜ç™½', 'è¨˜å¾—', 'å¿˜è¨˜', 'é–‹å§‹', 'çµæŸ', 'ç¹¼çºŒ', 'åœæ­¢', 'å®Œæˆ', 'åš', 'åšé', 'æ­£åœ¨åš', 'å°‡è¦åš', 'è¢«', 'æŠŠ', 'çµ¦', 'å°', 'å‘', 'å¾', 'åˆ°', 'åœ¨', 'æ–¼', 'ç‚º', 'ä»¥', 'ç”¨', 'é€šé', 'æ ¹æ“š', 'æŒ‰ç…§', 'ä¾ç…§', 'é—œæ–¼', 'å°æ–¼', 'è‡³æ–¼', 'é™¤äº†', 'åŒ…æ‹¬', 'ä»¥åŠ', 'èˆ‡', 'æˆ–', 'ä½†', 'ç„¶è€Œ', 'å› æ­¤', 'æ–¼æ˜¯', 'ç„¶å¾Œ', 'æ¥è‘—', 'æœ€å¾Œ', 'é¦–å…ˆ', 'å…¶æ¬¡', 'å†æ¬¡', 'å¦å¤–', 'æ­¤å¤–', 'ä¸¦ä¸”', 'åŒæ™‚', 'ä¸€èµ·', 'åˆ†åˆ¥', 'å„è‡ª', 'å…±åŒ', 'å–®ç¨', 'ç¨ç«‹', 'ç›¸é—œ', 'ç„¡é—œ', 'é‡è¦', 'ä¸é‡è¦', 'ä¸»è¦', 'æ¬¡è¦', 'åŸºæœ¬', 'æ ¹æœ¬', 'æ ¸å¿ƒ', 'é—œéµ', 'å¿…è¦', 'ä¸å¿…è¦', 'å¯èƒ½', 'ä¸å¯èƒ½', 'ä¸€å®š', 'ä¸ä¸€å®š', 'è‚¯å®š', 'ä¸è‚¯å®š', 'ç¢ºå®š', 'ä¸ç¢ºå®š', 'æ¸…æ¥š', 'ä¸æ¸…æ¥š', 'æ˜ç¢º', 'ä¸æ˜ç¢º', 'å…·é«”', 'ä¸å…·é«”', 'è©³ç´°', 'ä¸è©³ç´°', 'ç°¡å–®', 'è¤‡é›œ', 'å®¹æ˜“', 'å›°é›£', 'æ–¹ä¾¿', 'ä¸æ–¹ä¾¿', 'å¿«é€Ÿ', 'æ…¢é€Ÿ', 'é«˜æ•ˆ', 'ä½æ•ˆ', 'æœ‰æ•ˆ', 'ç„¡æ•ˆ', 'æˆåŠŸ', 'å¤±æ•—', 'æ­£ç¢º', 'éŒ¯èª¤', 'å°', 'éŒ¯', 'å¥½', 'å£', 'å„ª', 'åŠ£', 'é«˜', 'ä½', 'å¤§', 'å°', 'å¤š', 'å°‘', 'é•·', 'çŸ­', 'å¯¬', 'çª„', 'åš', 'è–„', 'æ·±', 'æ·º', 'æ–°', 'èˆŠ', 'å¹´è¼•', 'è€', 'æ—©', 'æ™š', 'å¿«', 'æ…¢', 'ç†±', 'å†·', 'æš–', 'æ¶¼', 'ä¹¾', 'æ¿•', 'äº®', 'æš—', 'æ˜', 'æ¸…', 'æ¿', 'éœ', 'å‹•', 'å®‰', 'å±', 'å¹³', 'é™¡', 'ç›´', 'å½', 'åœ“', 'æ–¹', 'å°–', 'éˆ', 'è»Ÿ', 'ç¡¬', 'è¼•', 'é‡', 'å¼·', 'å¼±', 'ç·Š', 'é¬†', 'å¯†', 'ç–', 'æ»¿', 'ç©º', 'å¯¦', 'è™›', 'çœŸ', 'å‡', 'æ­£', 'è² ', 'åŠ ', 'æ¸›', 'ä¹˜', 'é™¤', 'ç­‰æ–¼', 'ä¸ç­‰æ–¼', 'å¤§æ–¼', 'å°æ–¼', 'å¤§æ–¼ç­‰æ–¼', 'å°æ–¼ç­‰æ–¼', 'å’Œ', 'å·®', 'ç©', 'å•†', 'é¤˜', 'å€', 'åˆ†', 'æ¯”', 'ç‡', 'æ¯”ä¾‹', 'ç™¾åˆ†', 'åƒåˆ†', 'è¬åˆ†', 'å„„åˆ†', 'å…†åˆ†', 'äº¬åˆ†', 'å“åˆ†', 'ç§­åˆ†', 'ç©°åˆ†', 'æºåˆ†', 'æ¾—åˆ†', 'æ­£åˆ†', 'è¼‰åˆ†', 'æ¥µåˆ†', 'æ†æ²³æ²™åˆ†', 'é˜¿åƒ§ç¥‡åˆ†', 'é‚£ç”±ä»–åˆ†', 'ä¸å¯æ€è­°åˆ†', 'ç„¡é‡å¤§æ•¸åˆ†'
    }
    
    # éæ¿¾åœç”¨è©å’ŒçŸ­è©
    filtered_words = []
    for word in words:
        word = word.strip()
        if len(word) > 1 and word not in stop_words and not word.isdigit():
            filtered_words.append(word)
    
    return filtered_words


def calculate_tfidf_importance(texts: List[str], target_text: str) -> float:
    """
    ä½¿ç”¨TF-IDFè¨ˆç®—æ–‡æœ¬é‡è¦æ€§
    """
    if not texts or not target_text:
        return 1.0
    
    try:
        # é è™•ç†æ‰€æœ‰æ–‡æœ¬
        processed_texts = [' '.join(preprocess_text(text)) for text in texts]
        processed_target = ' '.join(preprocess_text(target_text))
        
        if not processed_target:
            return 1.0
        
        # è¨ˆç®—TF-IDF
        vectorizer = TfidfVectorizer(
            max_features=1000,
            ngram_range=(1, 2),
            min_df=1,
            max_df=0.95
        )
        
        # æ“¬åˆæ‰€æœ‰æ–‡æœ¬
        all_texts = processed_texts + [processed_target]
        tfidf_matrix = vectorizer.fit_transform(all_texts)
        
        # ç²å–ç›®æ¨™æ–‡æœ¬çš„TF-IDFå‘é‡ï¼ˆé¿å…ç¨€ç–çŸ©é™£çš„åˆ‡ç‰‡è­¦å‘Šï¼‰
        target_vector = tfidf_matrix.getrow(tfidf_matrix.shape[0] - 1)
        
        # è¨ˆç®—èˆ‡å…¶ä»–æ–‡æœ¬çš„å¹³å‡ç›¸ä¼¼åº¦
        similarities = cosine_similarity(target_vector, tfidf_matrix[:-1])
        avg_similarity = similarities.mean()
        
        # è¨ˆç®—TF-IDFåˆ†æ•¸ï¼ˆè©é »-é€†æ–‡æª”é »ç‡ï¼‰
        tfidf_scores = target_vector.toarray()[0]
        tfidf_sum = tfidf_scores.sum()
        
        # ç¶œåˆè©•åˆ†ï¼šTF-IDFåˆ†æ•¸ + ç›¸ä¼¼åº¦
        importance = (tfidf_sum * 0.7 + avg_similarity * 0.3) * 10
        
        # æ¨™æº–åŒ–åˆ°1-5ç¯„åœ
        importance = max(0.1, min(5.0, importance))
        
        return round(importance, 2)
        
    except Exception as e:
        print(f"TF-IDFè¨ˆç®—éŒ¯èª¤: {e}")
        return 1.0


def calculate_bm25_importance(texts: List[str], target_text: str) -> float:
    """
    ä½¿ç”¨BM25è¨ˆç®—æ–‡æœ¬é‡è¦æ€§
    """
    if not texts or not target_text or not BM25_AVAILABLE:
        return 1.0
    
    try:
        # é è™•ç†æ‰€æœ‰æ–‡æœ¬
        processed_texts = [preprocess_text(text) for text in texts]
        processed_target = preprocess_text(target_text)
        
        if not processed_target:
            return 1.0
        
        # åˆå§‹åŒ–BM25
        bm25 = BM25Okapi(processed_texts)
        
        # è¨ˆç®—BM25åˆ†æ•¸
        scores = bm25.get_scores(processed_target)
        
        if len(scores) == 0:
            return 1.0
        
        # è¨ˆç®—å¹³å‡åˆ†æ•¸
        avg_score = scores.mean()
        
        # è¨ˆç®—æœ€é«˜åˆ†æ•¸
        max_score = scores.max()
        
        # ç¶œåˆè©•åˆ†ï¼šå¹³å‡åˆ†æ•¸ + æœ€é«˜åˆ†æ•¸
        importance = (avg_score * 0.6 + max_score * 0.4) * 2
        
        # æ¨™æº–åŒ–åˆ°1-5ç¯„åœ
        importance = max(0.1, min(5.0, importance))
        
        return round(importance, 2)
        
    except Exception as e:
        print(f"BM25è¨ˆç®—éŒ¯èª¤: {e}")
        return 1.0


def calculate_importance(chapter: str, section: str, article: str, content: str = "", all_articles: List[Dict] = None) -> float:
    """
    è¨ˆç®—é‡è¦æ€§æ¬Šé‡ - ä½¿ç”¨TF-IDFå’ŒBM25å‹•æ…‹è¨ˆç®—
    
    åƒæ•¸:
    - chapter: ç« ç¯€åç¨±
    - section: ç¯€åç¨±  
    - article: æ¢æ–‡åç¨±
    - content: æ¢æ–‡å…§å®¹
    - all_articles: æ‰€æœ‰æ¢æ–‡åˆ—è¡¨ï¼Œç”¨æ–¼è¨ˆç®—ç›¸å°é‡è¦æ€§
    """
    # åŸºç¤æ¬Šé‡
    base_weight = 1.0
    
    # å¦‚æœæ²’æœ‰å…§å®¹æˆ–æ‰€æœ‰æ¢æ–‡ï¼Œä½¿ç”¨éœæ…‹æ¬Šé‡
    if not content or not all_articles:
        return calculate_static_importance(chapter, section, article)
    
    try:
        # æº–å‚™æ‰€æœ‰æ¢æ–‡çš„æ–‡æœ¬
        all_texts = []
        for art in all_articles:
            text = f"{art.get('article', '')} {art.get('content', '')}"
            if text.strip():
                all_texts.append(text)
        
        if len(all_texts) < 2:
            return calculate_static_importance(chapter, section, article)
        
        # ç›®æ¨™æ–‡æœ¬
        target_text = f"{article} {content}"
        
        # è¨ˆç®—TF-IDFé‡è¦æ€§
        tfidf_importance = calculate_tfidf_importance(all_texts, target_text)
        
        # è¨ˆç®—BM25é‡è¦æ€§
        bm25_importance = calculate_bm25_importance(all_texts, target_text)
        
        # ç¶œåˆè©•åˆ†ï¼šTF-IDF 60% + BM25 40%
        dynamic_weight = tfidf_importance * 0.6 + bm25_importance * 0.4
        
        # çµåˆéœæ…‹æ¬Šé‡ï¼ˆ30%ï¼‰å’Œå‹•æ…‹æ¬Šé‡ï¼ˆ70%ï¼‰
        final_weight = base_weight * 0.3 + dynamic_weight * 0.7
        
        return round(final_weight, 2)
        
    except Exception as e:
        print(f"å‹•æ…‹é‡è¦æ€§è¨ˆç®—éŒ¯èª¤: {e}")
        return calculate_static_importance(chapter, section, article)


def calculate_static_importance(chapter: str, section: str, article: str) -> float:
    """
    éœæ…‹é‡è¦æ€§æ¬Šé‡è¨ˆç®—ï¼ˆå‚™ç”¨æ–¹æ³•ï¼‰
    """
    weight = 1.0
    
    # ç¸½å‰‡ç« ç¯€æ¬Šé‡æ›´é«˜ (åŸºç¤æ€§æ¢æ–‡)
    if "ç¸½å‰‡" in chapter or "ç¬¬ä¸€ç« " in chapter or "é€šå‰‡" in chapter:
        weight *= 1.5
    
    # å®šç¾©æ€§æ¢æ–‡æ¬Šé‡æ›´é«˜ (æ ¸å¿ƒæ¦‚å¿µ)
    if "å®šç¾©" in article or "ç”¨è©" in article or "é‡‹ç¾©" in article:
        weight *= 1.3
    
    # ç½°å‰‡ç« ç¯€æ¬Šé‡è¼ƒé«˜ (æ³•å¾‹å¾Œæœ)
    if "ç½°å‰‡" in chapter or "ç½°" in chapter or "è™•ç½°" in chapter:
        weight *= 1.2
    
    # æ–½è¡Œç´°å‰‡æ¬Šé‡è¼ƒä½ (ç¨‹åºæ€§æ¢æ–‡)
    if "æ–½è¡Œ" in chapter or "ç¨‹åº" in chapter or "æµç¨‹" in chapter:
        weight *= 0.8
    
    # é™„å‰‡æ¬Šé‡æœ€ä½ (è£œå……æ€§æ¢æ–‡)
    if "é™„å‰‡" in chapter or "é™„" in chapter:
        weight *= 0.7
    
    # é€šå‰‡ç¯€æ¬Šé‡è¼ƒé«˜
    if "é€šå‰‡" in section:
        weight *= 1.2
    
    return round(weight, 2)






def get_text_position_in_document(full_text: str, target_text: str) -> Dict[str, Any]:
    """ç²å–æ–‡æœ¬åœ¨æ–‡æª”ä¸­çš„ä½ç½®ä¿¡æ¯"""
    if not target_text.strip():
        return {"start": 0, "end": 0, "found": False}
    
    # æ¸…ç†æ–‡æœ¬å…§å®¹
    clean_target = re.sub(r'\s+', ' ', target_text.strip())
    clean_full = re.sub(r'\s+', ' ', full_text)
    
    start_idx = clean_full.find(clean_target)
    if start_idx != -1:
        return {
            "start": start_idx,
            "end": start_idx + len(clean_target),
            "found": True,
            "confidence": 1.0
        }
    
    # å˜—è©¦éƒ¨åˆ†åŒ¹é…
    if len(clean_target) > 10:
        partial = clean_target[:15]
        start_idx = clean_full.find(partial)
        if start_idx != -1:
            return {
                "start": start_idx,
                "end": start_idx + len(clean_target),
                "found": True,
                "confidence": 0.7,
                "note": "partial_match"
            }
    
    return {"start": 0, "end": 0, "found": False, "confidence": 0.0}






@app.get("/health")
def health():
    return {"ok": True}


@app.post("/upload")
async def upload(file: UploadFile = File(...)):
    content = await file.read()
    doc_id = str(uuid.uuid4())
    
    # æª¢æŸ¥æ–‡ä»¶é¡å‹ä¸¦ç›¸æ‡‰è™•ç†
    if file.filename and file.filename.lower().endswith('.pdf'):
        # è™•ç†PDFæ–‡ä»¶
        try:
            import io
            if pdfplumber:
                # ä½¿ç”¨pdfplumberè§£æPDF
                pdf_file = io.BytesIO(content)
                text = ""
                with pdfplumber.open(pdf_file) as pdf:
                    for page in pdf.pages:
                        page_text = page.extract_text()
                        if page_text:
                            text += page_text + "\n"
            else:
                # å‚™ç”¨æ–¹æ¡ˆï¼šä½¿ç”¨PyPDF2
                pdf_file = io.BytesIO(content)
                if PYPDF2_AVAILABLE:
                    pdf_reader = PdfReader(pdf_file)
                    text = ""
                    for page in pdf_reader.pages:
                        text += page.extract_text() + "\n"
                else:
                    # å¦‚æœæ²’æœ‰PDFè§£æåº«ï¼Œè¿”å›éŒ¯èª¤
                    return JSONResponse(
                        status_code=400, 
                        content={"error": "PDF parsing libraries not available. Please install pdfplumber or PyPDF2."}
                    )
        except Exception as e:
            return JSONResponse(
                status_code=400, 
                content={"error": f"Failed to parse PDF: {str(e)}"}
            )
    else:
        # è™•ç†æ–‡æœ¬æ–‡ä»¶
        try:
            text = content.decode("utf-8", errors="ignore")
        except Exception:
            text = str(content)
    
    # æ¸…ç†æ–‡æœ¬
    text = text.strip()
    if not text:
        return JSONResponse(
            status_code=400, 
            content={"error": "No text content found in the file"}
        )
    
    doc_record = DocRecord(
        id=doc_id,
        filename=file.filename,
        text=text,
        json_data=None,  # åˆå§‹ç‚ºNoneï¼Œå¾ŒçºŒé€šé/update-jsonç«¯é»æ›´æ–°
        chunks=[],
        chunk_size=0,
        overlap=0,
    )
    store.add_doc(doc_record)
    
    # è‡ªå‹•ä¿å­˜æ•¸æ“š
    store.save_data()
    
    return {"doc_id": doc_id, "filename": file.filename, "num_chars": len(text)}


@app.post("/api/update-json")
async def update_json(request: dict):
    """æ›´æ–°æ–‡æª”çš„JSONçµæ§‹åŒ–æ•¸æ“š"""
    doc_id = request.get("doc_id")
    json_data = request.get("json_data")
    
    if not doc_id or not json_data:
        return JSONResponse(
            status_code=400,
            content={"error": "doc_id and json_data are required"}
        )
    
    if doc_id not in store.docs:
        return JSONResponse(
            status_code=404,
            content={"error": "Document not found"}
        )
    
    # æ›´æ–°æ–‡æª”çš„JSONæ•¸æ“š
    store.docs[doc_id].json_data = json_data
    
    # é‡ç½®ç›¸é—œç‹€æ…‹ï¼Œå› ç‚ºJSONæ•¸æ“šæ”¹è®Šå¯èƒ½å½±éŸ¿chunking
    store.docs[doc_id].chunks = []
    store.docs[doc_id].chunk_size = 0
    store.docs[doc_id].overlap = 0
    store.reset_embeddings()
    
    return {"success": True, "message": "JSON data updated successfully"}


def sliding_window_chunks(text: str, chunk_size: int, overlap: int) -> List[str]:
    """å›ºå®šå¤§å°æ»‘å‹•çª—å£åˆ†å‰²"""
    if chunk_size <= 0:
        return [text]
    if overlap >= chunk_size:
        overlap = max(0, chunk_size - 1)
    chunks: List[str] = []
    start = 0
    n = len(text)
    while start < n:
        end = min(n, start + chunk_size)
        chunks.append(text[start:end])
        if end == n:
            break
        start = end - overlap
    return chunks


def hierarchical_chunks(text: str, max_chunk_size: int, min_chunk_size: int, overlap: int, level_depth: int) -> List[str]:
    """å±¤æ¬¡åŒ–åˆ†å‰²ç­–ç•¥"""
    if max_chunk_size <= 0:
        return [text]
    
    # é¦–å…ˆæŒ‰æ®µè½åˆ†å‰²
    paragraphs = text.split('\n\n')
    chunks = []
    
    for para in paragraphs:
        if len(para) <= max_chunk_size:
            chunks.append(para)
        else:
            # å¦‚æœæ®µè½å¤ªé•·ï¼ŒæŒ‰å¥å­åˆ†å‰²
            sentences = para.split('ã€‚')
            current_chunk = ""
            
            for sentence in sentences:
                if len(current_chunk + sentence) <= max_chunk_size:
                    current_chunk += sentence + "ã€‚"
                else:
                    if current_chunk and len(current_chunk) >= min_chunk_size:
                        chunks.append(current_chunk.strip())
                    current_chunk = sentence + "ã€‚"
            
            if current_chunk and len(current_chunk) >= min_chunk_size:
                chunks.append(current_chunk.strip())
    
    # æ‡‰ç”¨é‡ç–Š
    if overlap > 0:
        overlapped_chunks = []
        for i, chunk in enumerate(chunks):
            overlapped_chunks.append(chunk)
            if i < len(chunks) - 1 and len(chunk) > overlap:
                # æ·»åŠ é‡ç–Šéƒ¨åˆ†
                overlap_text = chunk[-overlap:]
                next_chunk = chunks[i + 1]
                if len(next_chunk) > overlap:
                    overlapped_chunks.append(overlap_text + next_chunk[overlap:])
        return overlapped_chunks
    
    return chunks


def adaptive_chunks(text: str, target_size: int, tolerance: int, overlap: int, semantic_threshold: float) -> List[str]:
    """è‡ªé©æ‡‰åˆ†å‰²ç­–ç•¥"""
    if target_size <= 0:
        return [text]
    
    chunks = []
    start = 0
    n = len(text)
    
    while start < n:
        # å˜—è©¦æ‰¾åˆ°æœ€ä½³åˆ†å‰²é»
        end = min(n, start + target_size)
        
        # å¦‚æœæ¥è¿‘ç›®æ¨™å¤§å°ï¼Œå°‹æ‰¾èªç¾©é‚Šç•Œ
        if end - start >= target_size - tolerance:
            # å°‹æ‰¾å¥è™Ÿã€æ®µè½ç­‰èªç¾©é‚Šç•Œ
            for i in range(end, max(start + target_size - tolerance, start), -1):
                if i < n and text[i] in ['ã€‚', '\n', 'ï¼', 'ï¼Ÿ']:
                    end = i + 1
                    break
        
        chunk = text[start:end].strip()
        if chunk:
            chunks.append(chunk)
        
        if end == n:
            break
        
        # è¨ˆç®—ä¸‹ä¸€å€‹chunkçš„èµ·å§‹ä½ç½®ï¼ˆè€ƒæ…®é‡ç–Šï¼‰
        start = max(start + 1, end - overlap)
    
    return chunks


def hybrid_chunks(text: str, primary_size: int, secondary_size: int, overlap: int, switch_threshold: float) -> List[str]:
    """æ··åˆåˆ†å‰²ç­–ç•¥"""
    if primary_size <= 0:
        return [text]
    
    chunks = []
    start = 0
    n = len(text)
    
    while start < n:
        # æ±ºå®šä½¿ç”¨ä¸»è¦å¤§å°é‚„æ˜¯æ¬¡è¦å¤§å°
        remaining_text = text[start:]
        avg_sentence_length = len(remaining_text) / max(1, remaining_text.count('ã€‚'))
        
        if avg_sentence_length > primary_size * switch_threshold:
            chunk_size = secondary_size
        else:
            chunk_size = primary_size
        
        end = min(n, start + chunk_size)
        chunk = text[start:end].strip()
        
        if chunk:
            chunks.append(chunk)
        
        if end == n:
            break
        
        start = max(start + 1, end - overlap)
    
    return chunks


def semantic_chunks(text: str, target_size: int, similarity_threshold: float, overlap: int, context_window: int) -> List[str]:
    """èªç¾©åˆ†å‰²ç­–ç•¥"""
    if target_size <= 0:
        return [text]
    
    # ç°¡åŒ–å¯¦ç¾ï¼šæŒ‰å¥å­åˆ†å‰²ï¼Œç„¶å¾Œåˆä½µç›¸ä¼¼çš„å¥å­
    sentences = text.split('ã€‚')
    chunks = []
    current_chunk = ""
    
    for sentence in sentences:
        if not sentence.strip():
            continue
            
        sentence = sentence.strip() + "ã€‚"
        
        # å¦‚æœç•¶å‰chunkåŠ ä¸Šæ–°å¥å­ä¸è¶…éç›®æ¨™å¤§å°
        if len(current_chunk + sentence) <= target_size:
            current_chunk += sentence
        else:
            # ä¿å­˜ç•¶å‰chunk
            if current_chunk:
                chunks.append(current_chunk.strip())
            
            # é–‹å§‹æ–°chunk
            current_chunk = sentence
    
    # æ·»åŠ æœ€å¾Œä¸€å€‹chunk
    if current_chunk:
        chunks.append(current_chunk.strip())
    
    return chunks


def json_structured_chunks(json_data: Dict[str, Any], chunk_size: int, overlap: int) -> List[Dict[str, Any]]:
    """
    åŸºæ–¼JSONçµæ§‹çš„æ™ºèƒ½åˆ†å‰²
    ä¿ç•™æ³•å¾‹æ–‡æª”çš„çµæ§‹åŒ–ä¿¡æ¯
    æ”¯æŒå–®ä¸€æ³•å¾‹æ–‡æª”å’Œå¤šæ³•å¾‹æ–‡æª”æ ¼å¼
    """
    if not json_data or chunk_size <= 0:
        return []
    
    chunks = []
    
    # æª¢æŸ¥æ˜¯å¦ç‚ºå¤šæ³•å¾‹æ–‡æª”æ ¼å¼
    if "laws" in json_data:
        # å¤šæ³•å¾‹æ–‡æª”æ ¼å¼
        laws = json_data.get("laws", [])
        for law in laws:
            law_chunks = process_single_law(law, chunk_size, overlap)
            chunks.extend(law_chunks)
    else:
        # å–®ä¸€æ³•å¾‹æ–‡æª”æ ¼å¼
        law_chunks = process_single_law(json_data, chunk_size, overlap)
        chunks.extend(law_chunks)
    
    return chunks


def process_single_law(law_data: Dict[str, Any], chunk_size: int, overlap: int) -> List[Dict[str, Any]]:
    """
    è™•ç†å–®ä¸€æ³•å¾‹æ–‡æª”
    """
    chunks = []
    law_name = law_data.get("law_name", "æœªå‘½åæ³•è¦")
    
    def create_chunk(content: str, metadata: Dict[str, Any], chunk_id: str) -> Dict[str, Any]:
        """å‰µå»ºåŒ…å«metadataçš„chunk"""
        return {
            "chunk_id": chunk_id,
            "content": content,
            "metadata": {
                "id": metadata.get("id", ""),
                "spans": metadata.get("spans", {}),
                "page_range": metadata.get("page_range", {})
            }
        }
    
    def process_article(article: Dict[str, Any], chapter: str, section: str) -> List[Dict[str, Any]]:
        """è™•ç†å–®å€‹æ¢æ–‡"""
        article_chunks = []
        article_title = article.get("article", "")
        article_content = article.get("content", "")
        items = article.get("items", [])
        
        # è™•ç†æ¢æ–‡ä¸»é«”
        if article_content:
            # å¦‚æœæ¢æ–‡å…§å®¹è¼ƒçŸ­ï¼Œç›´æ¥ä½œç‚ºä¸€å€‹chunk
            if len(article_content) <= chunk_size:
                metadata = {
                    "id": article.get("metadata", {}).get("id", ""),
                    "spans": article.get("metadata", {}).get("spans", {}),
                    "page_range": article.get("metadata", {}).get("page_range", {})
                }
                chunk_id = f"{article_title}_main"
                article_chunks.append(create_chunk(article_content, metadata, chunk_id))
            else:
                # æ¢æ–‡å…§å®¹è¼ƒé•·ï¼Œéœ€è¦åˆ†å‰²
                text_chunks = sliding_window_chunks(article_content, chunk_size, overlap)
                for i, chunk_text in enumerate(text_chunks):
                    metadata = {
                        "id": article.get("metadata", {}).get("id", ""),
                        "spans": article.get("metadata", {}).get("spans", {}),
                        "page_range": article.get("metadata", {}).get("page_range", {})
                    }
                    chunk_id = f"{article_title}_part_{i+1}"
                    article_chunks.append(create_chunk(chunk_text, metadata, chunk_id))
        
        # è™•ç†æ¢æ–‡é …ç›® - æ”¯æ´æ–°çµæ§‹ (paragraphs) å’ŒèˆŠçµæ§‹ (items)
        paragraphs = article.get("paragraphs", [])
        items = article.get("items", [])
        
        # ä½¿ç”¨ paragraphs å¦‚æœå­˜åœ¨ï¼Œå¦å‰‡ä½¿ç”¨ items
        items_to_process = paragraphs if paragraphs else items
        
        for item in items_to_process:
            # æ”¯æ´æ–°çµæ§‹çš„éµå
            item_title = item.get("paragraph", item.get("item", ""))
            item_content = item.get("content", "")
            
            # è™•ç†é …ç›®ä¸»é«”
            if item_content:
                if len(item_content) <= chunk_size:
                    metadata = {
                        "id": item.get("metadata", {}).get("id", ""),
                        "spans": item.get("metadata", {}).get("spans", {}),
                        "page_range": item.get("metadata", {}).get("page_range", {})
                    }
                    chunk_id = f"{article_title}_{item_title}_main"
                    article_chunks.append(create_chunk(item_content, metadata, chunk_id))
                else:
                    # é …ç›®å…§å®¹è¼ƒé•·ï¼Œéœ€è¦åˆ†å‰²
                    text_chunks = sliding_window_chunks(item_content, chunk_size, overlap)
                    for i, chunk_text in enumerate(text_chunks):
                        metadata = {
                            "id": item.get("metadata", {}).get("id", ""),
                            "spans": item.get("metadata", {}).get("spans", {}),
                            "page_range": item.get("metadata", {}).get("page_range", {})
                        }
                        chunk_id = f"{article_title}_{item_title}_part_{i+1}"
                        article_chunks.append(create_chunk(chunk_text, metadata, chunk_id))
            
            # è™•ç†å­é …ç›® - æ”¯æ´æ–°çµæ§‹ (subparagraphs) å’ŒèˆŠçµæ§‹ (sub_items)
            subparagraphs = item.get("subparagraphs", [])
            sub_items = item.get("sub_items", [])
            
            # ä½¿ç”¨ subparagraphs å¦‚æœå­˜åœ¨ï¼Œå¦å‰‡ä½¿ç”¨ sub_items
            sub_items_to_process = subparagraphs if subparagraphs else sub_items
            
            for sub_item in sub_items_to_process:
                # æ”¯æ´æ–°çµæ§‹çš„éµå
                sub_item_title = sub_item.get("subparagraph", sub_item.get("sub_item", ""))
                sub_item_content = sub_item.get("content", "")
                
                if sub_item_content:
                    if len(sub_item_content) <= chunk_size:
                        metadata = {
                            "id": sub_item.get("metadata", {}).get("id", ""),
                            "spans": sub_item.get("metadata", {}).get("spans", {}),
                            "page_range": sub_item.get("metadata", {}).get("page_range", {})
                        }
                        chunk_id = f"{article_title}_{item_title}_{sub_item_title}"
                        article_chunks.append(create_chunk(sub_item_content, metadata, chunk_id))
                    else:
                        # å­é …ç›®å…§å®¹è¼ƒé•·ï¼Œéœ€è¦åˆ†å‰²
                        text_chunks = sliding_window_chunks(sub_item_content, chunk_size, overlap)
                        for i, chunk_text in enumerate(text_chunks):
                            metadata = {
                                "id": sub_item.get("metadata", {}).get("id", ""),
                                "spans": sub_item.get("metadata", {}).get("spans", {}),
                                "page_range": sub_item.get("metadata", {}).get("page_range", {})
                            }
                            chunk_id = f"{article_title}_{item_title}_{sub_item_title}_part_{i+1}"
                            article_chunks.append(create_chunk(chunk_text, metadata, chunk_id))
                
                # è™•ç†ç¬¬ä¸‰å±¤é …ç›® (items)
                third_level_items = sub_item.get("items", [])
                for third_item in third_level_items:
                    third_item_title = third_item.get("item", "")
                    third_item_content = third_item.get("content", "")
                    
                    if third_item_content:
                        if len(third_item_content) <= chunk_size:
                            metadata = {
                                "id": third_item.get("metadata", {}).get("id", ""),
                                "spans": third_item.get("metadata", {}).get("spans", {}),
                                "page_range": third_item.get("metadata", {}).get("page_range", {})
                            }
                            chunk_id = f"{article_title}_{item_title}_{sub_item_title}_{third_item_title}"
                            article_chunks.append(create_chunk(third_item_content, metadata, chunk_id))
                        else:
                            # ç¬¬ä¸‰å±¤é …ç›®å…§å®¹è¼ƒé•·ï¼Œéœ€è¦åˆ†å‰²
                            text_chunks = sliding_window_chunks(third_item_content, chunk_size, overlap)
                            for i, chunk_text in enumerate(text_chunks):
                                metadata = {
                                    "id": third_item.get("metadata", {}).get("id", ""),
                                    "spans": third_item.get("metadata", {}).get("spans", {}),
                                    "page_range": third_item.get("metadata", {}).get("page_range", {})
                                }
                                chunk_id = f"{article_title}_{item_title}_{sub_item_title}_{third_item_title}_part_{i+1}"
                                article_chunks.append(create_chunk(chunk_text, metadata, chunk_id))
        
        return article_chunks
    
    # éæ­·æ‰€æœ‰ç« ç¯€
    chapters = law_data.get("chapters", [])
    for chapter in chapters:
        chapter_title = chapter.get("chapter", "")
        sections = chapter.get("sections", [])
        
        for section in sections:
            section_title = section.get("section", "")
            articles = section.get("articles", [])
            
            for article in articles:
                article_chunks = process_article(article, chapter_title, section_title)
                chunks.extend(article_chunks)
    
    return chunks


# è©•æ¸¬ç›¸é—œå‡½æ•¸
def calculate_precision_at_k(retrieved_chunks: List[str], query: str, k: int) -> float:
    """
    è¨ˆç®—Precision@K - æª¢ç´¢å‡ºä¾†çš„tokensä¸­ï¼Œæœ‰å¤šå°‘æ˜¯çœŸæ­£ç›¸é—œçš„
    """
    if not retrieved_chunks or k <= 0:
        return 0.0
    
    # å–å‰kå€‹çµæœ
    top_k_chunks = retrieved_chunks[:k]
    
    # æ”¹é€²çš„é—œéµè©åŒ¹é…æ–¹æ³• - ä½¿ç”¨å­—ç¬¦ç´šåŒ¹é…
    query_chars = set(query.replace(' ', '').replace('ï¼Ÿ', '').replace('ï¼', '').replace('ï¼Œ', '').replace('ã€‚', ''))
    if not query_chars:
        return 0.0
    
    relevant_count = 0
    for chunk in top_k_chunks:
        chunk_chars = set(chunk.replace(' ', '').replace('ï¼Œ', '').replace('ã€‚', '').replace('ï¼›', '').replace('ï¼š', ''))
        # å¦‚æœæŸ¥è©¢ä¸­çš„å­—ç¬¦æœ‰50%ä»¥ä¸Šå‡ºç¾åœ¨chunkä¸­ï¼Œèªç‚ºç›¸é—œ
        overlap_chars = query_chars & chunk_chars
        if len(overlap_chars) >= len(query_chars) * 0.5:
            relevant_count += 1
    
    return relevant_count / len(top_k_chunks)


def calculate_precision_omega(retrieved_chunks: List[str], query: str) -> float:
    """
    è¨ˆç®—PrecisionÎ© - å‡è¨­Recallæ˜¯æ»¿åˆ†ï¼Œæœ€å¤§çš„æº–ç¢ºç‡æ˜¯å¤šå°‘
    """
    if not retrieved_chunks:
        return 0.0
    
    # æ”¹é€²çš„é—œéµè©åŒ¹é…æ–¹æ³• - ä½¿ç”¨å­—ç¬¦ç´šåŒ¹é…
    query_chars = set(query.replace(' ', '').replace('ï¼Ÿ', '').replace('ï¼', '').replace('ï¼Œ', '').replace('ã€‚', ''))
    if not query_chars:
        return 0.0
    
    relevant_count = 0
    for chunk in retrieved_chunks:
        chunk_chars = set(chunk.replace(' ', '').replace('ï¼Œ', '').replace('ã€‚', '').replace('ï¼›', '').replace('ï¼š', ''))
        # å¦‚æœæŸ¥è©¢ä¸­çš„å­—ç¬¦æœ‰30%ä»¥ä¸Šå‡ºç¾åœ¨chunkä¸­ï¼Œèªç‚ºç›¸é—œ
        overlap_chars = query_chars & chunk_chars
        if len(overlap_chars) >= len(query_chars) * 0.3:
            relevant_count += 1
    
    return relevant_count / len(retrieved_chunks)


def calculate_recall_at_k(retrieved_chunks: List[str], query: str, k: int, 
                         ground_truth_chunks: List[str] = None) -> float:
    """
    è¨ˆç®—Recall@K - åœ¨å‰Kå€‹æª¢ç´¢çµæœä¸­å‘½ä¸­ç›¸é—œchunkçš„æ¯”ä¾‹
    """
    if not retrieved_chunks or k <= 0:
        return 0.0
    
    # å–å‰kå€‹çµæœ
    top_k_chunks = retrieved_chunks[:k]
    
    # å¦‚æœæ²’æœ‰ground truthï¼Œä½¿ç”¨é—œéµè©åŒ¹é…ä½œç‚ºè¿‘ä¼¼
    if ground_truth_chunks is None:
        # æ”¹é€²çš„é—œéµè©åŒ¹é…æ–¹æ³• - ä½¿ç”¨å­—ç¬¦ç´šåŒ¹é…
        query_chars = set(query.replace(' ', '').replace('ï¼Ÿ', '').replace('ï¼', '').replace('ï¼Œ', '').replace('ã€‚', ''))
        if not query_chars:
            return 0.0
        
        # é¦–å…ˆè¨ˆç®—ç¸½ç›¸é—œæ–‡æª”æ•¸é‡ï¼ˆéœ€è¦å¾æ‰€æœ‰chunksä¸­è¨ˆç®—ï¼Œä¸åªæ˜¯top_kï¼‰
        # ä½†ç”±æ–¼æˆ‘å€‘æ²’æœ‰è¨ªå•æ‰€æœ‰chunksï¼Œæˆ‘å€‘éœ€è¦ä¸€å€‹è¿‘ä¼¼æ–¹æ³•
        # é€™è£¡æˆ‘å€‘å‡è¨­ç¸½ç›¸é—œæ–‡æª”æ•¸é‡ç­‰æ–¼æª¢ç´¢åˆ°çš„ç›¸é—œæ–‡æª”æ•¸é‡ï¼ˆé€™æ˜¯ä¸€å€‹è¿‘ä¼¼ï¼‰
        retrieved_relevant_count = 0
        for chunk in top_k_chunks:
            chunk_chars = set(chunk.replace(' ', '').replace('ï¼Œ', '').replace('ã€‚', '').replace('ï¼›', '').replace('ï¼š', ''))
            # å¦‚æœæŸ¥è©¢ä¸­çš„å­—ç¬¦æœ‰50%ä»¥ä¸Šå‡ºç¾åœ¨chunkä¸­ï¼Œèªç‚ºç›¸é—œ
            overlap_chars = query_chars & chunk_chars
            if len(overlap_chars) >= len(query_chars) * 0.5:
                retrieved_relevant_count += 1
        
        # ç”±æ–¼ç„¡æ³•æº–ç¢ºè¨ˆç®—ç¸½ç›¸é—œæ–‡æª”æ•¸é‡ï¼Œæˆ‘å€‘ä½¿ç”¨ä¸€å€‹ä¿å®ˆçš„ä¼°è¨ˆ
        # å‡è¨­ç¸½ç›¸é—œæ–‡æª”æ•¸é‡è‡³å°‘ç­‰æ–¼æª¢ç´¢åˆ°çš„ç›¸é—œæ–‡æª”æ•¸é‡
        total_relevant_estimate = max(retrieved_relevant_count, 1)
        
        return retrieved_relevant_count / total_relevant_estimate
    
    # ä½¿ç”¨ground truthè¨ˆç®— - é€™è£¡ground_truth_chunkså¯¦éš›ä¸Šæ˜¯æ‰€æœ‰chunks
    # é¦–å…ˆè¨ˆç®—æ‰€æœ‰chunksä¸­ç›¸é—œçš„æ•¸é‡
    query_chars = set(query.replace(' ', '').replace('ï¼Ÿ', '').replace('ï¼', '').replace('ï¼Œ', '').replace('ã€‚', ''))
    if not query_chars:
        return 0.0
    
    total_relevant_count = 0
    for chunk in ground_truth_chunks:
        chunk_chars = set(chunk.replace(' ', '').replace('ï¼Œ', '').replace('ã€‚', '').replace('ï¼›', '').replace('ï¼š', ''))
        overlap_chars = query_chars & chunk_chars
        if len(overlap_chars) >= len(query_chars) * 0.3:
            total_relevant_count += 1
    
    # è¨ˆç®—æª¢ç´¢åˆ°çš„ç›¸é—œchunksæ•¸é‡
    retrieved_relevant_count = 0
    for chunk in top_k_chunks:
        chunk_chars = set(chunk.replace(' ', '').replace('ï¼Œ', '').replace('ã€‚', '').replace('ï¼›', '').replace('ï¼š', ''))
        overlap_chars = query_chars & chunk_chars
        if len(overlap_chars) >= len(query_chars) * 0.3:
            retrieved_relevant_count += 1
    
    return retrieved_relevant_count / total_relevant_count if total_relevant_count > 0 else 0


def calculate_faithfulness(chunks: List[str]) -> float:
    """
    è¨ˆç®—å¿ å¯¦åº¦ - è©•ä¼°chunkæ˜¯å¦ä¿æŒå®Œæ•´èªç¾©
    åŸºæ–¼å¥å­å®Œæ•´æ€§ã€æ®µè½é‚Šç•Œç­‰
    """
    if not chunks:
        return 0.0
    
    total_score = 0.0
    
    for chunk in chunks:
        score = 1.0
        
        # æª¢æŸ¥å¥å­å®Œæ•´æ€§
        sentences = re.split(r'[ã€‚ï¼ï¼Ÿ]', chunk)
        incomplete_sentences = sum(1 for s in sentences if s.strip() and not s.endswith(('ã€‚', 'ï¼', 'ï¼Ÿ')))
        if len(sentences) > 1:
            score *= (1.0 - incomplete_sentences / len(sentences))
        
        # æª¢æŸ¥æ®µè½å®Œæ•´æ€§
        if chunk.startswith(('ç¬¬', 'æ¢', 'é …', 'æ¬¾')) and not chunk.endswith(('ã€‚', 'ï¼', 'ï¼Ÿ')):
            score *= 0.8
        
        total_score += score
    
    return total_score / len(chunks)


def calculate_fragmentation_score(chunks: List[str], original_text: str) -> float:
    """
    è¨ˆç®—ç¢ç‰‡åŒ–ç¨‹åº¦ - è©•ä¼°æ–‡æœ¬è¢«åˆ†å‰²çš„ç´°ç¢ç¨‹åº¦
    è¿”å›å€¼è¶Šé«˜è¡¨ç¤ºç¢ç‰‡åŒ–è¶Šåš´é‡
    """
    if not chunks or not original_text:
        return 0.0
    
    # è¨ˆç®—å¹³å‡chunké•·åº¦ç›¸å°æ–¼åŸæ–‡çš„æ¯”ä¾‹
    avg_chunk_length = sum(len(chunk) for chunk in chunks) / len(chunks)
    length_ratio = avg_chunk_length / len(original_text)
    
    # è¨ˆç®—chunkæ•¸é‡
    chunk_count_ratio = len(chunks) / (len(original_text) / 500)  # ä»¥500å­—ç¬¦ç‚ºåŸºæº–
    
    # ç¶œåˆè©•åˆ†
    fragmentation = (1.0 - length_ratio) * 0.6 + chunk_count_ratio * 0.4
    
    return min(1.0, max(0.0, fragmentation))


def generate_questions_with_gemini(text_content: str, num_questions: int, 
                                 question_types: List[str], difficulty_levels: List[str]) -> List[GeneratedQuestion]:
    """
    ä½¿ç”¨Geminiç”Ÿæˆç¹é«”ä¸­æ–‡æ³•å¾‹è€ƒå¤é¡Œ
    åƒè€ƒihoweræ–‡ç« çš„åšæ³•ï¼Œå¾æ–‡æœ¬ä¸­éš¨æ©Ÿé¸æ“‡å…§å®¹ç”Ÿæˆå•é¡Œ
    """
    if not GEMINI_AVAILABLE:
        return generate_questions_fallback(text_content, num_questions)
    
    try:
        # å„ªå…ˆä½¿ç”¨ GOOGLE_API_KEYï¼Œå¦‚æœæ²’æœ‰å‰‡ä½¿ç”¨ GEMINI_API_KEY
        api_key = GOOGLE_API_KEY or os.getenv('GEMINI_API_KEY')
        if not api_key:
            print("è­¦å‘Šï¼šGOOGLE_API_KEY å’Œ GEMINI_API_KEY éƒ½æœªè¨­ç½®ï¼Œä½¿ç”¨å‚™ç”¨æ–¹æ³•")
            return generate_questions_fallback(text_content, num_questions)
        
        cfg = getattr(genai, "configure", None)
        if callable(cfg):
            cfg(api_key=api_key)  # type: ignore[misc]
        ModelCls = getattr(genai, "GenerativeModel", None)
        if ModelCls is None:
            print("è­¦å‘Šï¼šç„¡æ³•ç²å– GenerativeModel é¡ï¼Œä½¿ç”¨å‚™ç”¨æ–¹æ³•")
            return generate_questions_fallback(text_content, num_questions)
        model = ModelCls('gemini-2.0-flash-exp')
        
        # å¾æ–‡æœ¬ä¸­éš¨æ©Ÿé¸æ“‡4000 tokensçš„å…§å®¹ï¼ˆæ¨¡æ“¬ihowerçš„åšæ³•ï¼‰
        import random
        text_chunks = text_content.split('\n')
        random.shuffle(text_chunks)
        
        # é¸æ“‡è¶³å¤ çš„å…§å®¹ä¾†ç”Ÿæˆå•é¡Œ
        selected_content = ""
        current_tokens = 0
        max_tokens = 4000
        
        for chunk in text_chunks:
            if current_tokens + len(chunk) > max_tokens:
                break
            selected_content += chunk + "\n"
            current_tokens += len(chunk)
        
        if not selected_content.strip():
            selected_content = text_content[:2000]  # å‚™ç”¨æ–¹æ¡ˆ
        
        prompt = f"""
ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„æ³•å¾‹æ•™è‚²å°ˆå®¶ï¼Œè«‹æ ¹æ“šä»¥ä¸‹æ³•å¾‹æ–‡æœ¬å…§å®¹ï¼Œç”Ÿæˆ{num_questions}é“ç¹é«”ä¸­æ–‡è€ƒå¤é¡Œã€‚

é‡è¦è¦æ±‚ï¼š
1. æ‰€æœ‰å•é¡Œå¿…é ˆä½¿ç”¨ç¹é«”ä¸­æ–‡ï¼ˆå°ç£ç”¨æ³•ï¼‰
2. å•é¡Œé¡å‹æ‡‰åŒ…å«ï¼š{', '.join(question_types)}ï¼Œéš¨æ©Ÿåˆ†é…ä½†ç¢ºä¿å¤šæ¨£æ€§
3. é›£åº¦ç­‰ç´šæ‡‰åŒ…å«ï¼š{', '.join(difficulty_levels)}ï¼Œéš¨æ©Ÿåˆ†é…ï¼ŒåŸºç¤å•é¡Œèšç„¦å–®ä¸€æ¦‚å¿µï¼Œé€²éšå•é¡Œæ¶‰åŠå¤šæ¦‚å¿µï¼Œæ‡‰ç”¨å•é¡Œæ¨¡æ“¬å¯¦å‹™å ´æ™¯
4. æ¯é“é¡Œç›®éƒ½è¦æ¨™æ˜ç›¸é—œçš„æ³•è¦æ¢æ–‡
5. å•é¡Œæ‡‰è©²åŸºæ–¼æ–‡æœ¬ä¸­çš„å…·é«”å…§å®¹ï¼Œä¸æ˜¯æ³›æ³›è€Œè«‡
6. å•é¡Œæ‡‰è©²æœ‰æ˜ç¢ºçš„ç­”æ¡ˆï¼Œå¯ä»¥åœ¨æ–‡æœ¬ä¸­æ‰¾åˆ°ä¾æ“š

æ ¸å¿ƒè¨­è¨ˆåŸå‰‡ï¼š
7. é‡é»ï¼šé¿å…ç´”ç²¹çš„æ¢æ–‡èƒŒèª¦é¡Œç›®ï¼Œæ”¹ç‚ºå¯¦éš›ç”Ÿæ´»æ¡ˆä¾‹æ‡‰ç”¨é¡Œ
8. å•é¡Œæ‡‰è©²è¨­è¨ˆæˆæƒ…å¢ƒå¼æ¡ˆä¾‹ï¼Œè®“å­¸ç”Ÿæ€è€ƒå¦‚ä½•åœ¨å¯¦éš›ç”Ÿæ´»ä¸­æ‡‰ç”¨æ³•å¾‹æ¦‚å¿µ
9. ä½¿ç”¨ã€Œå¦‚æœ...é‚£éº¼...ã€æˆ–ã€Œç•¶...æ™‚...ã€çš„æƒ…å¢ƒè¨­å®š
10. æä¾›å…·é«”çš„ç”Ÿæ´»å ´æ™¯ï¼ˆå¦‚ï¼šç¶²è·¯ä½¿ç”¨ã€å‰µä½œåˆ†äº«ã€å•†æ¥­æ´»å‹•ç­‰ï¼‰
11. è©¢å•ã€Œæ‡‰è©²å¦‚ä½•è™•ç†ã€ã€ã€Œæ˜¯å¦ç¬¦åˆæ³•å¾‹è¦å®šã€ã€ã€Œæœƒç”¢ç”Ÿä»€éº¼å¾Œæœã€ç­‰
12. é¿å…ç›´æ¥å•ã€Œç¬¬Xæ¢è¦å®šä»€éº¼ã€é€™é¡èƒŒèª¦é¡Œ

æ–‡æœ¬å…§å®¹ï¼š
{selected_content}

è«‹ä»¥JSONæ ¼å¼è¿”å›çµæœï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
{{
  "questions": [
    {{
      "question": "å•é¡Œå…§å®¹",
      "references": ["ç¬¬Xæ¢", "ç¬¬Yæ¢ç¬¬Zé …"],
      "question_type": "æ¡ˆä¾‹æ‡‰ç”¨/æƒ…å¢ƒåˆ†æ/å¯¦å‹™è™•ç†/æ³•å¾‹å¾Œæœ/åˆè¦åˆ¤æ–·",
      "difficulty": "åŸºç¤/é€²éš/æ‡‰ç”¨",
      "keywords": ["é—œéµè©1", "é—œéµè©2"],
      "estimated_tokens": ä¼°ç®—çš„tokenæ•¸é‡
    }}
  ]
}}

è«‹ç¢ºä¿ç”Ÿæˆçš„å•é¡Œéƒ½æ˜¯å¯¦éš›ç”Ÿæ´»æ¡ˆä¾‹æ‡‰ç”¨é¡Œï¼Œé¿å…æ¢æ–‡èƒŒèª¦ï¼Œè®“å­¸ç”Ÿèƒ½å¤ æ€è€ƒå¦‚ä½•åœ¨çœŸå¯¦æƒ…å¢ƒä¸­æ‡‰ç”¨æ³•å¾‹çŸ¥è­˜ã€‚
"""
        
        response = model.generate_content(prompt)
        response_text = response.text.strip()
        
        # è§£æJSONéŸ¿æ‡‰
        try:
            # æ¸…ç†éŸ¿æ‡‰æ–‡æœ¬ï¼Œç§»é™¤å¯èƒ½çš„markdownæ ¼å¼
            if response_text.startswith('```json'):
                response_text = response_text[7:]
            if response_text.endswith('```'):
                response_text = response_text[:-3]
            
            import json
            result = json.loads(response_text)
            
            questions = []
            for q_data in result.get('questions', []):
                question = GeneratedQuestion(
                    question=q_data.get('question', ''),
                    references=q_data.get('references', []),
                    question_type=q_data.get('question_type', ''),
                    difficulty=q_data.get('difficulty', ''),
                    keywords=q_data.get('keywords', []),
                    estimated_tokens=q_data.get('estimated_tokens', 0)
                )
                questions.append(question)
            
            return questions[:num_questions]  # ç¢ºä¿ä¸è¶…éè«‹æ±‚æ•¸é‡
            
        except json.JSONDecodeError as e:
            print(f"JSONè§£æéŒ¯èª¤: {e}")
            print(f"éŸ¿æ‡‰å…§å®¹: {response_text[:500]}...")  # åªé¡¯ç¤ºå‰500å­—ç¬¦
            return generate_questions_fallback(text_content, num_questions)
        
    except Exception as e:
        print(f"Geminiå•é¡Œç”Ÿæˆå¤±æ•—: {e}")
        return generate_questions_fallback(text_content, num_questions)


def generate_questions_fallback(text_content: str, num_questions: int) -> List[GeneratedQuestion]:
    """
    å‚™ç”¨å•é¡Œç”Ÿæˆæ–¹æ³•
    """
    questions = []
    
    # ç°¡å–®çš„æ­£å‰‡è¡¨é”å¼æå–æ³•æ¢
    import re
    articles = re.findall(r'ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒ0-9]+æ¢[^ã€‚]*ã€‚', text_content)
    
    print(f"å‚™ç”¨æ–¹æ³•ï¼šå¾æ–‡æœ¬ä¸­æ‰¾åˆ° {len(articles)} å€‹æ³•æ¢")
    
    # ç”ŸæˆåŸºç¤å•é¡Œ
    question_templates = [
        ("{article}çš„å®šç¾©æ˜¯ä»€éº¼ï¼Ÿ", "å®šç¾©", "åŸºç¤"),
        ("{article}çš„é©ç”¨æ¢ä»¶ç‚ºä½•ï¼Ÿ", "æ¢ä»¶", "åŸºç¤"),
        ("é•å{article}çš„æ³•å¾‹å¾Œæœæ˜¯ä»€éº¼ï¼Ÿ", "å¾Œæœ", "é€²éš"),
        ("{article}çš„ç”³è«‹ç¨‹åºç‚ºä½•ï¼Ÿ", "ç¨‹åº", "é€²éš"),
        ("{article}çš„ä¿è­·æœŸé™æ˜¯å¤šä¹…ï¼Ÿ", "æœŸé™", "åŸºç¤"),
    ]
    
    if articles:
        # å¦‚æœæœ‰æ³•æ¢ï¼ŒåŸºæ–¼æ³•æ¢ç”Ÿæˆå•é¡Œ
        for i in range(min(num_questions, len(articles))):
            article = articles[i % len(articles)]
            template, q_type, difficulty = question_templates[i % len(question_templates)]
            
            # æå–æ¢æ–‡è™Ÿç¢¼
            article_match = re.search(r'ç¬¬([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒ0-9]+)æ¢', article)
            article_num = article_match.group(1) if article_match else str(i+1)
            
            question = GeneratedQuestion(
                question=template.format(article=f"ç¬¬{article_num}æ¢"),
                references=[f"ç¬¬{article_num}æ¢"],
                question_type=q_type,
                difficulty=difficulty,
                keywords=extract_keywords(article, 3),
                estimated_tokens=len(article) + 50
            )
            questions.append(question)
    else:
        # å¦‚æœæ²’æœ‰æ‰¾åˆ°æ³•æ¢ï¼Œç”Ÿæˆé€šç”¨å•é¡Œ
        print("è­¦å‘Šï¼šæ²’æœ‰æ‰¾åˆ°æ³•æ¢ï¼Œç”Ÿæˆé€šç”¨å•é¡Œ")
        generic_questions = [
            "è«‹èªªæ˜æœ¬æ³•å¾‹æ–‡æª”çš„ä¸»è¦å…§å®¹å’Œç›®çš„ï¼Ÿ",
            "æœ¬æ³•å¾‹æ–‡æª”é©ç”¨æ–¼å“ªäº›æƒ…æ³ï¼Ÿ",
            "é•åæœ¬æ³•å¾‹è¦å®šæœƒç”¢ç”Ÿä»€éº¼å¾Œæœï¼Ÿ",
            "å¦‚ä½•ç”³è«‹æœ¬æ³•å¾‹è¦å®šçš„ç›¸é—œæ¬Šåˆ©ï¼Ÿ",
            "æœ¬æ³•å¾‹è¦å®šçš„ä¿è­·æœŸé™æ˜¯å¤šä¹…ï¼Ÿ"
        ]
        
        for i in range(min(num_questions, len(generic_questions))):
            question = GeneratedQuestion(
                question=generic_questions[i],
                references=["ç›¸é—œæ³•æ¢"],
                question_type="åŸºç¤æ¦‚å¿µ",
                difficulty="åŸºç¤",
                keywords=extract_keywords(text_content[:200], 3),
                estimated_tokens=100
            )
            questions.append(question)
    
    print(f"å‚™ç”¨æ–¹æ³•ç”Ÿæˆäº† {len(questions)} å€‹å•é¡Œ")
    return questions


def evaluate_chunk_config(doc: DocRecord, config: ChunkConfig, 
                         test_queries: List[str], k_values: List[int], 
                         strategy: str = "fixed_size") -> EvaluationResult:
    """
    è©•ä¼°å–®å€‹chunké…ç½®
    """
    # æ ¹æ“šç­–ç•¥ç”Ÿæˆchunksï¼Œå‚³éç­–ç•¥ç‰¹å®šåƒæ•¸
    if strategy == "fixed_size":
        chunks = sliding_window_chunks(doc.text, config.chunk_size, config.overlap)
    elif strategy == "hierarchical":
        from .chunking import chunk_text
        chunks = chunk_text(doc.text, strategy="hierarchical", 
                           max_chunk_size=config.chunk_size, 
                           overlap_ratio=config.overlap_ratio,
                           min_chunk_size=config.min_chunk_size,
                           level_depth=config.level_depth)
    elif strategy == "rcts_hierarchical":
        from .chunking import chunk_text
        chunks = chunk_text(doc.text, strategy="rcts_hierarchical", 
                           max_chunk_size=config.chunk_size, 
                           overlap_ratio=config.overlap_ratio,
                           preserve_structure=config.preserve_structure)
    elif strategy == "structured_hierarchical":
        from .chunking import chunk_text
        chunks = chunk_text(doc.text, strategy="structured_hierarchical", 
                           json_data=doc.json_data, 
                           max_chunk_size=config.chunk_size, 
                           overlap_ratio=config.overlap_ratio,
                           chunk_by=config.chunk_by)
    elif strategy == "semantic":
        from .chunking import chunk_text
        chunks = chunk_text(doc.text, strategy="semantic", 
                           max_chunk_size=config.chunk_size, 
                           similarity_threshold=config.similarity_threshold,
                           context_window=config.context_window,
                           overlap_ratio=config.overlap_ratio)
    elif strategy == "sliding_window":
        from .chunking import chunk_text
        chunks = chunk_text(doc.text, strategy="sliding_window", 
                           window_size=config.window_size, 
                           step_size=config.step_size,
                           overlap_ratio=config.overlap_ratio,
                           boundary_aware=config.boundary_aware,
                           min_chunk_size_sw=config.min_chunk_size_sw,
                           max_chunk_size_sw=config.max_chunk_size_sw,
                           preserve_sentences=config.preserve_sentences)
    elif strategy == "llm_semantic":
        from .chunking import chunk_text
        chunks = chunk_text(doc.text, strategy="llm_semantic", 
                           max_chunk_size=config.chunk_size, 
                           semantic_threshold=config.semantic_threshold,
                           context_window=config.context_window,
                           overlap_ratio=config.overlap_ratio)
    elif strategy == "hybrid":
        from .chunking import chunk_text
        chunks = chunk_text(doc.text, strategy="hybrid", 
                           primary_size=config.chunk_size, 
                           secondary_size=config.secondary_size,
                           switch_threshold=config.switch_threshold,
                           overlap_ratio=config.overlap_ratio)
    else:
        # é»˜èªä½¿ç”¨å›ºå®šå¤§å°åˆ†å¡Š
        chunks = sliding_window_chunks(doc.text, config.chunk_size, config.overlap)

    # è¨ˆç®—åŸºæœ¬çµ±è¨ˆ
    chunk_count = len(chunks)
    avg_chunk_length = sum(len(c) for c in chunks) / chunk_count if chunk_count else 0.0
    lengths = [len(c) for c in chunks]
    length_variance = (
        sum((l - avg_chunk_length) ** 2 for l in lengths) / chunk_count if chunk_count else 0.0
    )

    # ä½¿ç”¨TF-IDFç‚ºæ¯å€‹æŸ¥è©¢åšæª¢ç´¢æ‰“åˆ†ï¼ˆä¸­æ–‡ç”¨è‡ªå®šç¾©åˆ†è©ï¼‰
    def to_tokens(s: str) -> str:
        toks = preprocess_text(s)
        return " ".join(toks) if toks else s

    processed_chunks = [to_tokens(c) for c in chunks]
    # è‹¥æ–‡æª”éçŸ­ï¼Œé¿å…vectorizerå ±éŒ¯
    if not processed_chunks:
        processed_chunks = [""]

    vectorizer = TfidfVectorizer(
        analyzer="word",
        token_pattern=r"[^\s]+",
        max_features=5000,
        min_df=1,
        max_df=0.98,
        ngram_range=(1, 2),
    )
    X = vectorizer.fit_transform(processed_chunks)

    retrieval_results: Dict[str, List[Dict]] = {}
    precision_at_k_scores: Dict[int, List[float]] = {k: [] for k in k_values}
    recall_at_k_scores: Dict[int, List[float]] = {k: [] for k in k_values}
    precision_omega_scores: List[float] = []

    def compute_pr(retrieved_indices: List[int], relevant_set: set[int], k: int) -> Tuple[float, float]:
        if k <= 0:
            return 0.0, 0.0
        topk = retrieved_indices[:k]
        hit = sum(1 for i in topk if i in relevant_set)
        precision = hit / k
        recall = hit / max(1, len(relevant_set))
        return precision, recall

    max_k = max(k_values) if k_values else 10

    for query in test_queries:
        q = to_tokens(query)
        if not q.strip():
            q = query or ""

        q_vec = vectorizer.transform([q])
        # ä½™å¼¦ç›¸ä¼¼åº¦
        sims = cosine_similarity(q_vec, X).ravel()
        ranked_idx = sims.argsort()[::-1].tolist()

        # å®šç¾©ç›¸é—œé›†ï¼šåˆ†æ•¸é”åˆ°æœ€ä½³åˆ†æ•¸çš„æŸä¸€æ¯”ä¾‹é–¾å€¼ï¼ˆä¾‹å¦‚0.7ï¼‰ä¸”>0
        best = float(sims[ranked_idx[0]]) if ranked_idx else 0.0
        threshold = best * 0.7 if best > 0 else 0.0
        relevant_set = {i for i, s in enumerate(sims) if s >= threshold and s > 0}
        # é˜²æ­¢ç©ºé›†åˆå°è‡´recallç„¡æ„ç¾©ï¼Œè‹¥å…¨éƒ¨ç‚º0åˆ†ï¼Œå‰‡èªç‚ºæ²’æœ‰ç›¸é—œæ–‡æª”
        # è‹¥åªæœ‰æ¥µå°‘æ•¸éé›¶ï¼Œè‡³å°‘ä¿ç•™top1ç‚ºç›¸é—œ
        if best > 0 and not relevant_set:
            relevant_set = {ranked_idx[0]}

        # ä¿å­˜å‰max_kå€‹æª¢ç´¢çµæœä¾›å¯©æŸ¥
        retrieval_results[query] = [
            {
                "chunk_index": i,
                "score": float(sims[i]),
                "content": (chunks[i][:200] + "...") if len(chunks[i]) > 200 else chunks[i],
            }
            for i in ranked_idx[:max_k]
        ]

        # æŒ‡æ¨™è¨ˆç®—
        for k in k_values:
            p, r = compute_pr(ranked_idx, relevant_set, k)
            precision_at_k_scores[k].append(p)
            recall_at_k_scores[k].append(r)

        # PrecisionÎ©: ç†æƒ³æƒ…æ³ä¸‹ï¼ˆæœ€å„ªæ’åºï¼‰åœ¨k=max_kæ™‚å¯é”åˆ°çš„ç²¾åº¦
        # = min(|R|, max_k) / max_k
        precision_omega_scores.append(
            min(len(relevant_set), max_k) / max_k if max_k > 0 else 0.0
        )

    # èšåˆå¹³å‡
    avg_precision_omega = sum(precision_omega_scores) / len(precision_omega_scores) if precision_omega_scores else 0.0
    avg_precision_at_k = {k: (sum(v) / len(v) if v else 0.0) for k, v in precision_at_k_scores.items()}
    avg_recall_at_k = {k: (sum(v) / len(v) if v else 0.0) for k, v in recall_at_k_scores.items()}

    metrics = EvaluationMetrics(
        precision_omega=avg_precision_omega,
        precision_at_k=avg_precision_at_k,
        recall_at_k=avg_recall_at_k,
        chunk_count=chunk_count,
        avg_chunk_length=avg_chunk_length,
        length_variance=length_variance,
    )

    # å‰µå»ºè©³ç´°çš„é…ç½®ä¿¡æ¯ï¼ŒåŒ…å«æ‰€æœ‰ç­–ç•¥ç‰¹å®šåƒæ•¸
    detailed_config = {
        "chunk_size": config.chunk_size,
        "overlap": config.overlap,
        "overlap_ratio": config.overlap_ratio,
        "strategy": strategy,
    }
    
    # æ ¹æ“šç­–ç•¥æ·»åŠ ç‰¹å®šåƒæ•¸
    if strategy == "structured_hierarchical":
        detailed_config["chunk_by"] = config.chunk_by
    elif strategy == "rcts_hierarchical":
        detailed_config["preserve_structure"] = config.preserve_structure
    elif strategy == "hierarchical":
        detailed_config["level_depth"] = config.level_depth
        detailed_config["min_chunk_size"] = config.min_chunk_size
    elif strategy == "semantic":
        detailed_config["similarity_threshold"] = config.similarity_threshold
        detailed_config["context_window"] = config.context_window
    elif strategy == "llm_semantic":
        detailed_config["semantic_threshold"] = config.semantic_threshold
        detailed_config["context_window"] = config.context_window
    elif strategy == "sliding_window":
        detailed_config["window_size"] = config.window_size
        detailed_config["step_size"] = config.step_size
        detailed_config["boundary_aware"] = config.boundary_aware
        detailed_config["preserve_sentences"] = config.preserve_sentences
        detailed_config["min_chunk_size_sw"] = config.min_chunk_size_sw
        detailed_config["max_chunk_size_sw"] = config.max_chunk_size_sw
    elif strategy == "hybrid":
        detailed_config["switch_threshold"] = config.switch_threshold
        detailed_config["secondary_size"] = config.secondary_size

    return EvaluationResult(
        config=detailed_config,
        metrics=metrics,
        test_queries=test_queries,
        retrieval_results=retrieval_results,
        timestamp=datetime.now(),
    )


@app.post("/api/chunk")
def chunk(req: ChunkRequest):
    doc = store.docs.get(req.doc_id)
    if not doc:
        return JSONResponse(status_code=404, content={"error": "doc not found"})
    
    # å°å…¥æ–°çš„chunkingæ¨¡çµ„
    from .chunking import chunk_text
    
    # æ ¹æ“šä¸åŒç­–ç•¥é€²è¡Œåˆ†å¡Š
    strategy = req.strategy
    use_json_structure = req.use_json_structure
    
    # å¦‚æœå•Ÿç”¨JSONçµæ§‹åŒ–åˆ†å‰²ä¸”æœ‰JSONæ•¸æ“šï¼Œå„ªå…ˆä½¿ç”¨JSONçµæ§‹åŒ–åˆ†å‰²
    if use_json_structure and doc.json_data:
        structured_chunks = json_structured_chunks(doc.json_data, req.chunk_size, req.overlap)
        # æå–ç´”æ–‡æœ¬chunksç”¨æ–¼å¾ŒçºŒè™•ç†
        chunks = [chunk["content"] for chunk in structured_chunks]
        # å­˜å„²çµæ§‹åŒ–chunksåˆ°æ–‡æª”ä¸­
        doc.structured_chunks = structured_chunks
    else:
        # ä½¿ç”¨æ–°çš„chunkingæ¨¡çµ„
        chunk_kwargs = {
            "chunk_size": req.chunk_size,
            "overlap": req.overlap,
        }
        
        # æ ¹æ“šç­–ç•¥æ·»åŠ ç‰¹å®šåƒæ•¸
        if strategy == 'hierarchical' and req.hierarchical_params:
            chunk_kwargs.update({
                "max_chunk_size": req.chunk_size,
                "min_chunk_size": req.hierarchical_params.get('min_chunk_size', req.chunk_size // 2),
                "overlap_ratio": req.overlap / req.chunk_size if req.chunk_size > 0 else 0.1,
                "level_depth": req.hierarchical_params.get('level_depth', 2)
            })
        elif strategy == 'rcts_hierarchical' and req.rcts_hierarchical_params:
            chunk_kwargs.update({
                "max_chunk_size": req.chunk_size,
                "overlap_ratio": req.rcts_hierarchical_params.get('overlap_ratio', 0.1),
                "preserve_structure": req.rcts_hierarchical_params.get('preserve_structure', True)
            })
        elif strategy == 'structured_hierarchical' and req.structured_hierarchical_params:
            chunk_kwargs.update({
                "max_chunk_size": req.chunk_size,
                "overlap_ratio": req.structured_hierarchical_params.get('overlap_ratio', 0.1),
                "chunk_by": req.structured_hierarchical_params.get('chunk_by', 'article')
            })
        elif strategy == 'adaptive' and req.adaptive_params:
            chunk_kwargs.update({
                "target_size": req.chunk_size,
                "tolerance": req.adaptive_params.get('tolerance', req.chunk_size // 10),
                "semantic_threshold": req.adaptive_params.get('semantic_threshold', 0.7)
            })
        elif strategy == 'hybrid' and req.hybrid_params:
            chunk_kwargs.update({
                "primary_size": req.chunk_size,
                "secondary_size": req.hybrid_params.get('secondary_size', req.chunk_size // 2),
                "switch_threshold": req.hybrid_params.get('switch_threshold', 0.8)
            })
        elif strategy == 'semantic' and req.semantic_params:
            chunk_kwargs.update({
                "target_size": req.chunk_size,
                "similarity_threshold": req.semantic_params.get('similarity_threshold', 0.6),
                "context_window": req.semantic_params.get('context_window', 100)
            })
        
        # ä½¿ç”¨æ–°çš„chunkingæ¨¡çµ„
        chunks = chunk_text(doc.text, strategy=strategy, json_data=doc.json_data, **chunk_kwargs)
        
        # æ¸…ç©ºçµæ§‹åŒ–chunks
        doc.structured_chunks = []
    
    # è¨ˆç®—è©³ç´°æŒ‡æ¨™
    chunk_lengths = [len(chunk) for chunk in chunks]
    avg_length = sum(chunk_lengths) / len(chunk_lengths) if chunks else 0
    length_variance = 0
    if len(chunk_lengths) > 1:
        variance = sum((length - avg_length) ** 2 for length in chunk_lengths) / len(chunk_lengths)
        length_variance = variance / avg_length if avg_length > 0 else 0
    
    doc.chunks = chunks
    doc.chunk_size = req.chunk_size
    doc.overlap = req.overlap
    # invalidates embeddings for safety
    store.reset_embeddings()
    
    return {
        "doc_id": doc.id, 
        "num_chunks": len(chunks), 
        "chunk_size": req.chunk_size, 
        "overlap": req.overlap,
        "strategy": strategy,
        "sample": chunks[:3],  # å‰3å€‹chunksä½œç‚ºé è¦½
        "all_chunks": chunks,  # æ‰€æœ‰chunks
        "metrics": {
            "avg_length": round(avg_length, 2),
            "length_variance": round(length_variance, 3),
            "min_length": min(chunk_lengths) if chunks else 0,
            "max_length": max(chunk_lengths) if chunks else 0,
            "overlap_rate": req.overlap / req.chunk_size if req.chunk_size > 0 else 0
        }
    }


def clean_text_for_gemini(text: str) -> str:
    """æ¸…ç†æ–‡æœ¬ä»¥ç¬¦åˆGemini APIè¦æ±‚"""
    import re
    
    # ç§»é™¤æ§åˆ¶å­—ç¬¦
    text = re.sub(r'[\x00-\x08\x0b\x0c\x0e-\x1f\x7f]', '', text)
    
    # ç§»é™¤å…¨è§’ç‰¹æ®Šå­—ç¬¦ï¼ˆä½†ä¿ç•™ä¸­æ–‡å¸¸ç”¨æ ‡ç‚¹ï¼‰
    # ç§»é™¤ã€ã€‘ã€ã€–ã€—ã€ã€ã€ç­‰å…¨è§’æ–¹æ‹¬å·å’Œç‰¹æ®Šç¬¦å·
    text = re.sub(r'[ã€ã€‘ã€–ã€—ã€ã€ï¼»ï¼½ã€”ã€•ï½›ï½ã€ˆã€‰ã€Šã€‹ã€Œã€ã€ã€]', '', text)
    
    # å¤„ç†æ³•å¾‹æ–‡æ¡£çš„ç‰¹æ®Šæ ¼å¼
    # ç§»é™¤è¿‡å¤šçš„æ¢è¡Œç¬¦ï¼Œä½†ä¿ç•™æ®µè½ç»“æ„
    text = re.sub(r'\n\s*\n\s*\n+', '\n\n', text)
    
    # ç§»é™¤å¤šä½™çš„ç©ºæ ¼
    text = re.sub(r' +', ' ', text)
    
    # ç§»é™¤è¡Œé¦–è¡Œå°¾ç©ºæ ¼
    text = re.sub(r'^\s+|\s+$', '', text, flags=re.MULTILINE)
    
    # ç¡®ä¿æ–‡æœ¬ä¸ä¼šå¤ªé•¿
    if len(text) > 8000:
        # å°è¯•åœ¨åˆé€‚çš„ä½ç½®æˆªæ–­ï¼ˆå¦‚æ®µè½è¾¹ç•Œï¼‰
        truncated = text[:8000]
        last_paragraph = truncated.rfind('\n\n')
        if last_paragraph > 6000:  # å¦‚æœæœ€åä¸€ä¸ªæ®µè½ä¸å¤ªè¿œ
            text = truncated[:last_paragraph]
        else:
            text = truncated
    
    return text.strip()


async def embed_gemini(texts: List[str]) -> List[List[float]]:
    """Call Google Generative API (Gemini) embeddings endpoint using API key.

    Note: This uses the REST endpoint pattern with the API key in query params.
    """
    if not httpx:
        raise RuntimeError("httpx not available")
    if not GOOGLE_API_KEY:
        raise RuntimeError("GOOGLE_API_KEY not set")
    # Gemini embedding model: gemini-embedding-001 (ç¶­åº¦å¯é…ç½®: 128-3072)
    model = "gemini-embedding-001"
    # ä½¿ç”¨æ­£ç¢ºçš„ API ç«¯é»æ ¼å¼
    url = f"https://generativelanguage.googleapis.com/v1beta/models/{model}:embedContent"
    headers = {
        "x-goog-api-key": GOOGLE_API_KEY,
        "Content-Type": "application/json"
    }
    out: List[List[float]] = []
    total_texts = len(texts)
    print(f"ğŸ”§ é–‹å§‹Gemini embeddingè™•ç†ï¼Œå…± {total_texts} å€‹æ–‡æœ¬")
    
    async with httpx.AsyncClient(timeout=60) as client:
        # é€å€‹è™•ç†æ–‡æœ¬ï¼ˆGemini API éœ€è¦å–®å€‹è«‹æ±‚ï¼‰
        for i, text in enumerate(texts):
            try:
                # ä½¿ç”¨å°ˆé–€çš„æ–‡æœ¬æ¸…ç†å‡½æ•¸
                original_text = text
                text = clean_text_for_gemini(text)
                
                # å¦‚æœæ–‡æœ¬ç‚ºç©ºæˆ–éçŸ­ï¼Œè·³é
                if len(text.strip()) < 10:
                    print(f"âš ï¸ æ–‡æœ¬éçŸ­æˆ–ç‚ºç©ºï¼Œè·³éè™•ç†")
                    import numpy as np
                    fallback_vector = np.random.randn(EMBEDDING_DIMENSION).astype(np.float32).tolist()
                    out.append(fallback_vector)
                    continue
                
                payload = {
                    "model": f"models/{model}",
                    "content": {"parts": [{"text": text}]},
                    "output_dimensionality": EMBEDDING_DIMENSION  # ä½¿ç”¨å…¨å±€é…ç½®çš„ç¶­åº¦
                }
                
                r = await client.post(url, headers=headers, json=payload)
                
                if r.status_code == 400:
                    print(f"âŒ Gemini API 400éŒ¯èª¤ï¼Œå˜—è©¦æ¸…ç†æ–‡æœ¬...")
                    # å°è¯•è¯»å–é”™è¯¯è¯¦æƒ…
                    try:
                        error_data = r.json()
                        print(f"âŒ APIéŒ¯èª¤è©³æƒ…: {error_data}")
                    except:
                        print(f"âŒ APIéŒ¯èª¤éŸ¿æ‡‰: {r.text[:200]}")
                    
                    # å°è¯•æ›´æ¿€è¿›çš„æ–‡æœ¬æ¸…ç†ï¼ˆä¿ç•™ä¸­æ–‡å­—ç¬¦å’Œå¸¸ç”¨æ ‡ç‚¹ï¼‰
                    # åªç§»é™¤å¯èƒ½å¼•èµ·é—®é¢˜çš„ç‰¹æ®Šå­—ç¬¦
                    cleaned_text = re.sub(r'[^\w\s\u4e00-\u9fff\u3000-\u303f\uff00-\uffef\u3001\u3002\u300a\u300b\u300c\u300d\u300e\u300f\u2018\u2019\u201c\u201d]', ' ', text)
                    cleaned_text = re.sub(r'\s+', ' ', cleaned_text).strip()
                    
                    if len(cleaned_text) > 10:
                        payload["content"]["parts"][0]["text"] = cleaned_text
                        r = await client.post(url, headers=headers, json=payload)
                        
                        if r.status_code == 400:
                            print(f"âŒ æ¸…ç†å¾Œä»å¤±æ•—ï¼Œæ‹‹å‡ºç•°å¸¸è€Œä¸æ˜¯ä½¿ç”¨fallbackå‘é‡")
                            print(f"âŒ åŸå§‹æ–‡æœ¬å‰100å­—ç¬¦: {original_text[:100]}")
                            print(f"âŒ æ¸…ç†å¾Œæ–‡æœ¬å‰100å­—ç¬¦: {cleaned_text[:100]}")
                            # ä¸å†ä½¿ç”¨fallbackå‘é‡ï¼Œè€Œæ˜¯æŠ›å‡ºå¼‚å¸¸
                            raise RuntimeError(f"Gemini APIè¿”å›400éŒ¯èª¤ï¼Œç„¡æ³•è™•ç†æ–‡æœ¬ã€‚åŸå§‹æ–‡æœ¬å‰100å­—ç¬¦: {original_text[:100]}")
                    else:
                        print(f"âŒ æ¸…ç†å¾Œæ–‡æœ¬éçŸ­ï¼Œæ‹‹å‡ºç•°å¸¸")
                        raise RuntimeError(f"æ¸…ç†å¾Œæ–‡æœ¬éçŸ­ï¼ˆ{len(cleaned_text)}å­—ç¬¦ï¼‰ï¼Œç„¡æ³•ç”Ÿæˆembedding")
                
                r.raise_for_status()
                data = r.json()
                
                # èª¿è©¦ï¼šæ‰“å°å®Œæ•´çš„APIéŸ¿æ‡‰çµæ§‹
                if i == 0:  # åªåœ¨ç¬¬ä¸€æ¬¡æ‰“å°
                    print(f"ğŸ“‹ Gemini APIéŸ¿æ‡‰çµæ§‹: {list(data.keys())}")
                    if "embedding" in data:
                        print(f"ğŸ“‹ Embeddingçµæ§‹: {list(data['embedding'].keys())}")
                
                # æ ¹æ“šå®˜æ–¹æ–‡æª”ï¼ŒéŸ¿æ‡‰æ ¼å¼æ˜¯ {"embedding": {"values": [...]}}
                embedding_values = data.get("embedding", {}).get("values", [])
                
                if not embedding_values:
                    print(f"âŒ ç²å–åˆ°çš„embeddingç‚ºç©ºï¼Œä½¿ç”¨fallbackå‘é‡")
                    print(f"âŒ å®Œæ•´éŸ¿æ‡‰: {data}")
                    import numpy as np
                    fallback_vector = np.random.randn(EMBEDDING_DIMENSION).astype(np.float32).tolist()
                    out.append(fallback_vector)
                else:
                    # èª¿è©¦ï¼šæ‰“å°å¯¦éš›è¿”å›çš„ç¶­åº¦
                    actual_dimension = len(embedding_values)
                    if i == 0:  # åªåœ¨ç¬¬ä¸€æ¬¡æ‰“å°
                        print(f"âœ… Geminiè¿”å›çš„å‘é‡ç¶­åº¦: {actual_dimension}")
                    if actual_dimension != EMBEDDING_DIMENSION:
                        print(f"âš ï¸ è­¦å‘Šï¼šGeminiè¿”å›çš„å‘é‡ç¶­åº¦ç‚º {actual_dimension}ï¼Œèˆ‡é…ç½®çš„{EMBEDDING_DIMENSION}ä¸åŒ")
                        print(f"âš ï¸ é€™å¯èƒ½æœƒå°è‡´èˆ‡ä¹‹å‰å­˜å„²çš„embeddingç¶­åº¦ä¸åŒ¹é…")
                    out.append(embedding_values)
                
                # é¡¯ç¤ºé€²åº¦
                progress = ((i + 1) / total_texts) * 100
                print(f"ğŸ“Š Gemini embeddingé€²åº¦: {i + 1}/{total_texts} ({progress:.1f}%)")
                
            except Exception as e:
                print(f"âŒ è™•ç†ç¬¬{i+1}å€‹æ–‡æœ¬æ™‚å‡ºéŒ¯: {e}")
                print(f"âŒ éŒ¯èª¤æ–‡æœ¬å‰100å­—ç¬¦: {text[:100] if 'text' in locals() else 'N/A'}")
                
                # å˜—è©¦ç²å–æ›´è©³ç´°çš„éŒ¯èª¤ä¿¡æ¯
                if hasattr(e, 'response') and hasattr(e.response, 'text'):
                    try:
                        error_detail = e.response.json()
                        print(f"âŒ APIéŒ¯èª¤è©³æƒ…: {error_detail}")
                    except:
                        print(f"âŒ APIéŒ¯èª¤éŸ¿æ‡‰: {e.response.text[:200]}")
                
                # ä½¿ç”¨éš¨æ©Ÿå‘é‡ä½œç‚ºfallback
                import numpy as np
                fallback_vector = np.random.randn(EMBEDDING_DIMENSION).astype(np.float32).tolist()
                out.append(fallback_vector)
                continue
    
    print(f"âœ… Gemini embeddingå®Œæˆï¼Œå…±è™•ç† {len(out)} å€‹å‘é‡")
    return out


def embed_bge_m3(texts: List[str]) -> List[List[float]]:
    """ä½¿ç”¨ BGE-M3 æ¨¡å‹é€²è¡Œ embedding"""
    if not SENTENCE_TRANSFORMERS_AVAILABLE:
        raise RuntimeError("sentence-transformers not available")
    
    try:
        total_texts = len(texts)
        print(f"ğŸ”§ é–‹å§‹BGE-M3 embeddingè™•ç†ï¼Œå…± {total_texts} å€‹æ–‡æœ¬")
        
        # è¼‰å…¥ BGE-M3 æ¨¡å‹
        model = SentenceTransformer('BAAI/bge-m3')
        
        # æ‰¹é‡è™•ç†æ–‡æœ¬
        embeddings = model.encode(texts, batch_size=32, show_progress_bar=True)
        
        # è½‰æ›ç‚ºåˆ—è¡¨æ ¼å¼
        result = embeddings.tolist()
        print(f"âœ… BGE-M3 embeddingå®Œæˆï¼Œå…±è™•ç† {len(result)} å€‹å‘é‡")
        return result
        
    except Exception as e:
        raise RuntimeError(f"BGE-M3 embedding failed: {e}")


@app.post("/api/embed")
async def embed(req: EmbedRequest):
    print(f"ğŸ” Embedå‡½æ•°è¢«è°ƒç”¨ï¼Œè¯·æ±‚: {req}")
    
    # æ”¶é›†é¸å®šæ–‡æª”çš„chunks
    # å¦‚æœæ²’æœ‰æŒ‡å®šdoc_idsï¼Œåªé¸æ“‡ä½¿ç”¨structured_hierarchicalç­–ç•¥çš„æœ€è¿‘æ–‡æª”
    requested_doc_ids = req.doc_ids
    if requested_doc_ids:
        # å³ä½¿æŒ‡å®šäº†doc_idsï¼Œä¹Ÿè¦æŒ‰æ–‡ä»¶åå»é‡ï¼Œé¿å…é‡è¤‡embeddingåŒåæ–‡æª”
        candidates = []
        for doc_id in requested_doc_ids:
            doc = store.docs.get(doc_id)
            if doc:
                candidates.append((doc_id, doc))
        
        if not candidates:
            return JSONResponse(
                status_code=400,
                content={"error": "æŒ‡å®šçš„æ–‡æª”IDä¸å­˜åœ¨"}
            )
        
        # æŒ‰æ–‡ä»¶åå»é‡ï¼Œåªé¸æ“‡æ¯å€‹æ–‡ä»¶åçš„ç¬¬ä¸€å€‹æ–‡æª”ï¼ˆæˆ–è€…chunksæœ€å¤šçš„ï¼‰
        filename_to_doc = {}  # {filename: (doc_id, doc, chunk_count)}
        for doc_id, doc in candidates:
            # å„ªå…ˆä½¿ç”¨structured_chunksè¨ˆç®—chunkæ•¸é‡
            chunk_count = len(doc.structured_chunks) if doc.structured_chunks else (len(doc.chunks) if doc.chunks else 0)
            if doc.filename not in filename_to_doc:
                filename_to_doc[doc.filename] = (doc_id, doc, chunk_count)
            else:
                existing_count = filename_to_doc[doc.filename][2]
                if chunk_count > existing_count:
                    print(f"ğŸ”„ ç™¼ç¾æ›´æ–°çš„æ–‡æª” {doc.filename}: {chunk_count} > {existing_count} chunks")
                    filename_to_doc[doc.filename] = (doc_id, doc, chunk_count)
                else:
                    print(f"âš ï¸ è·³éé‡è¤‡æ–‡æª” {doc.filename} (doc_id: {doc_id})ï¼Œå·²é¸æ“‡chunksæ›´å¤šçš„ç‰ˆæœ¬")
        
        selected = [doc_id for doc_id, _, _ in filename_to_doc.values()]
        if len(selected) < len(requested_doc_ids):
            print(f"âš ï¸ å»é‡å¾Œï¼Œå¾ {len(requested_doc_ids)} å€‹æŒ‡å®šçš„æ–‡æª”ä¸­é¸æ“‡äº† {len(selected)} å€‹æ–‡æª”")
    else:
        # åªé¸æ“‡ä½¿ç”¨structured_hierarchicalæˆ–multi_level_structuredç­–ç•¥çš„æ–‡æª”
        candidates = [
            (doc_id, doc) for doc_id, doc in store.docs.items()
            if doc and getattr(doc, 'chunking_strategy', None) in ['structured_hierarchical', 'multi_level_structured']
        ]
        
        if not candidates:
            return JSONResponse(
                status_code=400,
                content={"error": "æ²’æœ‰æ‰¾åˆ°ä½¿ç”¨structured_hierarchicalç­–ç•¥çš„æ–‡æª”ã€‚è«‹å…ˆé€²è¡Œå¤šå±¤ç´šçµæ§‹åŒ–åˆ†å¡Šã€‚"}
            )
        
        # æŒ‰æ–‡ä»¶åå»é‡ï¼Œåªé¸æ“‡æ¯å€‹æ–‡ä»¶åçš„ç¬¬ä¸€å€‹æ–‡æª”ï¼ˆæˆ–è€…chunksæœ€å¤šçš„ï¼‰
        filename_to_doc = {}  # {filename: (doc_id, doc, chunk_count)}
        for doc_id, doc in candidates:
            # å„ªå…ˆä½¿ç”¨structured_chunksè¨ˆç®—chunkæ•¸é‡
            chunk_count = len(doc.structured_chunks) if doc.structured_chunks else (len(doc.chunks) if doc.chunks else 0)
            if doc.filename not in filename_to_doc:
                filename_to_doc[doc.filename] = (doc_id, doc, chunk_count)
            else:
                existing_count = filename_to_doc[doc.filename][2]
                if chunk_count > existing_count:
                    print(f"ğŸ”„ ç™¼ç¾æ›´æ–°çš„æ–‡æª” {doc.filename}: {chunk_count} > {existing_count} chunks")
                    filename_to_doc[doc.filename] = (doc_id, doc, chunk_count)
        
        selected = [doc_id for doc_id, _, _ in filename_to_doc.values()]
        print(f"ğŸ” æœªæŒ‡å®šdoc_idsï¼Œè‡ªå‹•é¸æ“‡ {len(selected)} å€‹ä½¿ç”¨structured_hierarchicalç­–ç•¥çš„æ–‡æª”ï¼ˆå·²å»é‡ï¼‰: {[store.docs[d].filename for d in selected]}")
    
    all_chunks: List[str] = []
    chunk_doc_ids: List[str] = []
    chunk_ids: List[str] = []
    
    # å„ªå…ˆä½¿ç”¨structured_chunksï¼Œå¦‚æœæ²’æœ‰æ‰ä½¿ç”¨doc.chunks
    for doc_id in selected:
        doc = store.docs.get(doc_id)
        if not doc:
            continue
        
        # å„ªå…ˆä½¿ç”¨structured_chunksï¼ˆå¯¦éš›é¡¯ç¤ºçš„428å€‹chunksï¼‰
        if doc.structured_chunks:
            print(f"âœ… ä½¿ç”¨æ–‡æª” {doc.filename} çš„structured_chunksï¼ˆ{len(doc.structured_chunks)}å€‹chunksï¼‰")
            for i, chunk_data in enumerate(doc.structured_chunks):
                if isinstance(chunk_data, dict):
                    content = chunk_data.get('content', '')
                else:
                    content = str(chunk_data)
                
                if content:
                    all_chunks.append(content)
                    chunk_doc_ids.append(doc.id)
                    chunk_id = chunk_data.get('chunk_id', '') if isinstance(chunk_data, dict) else f"{doc.id}_{i}"
                    chunk_ids.append(chunk_id if chunk_id else f"{doc.id}_{i}")
        elif doc.chunks:
            # å›é€€åˆ°èˆŠçš„doc.chunks
            print(f"âš ï¸ æ–‡æª” {doc.filename} æ²’æœ‰structured_chunksï¼Œä½¿ç”¨doc.chunksï¼ˆ{len(doc.chunks)}å€‹chunksï¼‰")
            all_chunks.extend(doc.chunks)
            chunk_doc_ids.extend([doc.id] * len(doc.chunks))
            # ç”Ÿæˆchunk_id
            for i in range(len(doc.chunks)):
                chunk_ids.append(f"{doc.id}_{i}")

    if not all_chunks:
        return JSONResponse(status_code=400, content={"error": "no chunks to embed"})
    
    # æ‰“å°çµ±è¨ˆä¿¡æ¯
    print(f"ğŸ“Š Embeddingçµ±è¨ˆ: å°‡ç‚º {len(selected)} å€‹æ–‡æª”é€²è¡Œembeddingï¼Œå…± {len(all_chunks)} å€‹chunks")
    for doc_id in selected:
        doc = store.docs.get(doc_id)
        if doc:
            chunk_count = len([c for c in chunk_doc_ids if c == doc_id])
            print(f"   æ–‡æª” {doc.filename}: {chunk_count} å€‹chunks")

    # èª¿è©¦ä¿¡æ¯
    print(f"ğŸ” Embedding èª¿è©¦ä¿¡æ¯:")
    print(f"   USE_GEMINI_EMBEDDING: {USE_GEMINI_EMBEDDING}")
    print(f"   GOOGLE_API_KEY: {'å·²è¨­ç½®' if GOOGLE_API_KEY else 'æœªè¨­ç½®'}")
    print(f"   USE_BGE_M3_EMBEDDING: {USE_BGE_M3_EMBEDDING}")
    print(f"   SENTENCE_TRANSFORMERS_AVAILABLE: {SENTENCE_TRANSFORMERS_AVAILABLE}")
    print(f"ğŸ¯ å¯¦é©—çµ„Açµ±ä¸€ä½¿ç”¨ {EMBEDDING_DIMENSION} ç¶­ç´¢å¼•")
    print(f"ğŸ“Š ç•¶å‰EMBEDDING_DIMENSIONé…ç½®: {EMBEDDING_DIMENSION}")
    
    # å˜—è©¦ä½¿ç”¨ Gemini embeddingï¼ˆä¸»è¦é¸é …ï¼‰
    if USE_GEMINI_EMBEDDING and GOOGLE_API_KEY:
        try:
            vectors = await embed_gemini(all_chunks)
            # ä½¿ç”¨å¯¦éš›å‘é‡ç¶­åº¦ï¼Œå¦‚æœç‚ºç©ºå‰‡ä½¿ç”¨å…¨å±€é…ç½®
            dimension = len(vectors[0]) if vectors and len(vectors) > 0 else EMBEDDING_DIMENSION
            print(f"ğŸ“Š æª¢æ¸¬åˆ°embeddingç¶­åº¦: {dimension} (é…ç½®: {EMBEDDING_DIMENSION})")
            
            # é©—è­‰ç¶­åº¦ä¸€è‡´æ€§
            if dimension != EMBEDDING_DIMENSION:
                print(f"âš ï¸ è­¦å‘Šï¼šå¯¦éš›embeddingç¶­åº¦({dimension})èˆ‡é…ç½®({EMBEDDING_DIMENSION})ä¸åŒ")
            
            # æ¸…é™¤èˆŠç´¢å¼•ï¼ˆå¦‚æœç¶­åº¦ä¸åŒï¼‰ï¼Œç¢ºä¿ä¸€è‡´æ€§
            if faiss_store.has_vectors() and faiss_store.dimension != dimension:
                print(f"âš ï¸ æª¢æ¸¬åˆ°èˆŠç´¢å¼•ç¶­åº¦({faiss_store.dimension})èˆ‡æ–°embeddingç¶­åº¦({dimension})ä¸åŒ¹é…ï¼Œæ¸…é™¤èˆŠç´¢å¼•")
                faiss_store.reset_vectors()
                # åŒæ™‚æ¸…é™¤BM25ç´¢å¼•ä»¥ä¿æŒä¸€è‡´æ€§
                bm25_index.reset_index()
            
            # å‰µå»ºFAISSç´¢å¼•
            faiss_store.create_index(dimension, "flat")
            faiss_store.add_vectors(vectors, chunk_ids, chunk_doc_ids, all_chunks)
            
            # æ§‹å»ºBM25ç´¢å¼•
            bm25_index.build_index(all_chunks, chunk_ids, chunk_doc_ids)
            
            # æª¢æŸ¥æ˜¯å¦å·²æœ‰enhanced metadataï¼ˆåœ¨åˆ†å¡Šéšæ®µç”Ÿæˆï¼‰
            enhanced_metadata = {}
            if hasattr(store, 'enhanced_metadata') and store.enhanced_metadata:
                print("ğŸ“‹ ä½¿ç”¨å·²å­˜åœ¨çš„enhanced metadata...")
                enhanced_metadata = store.enhanced_metadata
                
                # è¨­ç½®å¢å¼·metadataåˆ°FAISSå­˜å„²
                for chunk_id, metadata in enhanced_metadata.items():
                    faiss_store.set_enhanced_metadata(chunk_id, metadata)
            else:
                print("âš ï¸ æœªæ‰¾åˆ°enhanced metadataï¼ŒHybridRAGå°‡ä½¿ç”¨åŸºç¤metadata")
            
            # ä¿æŒåŸæœ‰storeçš„å…¼å®¹æ€§
            store.embeddings = vectors
            store.chunk_doc_ids = chunk_doc_ids
            store.chunks_flat = all_chunks
            
            # è‡ªå‹•ä¿å­˜æ•¸æ“š
            store.save_data()
            faiss_store.save_data()
            bm25_index.save_data()
            
            print(f"âœ… å®Œæˆembedding: FAISSç´¢å¼•({len(vectors)}å‘é‡), BM25ç´¢å¼•({len(all_chunks)}æ–‡æª”), å¢å¼·metadata({len(enhanced_metadata)}æ¢)")
            
            return {
                "provider": "gemini", 
                "model": "gemini-embedding-001",
                "num_vectors": len(vectors),
                "dimension": dimension,
                "enhanced_metadata_count": len(enhanced_metadata),
                "faiss_available": True,
                "bm25_available": True
            }
        except Exception as e:
            print(f"Gemini embedding failed: {e}")
            # å¦‚æœ Gemini å¤±æ•—ï¼Œå˜—è©¦ BGE-M3
    
    # å˜—è©¦ä½¿ç”¨ BGE-M3 embeddingï¼ˆå‚™ç”¨é¸é …ï¼‰
    if USE_BGE_M3_EMBEDDING and SENTENCE_TRANSFORMERS_AVAILABLE:
        try:
            vectors = embed_bge_m3(all_chunks)
            dimension = len(vectors[0]) if vectors else 1024
            
            # å‰µå»ºFAISSç´¢å¼•
            faiss_store.create_index(dimension, "flat")
            faiss_store.add_vectors(vectors, chunk_ids, chunk_doc_ids, all_chunks)
            
            # æ§‹å»ºBM25ç´¢å¼•
            bm25_index.build_index(all_chunks, chunk_ids, chunk_doc_ids)
            
            # æª¢æŸ¥æ˜¯å¦å·²æœ‰enhanced metadataï¼ˆåœ¨åˆ†å¡Šéšæ®µç”Ÿæˆï¼‰
            enhanced_metadata = {}
            if hasattr(store, 'enhanced_metadata') and store.enhanced_metadata:
                print("ğŸ“‹ ä½¿ç”¨å·²å­˜åœ¨çš„enhanced metadata...")
                enhanced_metadata = store.enhanced_metadata
                
                # è¨­ç½®å¢å¼·metadataåˆ°FAISSå­˜å„²
                for chunk_id, metadata in enhanced_metadata.items():
                    faiss_store.set_enhanced_metadata(chunk_id, metadata)
            else:
                print("âš ï¸ æœªæ‰¾åˆ°enhanced metadataï¼ŒHybridRAGå°‡ä½¿ç”¨åŸºç¤metadata")
            
            # ä¿æŒåŸæœ‰storeçš„å…¼å®¹æ€§
            store.embeddings = vectors
            store.chunk_doc_ids = chunk_doc_ids
            store.chunks_flat = all_chunks
            
            # è‡ªå‹•ä¿å­˜æ•¸æ“š
            store.save_data()
            faiss_store.save_data()
            bm25_index.save_data()
            
            print(f"âœ… å®Œæˆembedding: FAISSç´¢å¼•({len(vectors)}å‘é‡), BM25ç´¢å¼•({len(all_chunks)}æ–‡æª”), å¢å¼·metadata({len(enhanced_metadata)}æ¢)")
            
            return {
                "provider": "bge-m3", 
                "model": "BAAI/bge-m3",
                "num_vectors": len(vectors),
                "dimension": dimension,
                "enhanced_metadata_count": len(enhanced_metadata),
                "faiss_available": True,
                "bm25_available": True
            }
        except Exception as e:
            print(f"BGE-M3 embedding failed: {e}")
    
    # æ²’æœ‰å¯ç”¨çš„ embedding æ–¹æ³•
    return JSONResponse(
        status_code=500, 
        content={
            "error": "No embedding method available. Please configure Gemini API key or BGE-M3 model."
        }
    )


def convert_structured_to_multi_level(structured_chunks):
    """å°‡çµæ§‹åŒ–åˆ†å¡Šè½‰æ›ç‚ºè«–æ–‡ä¸­çš„å…­å€‹ç²’åº¦ç´šåˆ¥æ ¼å¼ï¼Œç¢ºä¿ä¸Šä¸‹æ–‡é€£è²«æ€§"""
    # è«–æ–‡ä¸­çš„å…­å€‹å±¤æ¬¡
    six_level_chunks = {
        'document': [],                    # 1. æ–‡ä»¶å±¤ç´š (Document Level)
        'document_component': [],          # 2. æ–‡ä»¶çµ„æˆéƒ¨åˆ†å±¤ç´š (Document Component Level)
        'basic_unit_hierarchy': [],        # 3. åŸºæœ¬å–®ä½å±¤æ¬¡çµæ§‹å±¤ç´š (Basic Unit Hierarchy Level)
        'basic_unit': [],                  # 4. åŸºæœ¬å–®ä½å±¤ç´š (Basic Unit Level)
        'basic_unit_component': [],        # 5. åŸºæœ¬å–®ä½çµ„æˆéƒ¨åˆ†å±¤ç´š (Basic Unit Component Level)
        'enumeration': []                  # 6. åˆ—èˆ‰å±¤ç´š (Enumeration Level)
    }
    
    for chunk in structured_chunks:
        content = chunk.get('content', '')
        metadata = chunk.get('metadata', {})
        
        # å„ªå…ˆä½¿ç”¨metadataä¸­çš„levelä¿¡æ¯ï¼ˆå¤šå±¤æ¬¡çµæ§‹åŒ–åˆ†å¡Šæœƒè¨­ç½®é€™å€‹ï¼‰
        if 'level' in metadata:
            chunk_by = metadata['level']
        else:
            chunk_by = metadata.get('chunk_by', 'article')
        
        # å°‡å¤§å¯«çš„levelåç¨±è½‰æ›ç‚ºå°å¯«ï¼ˆå…¼å®¹MultiLevelStructuredChunkingç”Ÿæˆçš„æ ¼å¼ï¼‰
        chunk_by = chunk_by.lower()
        
        # æ ¹æ“šchunk_byå’Œå…§å®¹ç‰¹å¾µåˆ†é¡åˆ°å°æ‡‰å±¤æ¬¡
        level_name, semantic_features = classify_chunk_to_level(content, metadata, chunk_by)
        
        # è™•ç†ä¸Šä¸‹æ–‡é€£è²«æ€§ï¼šç‚ºåˆ—èˆ‰å…ƒç´ æ·»åŠ çˆ¶ç´šä¸Šä¸‹æ–‡
        final_content = content
        if level_name == 'enumeration' and chunk_by == 'item':
            # æª¢æŸ¥æ˜¯å¦å·²ç¶“åŒ…å«çˆ¶ç´šå…§å®¹ï¼ˆé€šéæª¢æŸ¥æ˜¯å¦åŒ…å«æ¢æ–‡ä¸»æ–‡ï¼‰
            if not has_parent_context(content, metadata):
                # å˜—è©¦å¾å…¶ä»–chunksä¸­æ‰¾åˆ°çˆ¶ç´šæ¢æ–‡å…§å®¹
                parent_content = find_parent_article_content(structured_chunks, metadata)
                if parent_content:
                    final_content = f"{parent_content}\n{content}"
                    # æ›´æ–°èªç¾©ç‰¹å¾µä»¥åæ˜ ä¸Šä¸‹æ–‡é€£è²«æ€§
                    semantic_features['has_parent_context'] = True
                    semantic_features['parent_content_length'] = len(parent_content)
        
        if level_name in six_level_chunks:
            six_level_chunks[level_name].append({
                'content': final_content,
                'original_content': content,  # ä¿ç•™åŸå§‹å…§å®¹
                'metadata': {
                    **metadata,
                    'semantic_level': level_name,
                    'semantic_features': semantic_features,
                    'target_queries': get_target_queries_for_level(level_name),
                    'has_context_consistency': level_name == 'enumeration' and final_content != content
                }
            })
    
    return six_level_chunks


def classify_chunk_to_level(content: str, metadata: dict, chunk_by: str) -> tuple:
    """æ ¹æ“šå…§å®¹å’Œå…ƒæ•¸æ“šå°‡chunkåˆ†é¡åˆ°åˆé©çš„å±¤æ¬¡ - å°æ‡‰è«–æ–‡ä¸­çš„å…­å€‹ç²’åº¦ç´šåˆ¥"""
    import re
    
    # æ ¹æ“šè«–æ–‡å®šç¾©çš„å…­å€‹ç²’åº¦ç´šåˆ¥æ˜ å°„
    level_mapping = {
        # 1) law_name â†’ document
        'law': 'document',
        # 2) chapter â†’ document_component
        'chapter': 'document_component',
        # 3) section â†’ basic_unit_hierarchy
        'section': 'basic_unit_hierarchy',
        # 4) article â†’ basic_unit
        'article': 'basic_unit',
        # 5) paragraph/é … â†’ basic_unit_component
        'paragraph': 'basic_unit_component',
        # 6) subparagraph/æ¬¾ â†’ enumerationï¼›item/ç›® â†’ enumeration
        'subparagraph': 'enumeration',
        'item': 'enumeration'
    }
    
    # é¦–å…ˆæ ¹æ“šchunk_byç¢ºå®šåŸºæœ¬å±¤æ¬¡
    base_level = level_mapping.get(chunk_by, 'basic_unit')
    
    # åŸºæ–¼å…§å®¹ç‰¹å¾µé€²è¡Œèªç¾©åˆ†æ
    semantic_features = analyze_chunk_semantics(content)
    
    # æ ¹æ“šèªç¾©ç‰¹å¾µå’Œå…§å®¹é•·åº¦é€²è¡Œç²¾ç´°èª¿æ•´
    # ä»¥ä½ æŒ‡å®šçš„å›ºå®šæ˜ å°„ç‚ºä¸»ï¼›åªä¿ç•™å°‘é‡åˆç†åŒ–ï¼ˆä¾‹å¦‚ article çš„å®šç¾©æ€§é•·æ–‡å¯æ­¸åˆ° basic_unit_componentï¼‰
    if chunk_by == 'article':
        level = 'basic_unit' if not (semantic_features['is_definition'] and len(content) > 200) else 'basic_unit_component'
    elif chunk_by in ('paragraph',):
        # é …ï¼ˆparagraphï¼‰å›ºå®šç‚º basic_unit_component
        level = 'basic_unit_component'
    elif chunk_by in ('subparagraph', 'item'):
        # æ¬¾/ç›®å›ºå®šç‚º enumerationï¼ˆæ³¨æ„ï¼šæ­¤è™•çš„ item ä»£è¡¨ã€Œç›®ã€ï¼‰
        level = 'enumeration'
    elif chunk_by == 'chapter':
        level = 'document_component'
    elif chunk_by == 'section':
        level = 'basic_unit_hierarchy'
    elif chunk_by == 'law':
        level = 'document'
    else:
        level = base_level
    
    return level, semantic_features


def analyze_chunk_semantics(content: str) -> dict:
    """åˆ†æchunkçš„èªç¾©ç‰¹å¾µ"""
    import re
    
    features = {
        'is_definition': False,
        'is_procedural': False,
        'is_enumeration': False,
        'is_normative': False,
        'has_article_reference': False,
        'concept_density': 0.0,
        'legal_keywords': []
    }
    
    content_lower = content.lower()
    
    # æª¢æŸ¥å®šç¾©æ€§å…§å®¹
    definition_patterns = [
        r'æœ¬æ³•æ‰€ç¨±.*?æ˜¯æŒ‡',
        r'.*?æŒ‡.*?è€…',
        r'.*?ç‚º.*?è€…',
        r'å®šç¾©.*?ç‚º',
        r'æ‰€è¬‚.*?ä¿‚æŒ‡'
    ]
    for pattern in definition_patterns:
        if re.search(pattern, content):
            features['is_definition'] = True
            break
    
    # æª¢æŸ¥ç¨‹åºæ€§å…§å®¹
    procedural_patterns = [
        r'æ‡‰.*?ç”³è«‹',
        r'å¾—.*?è¾¦ç†',
        r'ä¾.*?ç¨‹åº',
        r'å¦‚ä½•.*?',
        r'ç¨‹åº.*?',
        r'æµç¨‹.*?'
    ]
    for pattern in procedural_patterns:
        if re.search(pattern, content):
            features['is_procedural'] = True
            break
    
    # æª¢æŸ¥åˆ—èˆ‰æ€§å…§å®¹
    enumeration_patterns = [
        r'[ï¼ˆ(]\d+[ï¼‰)]',
        r'[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+[ã€ï¼]',
        r'\d+[ã€ï¼]',
        r'ç¬¬.*?é …',
        r'ç¬¬.*?æ¬¾'
    ]
    for pattern in enumeration_patterns:
        if re.search(pattern, content):
            features['is_enumeration'] = True
            break
    
    # æª¢æŸ¥è¦ç¯„æ€§å…§å®¹
    normative_patterns = [
        r'æ‡‰.*?',
        r'å¾—.*?',
        r'ä¸å¾—.*?',
        r'ç¦æ­¢.*?',
        r'è¦å®š.*?'
    ]
    for pattern in normative_patterns:
        if re.search(pattern, content):
            features['is_normative'] = True
            break
    
    # æª¢æŸ¥æ³•æ¢å¼•ç”¨
    if re.search(r'ç¬¬\s*\d+\s*æ¢', content):
        features['has_article_reference'] = True
    
    # è¨ˆç®—æ¦‚å¿µå¯†åº¦
    legal_keywords = ['æœ¬æ³•', 'æ¢æ–‡', 'è¦å®š', 'æ¬Šåˆ©', 'ç¾©å‹™', 'ç”³è«‹', 'è¾¦ç†', 'ç¨‹åº', 'å®šç¾©', 'ç¯„åœ', 'è²¬ä»»', 'æ¬ŠåŠ›', 'è·æ¬Š', 'è·è²¬', 'æ³•å¾‹', 'æ³•è¦', 'æ¢ä¾‹']
    keyword_count = sum(1 for keyword in legal_keywords if keyword in content)
    features['concept_density'] = keyword_count / max(len(content.split()), 1)
    features['legal_keywords'] = [kw for kw in legal_keywords if kw in content]
    
    return features


def get_target_queries_for_level(level_name: str) -> list:
    """æ ¹æ“šå±¤æ¬¡è¿”å›ç›®æ¨™æŸ¥è©¢é—œéµè©"""
    query_mapping = {
        'document': ['æ•´éƒ¨', 'å…¨æ–‡', 'æ•´å€‹', 'å…¨éƒ¨'],
        'document_component': ['ç« ', 'éƒ¨åˆ†', 'ç·¨', 'ç¯‡'],
        'basic_unit_hierarchy': ['ç¯€', 'æ¨™é¡Œ', 'ç« ç¯€'],
        'basic_unit': ['ç¬¬.*æ¢', 'æ¢æ–‡', 'æ³•æ¢'],
        'basic_unit_component': ['æ®µè½', 'ä¸»æ–‡', 'å…§å®¹', 'å®šç¾©'],
        'enumeration': ['é …', 'ç›®', 'æ¬¾', 'å­é …']
    }
    return query_mapping.get(level_name, ['ç¬¬.*æ¢'])


def has_parent_context(content: str, metadata: dict) -> bool:
    """æª¢æŸ¥å…§å®¹æ˜¯å¦å·²ç¶“åŒ…å«çˆ¶ç´šä¸Šä¸‹æ–‡"""
    import re
    
    # æª¢æŸ¥æ˜¯å¦åŒ…å«æ¢æ–‡ä¸»æ–‡çš„ç‰¹å¾µ
    article_main_patterns = [
        r'æœ¬æ³•.*?å®šç¾©',
        r'æœ¬æ³•.*?è¦å®š',
        r'æœ¬æ³•.*?ç”¨è©',
        r'æ‡‰.*?ç”³è«‹',
        r'å¾—.*?è¾¦ç†',
        r'ä¾.*?ç¨‹åº'
    ]
    
    # å¦‚æœå…§å®¹é•·åº¦è¼ƒçŸ­ä¸”ä¸åŒ…å«æ¢æ–‡ä¸»æ–‡ç‰¹å¾µï¼Œå¯èƒ½ç¼ºå°‘çˆ¶ç´šä¸Šä¸‹æ–‡
    if len(content) < 200:
        for pattern in article_main_patterns:
            if re.search(pattern, content):
                return True
        return False
    
    return True


def find_parent_article_content(structured_chunks: list, current_metadata: dict) -> str:
    """å¾çµæ§‹åŒ–chunksä¸­æ‰¾åˆ°çˆ¶ç´šæ¢æ–‡å…§å®¹"""
    current_article = current_metadata.get('article', '')
    current_chapter = current_metadata.get('chapter', '')
    current_section = current_metadata.get('section', '')
    
    # æŸ¥æ‰¾å°æ‡‰çš„æ¢æ–‡chunk
    for chunk in structured_chunks:
        chunk_metadata = chunk.get('metadata', {})
        chunk_by = chunk_metadata.get('chunk_by', '')
        
        # æ‰¾åˆ°å°æ‡‰çš„æ¢æ–‡chunk
        if (chunk_by == 'article' and 
            chunk_metadata.get('article', '') == current_article and
            chunk_metadata.get('chapter', '') == current_chapter and
            chunk_metadata.get('section', '') == current_section):
            
            content = chunk.get('content', '')
            # æå–æ¢æ–‡ä¸»æ–‡éƒ¨åˆ†ï¼ˆæ’é™¤é …ç›®å…§å®¹ï¼‰
            lines = content.split('\n')
            main_content_lines = []
            
            for line in lines:
                line = line.strip()
                # å¦‚æœé‡åˆ°é …ç›®æ¨™è¨˜ï¼Œåœæ­¢æå–ä¸»æ–‡
                if re.match(r'^[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+[ã€ï¼]', line) or re.match(r'^\d+[ã€ï¼]', line):
                    break
                # åŒ…å«æ¡æ–‡æ ‡é¢˜å’Œä¸»æ–‡å†…å®¹ï¼Œä½†æ’é™¤ç»“æ„ä¿¡æ¯
                if line and not line.startswith('ã€') and not line.startswith('ç« ') and not line.startswith('ç¯€'):
                    main_content_lines.append(line)
            
            return '\n'.join(main_content_lines)
    
    return ""


@app.post("/api/generate-enhanced-metadata")
async def generate_enhanced_metadata(req: Dict[str, Any]):
    """åœ¨åˆ†å¡Šéšæ®µç”Ÿæˆenhanced metadata - å°ˆé–€ç”¨æ–¼HybridRAG"""
    print(f"ğŸ”§ ç”Ÿæˆenhanced metadataè«‹æ±‚: {req}")
    
    try:
        # ç²å–æ‰€æœ‰chunks
        all_chunks = []
        chunk_ids = []
        chunk_doc_ids = []
        
        for doc_id, doc in store.docs.items():
            if doc.structured_chunks:
                for chunk in doc.structured_chunks:
                    all_chunks.append(chunk.get("content", ""))
                    chunk_ids.append(chunk.get("chunk_id", f"{doc_id}_{len(chunk_ids)}"))
                    chunk_doc_ids.append(doc_id)
        
        if not all_chunks:
            return {"error": "æ²’æœ‰æ‰¾åˆ°å¯ç”¨çš„chunks"}
        
        # æº–å‚™chunksæ•¸æ“š
        chunks_data = [
            {
                "chunk_id": chunk_ids[i],
                "content": all_chunks[i],
                "metadata": {}
            }
            for i in range(len(all_chunks))
        ]
        
        # æ‰¹é‡å¢å¼·metadata
        print(f"ğŸ”§ é–‹å§‹ç‚º {len(chunks_data)} å€‹chunksç”Ÿæˆenhanced metadata...")
        enhanced_metadata = metadata_enhancer.enhance_metadata_batch(chunks_data)
        
        # ä¿å­˜åˆ°store
        store.enhanced_metadata = enhanced_metadata
        store.save_data()
        
        # çµ±è¨ˆä¿¡æ¯
        article_level_count = sum(1 for meta in enhanced_metadata.values() if meta.get("is_article_level", False))
        chapter_section_count = sum(1 for meta in enhanced_metadata.values() if meta.get("is_chapter_section_level", False))
        inherited_count = sum(1 for meta in enhanced_metadata.values() if meta.get("inherited_from"))
        
        return {
            "success": True,
            "message": "Enhanced metadataç”Ÿæˆå®Œæˆ",
            "stats": {
                "total_chunks": len(chunks_data),
                "enhanced_metadata_count": len(enhanced_metadata),
                "article_level_chunks": article_level_count,
                "chapter_section_chunks": chapter_section_count,
                "inherited_chunks": inherited_count
            }
        }
        
    except Exception as e:
        print(f"âŒ ç”Ÿæˆenhanced metadataå¤±æ•—: {e}")
        return {"error": f"ç”Ÿæˆenhanced metadataå¤±æ•—: {str(e)}"}

@app.get("/api/enhanced-metadata-stats")
async def get_enhanced_metadata_stats():
    """ç²å–enhanced metadataçµ±è¨ˆä¿¡æ¯"""
    try:
        if not hasattr(store, 'enhanced_metadata') or not store.enhanced_metadata:
            return {
                "enhanced_metadata_count": 0,
                "message": "å°šæœªç”Ÿæˆenhanced metadata"
            }
        
        enhanced_metadata = store.enhanced_metadata
        article_level_count = sum(1 for meta in enhanced_metadata.values() if meta.get("is_article_level", False))
        chapter_section_count = sum(1 for meta in enhanced_metadata.values() if meta.get("is_chapter_section_level", False))
        inherited_count = sum(1 for meta in enhanced_metadata.values() if meta.get("inherited_from"))
        
        return {
            "enhanced_metadata_count": len(enhanced_metadata),
            "article_level_chunks": article_level_count,
            "chapter_section_chunks": chapter_section_count,
            "inherited_chunks": inherited_count,
            "enhancement_levels": {
                "full": sum(1 for meta in enhanced_metadata.values() if meta.get("enhancement_level") == "full"),
                "medium": sum(1 for meta in enhanced_metadata.values() if meta.get("enhancement_level") == "medium"),
                "lightweight": sum(1 for meta in enhanced_metadata.values() if meta.get("enhancement_level") == "lightweight"),
                "none": sum(1 for meta in enhanced_metadata.values() if meta.get("enhancement_level") == "none"),
            }
        }
    except Exception as e:
        print(f"âŒ ç²å–enhanced metadataçµ±è¨ˆå¤±æ•—: {e}")
        return {"error": f"ç²å–çµ±è¨ˆå¤±æ•—: {str(e)}"}

@app.get("/api/chunking-hierarchy-stats")
async def get_chunking_hierarchy_stats():
    """ç²å–åˆ†å¡Šçµæœçš„æ³•å¾‹å±¤ç´šçµ±è¨ˆä¿¡æ¯ - çµ±è¨ˆå¯¦éš›é¡¯ç¤ºçš„åˆ†å¡Šåˆ—è¡¨ï¼ˆ428å€‹åˆ†å¡Šï¼‰"""
    try:
        # ç²å–æ‰€æœ‰æ–‡æª”çš„å¤šå±¤ç´šåˆ†å¡Šæ•¸æ“š
        hierarchy_stats = {
            'document': 0,                    # ç« ç´š (æ–‡ä»¶å±¤ç´š)
            'document_component': 0,          # ç¯€ç´š (æ–‡ä»¶çµ„æˆéƒ¨åˆ†å±¤ç´š) 
            'basic_unit_hierarchy': 0,        # æ¢ç´š (åŸºæœ¬å–®ä½å±¤æ¬¡çµæ§‹å±¤ç´š)
            'basic_unit': 0,                  # é …ç´š (åŸºæœ¬å–®ä½å±¤ç´š)
            'basic_unit_component': 0,        # æ¬¾ç´š (åŸºæœ¬å–®ä½çµ„æˆéƒ¨åˆ†å±¤ç´š)
            'enumeration': 0                  # ç›®ç´š (åˆ—èˆ‰å±¤ç´š)
        }
        
        total_chunks = 0
        
        # å±¤ç´šæ˜ å°„ï¼šå°‡level_enæˆ–chunk_byæ˜ å°„åˆ°å…­å±¤åˆ†é¡
        # æ³¨æ„ï¼šå‰ç«¯é¡¯ç¤ºæ¨™ç±¤ç‚º Chapter->ç« , Section->ç¯€, Article->æ¢, Paragraph->é …, Subparagraph->æ¬¾, Item->ç›®
        def map_level_to_hierarchy(level_en: str = None, chunk_by: str = None) -> str:
            """å°‡level_enæˆ–chunk_byæ˜ å°„åˆ°å…­å±¤åˆ†é¡"""
            # å„ªå…ˆä½¿ç”¨level_enï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            if level_en:
                level_en_lower = level_en.lower()
                if level_en_lower == 'law':
                    return 'document'  # ç« ç´š (æ–‡ä»¶å±¤ç´š)
                elif level_en_lower == 'chapter':
                    return 'document'  # ç« ç´š (æ–‡ä»¶å±¤ç´š)
                elif level_en_lower == 'section':
                    return 'document_component'  # ç¯€ç´š (æ–‡ä»¶çµ„æˆéƒ¨åˆ†å±¤ç´š)
                elif level_en_lower == 'article':
                    return 'basic_unit_hierarchy'  # æ¢ç´š (åŸºæœ¬å–®ä½å±¤æ¬¡çµæ§‹å±¤ç´š)
                elif level_en_lower == 'paragraph':
                    return 'basic_unit'  # é …ç´š (åŸºæœ¬å–®ä½å±¤ç´š)
                elif level_en_lower == 'subparagraph':
                    return 'basic_unit_component'  # æ¬¾ç´š (åŸºæœ¬å–®ä½çµ„æˆéƒ¨åˆ†å±¤ç´š)
                elif level_en_lower == 'item':
                    return 'enumeration'  # ç›®ç´š (åˆ—èˆ‰å±¤ç´š)
            
            # å¦‚æœæ²’æœ‰level_enï¼Œä½¿ç”¨chunk_by
            if chunk_by:
                chunk_by_lower = chunk_by.lower()
                if chunk_by_lower == 'law':
                    return 'document'  # ç« ç´š (æ–‡ä»¶å±¤ç´š)
                elif chunk_by_lower == 'chapter':
                    return 'document'  # ç« ç´š (æ–‡ä»¶å±¤ç´š)
                elif chunk_by_lower == 'section':
                    return 'document_component'  # ç¯€ç´š (æ–‡ä»¶çµ„æˆéƒ¨åˆ†å±¤ç´š)
                elif chunk_by_lower == 'article':
                    return 'basic_unit_hierarchy'  # æ¢ç´š (åŸºæœ¬å–®ä½å±¤æ¬¡çµæ§‹å±¤ç´š)
                elif chunk_by_lower == 'paragraph':
                    return 'basic_unit'  # é …ç´š (åŸºæœ¬å–®ä½å±¤ç´š)
                elif chunk_by_lower == 'subparagraph':
                    return 'basic_unit_component'  # æ¬¾ç´š (åŸºæœ¬å–®ä½çµ„æˆéƒ¨åˆ†å±¤ç´š)
                elif chunk_by_lower == 'item':
                    return 'enumeration'  # ç›®ç´š (åˆ—èˆ‰å±¤ç´š)
            
            # é»˜èªæ­¸é¡åˆ°é …ç´šï¼ˆbasic_unitï¼‰
            return 'basic_unit'
        
        # éæ­·æ‰€æœ‰æ–‡æª”
        # åªçµ±è¨ˆä½¿ç”¨structured_hierarchicalç­–ç•¥çš„æ–‡æª”ï¼ˆé¿å…çµ±è¨ˆæ‰€æœ‰æ–‡æª”å°è‡´æ•¸å­—éå¤§ï¼‰
        # æŒ‰æ–‡ä»¶åå»é‡ï¼Œåªçµ±è¨ˆæ¯å€‹æ–‡ä»¶åçš„ç¬¬ä¸€å€‹ç¬¦åˆæ¢ä»¶çš„æ–‡æª”ï¼ˆé¿å…é‡è¤‡çµ±è¨ˆï¼‰
        # å¦‚æœæœ‰åŒåæ–‡æª”ï¼Œå„ªå…ˆé¸æ“‡æœ‰structured_chunksä¸”chunksæ•¸é‡æœ€å¤šçš„
        filename_to_doc = {}  # {filename: (doc_id, doc, chunk_count)}
        
        # ç¬¬ä¸€æ¬¡éæ­·ï¼šæ‰¾å‡ºæ¯å€‹æ–‡ä»¶åçš„æœ€ä½³æ–‡æª”ï¼ˆæœ‰structured_chunksä¸”chunksæœ€å¤šçš„ï¼‰
        for doc_id, doc in store.docs.items():
            # åªçµ±è¨ˆstructured_hierarchicalç­–ç•¥çš„æ–‡æª”
            chunking_strategy = getattr(doc, 'chunking_strategy', None)
            if chunking_strategy not in ['structured_hierarchical', 'multi_level_structured']:
                continue
            
            # å„ªå…ˆçµ±è¨ˆstructured_chunksï¼ˆå¯¦éš›é¡¯ç¤ºçš„428å€‹chunksï¼‰
            if doc.structured_chunks:
                chunk_count = len(doc.structured_chunks)
                
                # å¦‚æœé€™å€‹æ–‡ä»¶åé‚„æ²’æœ‰è¨˜éŒ„ï¼Œæˆ–è€…é€™å€‹æ–‡æª”æœ‰æ›´å¤šçš„chunksï¼Œå‰‡æ›´æ–°
                if doc.filename not in filename_to_doc:
                    filename_to_doc[doc.filename] = (doc_id, doc, chunk_count)
                else:
                    existing_count = filename_to_doc[doc.filename][2]
                    if chunk_count > existing_count:
                        print(f"ğŸ”„ ç™¼ç¾æ›´æ–°çš„æ–‡æª” {doc.filename}: {chunk_count} > {existing_count} chunks")
                        filename_to_doc[doc.filename] = (doc_id, doc, chunk_count)
        
        # ç¬¬äºŒæ¬¡éæ­·ï¼šåªçµ±è¨ˆé¸ä¸­çš„æ–‡æª”
        for filename, (doc_id, doc, chunk_count) in filename_to_doc.items():
            doc_chunk_count = 0
            # çµ±è¨ˆæ¯å€‹chunkçš„å±¤ç´š
            for chunk in doc.structured_chunks:
                metadata = chunk.get('metadata', {})
                level_en = metadata.get('level_en') or metadata.get('level')
                chunk_by = metadata.get('chunk_by')
                
                # æ˜ å°„åˆ°å…­å±¤åˆ†é¡
                hierarchy_level = map_level_to_hierarchy(level_en, chunk_by)
                
                if hierarchy_level in hierarchy_stats:
                    hierarchy_stats[hierarchy_level] += 1
                    total_chunks += 1
                    doc_chunk_count += 1
            
            chunking_strategy = getattr(doc, 'chunking_strategy', None)
            print(f"ğŸ“Š çµ±è¨ˆæ–‡æª” {doc.filename} (ç­–ç•¥: {chunking_strategy}, doc_id: {doc_id}): {doc_chunk_count} å€‹åˆ†å¡Š")
        
        # æ·»åŠ ä¸­æ–‡å±¤ç´šåç¨±æ˜ å°„
        level_names = {
            'document': 'ç« ç´š (æ–‡ä»¶å±¤ç´š)',
            'document_component': 'ç¯€ç´š (æ–‡ä»¶çµ„æˆéƒ¨åˆ†å±¤ç´š)',
            'basic_unit_hierarchy': 'æ¢ç´š (åŸºæœ¬å–®ä½å±¤æ¬¡çµæ§‹å±¤ç´š)', 
            'basic_unit': 'é …ç´š (åŸºæœ¬å–®ä½å±¤ç´š)',
            'basic_unit_component': 'æ¬¾ç´š (åŸºæœ¬å–®ä½çµ„æˆéƒ¨åˆ†å±¤ç´š)',
            'enumeration': 'ç›®ç´š (åˆ—èˆ‰å±¤ç´š)'
        }
        
        print(f"ğŸ“Š ç¸½çµ±è¨ˆçµæœ: ç¸½åˆ†å¡Šæ•¸={total_chunks}, å„å±¤ç´šçµ±è¨ˆ={hierarchy_stats}")
        
        return {
            "total_chunks": total_chunks,
            "hierarchy_stats": hierarchy_stats,
            "level_names": level_names,
            "has_multi_level_chunks": any(count > 0 for count in hierarchy_stats.values())
        }
        
    except Exception as e:
        print(f"âŒ ç²å–åˆ†å¡Šå±¤ç´šçµ±è¨ˆå¤±æ•—: {e}")
        return {"error": f"ç²å–çµ±è¨ˆå¤±æ•—: {str(e)}"}

@app.get("/api/chunks-by-hierarchy/{level_name}")
async def get_chunks_by_hierarchy(level_name: str):
    """æ ¹æ“šæ³•å¾‹å±¤ç´šç²å–chunksåˆ—è¡¨"""
    try:
        chunks_by_level = []
        
        # éæ­·æ‰€æœ‰æ–‡æª”
        for doc_id, doc in store.docs.items():
            # å„ªå…ˆä½¿ç”¨multi_level_chunks
            if doc.multi_level_chunks and isinstance(doc.multi_level_chunks, dict):
                # å¾å¤šå±¤ç´šchunksä¸­ç²å–æŒ‡å®šå±¤ç´šçš„chunks
                if level_name in doc.multi_level_chunks:
                    chunks = doc.multi_level_chunks[level_name]
                    if chunks:
                        for i, chunk_data in enumerate(chunks):
                            chunk_info = {
                                'chunk_id': f"{doc_id}_{level_name}_{i}",
                                'doc_id': doc_id,
                                'doc_name': doc.filename,
                                'level': level_name,
                                'content': chunk_data.get('content', ''),
                                'metadata': chunk_data.get('metadata', {}),
                                'span': chunk_data.get('span', {}),
                                'chunk_index': i
                            }
                            chunks_by_level.append(chunk_info)
            elif doc.structured_chunks:
                # å¾çµæ§‹åŒ–chunksä¸­ç¯©é¸æŒ‡å®šå±¤ç´š
                for i, chunk in enumerate(doc.structured_chunks):
                    metadata = chunk.get('metadata', {})
                    chunk_by = metadata.get('chunk_by', 'article')
                    
                    # æª¢æŸ¥æ˜¯å¦åŒ¹é…æŒ‡å®šçš„å±¤ç´š
                    level_matches = False
                    if level_name == 'document' and chunk_by == 'law':
                        level_matches = True
                    elif level_name == 'document_component' and chunk_by == 'chapter':
                        level_matches = True
                    elif level_name == 'basic_unit_hierarchy' and chunk_by == 'section':
                        level_matches = True
                    elif level_name == 'basic_unit' and chunk_by == 'article':
                        level_matches = True
                    elif level_name == 'basic_unit_component' and chunk_by == 'paragraph':
                        level_matches = True
                    elif level_name == 'enumeration' and chunk_by in ['subparagraph', 'item']:
                        level_matches = True
                    
                    if level_matches:
                        chunk_info = {
                            'chunk_id': f"{doc_id}_structured_{i}",
                            'doc_id': doc_id,
                            'doc_name': doc.filename,
                            'level': level_name,
                            'content': chunk.get('content', ''),
                            'metadata': metadata,
                            'span': chunk.get('span', {}),
                            'chunk_index': i
                        }
                        chunks_by_level.append(chunk_info)
        
        return {
            "level_name": level_name,
            "chunks": chunks_by_level,
            "total_count": len(chunks_by_level)
        }
        
    except Exception as e:
        print(f"âŒ ç²å–å±¤ç´šchunkså¤±æ•—: {e}")
        return {"error": f"ç²å–chunkså¤±æ•—: {str(e)}"}

@app.get("/api/enhanced-metadata-list")
async def get_enhanced_metadata_list():
    """ç²å–enhanced metadataåˆ—è¡¨"""
    try:
        if not hasattr(store, 'enhanced_metadata') or not store.enhanced_metadata:
            return {"enhanced_metadata": {}}
        
        return {"enhanced_metadata": store.enhanced_metadata}
    except Exception as e:
        print(f"âŒ ç²å–enhanced metadataåˆ—è¡¨å¤±æ•—: {e}")
        return {"error": f"ç²å–åˆ—è¡¨å¤±æ•—: {str(e)}"}

@app.post("/api/update-enhanced-metadata")
async def update_enhanced_metadata(req: Dict[str, Any]):
    """æ›´æ–°ç‰¹å®šchunkçš„enhanced metadata"""
    try:
        chunk_id = req.get("chunk_id")
        enhanced_metadata = req.get("enhanced_metadata")
        
        if not chunk_id or not enhanced_metadata:
            return {"error": "ç¼ºå°‘å¿…è¦åƒæ•¸"}
        
        # æ›´æ–°storeä¸­çš„enhanced metadata
        if not hasattr(store, 'enhanced_metadata'):
            store.enhanced_metadata = {}
        
        store.enhanced_metadata[chunk_id] = enhanced_metadata
        store.save_data()
        
        # åŒæ™‚æ›´æ–°FAISSå­˜å„²ä¸­çš„metadata
        if faiss_store.has_vectors():
            faiss_store.set_enhanced_metadata(chunk_id, enhanced_metadata)
            faiss_store.save_data()
        
        return {"success": True, "message": "Enhanced metadataæ›´æ–°æˆåŠŸ"}
    except Exception as e:
        print(f"âŒ æ›´æ–°enhanced metadataå¤±æ•—: {e}")
        return {"error": f"æ›´æ–°å¤±æ•—: {str(e)}"}

@app.post("/api/multi-level-embed-fast")
async def multi_level_embed_fast(req: Dict[str, Any]):
    """å¿«é€Ÿå¤šå±¤æ¬¡embedding - ä¸é€²è¡Œmetadataå¢å¼·ï¼Œå°ˆé–€ç”¨æ–¼å¤šå±¤æ¬¡èåˆæª¢ç´¢"""
    print(f"ğŸš€ å¿«é€Ÿå¤šå±¤æ¬¡embeddingè«‹æ±‚: {req}")
    
    # è¨­ç½®ç‚ºä¸é€²è¡Œmetadataå¢å¼·
    req["enable_metadata_enhancement"] = False
    
    # èª¿ç”¨æ¨™æº–çš„å¤šå±¤æ¬¡embedding
    return await multi_level_embed(req)

@app.post("/api/multi-level-embed")
async def multi_level_embed(req: Dict[str, Any]):
    """å¤šå±¤æ¬¡embeddingç«¯é» - ç‚ºè«–æ–‡ä¸­çš„å…­å€‹ç²’åº¦ç´šåˆ¥å‰µå»ºç¨ç«‹çš„embedding"""
    print(f"ğŸ” å¤šå±‚çº§Embeddingå‡½æ•°è¢«è°ƒç”¨ï¼Œè¯·æ±‚: {req}")
    print(f"ğŸ” é…ç½®æ£€æŸ¥:")
    print(f"   USE_GEMINI_EMBEDDING: {USE_GEMINI_EMBEDDING}")
    print(f"   GOOGLE_API_KEY: {'å·²è¨­ç½®' if GOOGLE_API_KEY else 'æœªè¨­ç½®'}")
    print(f"   USE_BGE_M3_EMBEDDING: {USE_BGE_M3_EMBEDDING}")
    # æ”¶é›†é¸å®šæ–‡æª”çš„å¤šå±¤æ¬¡chunks
    # å¦‚æœæ²’æœ‰æŒ‡å®šdoc_idsï¼Œåªé¸æ“‡ä½¿ç”¨structured_hierarchicalç­–ç•¥çš„æœ€è¿‘æ–‡æª”
    requested_doc_ids = req.get("doc_ids")
    if requested_doc_ids:
        # å³ä½¿æŒ‡å®šäº†doc_idsï¼Œä¹Ÿè¦æŒ‰æ–‡ä»¶åå»é‡ï¼Œé¿å…é‡è¤‡embeddingåŒåæ–‡æª”
        candidates = []
        for doc_id in requested_doc_ids:
            doc = store.docs.get(doc_id)
            if doc:
                candidates.append((doc_id, doc))
        
        if not candidates:
            return JSONResponse(
                status_code=400,
                content={"error": "æŒ‡å®šçš„æ–‡æª”IDä¸å­˜åœ¨"}
            )
        
        # æŒ‰æ–‡ä»¶åå»é‡ï¼Œåªé¸æ“‡æ¯å€‹æ–‡ä»¶åçš„ç¬¬ä¸€å€‹æ–‡æª”ï¼ˆæˆ–è€…chunksæœ€å¤šçš„ï¼‰
        filename_to_doc = {}  # {filename: (doc_id, doc, chunk_count)}
        for doc_id, doc in candidates:
            chunk_count = len(doc.structured_chunks) if doc.structured_chunks else 0
            if doc.filename not in filename_to_doc:
                filename_to_doc[doc.filename] = (doc_id, doc, chunk_count)
            else:
                existing_count = filename_to_doc[doc.filename][2]
                if chunk_count > existing_count:
                    print(f"ğŸ”„ ç™¼ç¾æ›´æ–°çš„æ–‡æª” {doc.filename}: {chunk_count} > {existing_count} chunks")
                    filename_to_doc[doc.filename] = (doc_id, doc, chunk_count)
                else:
                    print(f"âš ï¸ è·³éé‡è¤‡æ–‡æª” {doc.filename} (doc_id: {doc_id})ï¼Œå·²é¸æ“‡chunksæ›´å¤šçš„ç‰ˆæœ¬")
        
        selected = [doc_id for doc_id, _, _ in filename_to_doc.values()]
        if len(selected) < len(requested_doc_ids):
            print(f"âš ï¸ å»é‡å¾Œï¼Œå¾ {len(requested_doc_ids)} å€‹æŒ‡å®šçš„æ–‡æª”ä¸­é¸æ“‡äº† {len(selected)} å€‹æ–‡æª”")
    else:
        # åªé¸æ“‡ä½¿ç”¨structured_hierarchicalæˆ–multi_level_structuredç­–ç•¥çš„æ–‡æª”
        candidates = [
            (doc_id, doc) for doc_id, doc in store.docs.items()
            if doc and getattr(doc, 'chunking_strategy', None) in ['structured_hierarchical', 'multi_level_structured']
        ]
        
        if not candidates:
            return JSONResponse(
                status_code=400,
                content={"error": "æ²’æœ‰æ‰¾åˆ°ä½¿ç”¨structured_hierarchicalç­–ç•¥çš„æ–‡æª”ã€‚è«‹å…ˆé€²è¡Œå¤šå±¤ç´šçµæ§‹åŒ–åˆ†å¡Šã€‚"}
            )
        
        # æŒ‰æ–‡ä»¶åå»é‡ï¼Œåªé¸æ“‡æ¯å€‹æ–‡ä»¶åçš„ç¬¬ä¸€å€‹æ–‡æª”ï¼ˆæˆ–è€…chunksæœ€å¤šçš„ï¼‰
        filename_to_doc = {}  # {filename: (doc_id, doc, chunk_count)}
        for doc_id, doc in candidates:
            chunk_count = len(doc.structured_chunks) if doc.structured_chunks else 0
            if doc.filename not in filename_to_doc:
                filename_to_doc[doc.filename] = (doc_id, doc, chunk_count)
            else:
                existing_count = filename_to_doc[doc.filename][2]
                if chunk_count > existing_count:
                    print(f"ğŸ”„ ç™¼ç¾æ›´æ–°çš„æ–‡æª” {doc.filename}: {chunk_count} > {existing_count} chunks")
                    filename_to_doc[doc.filename] = (doc_id, doc, chunk_count)
        
        selected = [doc_id for doc_id, _, _ in filename_to_doc.values()]
        print(f"ğŸ” æœªæŒ‡å®šdoc_idsï¼Œè‡ªå‹•é¸æ“‡ {len(selected)} å€‹ä½¿ç”¨structured_hierarchicalç­–ç•¥çš„æ–‡æª”ï¼ˆå·²å»é‡ï¼‰: {[store.docs[d].filename for d in selected]}")
    
    experimental_groups = req.get("experimental_groups", [])  # æ–°å¢ï¼šå¯¦é©—çµ„é¸æ“‡
    all_multi_level_chunks = {}
    
    for doc_id in selected:
        doc = store.docs.get(doc_id)
        if not doc:
            continue
            
        # å„ªå…ˆä½¿ç”¨å·²æœ‰çš„multi_level_chunks
        if doc and hasattr(doc, 'multi_level_chunks') and doc.multi_level_chunks:
            all_multi_level_chunks[doc_id] = doc.multi_level_chunks
            print(f"âœ… ä½¿ç”¨æ–‡æª” {doc.filename} å·²æœ‰çš„multi_level_chunks")
        # å„ªå…ˆä½¿ç”¨å·²æœ‰çš„structured_chunksï¼Œè€Œä¸æ˜¯é‡æ–°å¾JSONç”Ÿæˆ
        elif doc and hasattr(doc, 'structured_chunks') and doc.structured_chunks:
            print(f"ğŸ”„ å¾structured_chunksè½‰æ›ç‚ºmulti_level_chunksï¼Œæ–‡æª”: {doc.filename}")
            try:
                # ç›´æ¥å¾structured_chunksè½‰æ›ï¼Œè€Œä¸æ˜¯é‡æ–°å¾JSONç”Ÿæˆ
                converted_chunks = convert_structured_to_multi_level(doc.structured_chunks)
                all_multi_level_chunks[doc_id] = converted_chunks
                # ä¿å­˜åˆ°æ–‡æª”
                doc.multi_level_chunks = converted_chunks
                store.add_doc(doc)
                store.save_data()
                print(f"âœ… æˆåŠŸè½‰æ› {doc.filename} çš„structured_chunksç‚ºmulti_level_chunks")
            except Exception as e:
                print(f"âš ï¸ å¾structured_chunksè½‰æ›å¤±æ•—: {e}")
                # å¦‚æœè½‰æ›å¤±æ•—ï¼Œæ‰å›é€€åˆ°å¾JSONç”Ÿæˆ
                if hasattr(doc, 'json_data') and doc.json_data:
                    experimental_group = experimental_groups[0] if experimental_groups else 'group_d'
                    print(f"ğŸ”„ å›é€€ï¼šåŸºæ–¼JSONç”Ÿæˆå…­å€‹ç²’åº¦ç´šåˆ¥æ ¼å¼ï¼Œæ–‡æª”: {doc.filename}ï¼Œå¯¦é©—çµ„: {experimental_group}")
                    try:
                        from .chunking import MultiLevelStructuredChunking
                        ml_chunker = MultiLevelStructuredChunking()
                        raw_multi_level_list = ml_chunker.chunk_with_span(
                            doc.text, 
                            json_data=doc.json_data,
                            experimental_group=experimental_group
                        )
                        converted_chunks = convert_structured_to_multi_level(raw_multi_level_list)
                        all_multi_level_chunks[doc_id] = converted_chunks
                        doc.multi_level_chunks = converted_chunks
                        store.add_doc(doc)
                        store.save_data()
                    except Exception as e2:
                        print(f"âŒ åŸºæ–¼JSONç”Ÿæˆä¹Ÿå¤±æ•—: {e2}")
        # æœ€å¾Œæ‰è€ƒæ…®å¾JSONç”Ÿæˆï¼ˆé€šå¸¸ä¸æ‡‰è©²èµ°åˆ°é€™è£¡ï¼‰
        elif doc and hasattr(doc, 'json_data') and doc.json_data:
            experimental_group = experimental_groups[0] if experimental_groups else 'group_d'
            print(f"âš ï¸ è­¦å‘Šï¼šæ–‡æª” {doc.filename} æ²’æœ‰structured_chunksï¼Œå°‡å¾JSONé‡æ–°ç”Ÿæˆï¼ˆå¯èƒ½ç”¢ç”Ÿä¸ä¸€è‡´çš„çµæœï¼‰")
            try:
                from .chunking import MultiLevelStructuredChunking
                ml_chunker = MultiLevelStructuredChunking()
                raw_multi_level_list = ml_chunker.chunk_with_span(
                    doc.text, 
                    json_data=doc.json_data,
                    experimental_group=experimental_group
                )
                converted_chunks = convert_structured_to_multi_level(raw_multi_level_list)
                all_multi_level_chunks[doc_id] = converted_chunks
                doc.multi_level_chunks = converted_chunks
                doc.chunking_strategy = "structured_to_multi_level"
                store.add_doc(doc)
                store.save_data()
            except Exception as e:
                print(f"âŒ åŸºæ–¼JSONç”Ÿæˆå¤šå±¤ç´šå¤±æ•—: {e}")
    
    if not all_multi_level_chunks:
        return JSONResponse(
            status_code=400, 
            content={"error": "No multi-level chunks available. Please run structured hierarchical chunking or multi-level semantic chunking first."}
        )
    
    # æ‰“å°æ¯å€‹æ–‡æª”çš„multi_level_chunksçµ±è¨ˆ
    print(f"ğŸ“Š æ”¶é›†åˆ°çš„multi_level_chunksçµ±è¨ˆ:")
    for doc_id, multi_chunks in all_multi_level_chunks.items():
        doc = store.docs.get(doc_id)
        doc_name = doc.filename if doc else doc_id
        total_chunks = sum(len(chunks) for chunks in multi_chunks.values() if isinstance(chunks, list))
        level_counts = {level: len(chunks) for level, chunks in multi_chunks.items() if isinstance(chunks, list)}
        print(f"   æ–‡æª” {doc_name}: ç¸½è¨ˆ {total_chunks} å€‹chunks, å„å±¤ç´š: {level_counts}")
    
    # è«–æ–‡ä¸­çš„å…­å€‹å±¤æ¬¡
    six_levels = [
        'document',                    # 1. æ–‡ä»¶å±¤ç´š
        'document_component',          # 2. æ–‡ä»¶çµ„æˆéƒ¨åˆ†å±¤ç´š
        'basic_unit_hierarchy',        # 3. åŸºæœ¬å–®ä½å±¤æ¬¡çµæ§‹å±¤ç´š
        'basic_unit',                  # 4. åŸºæœ¬å–®ä½å±¤ç´š
        'basic_unit_component',        # 5. åŸºæœ¬å–®ä½çµ„æˆéƒ¨åˆ†å±¤ç´š
        'enumeration'                  # 6. åˆ—èˆ‰å±¤ç´š
    ]
    
    # å¦‚æœæŒ‡å®šäº†å¯¦é©—çµ„ï¼Œåªè™•ç†ç›¸é—œå±¤æ¬¡
    if experimental_groups:
        print(f"ğŸ¯ æ”¶åˆ°å¯¦é©—çµ„é¸æ“‡: {experimental_groups}")
        # æ”¶é›†æ‰€æœ‰éœ€è¦çš„å±¤æ¬¡
        required_levels = set()
        for group_key in experimental_groups:
            if group_key in GRANULARITY_COMBINATIONS:
                group_levels = GRANULARITY_COMBINATIONS[group_key]["levels"]
                print(f"   ğŸ“‹ {group_key}: {GRANULARITY_COMBINATIONS[group_key]['name']} -> å±¤æ¬¡: {group_levels}")
                required_levels.update(group_levels)
            else:
                print(f"   âš ï¸ æœªçŸ¥çš„å¯¦é©—çµ„: {group_key}")
        
        # åªè™•ç†éœ€è¦çš„å±¤æ¬¡
        original_levels = six_levels.copy()
        six_levels = [level for level in six_levels if level in required_levels]
        print(f"ğŸ¯ å¯¦é©—çµ„æ¨¡å¼ï¼šå¾ {len(original_levels)} å€‹å±¤æ¬¡ä¸­é¸æ“‡ {len(six_levels)} å€‹å±¤æ¬¡")
        print(f"   åŸå§‹å±¤æ¬¡: {original_levels}")
        print(f"   é¸ä¸­å±¤æ¬¡: {six_levels}")
        print(f"   è·³éå±¤æ¬¡: {[level for level in original_levels if level not in required_levels]}")
    
    # ç‚ºæ¯å€‹å±¤æ¬¡å‰µå»ºç¨ç«‹çš„embedding
    level_results = {}
    total_vectors = 0
    total_levels = len(six_levels)
    completed_levels = 0
    
    print(f"ğŸš€ é–‹å§‹å¤šå±¤æ¬¡embeddingè™•ç†ï¼Œå…± {total_levels} å€‹å±¤æ¬¡")
    print(f"ğŸ¯ æ‰€æœ‰å¯¦é©—çµ„ï¼ˆAã€Bã€Cã€Dï¼‰çµ±ä¸€ä½¿ç”¨ {EMBEDDING_DIMENSION} ç¶­ç´¢å¼•")
    print(f"ğŸ“Š ç•¶å‰EMBEDDING_DIMENSIONé…ç½®: {EMBEDDING_DIMENSION}")
    
    for level_idx, level_name in enumerate(six_levels):
        level_chunks = []
        level_doc_ids = []
        
        # æ”¶é›†è©²å±¤æ¬¡çš„æ‰€æœ‰chunks
        for doc_id, multi_chunks in all_multi_level_chunks.items():
            if level_name in multi_chunks:
                for chunk_data in multi_chunks[level_name]:
                    if isinstance(chunk_data, dict) and 'content' in chunk_data:
                        level_chunks.append(chunk_data['content'])
                        level_doc_ids.append(doc_id)
        
        if not level_chunks:
            print(f"âš ï¸ å±¤æ¬¡ '{level_name}' æ²’æœ‰å¯ç”¨çš„chunks")
            completed_levels += 1
            progress = (completed_levels / total_levels) * 100
            print(f"ğŸ“Š é€²åº¦: {completed_levels}/{total_levels} ({progress:.1f}%)")
            continue
        
        print(f"ğŸ” é–‹å§‹ç‚ºå±¤æ¬¡ '{level_name}' å‰µå»ºembeddingï¼Œå…± {len(level_chunks)} å€‹chunks")
        
            # ç‚ºè©²å±¤æ¬¡å‰µå»ºembedding
        try:
            print(f"â³ æ­£åœ¨è™•ç†å±¤æ¬¡ '{level_name}' çš„embedding...")
            if USE_GEMINI_EMBEDDING and GOOGLE_API_KEY:
                vectors = await embed_gemini(level_chunks)
                provider = "gemini"
                model = "gemini-embedding-001"
            elif USE_BGE_M3_EMBEDDING and SENTENCE_TRANSFORMERS_AVAILABLE:
                vectors = embed_bge_m3(level_chunks)
                provider = "bge-m3"
                model = "BAAI/bge-m3"
            else:
                print(f"âŒ å±¤æ¬¡ '{level_name}' embeddingå¤±æ•—ï¼šæ²’æœ‰å¯ç”¨çš„embeddingæ–¹æ³•")
                completed_levels += 1
                progress = (completed_levels / total_levels) * 100
                print(f"ğŸ“Š é€²åº¦: {completed_levels}/{total_levels} ({progress:.1f}%)")
                continue
            
            # é©—è­‰å‘é‡ç¶­åº¦
            dimension = len(vectors[0]) if vectors and len(vectors) > 0 else EMBEDDING_DIMENSION
            print(f"ğŸ“Š å±¤æ¬¡ '{level_name}' embeddingç¶­åº¦: {dimension} (é…ç½®: {EMBEDDING_DIMENSION})")
            
            # é©—è­‰ç¶­åº¦ä¸€è‡´æ€§ï¼ˆæ‡‰è©²éƒ½æ˜¯3072ï¼‰
            if dimension != EMBEDDING_DIMENSION:
                print(f"âš ï¸ è­¦å‘Šï¼šå±¤æ¬¡ '{level_name}' çš„embeddingç¶­åº¦({dimension})èˆ‡é…ç½®({EMBEDDING_DIMENSION})ä¸åŒ")
                print(f"âš ï¸ å¼·åˆ¶ä½¿ç”¨é…ç½®çš„ç¶­åº¦ {EMBEDDING_DIMENSION}ï¼Œè«‹æª¢æŸ¥embeddingé…ç½®")
            
            # æª¢æŸ¥ä¸¦æ¸…é™¤èˆŠçš„å¤šå±¤æ¬¡ç´¢å¼•ï¼ˆå¦‚æœç¶­åº¦ä¸åŒ¹é…ï¼‰
            if level_name in faiss_store.multi_level_index_info:
                old_dimension = faiss_store.multi_level_index_info[level_name].dimension
                if old_dimension != dimension:
                    print(f"âš ï¸ æª¢æ¸¬åˆ°å±¤æ¬¡ '{level_name}' èˆŠç´¢å¼•ç¶­åº¦({old_dimension})èˆ‡æ–°embeddingç¶­åº¦({dimension})ä¸åŒ¹é…ï¼Œæ¸…é™¤èˆŠç´¢å¼•")
                    # æ¸…é™¤è©²å±¤æ¬¡çš„èˆŠç´¢å¼•
                    if level_name in faiss_store.multi_level_indices:
                        del faiss_store.multi_level_indices[level_name]
                    if level_name in faiss_store.multi_level_index_info:
                        del faiss_store.multi_level_index_info[level_name]
                    if level_name in faiss_store.multi_level_chunk_ids:
                        faiss_store.multi_level_chunk_ids[level_name] = []
                    if level_name in faiss_store.multi_level_chunk_doc_ids:
                        faiss_store.multi_level_chunk_doc_ids[level_name] = []
                    if level_name in faiss_store.multi_level_chunks_flat:
                        faiss_store.multi_level_chunks_flat[level_name] = []
            
            # å­˜å„²è©²å±¤æ¬¡çš„embeddingå’Œå…ƒæ•¸æ“š
            metadata = {
                "provider": provider,
                "model": model,
                "dimension": dimension
            }
            store.set_multi_level_embeddings(level_name, vectors, level_chunks, level_doc_ids, metadata)
            
            # æ–°å¢ï¼šå­˜å„²åˆ°FAISSå’ŒBM25
            # ç¢ºä¿chunk_idåŒ…å«å±¤æ¬¡ä¿¡æ¯ï¼Œé¿å…è·¨å±¤æ¬¡é‡è¤‡
            level_chunk_ids = [f"{level_name}_{doc_id}_{i}" for i, doc_id in enumerate(level_doc_ids)]
            faiss_store.add_multi_level_vectors(level_name, vectors, level_chunk_ids, level_doc_ids, level_chunks)
            bm25_index.build_multi_level_index(level_name, level_chunks, level_chunk_ids, level_doc_ids)
            
            # æ–°å¢ï¼šæ‰¹é‡å¢å¼·è©²å±¤æ¬¡çš„metadataï¼ˆå¯é¸ï¼‰
            level_enhanced_metadata = {}
            if req.get("enable_metadata_enhancement", True):
                print(f"ğŸ”§ é–‹å§‹å¢å¼·å±¤æ¬¡ '{level_name}' çš„metadata...")
                level_chunks_data = [
                    {
                        "chunk_id": level_chunk_ids[i],
                        "content": level_chunks[i],
                        "metadata": {}
                    }
                    for i in range(len(level_chunks))
                ]
                level_enhanced_metadata = metadata_enhancer.enhance_metadata_batch(level_chunks_data)
                
                # è¨­ç½®å¢å¼·metadataåˆ°FAISSå­˜å„²
                for chunk_id, enhanced_metadata in level_enhanced_metadata.items():
                    faiss_store.set_multi_level_enhanced_metadata(level_name, chunk_id, enhanced_metadata)
            else:
                print(f"âš ï¸ è·³éå±¤æ¬¡ '{level_name}' çš„metadataå¢å¼·")
            
            level_results[level_name] = {
                "provider": provider,
                "model": model,
                "num_vectors": len(vectors),
                "dimension": len(vectors[0]) if vectors else 0,
                "num_chunks": len(level_chunks),
                "level_description": get_level_description(level_name)
            }
            
            total_vectors += len(vectors)
            completed_levels += 1
            progress = (completed_levels / total_levels) * 100
            print(f"âœ… å±¤æ¬¡ '{level_name}' embeddingå®Œæˆï¼š{len(vectors)} å€‹å‘é‡")
            print(f"ğŸ“Š é€²åº¦: {completed_levels}/{total_levels} ({progress:.1f}%)")
            
        except Exception as e:
            print(f"âŒ å±¤æ¬¡ '{level_name}' embeddingå¤±æ•—ï¼š{e}")
            # ä½¿ç”¨éš¨æ©Ÿå‘é‡ä½œç‚ºfallback
            try:
                import numpy as np
                fallback_vectors = np.random.randn(len(level_chunks), EMBEDDING_DIMENSION).astype(np.float32).tolist()
                metadata = {
                    "provider": "fallback_random",
                    "model": f"random_{EMBEDDING_DIMENSION}d",
                    "dimension": EMBEDDING_DIMENSION
                }
                store.set_multi_level_embeddings(level_name, fallback_vectors, level_chunks, level_doc_ids, metadata)
                
                level_results[level_name] = {
                    "provider": "fallback_random",
                    "model": f"random_{EMBEDDING_DIMENSION}d",
                    "num_vectors": len(fallback_vectors),
                    "dimension": EMBEDDING_DIMENSION,
                    "num_chunks": len(level_chunks),
                    "level_description": get_level_description(level_name),
                    "error": f"Original embedding failed, using fallback: {str(e)}"
                }
                
                total_vectors += len(fallback_vectors)
                print(f"âš ï¸ å±¤æ¬¡ '{level_name}' ä½¿ç”¨fallbackå‘é‡ï¼š{len(fallback_vectors)} å€‹")
                
            except Exception as fallback_error:
                print(f"âŒ å±¤æ¬¡ '{level_name}' fallbackä¹Ÿå¤±æ•—ï¼š{fallback_error}")
                level_results[level_name] = {
                    "error": f"Both original and fallback failed: {str(e)} | {str(fallback_error)}",
                    "num_chunks": len(level_chunks),
                    "level_description": get_level_description(level_name)
                }
            
            completed_levels += 1
            progress = (completed_levels / total_levels) * 100
            print(f"ğŸ“Š é€²åº¦: {completed_levels}/{total_levels} ({progress:.1f}%)")
    
    print(f"ğŸ‰ å¤šå±¤æ¬¡embeddingè™•ç†å®Œæˆï¼ç¸½å…±è™•ç†äº† {total_vectors} å€‹å‘é‡ï¼ŒæˆåŠŸå®Œæˆ {completed_levels}/{total_levels} å€‹å±¤æ¬¡")
    
    # è‡ªå‹•ä¿å­˜å¤šå±¤æ¬¡embeddingæ•¸æ“š
    store.save_data()
    faiss_store.save_data()
    bm25_index.save_data()
    
    # ç¢ºä¿å¤šå±¤æ¬¡embeddingç‹€æ…‹æ­£ç¢ºè¨­ç½®
    print(f"ğŸ‰ å¤šå±¤æ¬¡embeddingå®Œæˆï¼Œä¿å­˜çš„å±¤æ¬¡: {list(store.multi_level_embeddings.keys())}")
    print(f"ğŸ” store.has_multi_level_embeddings(): {store.has_multi_level_embeddings()}")
    print(f"ğŸ” å¯ç”¨å±¤æ¬¡: {store.get_available_levels()}")
    
    # å¦‚æœé€™æ˜¯Açµ„ï¼ˆåƒ…basic_unitï¼‰ï¼Œä¹Ÿå‰µå»ºæ¨™æº–embeddingä»¥ä¿æŒå…¼å®¹æ€§
    if experimental_groups and len(experimental_groups) == 1 and experimental_groups[0] == "group_a":
        if "basic_unit" in store.multi_level_embeddings:
            basic_unit_data = store.multi_level_embeddings["basic_unit"]
            store.embeddings = basic_unit_data.get('embeddings', [])
            store.chunk_doc_ids = basic_unit_data.get('doc_ids', [])
            store.chunks_flat = basic_unit_data.get('chunks', [])
            print(f"ğŸ”„ Açµ„ï¼šåŒæ­¥å‰µå»ºæ¨™æº–embeddingï¼Œå‘é‡æ•¸é‡: {len(store.embeddings)}")
            store.save_data()
    
    if not level_results:
        return JSONResponse(
            status_code=500,
            content={"error": "Failed to create embeddings for any level"}
        )
    
    # å¦‚æœæŒ‡å®šäº†å¯¦é©—çµ„ï¼Œè¨ˆç®—å„çµ„çš„embeddingç‹€æ…‹
    group_results = {}
    if experimental_groups:
        for group_key in experimental_groups:
            if group_key in GRANULARITY_COMBINATIONS:
                combination = GRANULARITY_COMBINATIONS[group_key]
                group_levels = combination["levels"]
                
                group_results[group_key] = {
                    "name": combination["name"],
                    "levels": group_levels,
                    "embedding_status": {},
                    "total_chunks": 0
                }
                
                for level in group_levels:
                    if level in level_results:
                        group_results[group_key]["embedding_status"][level] = "completed"
                        group_results[group_key]["total_chunks"] += level_results[level]["num_chunks"]
                    else:
                        group_results[group_key]["embedding_status"][level] = "missing"

    return {
        "message": "Six-level embeddings created successfully",
        "total_vectors": total_vectors,
        "levels": level_results,
        "available_levels": list(level_results.keys()),
        "level_descriptions": {
            level: get_level_description(level) for level in six_levels
        },
        "experimental_groups": group_results if experimental_groups else None,
        "faiss_available": True,
        "bm25_available": True,
        "enhanced_metadata_available": True
    }


def get_level_description(level_name: str) -> str:
    """ç²å–å±¤æ¬¡æè¿°"""
    descriptions = {
        'document': 'æ–‡ä»¶å±¤ç´š (Document Level) - æ•´å€‹æ³•å¾‹æ–‡æª”',
        'document_component': 'æ–‡ä»¶çµ„æˆéƒ¨åˆ†å±¤ç´š (Document Component Level) - æ–‡æª”çš„ä¸»è¦çµ„æˆéƒ¨åˆ†',
        'basic_unit_hierarchy': 'åŸºæœ¬å–®ä½å±¤æ¬¡çµæ§‹å±¤ç´š (Basic Unit Hierarchy Level) - æ›¸ç±ã€æ¨™é¡Œã€ç« ç¯€',
        'basic_unit': 'åŸºæœ¬å–®ä½å±¤ç´š (Basic Unit Level) - æ–‡ç« /æ¢æ–‡ (article)',
        'basic_unit_component': 'åŸºæœ¬å–®ä½çµ„æˆéƒ¨åˆ†å±¤ç´š (Basic Unit Component Level) - å¼·åˆ¶æ€§ä¸»æ–‡æˆ–æ®µè½',
        'enumeration': 'åˆ—èˆ‰å±¤ç´š (Enumeration Level) - é …ç›®ã€å­é …'
    }
    return descriptions.get(level_name, f"æœªçŸ¥å±¤æ¬¡: {level_name}")


def generate_hierarchical_description_from_metadata(doc_id: str, metadata: Dict[str, Any], content: str, store) -> str:
    """
    å¾metadataç›´æ¥ç”Ÿæˆå±¤ç´šæè¿°ï¼ˆç”¨æ–¼å¤šå±¤ç´šæª¢ç´¢ï¼‰
    
    Args:
        doc_id: æ–‡æª”ID
        metadata: chunkçš„metadata
        content: chunkå…§å®¹
        store: å­˜å„²å¯¦ä¾‹
    
    Returns:
        å±¤ç´šæè¿°å­—ç¬¦ä¸²
    """
    try:
        # ç²å–æ–‡æª”ä¿¡æ¯
        doc = store.get_doc(doc_id)
        if not doc:
            return f"doc={doc_id}"
        
        # ç²å–æ–‡æª”åç¨±ï¼ˆå»é™¤.jsonå¾Œç¶´ï¼‰
        doc_name = doc.filename
        if doc_name.endswith('.json'):
            doc_name = doc_name[:-5]
        
        # å„ªå…ˆå¾metadataä¸­ç²å–æ³•å¾‹åç¨±
        law_name = ""
        if metadata.get('id'):
            law_name = extract_law_name_from_metadata_id(metadata['id'])
        
        if not law_name and metadata.get('law_name'):
            law_name = metadata['law_name'].replace('æ³•è¦åç¨±ï¼š', '')
        
        if not law_name:
            law_name = extract_law_name_from_content(content)
        
        if not law_name:
            law_name = doc_name
        
        # æ§‹å»ºå±¤ç´šæè¿°
        hierarchy_parts = [law_name]
        
        # æ·»åŠ ç« ç¯€ä¿¡æ¯
        if metadata.get('chapter'):
            chapter = metadata['chapter']
            if chapter != "æœªåˆ†é¡ç« " and chapter != "æœªåˆ†é¡ç¯€":
                if not chapter.startswith('ç¬¬') and not chapter.startswith('ç« '):
                    chapter = f"ç¬¬{chapter}ç« "
                hierarchy_parts.append(chapter)
        
        # æ·»åŠ ç¯€ä¿¡æ¯
        if metadata.get('section'):
            section = metadata['section']
            if section != "æœªåˆ†é¡ç¯€":
                if not section.startswith('ç¬¬') and not section.startswith('ç¯€'):
                    section = f"ç¬¬{section}ç¯€"
                hierarchy_parts.append(section)
        
        # å¾å…§å®¹ä¸­æå–æ­£ç¢ºçš„æ¢æ–‡è™Ÿç¢¼ï¼Œå„ªå…ˆä½¿ç”¨å…§å®¹ä¸­çš„ä¿¡æ¯
        article_number = extract_article_number_from_content(content)
        if article_number:
            hierarchy_parts.append(article_number)
        elif metadata.get('article'):
            article = metadata['article']
            if not article.startswith('ç¬¬') and not article.startswith('æ¢'):
                article = f"ç¬¬{article}æ¢"
            hierarchy_parts.append(article)
        
        # æ·»åŠ é …ä¿¡æ¯
        if metadata.get('items') and len(metadata['items']) > 0:
            items = metadata['items']
            if len(items) == 1:
                hierarchy_parts.append(f"ç¬¬{items[0]}é …")
            else:
                hierarchy_parts.append(f"ç¬¬{items[0]}-{items[-1]}é …")
        
        return ' '.join(hierarchy_parts)
        
    except Exception as e:
        print(f"âŒ å¾metadataç”Ÿæˆå±¤ç´šæè¿°å¤±æ•—: {e}")
        return f"doc={doc_id}"


def generate_hierarchical_description(doc_id: str, level: str, chunk_index: int, store) -> str:
    """
    ç”Ÿæˆå±¤ç´šæè¿°ï¼Œä¾‹å¦‚ï¼šè‘—ä½œæ¬Šæ³• ç¬¬ä¸‰ç«  ç¬¬ä¸€ç¯€ ç¬¬11æ¢
    
    Args:
        doc_id: æ–‡æª”ID
        level: å±¤ç´šåç¨±
        chunk_index: chunkç´¢å¼•
        store: å­˜å„²å¯¦ä¾‹
    
    Returns:
        å±¤ç´šæè¿°å­—ç¬¦ä¸²
    """
    try:
        # ç²å–æ–‡æª”ä¿¡æ¯
        doc = store.get_doc(doc_id)
        if not doc:
            return f"doc={doc_id}"
        
        # ç²å–æ–‡æª”åç¨±ï¼ˆå»é™¤.jsonå¾Œç¶´ï¼‰
        doc_name = doc.filename
        if doc_name.endswith('.json'):
            doc_name = doc_name[:-5]
        
        # å¦‚æœæ˜¯å¤šå±¤æ¬¡embeddingï¼Œå˜—è©¦å¾åŸå§‹æ–‡æª”çš„structured_chunksä¸­ç²å–å±¤ç´šä¿¡æ¯
        if hasattr(doc, 'structured_chunks') and doc.structured_chunks and chunk_index < len(doc.structured_chunks):
            structured_chunk = doc.structured_chunks[chunk_index]
            metadata = structured_chunk.get('metadata', {})
            content = structured_chunk.get('content', '')
            
            # å„ªå…ˆå¾metadataä¸­ç²å–æ³•å¾‹åç¨±ï¼Œé€™æ˜¯æœ€å¯é çš„ä¾†æº
            law_name = ""
            if metadata.get('id'):
                # å¾metadata idä¸­æå–æ³•å¾‹åç¨±
                law_name = extract_law_name_from_metadata_id(metadata['id'])
            
            # å¦‚æœmetadataä¸­æœ‰law_nameå­—æ®µï¼Œä¹Ÿå˜—è©¦ä½¿ç”¨å®ƒ
            if not law_name and metadata.get('law_name'):
                law_name = metadata['law_name']
                # æ¸…ç†å¯èƒ½å­˜åœ¨çš„"æ³•è¦åç¨±ï¼š"å‰ç¶´
                law_name = law_name.replace('æ³•è¦åç¨±ï¼š', '')
            
            # å¦‚æœmetadataä¸­æ²’æœ‰ï¼Œå†å˜—è©¦å¾å…§å®¹ä¸­æå–
            if not law_name:
                law_name = extract_law_name_from_content(content)
            
            # æœ€å¾Œä½¿ç”¨æ–‡æª”åç¨±
            if not law_name:
                law_name = doc_name
            
            # æ§‹å»ºå±¤ç´šæè¿°
            hierarchy_parts = [law_name]
            
            # æ·»åŠ ç« ç¯€ä¿¡æ¯
            if metadata.get('chapter'):
                chapter = metadata['chapter']
                # æ¸…ç†ç« ç¯€æ ¼å¼
                if chapter != "æœªåˆ†é¡ç¯€":
                    if not chapter.startswith('ç¬¬') and not chapter.startswith('ç« '):
                        chapter = f"ç¬¬{chapter}ç« "
                    hierarchy_parts.append(chapter)
            
            # æ·»åŠ ç¯€ä¿¡æ¯
            if metadata.get('section'):
                section = metadata['section']
                # æ¸…ç†ç¯€æ ¼å¼
                if section != "æœªåˆ†é¡ç¯€":
                    if not section.startswith('ç¬¬') and not section.startswith('ç¯€'):
                        section = f"ç¬¬{section}ç¯€"
                    hierarchy_parts.append(section)
            
            # å¾å…§å®¹ä¸­æå–æ­£ç¢ºçš„æ¢æ–‡è™Ÿç¢¼ï¼Œå„ªå…ˆä½¿ç”¨å…§å®¹ä¸­çš„ä¿¡æ¯
            article_number = extract_article_number_from_content(content)
            if article_number:
                hierarchy_parts.append(article_number)
            elif metadata.get('article'):
                article = metadata['article']
                if not article.startswith('ç¬¬') and not article.startswith('æ¢'):
                    article = f"ç¬¬{article}æ¢"
                hierarchy_parts.append(article)
            
            # æ·»åŠ é …ä¿¡æ¯
            if metadata.get('items') and len(metadata['items']) > 0:
                items = metadata['items']
                if len(items) == 1:
                    hierarchy_parts.append(f"ç¬¬{items[0]}é …")
                else:
                    hierarchy_parts.append(f"ç¬¬{items[0]}-{items[-1]}é …")
            
            return ' '.join(hierarchy_parts)
        
        # å¦‚æœæ²’æœ‰çµæ§‹åŒ–ä¿¡æ¯ï¼Œæ ¹æ“šå±¤ç´šåç¨±ç”ŸæˆåŸºæœ¬æè¿°
        level_descriptions = {
            'document': f"{doc_name} å…¨æ–‡",
            'document_component': f"{doc_name} ç« ç¯€",
            'basic_unit_hierarchy': f"{doc_name} æ¢æ–‡å±¤æ¬¡",
            'basic_unit': f"{doc_name} æ¢æ–‡",
            'basic_unit_component': f"{doc_name} æ¢æ–‡çµ„æˆ",
            'enumeration': f"{doc_name} åˆ—èˆ‰é …"
        }
        
        return level_descriptions.get(level, f"{doc_name} {level}")
        
    except Exception as e:
        print(f"âŒ ç”Ÿæˆå±¤ç´šæè¿°å¤±æ•—: {e}")
        return f"doc={doc_id}"


def extract_law_name_from_content(content: str) -> str:
    """å¾å…§å®¹ä¸­æå–æ³•å¾‹åç¨±"""
    import re
    
    # åŒ¹é…ã€æ³•å¾‹åç¨±ã€‘æ ¼å¼
    law_pattern = r'ã€([^ã€‘]+)ã€‘'
    match = re.search(law_pattern, content)
    if match:
        return match.group(1)
    
    # å¦‚æœæ²’æœ‰æ‰¾åˆ°ã€ã€‘æ ¼å¼ï¼Œå˜—è©¦å…¶ä»–æ¨¡å¼
    law_patterns = [
        r'^([^ç¬¬ç« ç¯€æ¢é …]+æ³•)',
        r'([^ç¬¬ç« ç¯€æ¢é …]+æ³•)',
    ]
    
    for pattern in law_patterns:
        match = re.search(pattern, content)
        if match:
            return match.group(1).strip()
    
    return ""


def extract_law_name_from_metadata_id(metadata_id: str) -> str:
    """å¾metadata IDä¸­æå–æ³•å¾‹åç¨±"""
    import re
    
    # å˜—è©¦å…©ç¨®æ ¼å¼ï¼š
    # 1. åŸå§‹æ ¼å¼ï¼šæ³•è¦åç¨±ï¼šå•†æ¨™æ³•_0_ç¬¬ä¸€ç« _ç¸½å‰‡_æœªåˆ†é¡ç¯€_ç¬¬1æ¢
    # 2. æ¸…ç†å¾Œæ ¼å¼ï¼šå•†æ¨™æ³•_0_ç¬¬ä¸€ç« _ç¸½å‰‡_æœªåˆ†é¡ç¯€_ç¬¬1æ¢
    
    # é¦–å…ˆå˜—è©¦åŸå§‹æ ¼å¼
    law_pattern = r'æ³•è¦åç¨±ï¼š([^_]+)'
    match = re.search(law_pattern, metadata_id)
    if match:
        return match.group(1)
    
    # å¦‚æœæ²’æœ‰æ‰¾åˆ°ï¼Œå˜—è©¦æ¸…ç†å¾Œçš„æ ¼å¼ï¼ˆç¬¬ä¸€å€‹éƒ¨åˆ†å°±æ˜¯æ³•è¦åç¨±ï¼‰
    parts = metadata_id.split('_')
    if parts and parts[0]:
        # æª¢æŸ¥ç¬¬ä¸€å€‹éƒ¨åˆ†æ˜¯å¦åŒ…å«"æ³•"å­—ï¼Œç¢ºèªå®ƒæ˜¯æ³•è¦åç¨±
        first_part = parts[0].strip()
        if 'æ³•' in first_part or 'æ¢ä¾‹' in first_part:
            return first_part
    
    return ""


def extract_article_number_from_content(content: str) -> str:
    """å¾å…§å®¹ä¸­æå–æ¢æ–‡è™Ÿç¢¼"""
    import re
    
    # æŒ‰è¡Œåˆ†å‰²å…§å®¹ï¼Œå°‹æ‰¾æ¢æ–‡è™Ÿç¢¼
    lines = content.split('\n')
    
    for line in lines:
        line = line.strip()
        if not line:
            continue
            
        # åŒ¹é…å„ç¨®æ¢æ–‡è™Ÿç¢¼æ ¼å¼
        article_patterns = [
            r'ç¬¬(\d+æ¢)',           # ç¬¬43æ¢
            r'ç¬¬([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒ]+)æ¢',  # ç¬¬å››åä¸‰æ¢
            r'ç¬¬(\d+-\d+æ¢)',      # ç¬¬43-1æ¢
            r'ç¬¬(\d+[ä¹‹-]\d+æ¢)',   # ç¬¬43ä¹‹1æ¢
        ]
        
        for pattern in article_patterns:
            match = re.search(pattern, line)
            if match:
                # æª¢æŸ¥é€™è¡Œæ˜¯å¦çœ‹èµ·ä¾†åƒæ¢æ–‡æ¨™é¡Œï¼ˆé€šå¸¸æ¯”è¼ƒç°¡çŸ­ï¼Œä¸åŒ…å«å¤ªå¤šå…§å®¹ï¼‰
                if len(line) < 50:  # æ¢æ–‡æ¨™é¡Œé€šå¸¸æ¯”è¼ƒçŸ­
                    return f"ç¬¬{match.group(1)}"
    
    # å¦‚æœæ²’æœ‰æ‰¾åˆ°ç°¡çŸ­çš„æ¢æ–‡æ¨™é¡Œï¼Œå˜—è©¦åœ¨æ•´å€‹å…§å®¹ä¸­æ‰¾ç¬¬ä¸€å€‹æ¢æ–‡è™Ÿç¢¼
    article_patterns = [
        r'ç¬¬(\d+æ¢)',
        r'ç¬¬([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒ]+)æ¢',
        r'ç¬¬(\d+-\d+æ¢)',
        r'ç¬¬(\d+[ä¹‹-]\d+æ¢)',
    ]
    
    for pattern in article_patterns:
        match = re.search(pattern, content)
        if match:
            return f"ç¬¬{match.group(1)}"
    
    return ""


def rank_with_dense_vectors(query: str, k: int):
    """ä½¿ç”¨å¯†é›†å‘é‡é€²è¡Œç›¸ä¼¼åº¦è¨ˆç®—ï¼ˆæ”¯æŒ Gemini å’Œ BGE-M3ï¼Œå„ªå…ˆä½¿ç”¨FAISSï¼‰"""
    import numpy as np
    
    # å„ªå…ˆä½¿ç”¨FAISS
    if faiss_store.has_vectors():
        try:
            # ç”ŸæˆæŸ¥è©¢å‘é‡
            if USE_GEMINI_EMBEDDING and GOOGLE_API_KEY:
                try:
                    query_vector = asyncio_run(embed_gemini([query]))[0]
                except Exception as e:
                    print(f"Gemini query embedding failed: {e}")
                    if USE_BGE_M3_EMBEDDING and SENTENCE_TRANSFORMERS_AVAILABLE:
                        query_vector = embed_bge_m3([query])[0]
                    else:
                        raise RuntimeError("Both Gemini and BGE-M3 query embedding failed")
            elif USE_BGE_M3_EMBEDDING and SENTENCE_TRANSFORMERS_AVAILABLE:
                query_vector = embed_bge_m3([query])[0]
            else:
                raise RuntimeError("No dense embedding method available")
            
            # FAISSæœç´¢
            indices, scores = faiss_store.search(query_vector, k)
            return indices, scores
            
        except Exception as e:
            print(f"FAISS search failed, falling back to NumPy: {e}")
    
    # å›é€€åˆ°NumPyï¼ˆä¿æŒå‘å¾Œå…¼å®¹ï¼‰
    if store.embeddings is None:
        raise ValueError("No embeddings available")
    if isinstance(store.embeddings, list):
        vecs = np.array(store.embeddings, dtype=float)
    else:
        vecs = np.array(store.embeddings, dtype=float)  # type: ignore[assignment]
    
    # æ ¹æ“šç•¶å‰é…ç½®é¸æ“‡æŸ¥è©¢å‘é‡åŒ–æ–¹æ³•
    if USE_GEMINI_EMBEDDING and GOOGLE_API_KEY:
        try:
            qvec = np.array(asyncio_run(embed_gemini([query]))[0], dtype=float)
        except Exception as e:
            print(f"Gemini query embedding failed: {e}")
            # å¦‚æœ Gemini å¤±æ•—ï¼Œå˜—è©¦ BGE-M3
            if USE_BGE_M3_EMBEDDING and SENTENCE_TRANSFORMERS_AVAILABLE:
                try:
                    qvec = np.array(embed_bge_m3([query])[0], dtype=float)
                except Exception as e2:
                    print(f"BGE-M3 query embedding failed: {e2}")
                    raise RuntimeError("Both Gemini and BGE-M3 query embedding failed")
            else:
                raise RuntimeError("Gemini query embedding failed and BGE-M3 not available")
    elif USE_BGE_M3_EMBEDDING and SENTENCE_TRANSFORMERS_AVAILABLE:
        try:
            qvec = np.array(embed_bge_m3([query])[0], dtype=float)
        except Exception as e:
            print(f"BGE-M3 query embedding failed: {e}")
            raise RuntimeError("BGE-M3 query embedding failed")
    else:
        raise RuntimeError("No dense embedding method available")
    
    # normalize
    vecs_norm = vecs / (np.linalg.norm(vecs, axis=1, keepdims=True) + 1e-8)
    q_norm = qvec / (np.linalg.norm(qvec) + 1e-8)
    sims = vecs_norm @ q_norm
    idxs = np.argsort(-sims)[:k]
    return idxs.tolist(), sims[idxs].tolist()


def calculate_query_qa_similarity(query: str, qa_query: str) -> float:
    """è¨ˆç®—æŸ¥è©¢èˆ‡QAçš„ç›¸ä¼¼åº¦"""
    import re
    
    query_lower = query.lower().strip()
    qa_lower = qa_query.lower().strip()
    
    # æ–¹æ³•1: å®Œå…¨åŒ¹é…
    if query_lower == qa_lower:
        return 1.0
    
    # æ–¹æ³•2: åŒ…å«åŒ¹é…
    if query_lower in qa_lower or qa_lower in query_lower:
        return 0.9
    
    # æ–¹æ³•3: æ³•æ¢è™Ÿç¢¼åŒ¹é…
    query_article_match = re.search(r'ç¬¬\s*(\d+(?:ä¹‹\d+)?)\s*æ¢', query_lower)
    qa_article_match = re.search(r'ç¬¬\s*(\d+(?:ä¹‹\d+)?)\s*æ¢', qa_lower)
    
    if query_article_match and qa_article_match:
        query_article = query_article_match.group(1)
        qa_article = qa_article_match.group(1)
        if query_article == qa_article:
            return 0.8
    
    # æ–¹æ³•4: æ³•å¾‹åç¨±åŒ¹é…
    law_names = ["è‘—ä½œæ¬Šæ³•", "å•†æ¨™æ³•", "å°ˆåˆ©æ³•", "æ°‘æ³•", "åˆ‘æ³•"]
    query_laws = [law for law in law_names if law in query_lower]
    qa_laws = [law for law in law_names if law in qa_lower]
    
    if query_laws and qa_laws and any(law in qa_laws for law in query_laws):
        return 0.7
    
    # æ–¹æ³•5: é—œéµè©é‡ç–Š
    query_words = set(re.findall(r'\w+', query_lower))
    qa_words = set(re.findall(r'\w+', qa_lower))
    
    if query_words and qa_words:
        overlap = len(query_words.intersection(qa_words))
        union = len(query_words.union(qa_words))
        jaccard_similarity = overlap / union if union > 0 else 0
        return jaccard_similarity * 0.6  # é™ä½æ¬Šé‡
    
    return 0.0


def expand_query_with_legal_domain(query: str) -> Dict[str, Any]:
    """ä½¿ç”¨é ˜åŸŸå°ˆå±¬è©åº«é€²è¡ŒæŸ¥è©¢æ“´å±•"""
    
    # å¦‚æœæœ‰æ³•å¾‹æ¨ç†å¼•æ“ï¼Œå„ªå…ˆä½¿ç”¨
    if legal_reasoning_engine:
        try:
            analysis = legal_reasoning_engine.analyze_query(query)
            expanded_query = legal_reasoning_engine.get_expanded_query(query)
            
            return {
                "original_query": query,
                "expanded_query": expanded_query,
                "expansion_ratio": len(expanded_query.split()) / len(query.split()),
                "domain_matches": analysis["concept_mappings"],
                "detected_domains": ["copyright"] if analysis["detected_concepts"] else [],
                "applicable_articles": analysis["applicable_articles"],
                "reasoning_explanation": analysis["reasoning_explanation"],
                "confidence_scores": analysis["confidence_scores"],
                "reasoning_engine_used": True
            }
        except Exception as e:
            print(f"âš ï¸ æ³•å¾‹æ¨ç†å¼•æ“åŸ·è¡Œå¤±æ•—: {e}")
            # å›é€€åˆ°åŸæœ‰æ–¹æ³•
    
    # é ˜åŸŸå°ˆå±¬è©åº« - æ³•å¾‹æ¦‚å¿µæ˜ å°„
    legal_domain_dict = {
        # è‘—ä½œæ¬Šæ³•å°ˆå±¬è©å½™
        "copyright": {
                   "æ ¸å¿ƒæ¦‚å¿µ": {
                       "é‡è£½": ["è¤‡è£½", "æŠ„è¥²", "ç›œç‰ˆ", "ç¿»å°", "å½±å°", "æƒæ", "ä¸‹è¼‰", "ä¿å­˜", "ç›´æ¥è¤‡è£½"],
                       "æ”¹ä½œ": ["æ”¹å¯«", "æ”¹ç·¨", "ä¿®æ”¹", "è¡ç”Ÿ", "å‰µä½œ", "é‡æ–°å‰µä½œ", "äºŒæ¬¡å‰µä½œ", "ç”¨è‡ªå·±çš„èªæ°£", "æ”¹å¯«æˆè‡ªå·±çš„èªæ°£", "æ”¹å¯«æˆ", "èªæ°£", "ç¿»è­¯", "è­¯", "è½‰è­¯", "ä¸­è­¯", "è‹±è­¯", "æ—¥è­¯"],
                       "æ•£å¸ƒ": ["åˆ†äº«", "å‚³æ’­", "ç™¼å¸ƒ", "ä¸Šå‚³", "è½‰è¼‰", "è½‰ç™¼", "å‚³é€"],
                "å…¬é–‹å‚³è¼¸": ["ä¸Šç¶²", "ç¶²è·¯å‚³æ’­", "ç·šä¸Šåˆ†äº«", "ä¸²æµ", "ç›´æ’­"],
                "å…¬é–‹æ¼”å‡º": ["è¡¨æ¼”", "æ¼”å¥", "æ¼”å”±", "æ’­æ”¾", "æ”¾æ˜ "],
                "å…¬é–‹å±•ç¤º": ["å±•è¦½", "å±•ç¤º", "é™³åˆ—", "å±•å‡º"],
                "å‡ºç§Ÿ": ["ç§Ÿå€Ÿ", "ç§Ÿè³ƒ", "å‡ºå€Ÿ"],
                "ä¾µå®³": ["é•å", "ä¾µçŠ¯", "æå®³", "é•æ³•", "ä¸æ³•"],
                       "åˆç†ä½¿ç”¨": ["å¼•ç”¨", "è©•è«–", "æ•™å­¸", "ç ”ç©¶", "å ±å°", "å­¸è¡“"],
                       "æ•™è‚²ç”¨é€”": ["èª²å ‚", "å­¸æ ¡", "æ•™è‚²", "æ•™å­¸", "æˆèª²", "å­¸ç”Ÿ", "æ’­æ”¾", "å½±ç‰‡", "youtube", "å½±ç‰‡"],
                       "è‘—ä½œè²¡ç”¢æ¬Š": ["ç‰ˆæ¬Š", "è²¡ç”¢æ¬Š", "ç¶“æ¿Ÿæ¬Šåˆ©"],
                       "è‘—ä½œäººæ ¼æ¬Š": ["ç½²åæ¬Š", "å®Œæ•´æ€§æ¬Š", "åè­½æ¬Š"],
                "å…¬é–‹ç™¼è¡¨": ["ç™¼è¡¨", "å‡ºç‰ˆ", "å…¬é–‹", "ç™¼å¸ƒ"],
                "å‰µä½œ": ["è£½ä½œ", "ç”¢ç”Ÿ", "å®Œæˆ", "å¯«ä½œ", "ç¹ªè£½"],
                "è‘—ä½œ": ["ä½œå“", "å‰µä½œ", "è—è¡“å“", "æ–‡å­¸", "éŸ³æ¨‚", "ç¾è¡“", "æ”å½±"],
                "è‘—ä½œäºº": ["ä½œè€…", "å‰µä½œè€…", "è—è¡“å®¶", "ä½œå®¶"],
            },
            "æ³•å¾‹æ¢æ–‡": {
                "ç¬¬3æ¢": ["å®šç¾©", "æ¦‚å¿µ", "è§£é‡‹", "èªªæ˜", "ä½•è¬‚"],
                "ç¬¬10æ¢": ["è‘—ä½œæ¬Šå–å¾—", "å®Œæˆæ™‚", "äº«æœ‰", "ç”¢ç”Ÿ"],
                "ç¬¬22æ¢": ["é‡è£½æ¬Š", "è¤‡è£½æ¬Š"],
                "ç¬¬26æ¢": ["å…¬é–‹æ¼”å‡ºæ¬Š"],
                "ç¬¬26-1æ¢": ["å…¬é–‹å‚³è¼¸æ¬Š"],
                "ç¬¬28æ¢": ["æ”¹ä½œæ¬Š", "è¡ç”Ÿè‘—ä½œ", "ç¿»è­¯", "è­¯", "è½‰è­¯", "ä¸­è­¯", "è‹±è­¯", "æ—¥è­¯", "æ”¹ä½œ", "æ”¹ç·¨", "ä¿®æ”¹", "è¡ç”Ÿ", "å‰µä½œ", "é‡æ–°å‰µä½œ", "äºŒæ¬¡å‰µä½œ"],
                "ç¬¬28-1æ¢": ["æ•£å¸ƒæ¬Š"],
                "ç¬¬29æ¢": ["å‡ºç§Ÿæ¬Š"],
                       "ç¬¬44æ¢": ["å¸æ³•ç¨‹åº", "é‡è£½"],
                       "ç¬¬46æ¢": ["å­¸æ ¡", "æˆèª²", "æ•™å­¸", "é‡è£½"],
                       "ç¬¬47æ¢": ["æ•™è‚²", "å­¸æ ¡", "å…¬é–‹æ’­é€", "å…¬é–‹å‚³è¼¸"],
                       "ç¬¬65æ¢": ["åˆç†ä½¿ç”¨", "ä¾‹å¤–", "é™åˆ¶"],
                       "ç¬¬87æ¢": ["è¦–ç‚ºä¾µå®³", "ç¦æ­¢è¡Œç‚º"],
                       "ç¬¬91æ¢": ["é‡è£½ç½ª", "åˆ‘ç½°", "ç½°é‡‘"],
            }
        },
        
        # å•†æ¨™æ³•å°ˆå±¬è©å½™
        "trademark": {
            "æ ¸å¿ƒæ¦‚å¿µ": {
                "å•†æ¨™": ["æ¨™èªŒ", "æ¨™è­˜", "å“ç‰Œ", "å•†è™Ÿ", "logo", "æ¨™è¨˜"],
                "è¨»å†Š": ["ç”³è«‹", "ç™»è¨˜", "æ ¸å‡†", "å–å¾—"],
                "ä»¿å†’": ["å‡å†’", "å½é€ ", "ä»¿è£½", "å±±å¯¨", "ç›œç”¨"],
                "æ··æ·†": ["ç›¸ä¼¼", "è¿‘ä¼¼", "èª¤èª", "æ··åŒ"],
                "ä½¿ç”¨": ["ä½¿ç”¨", "ç¶“ç‡Ÿ", "éŠ·å”®", "å»£å‘Š"],
                "å°ˆç”¨æ¬Š": ["ç¨å ", "æ’ä»–", "å°ˆæœ‰"],
                "ä¾µå®³": ["ä¾µæ¬Š", "é•å", "æå®³"],
            },
            "æ³•å¾‹æ¢æ–‡": {
                "ç¬¬2æ¢": ["å®šç¾©", "å•†æ¨™", "æœå‹™æ¨™ç« "],
                "ç¬¬5æ¢": ["è¨»å†Š", "ç”³è«‹", "æ ¸å‡†"],
                "ç¬¬29æ¢": ["è¿‘ä¼¼", "æ··æ·†", "é¡ä¼¼"],
                "ç¬¬68æ¢": ["ä¾µå®³", "ä¾µæ¬Š", "ç¦æ­¢"],
                "ç¬¬95æ¢": ["åˆ‘ç½°", "ä»¿å†’ç½ª"],
            }
        },
        
        # å°ˆåˆ©æ³•å°ˆå±¬è©å½™
        "patent": {
            "æ ¸å¿ƒæ¦‚å¿µ": {
                "ç™¼æ˜": ["å‰µæ–°", "æŠ€è¡“", "æ”¹è‰¯", "è¨­è¨ˆ"],
                "å°ˆåˆ©": ["å°ˆåˆ©æ¬Š", "ç¨å æ¬Š"],
                "æ–°ç©æ€§": ["æ–°", "æœªå…¬é–‹", "é¦–å‰µ"],
                "é€²æ­¥æ€§": ["éé¡¯è€Œæ˜“è¦‹", "æŠ€è¡“é€²æ­¥"],
                "ç”¢æ¥­åˆ©ç”¨æ€§": ["å¯¦ç”¨", "å¯è¡Œ", "è£½é€ "],
                "ç”³è«‹": ["æå‡º", "æäº¤", "ç”³å ±"],
                "æ ¸å‡†": ["é€šé", "æˆæ¬Š", "å…¬å‘Š"],
            }
        }
    }
    
    # æŸ¥è©¢æ“´å±•é‚è¼¯
    expanded_terms = set()
    # æ”¹é€²æŸ¥è©¢åˆ†å‰²ï¼Œè™•ç†ä¸­æ–‡å’Œæ¨™é»ç¬¦è™Ÿ
    import re
    # ç§»é™¤æ¨™é»ç¬¦è™Ÿï¼Œç„¶å¾Œåˆ†å‰²
    cleaned_query = re.sub(r'[ï¼Œã€‚ï¼Ÿï¼ã€ï¼›ï¼šï¼Ÿ]', ' ', query.lower())
    # ä½¿ç”¨ç©ºæ ¼å’Œæ¨™é»ç¬¦è™Ÿåˆ†å‰²
    original_terms = set(re.split(r'[\sï¼Œã€‚ï¼Ÿï¼ã€ï¼›ï¼šï¼Ÿ]+', cleaned_query))
    # ç§»é™¤ç©ºå­—ç¬¦ä¸²
    original_terms = {term for term in original_terms if term.strip()}
    domain_matches = []
    
    # 1. è­˜åˆ¥æŸ¥è©¢é ˜åŸŸ
    detected_domains = []
    if any(term in query for term in ["è‘—ä½œæ¬Š", "ç‰ˆæ¬Š", "è‘—ä½œ", "å‰µä½œ", "é‡è£½", "æ”¹ä½œ", "èª²å ‚", "æ•™è‚²", "æ•™å­¸", "å­¸æ ¡", "æ’­æ”¾", "å½±ç‰‡", "youtube", "æˆæ¬Š"]):
        detected_domains.append("copyright")
    if any(term in query for term in ["å•†æ¨™", "å“ç‰Œ", "æ¨™èªŒ", "ä»¿å†’"]):
        detected_domains.append("trademark")
    if any(term in query for term in ["å°ˆåˆ©", "ç™¼æ˜", "æŠ€è¡“", "å‰µæ–°"]):
        detected_domains.append("patent")
    
    # 2. æŸ¥è©¢æ“´å±•
    for domain in detected_domains:
        if domain in legal_domain_dict:
            domain_data = legal_domain_dict[domain]
            
            # æ ¸å¿ƒæ¦‚å¿µæ“´å±•
            for legal_concept, synonyms in domain_data["æ ¸å¿ƒæ¦‚å¿µ"].items():
                # ç›´æ¥æª¢æŸ¥æŸ¥è©¢ä¸­æ˜¯å¦åŒ…å«åŒç¾©è©
                for synonym in synonyms:
                    if synonym in query:
                        expanded_terms.update(synonyms)
                        expanded_terms.add(legal_concept)
                        domain_matches.append(f"{synonym}â†’{legal_concept}")
                # ä¹Ÿæª¢æŸ¥æŸ¥è©¢ä¸­æ˜¯å¦åŒ…å«æ¦‚å¿µæœ¬èº«
                if legal_concept in query:
                    expanded_terms.update(synonyms)
                    expanded_terms.add(legal_concept)
                    domain_matches.append(f"æŸ¥è©¢â†’{legal_concept}")
            
            # æ³•å¾‹æ¢æ–‡æ“´å±•
            for article, keywords in domain_data["æ³•å¾‹æ¢æ–‡"].items():
                for term in original_terms:
                    if term in keywords:
                        expanded_terms.update(keywords)
                        expanded_terms.add(article)
                        domain_matches.append(f"{term}â†’{article}")
                # ä¹Ÿæª¢æŸ¥æŸ¥è©¢ä¸­æ˜¯å¦åŒ…å«æ¢æ–‡é—œéµå­—
                for keyword in keywords:
                    if keyword in query:
                        expanded_terms.update(keywords)
                        expanded_terms.add(article)
                        domain_matches.append(f"{keyword}â†’{article}")
    
    # 3. ç”Ÿæˆæ“´å±•æŸ¥è©¢
    expanded_query_terms = list(original_terms.union(expanded_terms))
    expanded_query = " ".join(expanded_query_terms)
    
    return {
        "original_query": query,
        "expanded_query": expanded_query,
        "detected_domains": detected_domains,
        "expanded_terms": list(expanded_terms),
        "domain_matches": domain_matches,
        "expansion_ratio": len(expanded_terms) / len(original_terms) if original_terms else 0
    }


def detect_content_hierarchy(content: str) -> str:
    """åŸºæ–¼å…§å®¹åˆ†ææª¢æ¸¬å±¤æ¬¡ç´šåˆ¥"""
    import re
    
    # æª¢æ¸¬æ³•æ¢ç´šåˆ¥ï¼ˆåŒ…å«"ç¬¬Xæ¢"ï¼‰
    if re.search(r'ç¬¬\s*\d+\s*æ¢', content):
        return "article"
    
    # æª¢æ¸¬ç¯€ç´šåˆ¥ï¼ˆåŒ…å«"ç¬¬Xç¯€"æˆ–"ç¬¬Xç« "ï¼‰
    if re.search(r'ç¬¬\s*\d+\s*[ç¯€ç« ]', content):
        return "section"
    
    # æª¢æ¸¬ç« ç´šåˆ¥ï¼ˆåŒ…å«"ç¬¬Xç« "æˆ–"ç¸½å‰‡"ã€"é™„å‰‡"ç­‰ï¼‰
    if re.search(r'ç¬¬\s*\d+\s*ç« |ç¸½å‰‡|é™„å‰‡', content):
        return "chapter"
    
    # æª¢æ¸¬æ˜¯å¦ç‚ºå…·é«”æ³•å¾‹æ¢æ–‡å…§å®¹
    if re.search(r'æ¢æ–‡|è¦å®š|æ¬Šåˆ©|ç¾©å‹™|ç¦æ­¢|è™•ç½°', content):
        return "article"
    
    # é»˜èªç‚ºä¸€èˆ¬å…§å®¹
    return "general"


def calculate_hierarchical_relevance(query: str, result: Dict) -> Dict[str, Any]:
    """è¨ˆç®—å±¤æ¬¡åŒ–ç›¸é—œæ€§åˆ†æ•¸ - åŸºæ–¼è«–æ–‡çš„Aboutnessæ¦‚å¿µå’Œå…§å®¹åˆ†æ"""
    content = result.get("content", "")
    metadata = result.get("metadata", {})
    
    # åŸºæ–¼å…§å®¹åˆ†ææª¢æ¸¬å±¤æ¬¡ç´šåˆ¥ï¼ˆå‚™ç”¨æ–¹æ¡ˆï¼‰
    content_hierarchy = detect_content_hierarchy(content)
    
    # æå–å±¤æ¬¡ä¿¡æ¯ï¼ˆå„ªå…ˆä½¿ç”¨metadataï¼Œå‚™ç”¨å…§å®¹åˆ†æï¼‰
    hierarchy_level = "article"  # é»˜èªå±¤ç´š
    if metadata and metadata.get("article"):
        hierarchy_level = "article"
    elif metadata and metadata.get("section"):
        hierarchy_level = "section"
    elif metadata and metadata.get("chapter"):
        hierarchy_level = "chapter"
    else:
        # ä½¿ç”¨å…§å®¹åˆ†æçµæœ
        hierarchy_level = content_hierarchy
    
    # Aboutnessåˆ†æ - è­˜åˆ¥æ–‡æœ¬çš„ä¸»è¦ä¸»é¡Œ
    aboutness_score = 0.0
    aboutness_keywords = []
    
    # æ³•å¾‹æ¦‚å¿µaboutness
    legal_concepts = ["è‘—ä½œæ¬Š", "ç‰ˆæ¬Š", "ä¾µæ¬Š", "é‡è£½", "æ”¹ä½œ", "æ•£å¸ƒ", "åˆç†ä½¿ç”¨", "æˆæ¬Š", "å•†æ¨™", "å°ˆåˆ©"]
    for concept in legal_concepts:
        if concept in content:
            aboutness_score += 1.0
            aboutness_keywords.append(concept)
    
    # çµæ§‹å±¤ç´šæ¬Šé‡ï¼ˆåŸºæ–¼è«–æ–‡çš„å¤šå±¤æ¬¡æ–¹æ³•ï¼‰
    hierarchy_weights = {
        "article": 1.0,    # æ³•æ¢ç´šåˆ¥ - æœ€é«˜ç²¾åº¦
        "section": 0.8,    # ç¯€ç´šåˆ¥ - ä¸­ç­‰ç²¾åº¦
        "chapter": 0.6,    # ç« ç´šåˆ¥ - è¼ƒä½ç²¾åº¦ä½†å»£åº¦æ›´å¤§
        "general": 0.4     # ä¸€èˆ¬å…§å®¹ - æœ€ä½æ¬Šé‡
    }
    
    hierarchy_weight = hierarchy_weights.get(hierarchy_level, 1.0)
    
    return {
        "aboutness_score": aboutness_score,
        "aboutness_keywords": aboutness_keywords,
        "hierarchy_level": hierarchy_level,
        "hierarchy_weight": hierarchy_weight,
        "content_hierarchy": content_hierarchy  # å…§å®¹åˆ†æçš„çµæœ
    }


def calculate_retrieval_metrics(query: str, results: List[Dict], k: int) -> Dict[str, float]:
    """è¨ˆç®—æª¢ç´¢æŒ‡æ¨™ P@K å’Œ R@K - æ•´åˆæŸ¥è©¢æ“´å±•ã€æ™ºèƒ½ç›¸é—œæ€§åˆ¤æ–·å’Œå¤šå±¤æ¬¡æª¢ç´¢"""
    try:
        print(f"ğŸ” é–‹å§‹è¨ˆç®—æª¢ç´¢æŒ‡æ¨™ï¼ŒæŸ¥è©¢: '{query}', k={k}")
        
        if not results:
            print("âŒ æ²’æœ‰æª¢ç´¢çµæœ")
            return {"p_at_k": 0.0, "r_at_k": 0.0, "note": "No retrieval results"}
        
        # 1. æŸ¥è©¢æ“´å±•è™•ç†
        query_expansion = expand_query_with_legal_domain(query)
        expanded_query = query_expansion["expanded_query"]
        detected_domains = query_expansion["detected_domains"]
        domain_matches = query_expansion["domain_matches"]
        
        print(f"ğŸ” æŸ¥è©¢æ“´å±•: åŸæŸ¥è©¢='{query}'")
        print(f"ğŸ” æ“´å±•æŸ¥è©¢: '{expanded_query}'")
        print(f"ğŸ” æª¢æ¸¬é ˜åŸŸ: {detected_domains}")
        print(f"ğŸ” é ˜åŸŸæ˜ å°„: {domain_matches[:5]}...")  # åªé¡¯ç¤ºå‰5å€‹
        
        # åŸºæ–¼æŸ¥è©¢å…§å®¹å’Œæª¢ç´¢çµæœè¨ˆç®—ç›¸é—œæ€§
        relevant_chunks = []
        query_lower = query.lower()
        expanded_query_lower = expanded_query.lower()
        
        # æå–æŸ¥è©¢ä¸­çš„é—œéµä¿¡æ¯
        import re
        
        # æå–æ³•æ¢è™Ÿç¢¼
        article_patterns = [
            r'ç¬¬\s*(\d+)\s*æ¢',
            r'æ¢\s*(\d+)',
            r'article\s*(\d+)',
        ]
        
        article_numbers = []
        for pattern in article_patterns:
            matches = re.findall(pattern, query)
            article_numbers.extend([int(m) for m in matches])
        
        # æå–æ³•å¾‹åç¨±
        law_keywords = []
        law_patterns = ['è‘—ä½œæ¬Šæ³•', 'å•†æ¨™æ³•', 'å°ˆåˆ©æ³•', 'æ°‘æ³•', 'åˆ‘æ³•']
        for law in law_patterns:
            if law in query:
                law_keywords.append(law)
        
        # æª¢æ¸¬æŸ¥è©¢é¡å‹
        has_explicit_article = len(article_numbers) > 0
        query_type = "explicit_article" if has_explicit_article else "semantic_query"
        
        print(f"ğŸ“‹ æŸ¥è©¢åˆ†æ: é¡å‹={query_type}, æ³•æ¢è™Ÿç¢¼={article_numbers}, æ³•å¾‹é—œéµå­—={law_keywords}")
        
        # åˆ¤æ–·æ¯å€‹æª¢ç´¢çµæœçš„ç›¸é—œæ€§ï¼ˆæ•´åˆæŸ¥è©¢æ“´å±•å’Œå¤šå±¤æ¬¡æª¢ç´¢ï¼‰
        for i, result in enumerate(results):
            content = result.get("content", "")
            content_lower = content.lower()
            
            relevance_score = 0
            relevance_reasons = []
            
            # è¨ˆç®—å±¤æ¬¡åŒ–ç›¸é—œæ€§ï¼ˆåŸºæ–¼è«–æ–‡çš„å¤šå±¤æ¬¡æ–¹æ³•ï¼‰
            hierarchical_analysis = calculate_hierarchical_relevance(query, result)
            aboutness_score = hierarchical_analysis["aboutness_score"]
            hierarchy_weight = hierarchical_analysis["hierarchy_weight"]
            hierarchy_level = hierarchical_analysis["hierarchy_level"]
            
            # å±¤æ¬¡åŒ–ç›¸é—œæ€§åŠ åˆ†
            if aboutness_score > 0:
                relevance_score += aboutness_score * hierarchy_weight * 0.5  # é©åº¦æ¬Šé‡
                relevance_reasons.append(f"å±¤æ¬¡åŒ–aboutness({hierarchy_level}):{aboutness_score:.1f}")
            
            # 1. æ³•æ¢è™Ÿç¢¼åŒ¹é…ï¼ˆæ¬Šé‡æœ€é«˜ï¼Œåƒ…é©ç”¨æ–¼æ˜ç¢ºæ³•æ¢æŸ¥è©¢ï¼‰
            if has_explicit_article:
                for article_num in article_numbers:
                    if f'ç¬¬{article_num}æ¢' in content or f'ç¬¬ {article_num} æ¢' in content:
                        relevance_score += 4  # æé«˜æ¬Šé‡
                        relevance_reasons.append(f"ç²¾ç¢ºåŒ¹é…æ³•æ¢{article_num}")
                        break
            
            # 2. æ³•å¾‹åç¨±åŒ¹é…
            for law in law_keywords:
                if law in content:
                    relevance_score += 2
                    relevance_reasons.append(f"åŒ¹é…æ³•å¾‹{law}")
            
            # 3. æŸ¥è©¢æ“´å±•åŒ¹é…ï¼ˆæ–°å¢ï¼‰
            expanded_words = set(expanded_query_lower.split())
            content_words = set(content_lower.split())
            expanded_matches = expanded_words.intersection(content_words)
            
            if len(expanded_matches) > 0:
                # è¨ˆç®—æ“´å±•åŒ¹é…çš„æ¬Šé‡
                expansion_weight = min(len(expanded_matches) * 0.8, 3.0)  # æœ€å¤š3åˆ†
                relevance_score += expansion_weight
                relevance_reasons.append(f"æ“´å±•åŒ¹é…{len(expanded_matches)}å€‹è©:{list(expanded_matches)[:3]}")
            
            # 4. é ˜åŸŸå°ˆå±¬æ¦‚å¿µåŒ¹é…ï¼ˆæ–°å¢ï¼‰- æ”¹é€²é‚è¼¯
            for domain_match in domain_matches[:5]:  # é™åˆ¶åŒ¹é…æ•¸é‡
                concept = domain_match.split("â†’")[-1]
                if concept in content:
                    # å°æ–¼"æ”¹ä½œ"æ¦‚å¿µï¼Œçµ¦äºˆæ›´é«˜æ¬Šé‡
                    if concept == "æ”¹ä½œ":
                        relevance_score += 2.5  # é«˜æ¬Šé‡
                        relevance_reasons.append(f"æ ¸å¿ƒæ¦‚å¿µ:{concept}")
                    else:
                        relevance_score += 1.5
                        relevance_reasons.append(f"é ˜åŸŸæ¦‚å¿µ:{concept}")
            
            # 5. ç›´æ¥æ¦‚å¿µåŒ¹é…ï¼ˆæ–°å¢ï¼‰
            if "æ”¹ä½œ" in content and ("æ”¹å¯«" in query_lower or "èªæ°£" in query_lower):
                relevance_score += 3.0  # æœ€é«˜æ¬Šé‡
                relevance_reasons.append("ç›´æ¥æ¦‚å¿µåŒ¹é…:æ”¹ä½œ")
            
            # 6. åŸå§‹æŸ¥è©¢é—œéµè©åŒ¹é…
            query_words = set(query_lower.split())
            original_matches = query_words.intersection(content_words)
            
            if len(original_matches) >= 1:
                relevance_score += len(original_matches) * 0.3  # è¼ƒä½æ¬Šé‡ï¼Œé¿å…é‡è¤‡è¨ˆç®—
                relevance_reasons.append(f"åŸå§‹åŒ¹é…{len(original_matches)}å€‹è©:{list(original_matches)}")
            
            # 7. ç›¸ä¼¼åº¦åˆ†æ•¸ï¼ˆèª¿æ•´é–¾å€¼ï¼‰
            if 'score' in result:
                if result['score'] > 0.3:  # é€²ä¸€æ­¥é™ä½é–¾å€¼
                    relevance_score += result['score'] * 1.5  # é©åº¦æ¬Šé‡
                    relevance_reasons.append(f"ç›¸ä¼¼åº¦{result['score']:.2f}")
            
            # 8. èªç¾©æŸ¥è©¢çš„ç‰¹æ®ŠåŠ åˆ†ï¼ˆæ›´åš´æ ¼çš„æ¢ä»¶ï¼‰
            if not has_explicit_article and relevance_score > 1.0:  # åªæœ‰ç•¶åŸºç¤åˆ†æ•¸å¤ é«˜æ™‚æ‰åŠ åˆ†
                relevance_score += 0.3  # é€²ä¸€æ­¥é™ä½é¡å¤–åŠ åˆ†
                relevance_reasons.append("èªç¾©æŸ¥è©¢åŠ åˆ†")
            
            # å‹•æ…‹é–¾å€¼ï¼šæ›´å¯¬é¬†çš„æ¨™æº–ä»¥è­˜åˆ¥ç›¸é—œå…§å®¹
            if has_explicit_article:
                base_threshold = 3.0  # æ˜ç¢ºæ³•æ¢æŸ¥è©¢çš„é–¾å€¼
            else:
                base_threshold = 1.5  # èªç¾©æŸ¥è©¢çš„é–¾å€¼ï¼Œç¢ºä¿èƒ½è­˜åˆ¥ç›¸é—œå…§å®¹
            
            # å¦‚æœæœ‰æŸ¥è©¢æ“´å±•ï¼Œé©åº¦é™ä½é–¾å€¼
            if query_expansion["expansion_ratio"] > 2.0:  # ç•¶æœ‰é¡¯è‘—æ“´å±•æ™‚
                base_threshold *= 0.7  # æ›´ç©æ¥µçš„èª¿æ•´
            
            if relevance_score >= base_threshold:
                relevant_chunks.append(i)
                print(f"   âœ… Chunk {i+1} ç›¸é—œ (åˆ†æ•¸:{relevance_score:.1f}): {relevance_reasons} - {content[:50]}...")
            else:
                print(f"   âŒ Chunk {i+1} ä¸ç›¸é—œ (åˆ†æ•¸:{relevance_score:.1f}): {content[:50]}...")
        
        print(f"ğŸ“Š æ‰¾åˆ° {len(relevant_chunks)} å€‹ç›¸é—œchunks: {relevant_chunks}")
        
        # è¨ˆç®—P@Kå’ŒR@K
        top_k_results = results[:k]
        relevant_in_top_k = 0
        
        for i, result in enumerate(top_k_results):
            if i in relevant_chunks:
                relevant_in_top_k += 1
        
        p_at_k = relevant_in_top_k / k if k > 0 else 0.0
        r_at_k = relevant_in_top_k / len(relevant_chunks) if relevant_chunks else 0.0
        
        print(f"ğŸ“ˆ è©•æ¸¬çµæœ: P@{k}={p_at_k:.3f}, R@{k}={r_at_k:.3f}")
        
        return {
            "p_at_k": p_at_k,
            "r_at_k": r_at_k,
            "relevant_chunks_count": len(relevant_chunks),
            "relevant_chunks_indices": relevant_chunks,
            "query_analysis": {
                "query_type": query_type,
                "article_numbers": article_numbers,
                "law_keywords": law_keywords,
                "total_results": len(results),
                "threshold_used": base_threshold,
                "expansion_ratio": query_expansion["expansion_ratio"]
            },
            "query_expansion": {
                "original_query": query,
                "expanded_query": expanded_query,
                "detected_domains": detected_domains,
                "expansion_ratio": query_expansion["expansion_ratio"],
                "domain_matches": domain_matches[:10]  # é™åˆ¶è¿”å›æ•¸é‡
            },
            "note": f"æ™ºèƒ½åˆ†æ({query_type}+æŸ¥è©¢æ“´å±•)ï¼Œæ‰¾åˆ°{len(relevant_chunks)}å€‹ç›¸é—œçµæœ"
        }
        
    except Exception as e:
        print(f"âŒ è¨ˆç®—æª¢ç´¢æŒ‡æ¨™æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        import traceback
        traceback.print_exc()
        return {"p_at_k": 0.0, "r_at_k": 0.0, "error": str(e)}


def load_qa_data() -> List[Dict]:
    """è¼‰å…¥ QA æ•¸æ“š"""
    try:
        import json
        import os
        
        # å˜—è©¦è¼‰å…¥ä¸åŒçš„ QA æ–‡ä»¶ï¼ˆæŒ‰å„ªå…ˆç´šæ’åºï¼‰
        qa_files = [
            "QA/qa_gold.json",  # å„ªå…ˆä½¿ç”¨qa_gold.json
            "QA/copyright.json",
            "QA/copyright_p.json",
            "QA/copyright_n.json"
        ]
        
        # ç²å–é …ç›®æ ¹ç›®éŒ„
        current_dir = os.path.dirname(__file__)
        project_root = os.path.join(current_dir, "..", "..")
        project_root = os.path.abspath(project_root)
        
        print(f"ğŸ” æ­£åœ¨è¼‰å…¥QAæ•¸æ“šï¼Œé …ç›®æ ¹ç›®éŒ„: {project_root}")
        
        for qa_file in qa_files:
            qa_path = os.path.join(project_root, qa_file)
            print(f"   å˜—è©¦è¼‰å…¥: {qa_path}")
            
            if os.path.exists(qa_path):
                try:
                    with open(qa_path, 'r', encoding='utf-8') as f:
                        data = json.load(f)
                        if isinstance(data, list) and len(data) > 0:
                            print(f"âœ… æˆåŠŸè¼‰å…¥ {qa_file}ï¼Œå…± {len(data)} æ¢QAæ•¸æ“š")
                            return data
                        else:
                            print(f"âš ï¸  {qa_file} æ ¼å¼ä¸æ­£ç¢ºæˆ–ç‚ºç©º")
                except Exception as e:
                    print(f"âŒ è¼‰å…¥ {qa_file} å¤±æ•—: {e}")
            else:
                print(f"âŒ æ–‡ä»¶ä¸å­˜åœ¨: {qa_path}")
        
        print("âŒ æ‰€æœ‰QAæ–‡ä»¶éƒ½ç„¡æ³•è¼‰å…¥")
        return []
    except Exception as e:
        print(f"âŒ è¼‰å…¥ QA æ•¸æ“šå¤±æ•—: {e}")
        return []


def is_article_match(chunk_content: str, article_number: int, article_suffix: int = None) -> bool:
    """æª¢æŸ¥chunkå…§å®¹æ˜¯å¦åŒ…å«æŒ‡å®šçš„æ³•æ¢è™Ÿç¢¼"""
    import re
    
    if not chunk_content or article_number is None:
        return False
    
    # æ¨™æº–åŒ–æ–‡æœ¬
    content = chunk_content.replace(" ", "").replace("ã€€", "")
    
    if article_suffix is not None:
        # ç¬¬10æ¢ä¹‹1 æˆ– ç¬¬10-1æ¢ æ ¼å¼
        patterns = [
            rf"ç¬¬\s*{article_number}\s*æ¢\s*(?:ä¹‹|-)\s*{article_suffix}",
            rf"ç¬¬\s*{article_number}\s*æ¢\s*ä¹‹\s*{article_suffix}",
            rf"ç¬¬\s*{article_number}\s*æ¢\s*-\s*{article_suffix}",
            rf"ç¬¬\s*{article_number}\s*æ¢ä¹‹{article_suffix}",
            rf"ç¬¬\s*{article_number}\s*æ¢-{article_suffix}"
        ]
    else:
        # ç¬¬3æ¢ æ ¼å¼ï¼ˆä¸åŒ…å«ä¹‹æˆ–-ï¼‰
        patterns = [
            rf"ç¬¬\s*{article_number}\s*æ¢(?![\dä¹‹-])",
            rf"ç¬¬\s*{article_number}\s*æ¢$",
            rf"ç¬¬\s*{article_number}\s*æ¢[^0-9ä¹‹-]"
        ]
    
    for pattern in patterns:
        if re.search(pattern, content):
            return True
    
    return False


def is_law_match(chunk_content: str, law_name: str) -> bool:
    """æª¢æŸ¥chunkå…§å®¹æ˜¯å¦åŒ…å«æŒ‡å®šçš„æ³•å¾‹åç¨±"""
    if not law_name or not chunk_content:
        return True  # å¦‚æœæ²’æœ‰æŒ‡å®šæ³•å¾‹åç¨±ï¼Œä¸é€²è¡ŒåŒ¹é…
    
    # æ³•å¾‹åç¨±è®Šé«”æ˜ å°„
    law_variants = {
        "è‘—ä½œæ¬Šæ³•": ["è‘—ä½œæ¬Šæ³•", "è‘—ä½œæ¬Š", "ç‰ˆæ¬Šæ³•", "ç‰ˆæ¬Š"],
        "å•†æ¨™æ³•": ["å•†æ¨™æ³•", "å•†æ¨™"],
        "å°ˆåˆ©æ³•": ["å°ˆåˆ©æ³•", "å°ˆåˆ©"],
        "æ°‘æ³•": ["æ°‘æ³•", "æ°‘äº‹"],
        "åˆ‘æ³•": ["åˆ‘æ³•", "åˆ‘äº‹"]
    }
    
    variants = law_variants.get(law_name, [law_name])
    content_lower = chunk_content.lower()
    
    # å¦‚æœchunkä¸­åŒ…å«ä»»ä½•æ³•å¾‹åç¨±è®Šé«”ï¼Œå‰‡åŒ¹é…
    if any(variant in content_lower for variant in variants):
        return True
    
    # å¦‚æœchunkä¸­æ²’æœ‰æ˜ç¢ºçš„æ³•å¾‹åç¨±ï¼Œä½†åŒ…å«æ³•æ¢è™Ÿç¢¼ï¼Œä¹Ÿèªç‚ºåŒ¹é…
    # é€™æ˜¯å› ç‚ºæ³•æ¢å…§å®¹æœ¬èº«å¯èƒ½ä¸åŒ…å«æ³•å¾‹åç¨±
    import re
    if re.search(r'ç¬¬\s*\d+\s*æ¢', content_lower):
        return True
    
    return False


def is_relevant_chunk(chunk_content: str, gold_info: Dict[str, Any]) -> bool:
    """åˆ¤æ–·chunkæ˜¯å¦èˆ‡goldæ¨™æº–ç›¸é—œ"""
    if not chunk_content or not gold_info:
        return False
    
    # æ³•æ¢è™Ÿç¢¼åŒ¹é…ï¼ˆå¿…é ˆï¼‰
    article_number = gold_info.get("article_number")
    article_suffix = gold_info.get("article_suffix")
    
    if article_number is None:
        return False  # æ²’æœ‰æ³•æ¢è™Ÿç¢¼ï¼Œç„¡æ³•åˆ¤æ–·ç›¸é—œæ€§
    
    article_match = is_article_match(chunk_content, article_number, article_suffix)
    if not article_match:
        return False
    
    # æ³•å¾‹åç¨±åŒ¹é…ï¼ˆåŠ åˆ†é …ï¼‰
    law_name = gold_info.get("law", "")
    law_match = is_law_match(chunk_content, law_name)
    
    return law_match


def extract_articles_from_text(text: str) -> List[str]:
    """å¾æ–‡æœ¬ä¸­æå–æ³•æ¢ä¿¡æ¯"""
    import re
    
    articles = []
    
    # åŒ¹é… "ç¬¬Xæ¢" æ¨¡å¼
    patterns = [
        r"ç¬¬(\d+)æ¢",
        r"ç¬¬(\d+)æ¢ä¹‹(\d+)",
        r"ç¬¬(\d+)-(\d+)æ¢"
    ]
    
    for pattern in patterns:
        matches = re.findall(pattern, text)
        for match in matches:
            if isinstance(match, tuple):
                if len(match) == 2:
                    if match[1]:  # æœ‰ä¹‹N
                        articles.append(f"ç¬¬{match[0]}æ¢ä¹‹{match[1]}")
                    else:  # ç¯„åœ
                        articles.append(f"ç¬¬{match[0]}-{match[1]}æ¢")
                else:
                    articles.append(f"ç¬¬{match[0]}æ¢")
            else:
                articles.append(f"ç¬¬{match}æ¢")
    
    return list(set(articles))  # å»é‡


def asyncio_run(coro):
    import asyncio
    try:
        loop = asyncio.get_running_loop()
    except RuntimeError:
        loop = None
    if loop and loop.is_running():
        # create task and wait
        return asyncio.run_coroutine_threadsafe(coro, loop).result()
    return asyncio.run(coro)


@app.post("/api/retrieve")
def retrieve(req: RetrieveRequest):
    if store.embeddings is None:
        return JSONResponse(status_code=400, content={"error": "run /embed first"})
    
    # è¨ˆç®—ç›¸ä¼¼åº¦ä¸¦æ’åºï¼ˆåªä½¿ç”¨å¯†é›†å‘é‡ï¼‰
    idxs, sims = rank_with_dense_vectors(req.query, req.k)

    # Use the same order as built in /embed
    chunks_flat = store.chunks_flat
    mapping_doc_ids = store.chunk_doc_ids

    results = []
    for rank, (i, score) in enumerate(zip(idxs, sims), start=1):
        if i < 0 or i >= len(chunks_flat):
            continue
        
        # ç²å–æ–‡æª”ä¿¡æ¯
        doc_id = mapping_doc_ids[i]
        doc = store.docs.get(doc_id)
        
        # åŸºæœ¬çµæœ
        hierarchical_desc = generate_hierarchical_description(doc_id, "standard", i, store)
        
        result = {
            "rank": rank,
            "score": float(score),
            "doc_id": doc_id,
            "chunk_index": i,
            "content": chunks_flat[i][:2000],
            "hierarchical_description": hierarchical_desc,  # æ–°å¢å±¤ç´šæè¿°
        }
        
        # å¦‚æœæœ‰çµæ§‹åŒ–chunksï¼Œæ·»åŠ metadata
        if doc and hasattr(doc, 'structured_chunks') and doc.structured_chunks and i < len(doc.structured_chunks):
            structured_chunk = doc.structured_chunks[i]
            result["metadata"] = structured_chunk.get("metadata", {})
            result["chunk_id"] = structured_chunk.get("chunk_id", "")
            
            # æ·»åŠ æ³•å¾‹çµæ§‹ä¿¡æ¯
            metadata = structured_chunk.get("metadata", {})
            result["legal_structure"] = {
                "id": metadata.get("id", ""),
                "spans": metadata.get("spans", {}),
                "page_range": metadata.get("page_range", {})
            }
        
        results.append(result)
    
    # è¨ˆç®— P@K å’Œ R@Kï¼ˆå¦‚æœæœ‰ QA æ•¸æ“šï¼‰
    metrics = calculate_retrieval_metrics(req.query, results, req.k)
    
    # åˆ¤æ–· embedding provider å’Œ modelï¼ˆä¸å†æ”¯æŒ TF-IDFï¼‰
    if USE_GEMINI_EMBEDDING and GOOGLE_API_KEY:
        embedding_provider = "gemini"
        embedding_model = "gemini-embedding-001"
    elif USE_BGE_M3_EMBEDDING and SENTENCE_TRANSFORMERS_AVAILABLE:
        embedding_provider = "bge-m3"
        embedding_model = "BAAI/bge-m3"
    else:
        embedding_provider = "unknown"
        embedding_model = "unknown"

    return {
        "query": req.query, 
        "k": req.k, 
        "results": results,
        "metrics": metrics,
        "embedding_provider": embedding_provider,
        "embedding_model": embedding_model
    }


@app.post("/api/hierarchical-retrieve")
def hierarchical_retrieve(req: RetrieveRequest):
    """å¤šå±¤æ¬¡æª¢ç´¢ï¼šåŸºæ–¼è«–æ–‡çš„å¤šå±¤æ¬¡åµŒå…¥æª¢ç´¢æ–¹æ³•"""
    if store.embeddings is None:
        return JSONResponse(status_code=400, content={"error": "run /embed first"})
    
    # ç²å–æ‰€æœ‰ chunks å’Œ metadata
    chunks_flat = store.chunks_flat
    mapping_doc_ids = store.chunk_doc_ids
    
    if not chunks_flat:
        return JSONResponse(status_code=400, content={"error": "no chunks available"})
    
    # æ§‹å»ºå±¤æ¬¡åŒ–ç¯€é»
    hierarchical_nodes = []
    for i, (chunk, doc_id) in enumerate(zip(chunks_flat, mapping_doc_ids)):
        doc = store.docs.get(doc_id)
        metadata = {}
        
        # æå–å±¤æ¬¡åŒ–metadata
        if doc and hasattr(doc, 'structured_chunks') and doc.structured_chunks and i < len(doc.structured_chunks):
            structured_chunk = doc.structured_chunks[i]
            metadata = structured_chunk.get("metadata", {})
            
            # ç¢ºå®šå±¤æ¬¡ç´šåˆ¥
            hierarchy_level = "article"  # é»˜èª
            if metadata.get("article"):
                hierarchy_level = "article"
            elif metadata.get("section"):
                hierarchy_level = "section"
            elif metadata.get("chapter"):
                hierarchy_level = "chapter"
            
            metadata["hierarchy_level"] = hierarchy_level
        
        hierarchical_nodes.append({
            "content": chunk,
            "metadata": metadata,
            "doc_id": doc_id,
            "chunk_index": i
        })
    
    # å¤šå±¤æ¬¡æª¢ç´¢é‚è¼¯ - æ”¹é€²ç‰ˆ
    # 1. å…ˆæŒ‰å±¤æ¬¡åˆ†çµ„ï¼ˆåŸºæ–¼å…§å®¹åˆ†æï¼‰
    article_nodes = []
    section_nodes = []
    chapter_nodes = []
    general_nodes = []
    
    for node in hierarchical_nodes:
        content = node.get("content", "")
        hierarchy_level = detect_content_hierarchy(content)
        
        if hierarchy_level == "article":
            article_nodes.append(node)
        elif hierarchy_level == "section":
            section_nodes.append(node)
        elif hierarchy_level == "chapter":
            chapter_nodes.append(node)
        else:
            general_nodes.append(node)
    
    print(f"ğŸ” å±¤æ¬¡åˆ†çµ„: æ³•æ¢{len(article_nodes)}å€‹, ç¯€{len(section_nodes)}å€‹, ç« {len(chapter_nodes)}å€‹, ä¸€èˆ¬{len(general_nodes)}å€‹")
    
    # 2. å°æ¯å€‹å±¤æ¬¡é€²è¡Œæª¢ç´¢
    all_results = []
    
    # æ³•æ¢ç´šåˆ¥æª¢ç´¢ï¼ˆæœ€é«˜ç²¾åº¦ï¼‰
    if article_nodes:
        article_indices = [i for i, node in enumerate(hierarchical_nodes) if node in article_nodes]
        if article_indices:
            article_idxs, article_sims = rank_with_dense_vectors(req.query, k=min(len(article_indices), req.k * 2))
            for idx, sim in zip(article_idxs, article_sims):
                if idx in article_indices:
                    node = hierarchical_nodes[idx]
                    all_results.append({
                        "rank": len(all_results) + 1,
                        "score": float(sim),
                        "doc_id": node["doc_id"],
                        "chunk_index": idx,
                        "content": node["content"][:2000],
                        "metadata": node["metadata"],
                        "hierarchy_level": "article",
                        "hierarchy_weight": 1.0
                    })
    
    # ç¯€ç´šåˆ¥æª¢ç´¢ï¼ˆä¸­ç­‰ç²¾åº¦ï¼‰
    if section_nodes and len(all_results) < req.k:
        section_indices = [i for i, node in enumerate(hierarchical_nodes) if node in section_nodes]
        if section_indices:
            section_idxs, section_sims = rank_with_dense_vectors(req.query, k=min(len(section_indices), req.k))
            for idx, sim in zip(section_idxs, section_sims):
                if idx in section_indices and len(all_results) < req.k:
                    node = hierarchical_nodes[idx]
                    all_results.append({
                        "rank": len(all_results) + 1,
                        "score": float(sim) * 0.8,  # ç¯€ç´šåˆ¥æ¬Šé‡
                        "doc_id": node["doc_id"],
                        "chunk_index": idx,
                        "content": node["content"][:2000],
                        "metadata": node["metadata"],
                        "hierarchy_level": "section",
                        "hierarchy_weight": 0.8
                    })
    
    # ç« ç´šåˆ¥æª¢ç´¢ï¼ˆè¼ƒä½ç²¾åº¦ä½†å»£åº¦æ›´å¤§ï¼‰
    if chapter_nodes and len(all_results) < req.k:
        chapter_indices = [i for i, node in enumerate(hierarchical_nodes) if node in chapter_nodes]
        if chapter_indices:
            chapter_idxs, chapter_sims = rank_with_dense_vectors(req.query, k=min(len(chapter_indices), req.k))
            for idx, sim in zip(chapter_idxs, chapter_sims):
                if idx in chapter_indices and len(all_results) < req.k:
                    node = hierarchical_nodes[idx]
                    all_results.append({
                        "rank": len(all_results) + 1,
                        "score": float(sim) * 0.6,  # ç« ç´šåˆ¥æ¬Šé‡
                        "doc_id": node["doc_id"],
                        "chunk_index": idx,
                        "content": node["content"][:2000],
                        "metadata": node["metadata"],
                        "hierarchy_level": "chapter",
                        "hierarchy_weight": 0.6
                    })
    
    # å–å‰kå€‹çµæœ
    results = all_results[:req.k]
    
    # è¨ˆç®—å¤šå±¤æ¬¡æª¢ç´¢æŒ‡æ¨™
    metrics = calculate_retrieval_metrics(req.query, results, req.k)
    
    # æ·»åŠ å¤šå±¤æ¬¡æª¢ç´¢ç‰¹å®šä¿¡æ¯
    hierarchy_stats = {
        "article_results": len([r for r in results if r.get("hierarchy_level") == "article"]),
        "section_results": len([r for r in results if r.get("hierarchy_level") == "section"]),
        "chapter_results": len([r for r in results if r.get("hierarchy_level") == "chapter"])
    }
    
    metrics["hierarchical_analysis"] = hierarchy_stats
    metrics["note"] = f"å¤šå±¤æ¬¡æª¢ç´¢: æ³•æ¢{hierarchy_stats['article_results']}å€‹, ç¯€{hierarchy_stats['section_results']}å€‹, ç« {hierarchy_stats['chapter_results']}å€‹"
    
    # åˆ¤æ–· embedding provider å’Œ model
    embedding_provider = "gemini"
    embedding_model = "text-embedding-004"
    
    return {
        "results": results,
        "metrics": metrics,
        "embedding_provider": embedding_provider,
        "embedding_model": embedding_model
    }


@app.post("/api/multi-level-retrieve")
async def multi_level_retrieve(req: Dict[str, Any]):
    """å¤šå±¤æ¬¡æª¢ç´¢ï¼šåŸºæ–¼æŸ¥è©¢åˆ†é¡çš„æ™ºèƒ½å±¤æ¬¡é¸æ“‡æª¢ç´¢ï¼Œæ”¯æŒå¯¦é©—çµ„é™åˆ¶"""
    query = req.get("query")
    k = req.get("k", 10)
    experimental_groups = req.get("experimental_groups", [])
    
    if not query:
        return JSONResponse(status_code=400, content={"error": "Query is required"})
    
    # æª¢æŸ¥æ˜¯å¦æœ‰å¯ç”¨çš„å¤šå±¤æ¬¡embedding
    if not store.has_multi_level_embeddings():
        return JSONResponse(
            status_code=400, 
            content={"error": "Multi-level embeddings not available. Please run /api/multi-level-embed first."}
        )
    
    # å¦‚æœæŒ‡å®šäº†å¯¦é©—çµ„ï¼Œé™åˆ¶å¯ç”¨çš„å±¤æ¬¡
    available_levels = store.get_available_levels()
    if experimental_groups:
        print(f"ğŸ¯ å¯¦é©—çµ„é™åˆ¶æª¢ç´¢: {experimental_groups}")
        # æ”¶é›†å¯¦é©—çµ„éœ€è¦çš„å±¤æ¬¡
        required_levels = set()
        for group_key in experimental_groups:
            if group_key in GRANULARITY_COMBINATIONS:
                required_levels.update(GRANULARITY_COMBINATIONS[group_key]["levels"])
        
        # åªä½¿ç”¨å¯¦é©—çµ„éœ€è¦çš„å±¤æ¬¡
        available_levels = [level for level in available_levels if level in required_levels]
        print(f"ğŸ¯ å¯¦é©—çµ„å¯ç”¨å±¤æ¬¡: {available_levels}")
    
    # åˆ†ææŸ¥è©¢ä¸¦åˆ†é¡
    query_analysis = get_query_analysis(query)
    recommended_level = query_analysis['recommended_level']
    query_type = query_analysis['query_type']
    confidence = query_analysis['confidence']
    
    print(f"ğŸ” æŸ¥è©¢åˆ†æï¼šé¡å‹={query_type}, ç½®ä¿¡åº¦={confidence:.3f}, æ¨è–¦å±¤æ¬¡={recommended_level}")
    print(f"ğŸ“Š å¯ç”¨å±¤æ¬¡: {available_levels}")
    
    # æª¢æŸ¥æ¨è–¦å±¤æ¬¡æ˜¯å¦å¯ç”¨ï¼Œå¦‚æœä¸å¯ç”¨å‰‡é¸æ“‡æœ€ä½³å¯ç”¨å±¤æ¬¡
    if recommended_level not in available_levels:
        # æŒ‰å„ªå…ˆç´šé¸æ“‡å¯ç”¨çš„å±¤æ¬¡
        fallback_levels = ['basic_unit', 'basic_unit_component', 'enumeration', 'basic_unit_hierarchy', 'document_component', 'document']
        for fallback_level in fallback_levels:
            if fallback_level in available_levels:
                recommended_level = fallback_level
                print(f"âš ï¸  æ¨è–¦å±¤æ¬¡ {query_analysis['recommended_level']} ä¸å¯ç”¨ï¼Œä½¿ç”¨å‚™é¸å±¤æ¬¡: {recommended_level}")
                break
    
    # ç²å–æ¨è–¦å±¤æ¬¡çš„embedding
    level_data = store.get_multi_level_embeddings(recommended_level)
    if not level_data:
        return JSONResponse(
            status_code=400, 
            content={"error": f"No embeddings available for level: {recommended_level}. Available levels: {available_levels}"}
        )
    
    vectors = level_data['embeddings']
    chunks = level_data['chunks']
    doc_ids = level_data['doc_ids']
    metadata = level_data.get('metadata', {})
    
    print(f"ğŸ“Š ä½¿ç”¨å±¤æ¬¡ '{recommended_level}' é€²è¡Œæª¢ç´¢ï¼Œå…± {len(chunks)} å€‹chunks")
    
    # åŸ·è¡Œæª¢ç´¢
    try:
        import numpy as np
        
        # æª¢æ¸¬å­˜å„²çš„embeddingæ¨¡å‹ä¿¡æ¯
        embedding_provider = metadata.get('provider')
        embedding_dimension = metadata.get('dimension')
        
        if embedding_provider:
            print(f"ğŸ” æª¢æ¸¬åˆ°å­˜å„²çš„embeddingæä¾›è€…: {embedding_provider}, ç¶­åº¦: {embedding_dimension}")
        
        # æ ¹æ“šå­˜å„²çš„embeddingæ¨¡å‹é¸æ“‡æŸ¥è©¢å‘é‡åŒ–æ–¹æ³•
        query_vector = None
        if embedding_provider == 'gemini' or (not embedding_provider and USE_GEMINI_EMBEDDING and GOOGLE_API_KEY):
            query_vector = (await embed_gemini([req.query]))[0]
            print(f"âœ… ä½¿ç”¨Geminiç”ŸæˆæŸ¥è©¢å‘é‡ï¼Œç¶­åº¦: {len(query_vector)}")
        elif embedding_provider == 'bge-m3' or (not embedding_provider and USE_BGE_M3_EMBEDDING and SENTENCE_TRANSFORMERS_AVAILABLE):
            query_vector = embed_bge_m3([req.query])[0]
            print(f"âœ… ä½¿ç”¨BGE-M3ç”ŸæˆæŸ¥è©¢å‘é‡ï¼Œç¶­åº¦: {len(query_vector)}")
        else:
            return JSONResponse(
                status_code=500,
                content={"error": "No embedding method available for query"}
            )
        
        # é©—è­‰ç¶­åº¦åŒ¹é…
        if embedding_dimension and len(query_vector) != embedding_dimension:
            print(f"âš ï¸ è­¦å‘Šï¼šæŸ¥è©¢å‘é‡ç¶­åº¦({len(query_vector)})èˆ‡å­˜å„²å‘é‡ç¶­åº¦({embedding_dimension})ä¸åŒ¹é…")
            return JSONResponse(
                status_code=500,
                content={"error": f"Dimension mismatch: query vector has {len(query_vector)} dimensions but stored embeddings have {embedding_dimension} dimensions. Please re-run /api/multi-level-embed with the current embedding provider."}
            )
        
        # è¨ˆç®—ç›¸ä¼¼åº¦
        if isinstance(vectors, list):
            vectors = np.array(vectors)
        if isinstance(query_vector, list):
            query_vector = np.array(query_vector)
        
        similarities = cosine_similarity([query_vector], vectors)[0]
        
        # ç²å–top-kçµæœ
        top_indices = np.argsort(similarities)[::-1][:req.k]
        
        results = []
        for i, idx in enumerate(top_indices):
            doc_id = doc_ids[idx]
            doc = store.get_doc(doc_id)
            
            # ç”Ÿæˆå±¤ç´šæè¿°
            hierarchical_desc = generate_hierarchical_description(doc_id, recommended_level, idx, store)
            
            result = {
                "rank": i + 1,
                "content": chunks[idx],
                "similarity": float(similarities[idx]),
                "doc_id": doc_id,
                "doc_name": doc.filename if doc else "Unknown",
                "chunk_index": idx,
                "hierarchical_description": hierarchical_desc,  # æ–°å¢å±¤ç´šæè¿°
                "metadata": {
                    "level": recommended_level,
                    "query_type": query_type,
                    "confidence": confidence
                }
            }
            results.append(result)
        
        # è¨ˆç®—æª¢ç´¢æŒ‡æ¨™
        metrics = {
            "total_chunks_searched": len(chunks),
            "query_type": query_type,
            "recommended_level": recommended_level,
            "classification_confidence": confidence,
            "embedding_provider": "gemini" if USE_GEMINI_EMBEDDING else "bge-m3",
            "embedding_model": "text-embedding-004" if USE_GEMINI_EMBEDDING else "BAAI/bge-m3"
        }
        
        return {
            "results": results,
            "metrics": metrics,
            "query_analysis": query_analysis
        }
        
    except Exception as e:
        print(f"âŒ å¤šå±¤æ¬¡æª¢ç´¢éŒ¯èª¤: {e}")
        return JSONResponse(
            status_code=500,
            content={"error": f"Multi-level retrieval failed: {str(e)}"}
        )


@app.post("/api/query-analysis")
def analyze_query(req: RetrieveRequest):
    """æŸ¥è©¢åˆ†æç«¯é»ï¼šåˆ†ææŸ¥è©¢é¡å‹ä¸¦æ¨è–¦æª¢ç´¢ç­–ç•¥"""
    query_analysis = get_query_analysis(req.query)
    
    # æª¢æŸ¥å¯ç”¨çš„embeddingå±¤æ¬¡
    available_levels = store.get_available_levels()
    has_multi_level = store.has_multi_level_embeddings()
    
    # ç”Ÿæˆæª¢ç´¢å»ºè­°
    retrieval_suggestions = {
        "recommended_method": "multi-level" if has_multi_level else "standard",
        "recommended_level": query_analysis['recommended_level'],
        "available_levels": available_levels,
        "alternative_levels": [level for level in available_levels if level != query_analysis['recommended_level']]
    }
    
    return {
        "query_analysis": query_analysis,
        "retrieval_suggestions": retrieval_suggestions,
        "system_status": {
            "has_multi_level_embeddings": has_multi_level,
            "has_standard_embeddings": store.embeddings is not None
        }
    }


@app.post("/api/multi-level-fusion-retrieve")
async def multi_level_fusion_retrieve(req: MultiLevelFusionRequest):
    """å¤šå±¤æ¬¡èåˆæª¢ç´¢ï¼šå¾æ‰€æœ‰å±¤æ¬¡æª¢ç´¢ä¸¦èåˆçµæœ"""
    # æª¢æŸ¥æ˜¯å¦æœ‰å¯ç”¨çš„å¤šå±¤æ¬¡embedding
    if not store.has_multi_level_embeddings():
        return JSONResponse(
            status_code=400, 
            content={"error": "Multi-level embeddings not available. Please run /api/multi-level-embed first."}
        )
    
    # åˆ†ææŸ¥è©¢
    query_analysis = get_query_analysis(req.query)
    available_levels = store.get_available_levels()
    
    print(f"ğŸ” å¤šå±¤æ¬¡èåˆæª¢ç´¢ï¼šæŸ¥è©¢é¡å‹={query_analysis['query_type']}, å¯ç”¨å±¤æ¬¡={available_levels}")
    
    # å¦‚æœæ²’æœ‰å¯ç”¨çš„å±¤æ¬¡ï¼Œè¿”å›éŒ¯èª¤
    if not available_levels:
        return JSONResponse(
            status_code=400,
            content={"error": "No multi-level embeddings available. Please run /api/multi-level-embed first."}
        )
    
    # å¾æ‰€æœ‰å¯ç”¨å±¤æ¬¡æª¢ç´¢
    level_results = {}
    total_chunks_searched = 0
    
    try:
        import numpy as np
        
        # æª¢æ¸¬ç¬¬ä¸€å€‹å¯ç”¨å±¤æ¬¡çš„æ¨¡å‹ä¿¡æ¯ï¼Œç¢ºä¿ä½¿ç”¨ç›¸åŒçš„æ¨¡å‹
        first_level = available_levels[0] if available_levels else None
        embedding_provider = None
        embedding_dimension = None
        
        if first_level:
            first_level_data = store.get_multi_level_embeddings(first_level)
            if first_level_data and 'metadata' in first_level_data:
                embedding_provider = first_level_data['metadata'].get('provider')
                embedding_dimension = first_level_data['metadata'].get('dimension')
                print(f"ğŸ” æª¢æ¸¬åˆ°å­˜å„²çš„embeddingæä¾›è€…: {embedding_provider}, ç¶­åº¦: {embedding_dimension}")
        
        # æ ¹æ“šå­˜å„²çš„embeddingæ¨¡å‹é¸æ“‡æŸ¥è©¢å‘é‡åŒ–æ–¹æ³•
        query_vector = None
        if embedding_provider == 'gemini' or (not embedding_provider and USE_GEMINI_EMBEDDING and GOOGLE_API_KEY):
            query_vector = (await embed_gemini([req.query]))[0]
            print(f"âœ… ä½¿ç”¨Geminiç”ŸæˆæŸ¥è©¢å‘é‡ï¼Œç¶­åº¦: {len(query_vector)}")
        elif embedding_provider == 'bge-m3' or (not embedding_provider and USE_BGE_M3_EMBEDDING and SENTENCE_TRANSFORMERS_AVAILABLE):
            query_vector = embed_bge_m3([req.query])[0]
            print(f"âœ… ä½¿ç”¨BGE-M3ç”ŸæˆæŸ¥è©¢å‘é‡ï¼Œç¶­åº¦: {len(query_vector)}")
        else:
            return JSONResponse(
                status_code=500,
                content={"error": "No embedding method available for query"}
            )
        
        # é©—è­‰ç¶­åº¦åŒ¹é…
        if embedding_dimension and len(query_vector) != embedding_dimension:
            print(f"âš ï¸ è­¦å‘Šï¼šæŸ¥è©¢å‘é‡ç¶­åº¦({len(query_vector)})èˆ‡å­˜å„²å‘é‡ç¶­åº¦({embedding_dimension})ä¸åŒ¹é…")
            return JSONResponse(
                status_code=500,
                content={"error": f"Dimension mismatch: query vector has {len(query_vector)} dimensions but stored embeddings have {embedding_dimension} dimensions. Please re-run /api/multi-level-embed with the current embedding provider."}
            )
        
        # å°æ¯å€‹å±¤æ¬¡é€²è¡Œæª¢ç´¢
        for level_name in available_levels:
            level_data = store.get_multi_level_embeddings(level_name)
            if not level_data:
                continue
            
            vectors = level_data['embeddings']
            chunks = level_data['chunks']
            doc_ids = level_data['doc_ids']
            
            print(f"ğŸ“Š æª¢ç´¢å±¤æ¬¡ '{level_name}'ï¼š{len(chunks)} å€‹chunks")
            total_chunks_searched += len(chunks)
            
            # è¨ˆç®—ç›¸ä¼¼åº¦
            if isinstance(vectors, list):
                vectors = np.array(vectors)
            if isinstance(query_vector, list):
                query_vector = np.array(query_vector)
            
            similarities = cosine_similarity([query_vector], vectors)[0]
            
            # ç²å–top-kçµæœ
            top_indices = np.argsort(similarities)[::-1][:req.k]
            
            level_results[level_name] = []
            for i, idx in enumerate(top_indices):
                doc_id = doc_ids[idx]
                doc = store.get_doc(doc_id)
                
                # ç”Ÿæˆå±¤ç´šæè¿°
                hierarchical_desc = generate_hierarchical_description(doc_id, level_name, idx, store)
                
                result = {
                    "rank": int(i + 1),
                    "content": chunks[idx],
                    "similarity": float(similarities[idx]),
                    "doc_id": doc_id,
                    "doc_name": doc.filename if doc else "Unknown",
                    "chunk_index": int(idx),
                    "hierarchical_description": hierarchical_desc,  # æ–°å¢å±¤ç´šæè¿°
                    "metadata": {
                        "level": level_name,
                        "query_type": query_analysis['query_type'],
                        "confidence": query_analysis['confidence']
                    }
                }
                level_results[level_name].append(result)
        
        if not level_results:
            return JSONResponse(
                status_code=400,
                content={"error": "No results found from any level"}
            )
        
        # å‰µå»ºèåˆé…ç½®
        fusion_config = FusionConfig(
            strategy=req.fusion_strategy,
            level_weights=req.level_weights,
            similarity_threshold=req.similarity_threshold,
            max_results=req.max_results,
            normalize_scores=req.normalize_scores
        )
        
        # åŸ·è¡Œçµæœèåˆ
        print(f"ğŸ”„ åŸ·è¡Œçµæœèåˆï¼šç­–ç•¥={req.fusion_strategy}")
        fused_results = fuse_multi_level_results(level_results, fusion_config)
        
        # è¨ˆç®—èåˆæŒ‡æ¨™
        fusion_metrics = {
            "total_chunks_searched": total_chunks_searched,
            "levels_searched": list(level_results.keys()),
            "fusion_strategy": req.fusion_strategy,
            "level_weights": req.level_weights or fusion_config.level_weights,
            "similarity_threshold": req.similarity_threshold,
            "max_results": req.max_results,
            "query_type": query_analysis['query_type'],
            "classification_confidence": query_analysis['confidence'],
            "embedding_provider": "gemini" if USE_GEMINI_EMBEDDING else "bge-m3",
            "embedding_model": "text-embedding-004" if USE_GEMINI_EMBEDDING else "BAAI/bge-m3"
        }
        
        # çµ±è¨ˆå„å±¤æ¬¡çš„è²¢ç»
        level_contributions = {}
        for level, results in level_results.items():
            level_contributions[level] = {
                "num_results": len(results),
                "avg_similarity": sum(r['similarity'] for r in results) / len(results) if results else 0,
                "max_similarity": max(r['similarity'] for r in results) if results else 0
            }
        
        fusion_metrics["level_contributions"] = level_contributions
        
        return {
            "results": fused_results,
            "metrics": fusion_metrics,
            "query_analysis": query_analysis,
            "level_results": level_results  # åŒ…å«åŸå§‹å„å±¤æ¬¡çµæœ
        }
        
    except Exception as e:
        print(f"âŒ å¤šå±¤æ¬¡èåˆæª¢ç´¢éŒ¯èª¤: {e}")
        return JSONResponse(
            status_code=500,
            content={"error": f"Multi-level fusion retrieval failed: {str(e)}"}
        )


@app.post("/api/hybrid-retrieve")
def hybrid_retrieve(req: RetrieveRequest):
    """HybridRAG æª¢ç´¢ï¼šçµåˆå‘é‡ç›¸ä¼¼åº¦å’Œæ³•å¾‹çµæ§‹è¦å‰‡"""
    if store.embeddings is None:
        return JSONResponse(status_code=400, content={"error": "run /embed first"})
    
    # ç²å–æ‰€æœ‰ chunks å’Œ metadata
    chunks_flat = store.chunks_flat
    mapping_doc_ids = store.chunk_doc_ids
    
    if not chunks_flat:
        return JSONResponse(status_code=400, content={"error": "no chunks available"})
    
    # æ§‹å»º nodes æ ¼å¼ä¾› hybrid_rank ä½¿ç”¨
    nodes = []
    for i, (chunk, doc_id) in enumerate(zip(chunks_flat, mapping_doc_ids)):
        doc = store.docs.get(doc_id)
        metadata = {}
        
        # å¦‚æœæœ‰çµæ§‹åŒ–chunksï¼Œæå–metadata
        if doc and hasattr(doc, 'structured_chunks') and doc.structured_chunks and i < len(doc.structured_chunks):
            structured_chunk = doc.structured_chunks[i]
            metadata = structured_chunk.get("metadata", {})
        
        nodes.append({
            "content": chunk,
            "metadata": metadata,
            "doc_id": doc_id,
            "chunk_index": i
        })
    
    # å…ˆç”¨å¯†é›†å‘é‡å¾—åˆ°æ¯å€‹ç¯€é»çš„å‘é‡åˆ†æ•¸
    # æˆ‘å€‘å°æ‰€æœ‰ç¯€é»é€²è¡Œç›¸ä¼¼åº¦è¨ˆç®—ï¼Œç„¶å¾Œåªå–å‰ k çš„çµæœåš Hybrid æ’åº
    dense_top_k = min(len(nodes), max(req.k * 4, req.k))
    all_vec_idxs, all_vec_sims = rank_with_dense_vectors(req.query, k=len(nodes))
    # æ˜ å°„å‡ºç¯€é»é †åºå°æ‡‰çš„åˆ†æ•¸ï¼Œåˆå§‹åŒ–ç‚º0
    node_vector_scores = [0.0] * len(nodes)
    for rank_idx, node_idx in enumerate(all_vec_idxs):
        node_vector_scores[node_idx] = float(all_vec_sims[rank_idx])

    # å–å‘é‡åˆ†æ•¸æœ€é«˜çš„å‰ dense_top_k ç¯€é»ä½œç‚º Hybrid å€™é¸
    top_vec_pairs = sorted(
        [(i, s) for i, s in enumerate(node_vector_scores)], key=lambda x: x[1], reverse=True
    )[:dense_top_k]
    candidate_nodes = [nodes[i] for i, _ in top_vec_pairs]
    candidate_scores = [s for _, s in top_vec_pairs]

    # ä½¿ç”¨ hybrid_rank é€²è¡Œæª¢ç´¢ï¼ˆå‘é‡åˆ†æ•¸ + metadata åŠ åˆ†ï¼‰
    config = HybridConfig(
        alpha=0.8,  # å‘é‡ç›¸ä¼¼åº¦æ¬Šé‡
        w_law_match=0.15,  # æ³•åå°é½Šæ¬Šé‡
        w_article_match=0.15,  # æ¢è™Ÿå°é½Šæ¬Šé‡
        w_keyword_hit=0.05,  # è¡“èªå‘½ä¸­æ¬Šé‡
        max_bonus=0.4  # æœ€å¤§åŠ åˆ†
    )

    hybrid_results = hybrid_rank(
        req.query, candidate_nodes, k=req.k, config=config, vector_scores=candidate_scores
    )
    
    # è½‰æ›ç‚ºæ¨™æº–æ ¼å¼
    results = []
    for rank, item in enumerate(hybrid_results, start=1):
        # ç”Ÿæˆå±¤ç´šæè¿°
        hierarchical_desc = generate_hierarchical_description(item["doc_id"], "hybrid", item["chunk_index"], store)
        
        result = {
            "rank": rank,
            "score": item["score"],
            "vector_score": item["vector_score"],
            "bonus": item["bonus"],
            "doc_id": item["doc_id"],
            "chunk_index": item["chunk_index"],
            "content": item["content"][:2000],
            "metadata": item["metadata"],
            "hierarchical_description": hierarchical_desc,  # æ–°å¢å±¤ç´šæè¿°
        }
        
        # æ·»åŠ æ³•å¾‹çµæ§‹ä¿¡æ¯
        if item["metadata"]:
            result["legal_structure"] = {
                "id": item["metadata"].get("id", ""),
                "category": item["metadata"].get("category", ""),
                "article_label": item["metadata"].get("article_label", ""),
                "article_number": item["metadata"].get("article_number"),
                "article_suffix": item["metadata"].get("article_suffix"),
                "spans": item["metadata"].get("spans", {}),
                "page_range": item["metadata"].get("page_range", {})
            }
        
        results.append(result)
    
    # è¨ˆç®— P@K å’Œ R@Kï¼ˆå¦‚æœæœ‰ QA æ•¸æ“šï¼‰
    metrics = calculate_retrieval_metrics(req.query, results, req.k)
    
    # åˆ¤æ–· embedding provider å’Œ modelï¼ˆä¸å†æ”¯æŒ TF-IDFï¼‰
    if USE_GEMINI_EMBEDDING and GOOGLE_API_KEY:
        embedding_provider = "gemini"
        embedding_model = "gemini-embedding-001"
    elif USE_BGE_M3_EMBEDDING and SENTENCE_TRANSFORMERS_AVAILABLE:
        embedding_provider = "bge-m3"
        embedding_model = "BAAI/bge-m3"
    else:
        embedding_provider = "unknown"
        embedding_model = "unknown"
    
    return {
        "query": req.query, 
        "k": req.k, 
        "results": results,
        "method": "hybrid_rag",
        "metrics": metrics,
        "embedding_provider": embedding_provider,
        "embedding_model": embedding_model,
        "config": {
            "alpha": config.alpha,
            "w_law_match": config.w_law_match,
            "w_article_match": config.w_article_match,
            "w_keyword_hit": config.w_keyword_hit,
            "max_bonus": config.max_bonus
        }
    }


@app.post("/api/enhanced-hybrid-retrieve")
def enhanced_hybrid_retrieve(req: RetrieveRequest):
    """ä½¿ç”¨å¢å¼·ç‰ˆHybridRAGé€²è¡Œæª¢ç´¢"""
    print(f"ğŸš€ å¢å¼·ç‰ˆHybridRAGæª¢ç´¢è«‹æ±‚: {req.query}, k={req.k}")
    
    # æª¢æŸ¥æ˜¯å¦æœ‰FAISSå’ŒBM25ç´¢å¼•ï¼ˆæ¨™æº–æˆ–å¤šå±¤æ¬¡ï¼‰
    faiss_available = faiss_store.has_vectors() or faiss_store.has_multi_level_vectors()
    bm25_available = bm25_index.has_index() or bm25_index.has_multi_level_index()
    print(f"ğŸ“Š ç´¢å¼•ç‹€æ…‹: FAISS={faiss_available}, BM25={bm25_available}")
    
    # å¦‚æœç´¢å¼•ä¸å¯ç”¨ï¼Œå˜—è©¦è‡ªå‹•é‡æ–°åŠ è¼‰
    if not faiss_available or not bm25_available:
        print("âš ï¸ ç´¢å¼•ä¸å®Œæ•´ï¼Œå˜—è©¦è‡ªå‹•é‡æ–°åŠ è¼‰...")
        try:
            # å˜—è©¦å¾ç£ç›¤åŠ è¼‰ç´¢å¼•
            if not faiss_available:
                print("ğŸ”„ å˜—è©¦é‡æ–°åŠ è¼‰FAISSç´¢å¼•...")
                faiss_store.load_data()
                faiss_available = faiss_store.has_vectors() or faiss_store.has_multi_level_vectors()
                print(f"   FAISSåŠ è¼‰çµæœ: {faiss_available}")
            
            if not bm25_available:
                print("ğŸ”„ å˜—è©¦é‡æ–°åŠ è¼‰BM25ç´¢å¼•...")
                bm25_index.load_data()
                bm25_available = bm25_index.has_index() or bm25_index.has_multi_level_index()
                print(f"   BM25åŠ è¼‰çµæœ: {bm25_available}")
            
            # å¦‚æœåŠ è¼‰å¤±æ•—ï¼Œå˜—è©¦å¾storeé‡å»ºï¼ˆæ¨™æº–ç´¢å¼•ï¼‰
            if (not faiss_available or not bm25_available) and (store.embeddings is not None and store.chunks_flat):
                print("âš ï¸ ç´¢å¼•æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå˜—è©¦å¾storeé‡å»ºæ¨™æº–ç´¢å¼•...")
                vectors = store.embeddings
                chunks = store.chunks_flat
                chunk_ids = [f"{doc_id}_{i}" for i, doc_id in enumerate(store.chunk_doc_ids)]
                
                # é‡å»ºFAISSç´¢å¼•
                if not faiss_available and vectors:
                    print("ğŸ”§ é‡å»ºFAISSç´¢å¼•...")
                    dimension = len(vectors[0]) if vectors else EMBEDDING_DIMENSION
                    faiss_store.create_index(dimension, "flat")
                    faiss_store.add_vectors(vectors, chunk_ids, store.chunk_doc_ids, chunks)
                    
                    # æ¢å¾©enhanced metadata
                    if hasattr(store, 'enhanced_metadata') and store.enhanced_metadata:
                        for chunk_id, metadata in store.enhanced_metadata.items():
                            faiss_store.set_enhanced_metadata(chunk_id, metadata)
                    
                    faiss_store.save_data()
                    faiss_available = faiss_store.has_vectors() or faiss_store.has_multi_level_vectors()
                    print(f"   âœ… FAISSç´¢å¼•å·²é‡å»º: {faiss_available}")
                
                # é‡å»ºBM25ç´¢å¼•
                if not bm25_available and chunks:
                    print("ğŸ”§ é‡å»ºBM25ç´¢å¼•...")
                    bm25_index.build_index(chunks, chunk_ids, store.chunk_doc_ids)
                    bm25_index.save_data()
                    bm25_available = bm25_index.has_index() or bm25_index.has_multi_level_index()
                    print(f"   âœ… BM25ç´¢å¼•å·²é‡å»º: {bm25_available}")
            
        except Exception as e:
            print(f"âš ï¸ è‡ªå‹•é‡æ–°åŠ è¼‰ç´¢å¼•å¤±æ•—: {e}")
    
    # å†æ¬¡æª¢æŸ¥ç´¢å¼•ç‹€æ…‹
    if not faiss_available and not bm25_available:
        return JSONResponse(
            status_code=400,
            content={
                "error": "No enhanced indices available. Please run /api/embed or /api/multi-level-embed first.",
                "faiss_available": faiss_available,
                "bm25_available": bm25_available,
                "suggestion": "è«‹å…ˆåŸ·è¡Œ /api/embed æˆ– /api/multi-level-embed ä¾†å‰µå»ºç´¢å¼•ï¼Œæˆ–å¾Uploadé é¢é¸æ“‡å·²å­˜åœ¨çš„Embeddingè³‡æ–™åº«"
            }
        )
    
    try:
        # é…ç½®å¢å¼·ç‰ˆHybridRAG
        config = EnhancedHybridConfig(
            vector_weight=0.6,
            bm25_weight=0.25,
            metadata_weight=0.15,
            w_law_match=0.15,
            w_article_match=0.15,
            w_concept_match=0.1,
            w_keyword_hit=0.05,
            w_domain_match=0.05,
            w_title_match=0.1,
            w_category_match=0.05,
            max_bonus=0.4,
            title_boost_factor=1.5,
            category_boost_factor=1.3,
            # Metadataå‘ä¸‹ç¹¼æ‰¿é…ç½®
            enable_inheritance_strategy=True,
            metadata_match_threshold=0.3,
            inheritance_bonus=0.1,
            inheritance_boost_factor=1.2
        )
        
        # åŸ·è¡Œå¢å¼·ç‰ˆHybridRAGæª¢ç´¢
        enhanced_results = enhanced_hybrid_rag.retrieve(req.query, req.k, config)
        
        # ç”Ÿæˆå±¤ç´šæè¿°
        for result in enhanced_results:
            if 'doc_id' in result:
                doc_id = result.get('doc_id', 'unknown')
                level = 'basic_unit'  # é»˜èªå±¤ç´š
                chunk_index = result.get('chunk_index', 0)
                result['hierarchical_description'] = generate_hierarchical_description(
                    doc_id, level, chunk_index, store
                )
        
        print(f"âœ… å¢å¼·ç‰ˆHybridRAGæª¢ç´¢å®Œæˆï¼Œè¿”å› {len(enhanced_results)} å€‹çµæœ")
        
        return {
            "results": enhanced_results,
            "query": req.query,
            "final_results": len(enhanced_results),
            "config": config.__dict__,
            "retrieval_stats": enhanced_hybrid_rag.get_retrieval_stats()
        }
        
    except Exception as e:
        print(f"âŒ å¢å¼·ç‰ˆHybridRAGæª¢ç´¢å¤±æ•—: {e}")
        return JSONResponse(
            status_code=500,
            content={"error": f"Enhanced HybridRAG retrieval failed: {str(e)}"}
        )


@app.post("/api/hybrid-rrf-retrieve")
async def hybrid_rrf_retrieve(req: RetrieveRequest):
    """HybridRAG(RRF)æª¢ç´¢ï¼šç´”RRFèåˆå‘é‡+BM25ï¼Œä¸è€ƒæ…®MetadataåŠ åˆ†"""
    print(f"ğŸ”„ HybridRAG(RRF)æª¢ç´¢è«‹æ±‚: {req.query}, k={req.k}")
    
    # é©—è­‰æŸ¥è©¢
    if not req.query or not req.query.strip():
        error_msg = "Query cannot be empty"
        print(f"âŒ é©—è­‰å¤±æ•—: {error_msg}")
        return JSONResponse(
            status_code=400,
            content={"error": error_msg}
        )
    
    if req.k <= 0:
        error_msg = f"k must be greater than 0, got {req.k}"
        print(f"âŒ é©—è­‰å¤±æ•—: {error_msg}")
        return JSONResponse(
            status_code=400,
            content={"error": error_msg}
        )
    
    # æª¢æŸ¥æ˜¯å¦æœ‰FAISSå’ŒBM25ç´¢å¼•ï¼ˆæ¨™æº–æˆ–å¤šå±¤æ¬¡ï¼‰
    faiss_available = faiss_store.has_vectors() or faiss_store.has_multi_level_vectors()
    bm25_available = bm25_index.has_index() or bm25_index.has_multi_level_index()
    print(f"ğŸ“Š ç´¢å¼•ç‹€æ…‹: FAISS={faiss_available}, BM25={bm25_available}")
    
    # å¦‚æœç´¢å¼•ä¸å¯ç”¨ï¼Œå˜—è©¦è‡ªå‹•é‡æ–°åŠ è¼‰
    if not faiss_available or not bm25_available:
        print("âš ï¸ ç´¢å¼•ä¸å®Œæ•´ï¼Œå˜—è©¦è‡ªå‹•é‡æ–°åŠ è¼‰...")
        try:
            # å˜—è©¦å¾ç£ç›¤åŠ è¼‰ç´¢å¼•
            if not faiss_available:
                print("ğŸ”„ å˜—è©¦é‡æ–°åŠ è¼‰FAISSç´¢å¼•...")
                faiss_store.load_data()
                faiss_available = faiss_store.has_vectors() or faiss_store.has_multi_level_vectors()
                print(f"   FAISSåŠ è¼‰çµæœ: {faiss_available}")
            
            if not bm25_available:
                print("ğŸ”„ å˜—è©¦é‡æ–°åŠ è¼‰BM25ç´¢å¼•...")
                bm25_index.load_data()
                bm25_available = bm25_index.has_index() or bm25_index.has_multi_level_index()
                print(f"   BM25åŠ è¼‰çµæœ: {bm25_available}")
            
            # å¦‚æœåŠ è¼‰å¤±æ•—ï¼Œå˜—è©¦å¾storeé‡å»ºï¼ˆæ¨™æº–ç´¢å¼•ï¼‰
            if (not faiss_available or not bm25_available) and (store.embeddings is not None and store.chunks_flat):
                print("âš ï¸ ç´¢å¼•æ–‡ä»¶ä¸å­˜åœ¨ï¼Œå˜—è©¦å¾storeé‡å»ºæ¨™æº–ç´¢å¼•...")
                vectors = store.embeddings
                chunks = store.chunks_flat
                chunk_ids = [f"{doc_id}_{i}" for i, doc_id in enumerate(store.chunk_doc_ids)]
                
                # é‡å»ºFAISSç´¢å¼•
                if not faiss_available and vectors:
                    print("ğŸ”§ é‡å»ºFAISSç´¢å¼•...")
                    dimension = len(vectors[0]) if vectors else EMBEDDING_DIMENSION
                    faiss_store.create_index(dimension, "flat")
                    faiss_store.add_vectors(vectors, chunk_ids, store.chunk_doc_ids, chunks)
                    
                    # æ¢å¾©enhanced metadata
                    if hasattr(store, 'enhanced_metadata') and store.enhanced_metadata:
                        for chunk_id, metadata in store.enhanced_metadata.items():
                            faiss_store.set_enhanced_metadata(chunk_id, metadata)
                    
                    faiss_store.save_data()
                    faiss_available = faiss_store.has_vectors() or faiss_store.has_multi_level_vectors()
                    print(f"   âœ… FAISSç´¢å¼•å·²é‡å»º: {faiss_available}")
                
                # é‡å»ºBM25ç´¢å¼•
                if not bm25_available and chunks:
                    print("ğŸ”§ é‡å»ºBM25ç´¢å¼•...")
                    bm25_index.build_index(chunks, chunk_ids, store.chunk_doc_ids)
                    bm25_index.save_data()
                    bm25_available = bm25_index.has_index() or bm25_index.has_multi_level_index()
                    print(f"   âœ… BM25ç´¢å¼•å·²é‡å»º: {bm25_available}")
            
        except Exception as e:
            print(f"âš ï¸ è‡ªå‹•é‡æ–°åŠ è¼‰ç´¢å¼•å¤±æ•—: {e}")
    
    # å†æ¬¡æª¢æŸ¥ç´¢å¼•ç‹€æ…‹
    if not faiss_available and not bm25_available:
        error_msg = "No enhanced indices available. Please run /api/embed or /api/multi-level-embed first."
        print(f"âŒ é©—è­‰å¤±æ•—: {error_msg}")
        return JSONResponse(
            status_code=400,
            content={
                "error": error_msg,
                "faiss_available": faiss_available,
                "bm25_available": bm25_available,
                "suggestion": "è«‹å…ˆåŸ·è¡Œ /api/embed æˆ– /api/multi-level-embed ä¾†å‰µå»ºç´¢å¼•ï¼Œæˆ–å¾Uploadé é¢é¸æ“‡å·²å­˜åœ¨çš„Embeddingè³‡æ–™åº«"
            }
        )
    
    try:
        # 1. å‘é‡æª¢ç´¢ - ç”ŸæˆæŸ¥è©¢å‘é‡
        print("ğŸ“Š åŸ·è¡Œå‘é‡æª¢ç´¢...")
        query_vector = None
        if USE_GEMINI_EMBEDDING and GOOGLE_API_KEY:
            try:
                query_vector = (await embed_gemini([req.query]))[0]
                # é©—è­‰å‘é‡ç¶­åº¦
                if not query_vector or len(query_vector) != EMBEDDING_DIMENSION:
                    raise ValueError(f"Query vector dimension mismatch: expected {EMBEDDING_DIMENSION}, got {len(query_vector) if query_vector else 0}")
                print(f"âœ… ä½¿ç”¨Geminiç”ŸæˆæŸ¥è©¢å‘é‡ï¼Œç¶­åº¦: {len(query_vector)}")
            except Exception as e:
                print(f"âŒ Gemini query embedding failed: {e}")
                # å…¨éƒ¨ä½¿ç”¨Geminiï¼Œä¸ä½¿ç”¨BGE-M3 fallback
                raise RuntimeError(
                    f"Gemini embedding failed: {str(e)}. "
                    f"è«‹æª¢æŸ¥ï¼š1) GOOGLE_API_KEYæ˜¯å¦æ­£ç¢º 2) ç¶²çµ¡é€£æ¥æ˜¯å¦æ­£å¸¸ 3) æŸ¥è©¢æ–‡æœ¬æ˜¯å¦åŒ…å«ç„¡æ³•è™•ç†çš„ç‰¹æ®Šå­—ç¬¦"
                )
        else:
            raise RuntimeError(
                f"Gemini embeddingæœªå•Ÿç”¨æˆ–API keyæœªè¨­ç½®ã€‚"
                f"USE_GEMINI_EMBEDDING={USE_GEMINI_EMBEDDING}, GOOGLE_API_KEY={'å·²è¨­ç½®' if GOOGLE_API_KEY else 'æœªè¨­ç½®'}"
            )
        
        # é©—è­‰query_vector
        if not query_vector:
            raise RuntimeError("Failed to generate query vector")
        
        # æª¢æ¸¬æ˜¯æ¨™æº–ç´¢å¼•é‚„æ˜¯å¤šå±¤æ¬¡ç´¢å¼•
        use_standard_index = faiss_store.has_vectors()
        use_multi_level_index = faiss_store.has_multi_level_vectors()
        
        # å„ªå…ˆä½¿ç”¨å¤šå±¤æ¬¡ç´¢å¼•ï¼ˆå¦‚æœå­˜åœ¨ï¼‰ï¼Œå› ç‚ºé€™æ˜¯å¯¦é©—çµ„Bã€Cã€Dä½¿ç”¨çš„ç´¢å¼•
        # å¦‚æœæ¨™æº–ç´¢å¼•ç¶­åº¦ä¸åŒ¹é…ï¼Œè‡ªå‹•æ¸…é™¤ä¸¦ä½¿ç”¨å¤šå±¤æ¬¡ç´¢å¼•
        if use_standard_index:
            expected_dim = faiss_store.dimension
            query_dim = len(query_vector)
            if query_dim != expected_dim:
                print(f"âš ï¸ æª¢æ¸¬åˆ°æ¨™æº–ç´¢å¼•ç¶­åº¦({expected_dim})èˆ‡æŸ¥è©¢å‘é‡ç¶­åº¦({query_dim})ä¸åŒ¹é…")
                if use_multi_level_index:
                    print(f"ğŸ’¡ è‡ªå‹•æ¸…é™¤èˆŠçš„æ¨™æº–ç´¢å¼•ï¼Œæ”¹ç”¨å¤šå±¤æ¬¡ç´¢å¼•ï¼ˆå¯¦é©—çµ„B/C/Dï¼‰")
                    faiss_store.reset_vectors()
                    bm25_index.reset_index()
                    use_standard_index = False
                else:
                    print(f"âŒ ç¶­åº¦ä¸åŒ¹é…: æŸ¥è©¢å‘é‡ç¶­åº¦={query_dim}, FAISSç´¢å¼•ç¶­åº¦={expected_dim}")
                    print(f"ğŸ’¡ è§£æ±ºæ–¹æ¡ˆ: è«‹é‡æ–°é‹è¡Œ /api/embed æˆ– /api/multi-level-embed ä»¥çµ±ä¸€ç¶­åº¦")
                    raise ValueError(
                        f"Query vector dimension ({query_dim}) does not match FAISS index dimension ({expected_dim}). "
                        f"Please re-run /api/embed or /api/multi-level-embed to regenerate embeddings with the same dimension. "
                        f"Current EMBEDDING_DIMENSION setting: {EMBEDDING_DIMENSION}"
                    )
        
        all_candidates = {}
        
        # 1. å‘é‡æª¢ç´¢ï¼ˆæ”¯æŒæ¨™æº–å’Œå¤šå±¤æ¬¡ï¼‰
        print("ğŸ“Š åŸ·è¡Œå‘é‡æª¢ç´¢...")
        # å„ªå…ˆä½¿ç”¨å¤šå±¤æ¬¡ç´¢å¼•ï¼ˆå¦‚æœå­˜åœ¨ï¼‰ï¼Œå› ç‚ºé€™æ˜¯å¯¦é©—çµ„Bã€Cã€Dä½¿ç”¨çš„ç´¢å¼•
        # åªæœ‰åœ¨æ²’æœ‰å¤šå±¤æ¬¡ç´¢å¼•æ™‚æ‰ä½¿ç”¨æ¨™æº–ç´¢å¼•
        if use_multi_level_index:
            # å¤šå±¤æ¬¡ç´¢å¼•æª¢ç´¢ï¼šæª¢ç´¢æ‰€æœ‰å±¤æ¬¡ä¸¦åˆä½µï¼ˆå¯¦é©—çµ„Bã€Cã€Dï¼‰
            print(f"âœ… ä½¿ç”¨å¤šå±¤æ¬¡ç´¢å¼•é€²è¡Œæª¢ç´¢ï¼ˆå¯¦é©—çµ„B/C/Dï¼‰")
            available_levels = faiss_store.get_available_levels()
            print(f"ğŸ” å¤šå±¤æ¬¡ç´¢å¼•å¯ç”¨å±¤æ¬¡: {available_levels}")
            
            for level_name in available_levels:
                try:
                    level_indices, level_scores = faiss_store.search_multi_level(level_name, query_vector, req.k * 10)
                    print(f"   âœ… å±¤æ¬¡ '{level_name}' è¿”å› {len(level_indices)} å€‹å€™é¸")
                    
                    # ç‚ºè©²å±¤æ¬¡çš„çµæœåˆ†é…rank
                    for rank, (idx, score) in enumerate(zip(level_indices, level_scores), start=1):
                        chunk_info = faiss_store.get_multi_level_chunk_by_index(level_name, idx)
                        if chunk_info and 'chunk_id' in chunk_info:
                            chunk_id = chunk_info['chunk_id']
                            # è·¨å±¤æ¬¡å¯èƒ½æœƒæœ‰ç›¸åŒchunk_idï¼Œä½¿ç”¨ç¬¬ä¸€å€‹å±¤æ¬¡çš„æ’å
                            if chunk_id not in all_candidates:
                                # å¾multi_level_chunksä¸­ç²å–åŸå§‹metadataï¼ˆå¦‚æœå¯ç”¨ï¼‰
                                doc_id = chunk_info.get('doc_id', 'unknown')
                                content = chunk_info.get('content', '')
                                enhanced_metadata = chunk_info.get('enhanced_metadata', {})
                                
                                # å˜—è©¦å¾docçš„multi_level_chunksä¸­æ‰¾åˆ°å°æ‡‰çš„chunkä»¥ç²å–åŸå§‹metadata
                                original_metadata = {}
                                doc = store.docs.get(doc_id) if doc_id != 'unknown' else None
                                if doc and hasattr(doc, 'multi_level_chunks') and doc.multi_level_chunks:
                                    if level_name in doc.multi_level_chunks:
                                        doc_level_chunks = doc.multi_level_chunks[level_name]
                                        # é€šécontentç²¾ç¢ºåŒ¹é…æ‰¾åˆ°å°æ‡‰çš„chunkï¼ˆæœ€å¯é çš„æ–¹æ³•ï¼‰
                                        # å› ç‚ºmulti_level_chunksä¸­çš„contentå’Œæª¢ç´¢è¿”å›çš„contentæ‡‰è©²å®Œå…¨ä¸€è‡´
                                        matched = False
                                        for chunk_data in doc_level_chunks:
                                            chunk_content = chunk_data.get('content', '')
                                            # ç²¾ç¢ºåŒ¹é…æˆ–å‰200å­—ç¬¦åŒ¹é…ï¼ˆè€ƒæ…®å¯èƒ½çš„å¾®å°å·®ç•°ï¼‰
                                            if chunk_content == content or (
                                                len(chunk_content) > 100 and 
                                                len(content) > 100 and
                                                chunk_content[:200] == content[:200]
                                            ):
                                                original_metadata = chunk_data.get('metadata', {})
                                                matched = True
                                                print(f"ğŸ“‹ chunk_id {chunk_id}: é€šécontentåŒ¹é…æ‰¾åˆ°metadata - ç« :{original_metadata.get('chapter', '')}, ç¯€:{original_metadata.get('section', '')}, æ¢:{original_metadata.get('article', '')}")
                                                break
                                        
                                        # å¦‚æœä»ç„¶æ²’æ‰¾åˆ°ï¼Œå˜—è©¦é€šéchunk_idä¸­çš„ç´¢å¼•æ¨ç®—
                                        if not matched:
                                            try:
                                                import re
                                                match = re.search(r'_(\d+)$', chunk_id)
                                                if match:
                                                    global_idx = int(match.group(1))
                                                    # çµ±è¨ˆåœ¨level_chunksä¸­ï¼Œå±¬æ–¼ç•¶å‰doc_idçš„chunksæ•¸é‡ï¼ˆåˆ°global_idxç‚ºæ­¢ï¼‰
                                                    # é€šéstore.multi_level_chunk_doc_idsä¾†çµ±è¨ˆ
                                                    if level_name in store.multi_level_chunk_doc_ids:
                                                        level_doc_ids = store.multi_level_chunk_doc_ids[level_name]
                                                        # çµ±è¨ˆå‰global_idxå€‹chunksä¸­å±¬æ–¼ç•¶å‰doc_idçš„æ•¸é‡
                                                        doc_chunk_count = sum(1 for i in range(min(global_idx + 1, len(level_doc_ids))) if level_doc_ids[i] == doc_id)
                                                        relative_idx = doc_chunk_count - 1  # æ¸›1å› ç‚ºç•¶å‰chunkæ˜¯ç¬¬doc_chunk_countå€‹
                                                        if 0 <= relative_idx < len(doc_level_chunks):
                                                            original_metadata = doc_level_chunks[relative_idx].get('metadata', {})
                                                            print(f"ğŸ“‹ chunk_id {chunk_id}: é€šéç´¢å¼•æ¨ç®—æ‰¾åˆ°metadata - ç« :{original_metadata.get('chapter', '')}, ç¯€:{original_metadata.get('section', '')}, æ¢:{original_metadata.get('article', '')}")
                                            except (ValueError, IndexError, AttributeError) as e:
                                                print(f"âš ï¸ è§£æchunk_idå¤±æ•—: {chunk_id}, éŒ¯èª¤: {e}")
                                
                                all_candidates[chunk_id] = {
                                    'chunk_id': chunk_id,
                                    'doc_id': doc_id,
                                    'content': content,
                                    'enhanced_metadata': enhanced_metadata,
                                    'original_metadata': original_metadata,  # ä¿å­˜åŸå§‹metadata
                                    'chunk_index': idx,
                                    'level': level_name,
                                    'vector_rank': rank,
                                    'vector_score': float(score),
                                    'bm25_rank': None,
                                    'bm25_score': 0.0
                                }
                except Exception as e:
                    print(f"   âš ï¸ å±¤æ¬¡ '{level_name}' æª¢ç´¢å¤±æ•—: {e}")
        elif use_standard_index:
            # æ¨™æº–ç´¢å¼•æª¢ç´¢ï¼ˆå¯¦é©—çµ„Aï¼‰
            print(f"âœ… ä½¿ç”¨æ¨™æº–ç´¢å¼•é€²è¡Œæª¢ç´¢ï¼ˆå¯¦é©—çµ„Aï¼‰")
            vector_indices, vector_scores = faiss_store.search(query_vector, req.k * 10)
            print(f"âœ… æ¨™æº–å‘é‡æª¢ç´¢è¿”å› {len(vector_indices)} å€‹å€™é¸")
            
            # ç‚ºå‘é‡çµæœåˆ†é…rank
            for rank, (idx, score) in enumerate(zip(vector_indices, vector_scores), start=1):
                chunk_info = faiss_store.get_chunk_by_index(idx)
                if chunk_info and 'chunk_id' in chunk_info:
                    chunk_id = chunk_info['chunk_id']
                    all_candidates[chunk_id] = {
                        'chunk_id': chunk_id,
                        'doc_id': chunk_info.get('doc_id', 'unknown'),
                        'content': chunk_info.get('content', ''),
                        'enhanced_metadata': chunk_info.get('enhanced_metadata', {}),
                        'chunk_index': idx,
                        'level': 'standard',
                        'vector_rank': rank,
                        'vector_score': float(score),
                        'bm25_rank': None,
                        'bm25_score': 0.0
                    }
        else:
            print("âš ï¸ FAISSç´¢å¼•ä¸å¯ç”¨ï¼Œè·³éå‘é‡æª¢ç´¢")
        
        # 2. BM25æª¢ç´¢ï¼ˆæ”¯æŒæ¨™æº–å’Œå¤šå±¤æ¬¡ï¼‰
        print("ğŸ“Š åŸ·è¡ŒBM25æª¢ç´¢...")
        # å„ªå…ˆä½¿ç”¨å¤šå±¤æ¬¡ç´¢å¼•ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if bm25_index.has_multi_level_index():
            # å¤šå±¤æ¬¡BM25æª¢ç´¢ï¼šæª¢ç´¢æ‰€æœ‰å±¤æ¬¡ä¸¦åˆä½µï¼ˆå¯¦é©—çµ„Bã€Cã€Dï¼‰
            print(f"âœ… ä½¿ç”¨å¤šå±¤æ¬¡BM25ç´¢å¼•é€²è¡Œæª¢ç´¢ï¼ˆå¯¦é©—çµ„B/C/Dï¼‰")
            available_levels = bm25_index.get_available_levels()
            print(f"ğŸ” å¤šå±¤æ¬¡BM25ç´¢å¼•å¯ç”¨å±¤æ¬¡: {available_levels}")
            
            for level_name in available_levels:
                try:
                    level_indices, level_scores = bm25_index.search_multi_level(level_name, req.query, req.k * 10)
                    print(f"   âœ… å±¤æ¬¡ '{level_name}' BM25è¿”å› {len(level_indices)} å€‹å€™é¸")
                    
                    # ç‚ºè©²å±¤æ¬¡çš„çµæœåˆ†é…rankä¸¦åˆä½µ
                    for rank, (idx, score) in enumerate(zip(level_indices, level_scores), start=1):
                        chunk_info = bm25_index.get_multi_level_chunk_by_index(level_name, idx)
                        if chunk_info and 'chunk_id' in chunk_info:
                            chunk_id = chunk_info['chunk_id']
                            if chunk_id in all_candidates:
                                all_candidates[chunk_id]['bm25_rank'] = rank
                                all_candidates[chunk_id]['bm25_score'] = float(score)
                            else:
                                all_candidates[chunk_id] = {
                                    'chunk_id': chunk_id,
                                    'doc_id': chunk_info.get('doc_id', 'unknown'),
                                    'content': chunk_info.get('content', ''),
                                    'enhanced_metadata': {},
                                    'chunk_index': idx,
                                    'level': level_name,
                                    'vector_rank': None,
                                    'vector_score': 0.0,
                                    'bm25_rank': rank,
                                    'bm25_score': float(score)
                                }
                except Exception as e:
                    print(f"   âš ï¸ å±¤æ¬¡ '{level_name}' BM25æª¢ç´¢å¤±æ•—: {e}")
        elif bm25_index.has_index():
            # æ¨™æº–BM25æª¢ç´¢
            bm25_indices, bm25_scores = bm25_index.search(req.query, req.k * 10)
            print(f"âœ… æ¨™æº–BM25æª¢ç´¢è¿”å› {len(bm25_indices)} å€‹å€™é¸")
            
            # ç‚ºBM25çµæœåˆ†é…rankä¸¦åˆä½µ
            for rank, (idx, score) in enumerate(zip(bm25_indices, bm25_scores), start=1):
                chunk_info = bm25_index.get_chunk_by_index(idx)
                if chunk_info and 'chunk_id' in chunk_info:
                    chunk_id = chunk_info['chunk_id']
                    if chunk_id in all_candidates:
                        all_candidates[chunk_id]['bm25_rank'] = rank
                        all_candidates[chunk_id]['bm25_score'] = float(score)
                    else:
                        all_candidates[chunk_id] = {
                            'chunk_id': chunk_id,
                            'doc_id': chunk_info.get('doc_id', 'unknown'),
                            'content': chunk_info.get('content', ''),
                            'enhanced_metadata': {},
                            'chunk_index': idx,
                            'level': 'standard',
                            'vector_rank': None,
                            'vector_score': 0.0,
                            'bm25_rank': rank,
                            'bm25_score': float(score)
                        }
        else:
            print("âš ï¸ BM25ç´¢å¼•ä¸å¯ç”¨ï¼Œè·³éBM25æª¢ç´¢")
        
        # 3. RRFèåˆ - è¨ˆç®—RRFåˆ†æ•¸ï¼š1 / (60 + rank)
        k_rrf = 60
        for chunk_id, candidate in all_candidates.items():
            rrf_score = 0.0
            
            # å‘é‡æ’ååˆ†æ•¸
            if candidate['vector_rank'] is not None:
                rrf_score += 1.0 / (k_rrf + candidate['vector_rank'])
            
            # BM25æ’ååˆ†æ•¸
            if candidate['bm25_rank'] is not None:
                rrf_score += 1.0 / (k_rrf + candidate['bm25_rank'])
            
            candidate['rrf_score'] = rrf_score
            candidate['hybrid_score'] = rrf_score
            
            # æ·»åŠ åˆ†æ•¸åˆ†è§£
            candidate['score_breakdown'] = {
                'vector_rank': candidate['vector_rank'],
                'bm25_rank': candidate['bm25_rank'],
                'rrf_score': rrf_score
            }
        
        # æª¢æŸ¥æ˜¯å¦æœ‰å€™é¸çµæœ
        if not all_candidates:
            print("âš ï¸ æ²’æœ‰æ‰¾åˆ°ä»»ä½•å€™é¸çµæœ")
            return {
                "results": [],
                "query": req.query,
                "final_results": 0,
                "fusion_method": "RRF",
                "k_rrf": 60,
                "warning": "No candidates found from vector or BM25 search"
            }
        
        # æŒ‰RRFåˆ†æ•¸æ’åº
        final_results = sorted(all_candidates.values(), key=lambda x: x['rrf_score'], reverse=True)
        final_results = final_results[:req.k]
        
        # ç”Ÿæˆå±¤ç´šæè¿°
        for result in final_results:
            if 'doc_id' in result:
                doc_id = result.get('doc_id', 'unknown')
                level = result.get('level', 'basic_unit')  # ä½¿ç”¨å¯¦éš›å±¤ç´š
                content = result.get('content', '')
                original_metadata = result.get('original_metadata', {})
                
                # å°æ–¼å¤šå±¤ç´šæª¢ç´¢ï¼Œå„ªå…ˆä½¿ç”¨contentå’Œoriginal_metadataä¾†ç”Ÿæˆæè¿°
                if original_metadata:
                    # å¾original_metadataç”Ÿæˆæè¿°
                    hierarchical_desc = generate_hierarchical_description_from_metadata(
                        doc_id, original_metadata, content, store
                    )
                    result['hierarchical_description'] = hierarchical_desc
                else:
                    # å›é€€åˆ°èˆŠçš„æ–¹æ³•ï¼ˆæ¨™æº–ç´¢å¼•ï¼‰
                    chunk_index = result.get('chunk_index', 0)
                    result['hierarchical_description'] = generate_hierarchical_description(
                        doc_id, level, chunk_index, store
                    )
        
        print(f"âœ… HybridRAG(RRF)æª¢ç´¢å®Œæˆï¼Œè¿”å› {len(final_results)} å€‹çµæœ")
        
        return {
            "results": final_results,
            "query": req.query,
            "final_results": len(final_results),
            "fusion_method": "RRF",
            "k_rrf": k_rrf
        }
        
    except Exception as e:
        print(f"âŒ HybridRAG(RRF)æª¢ç´¢å¤±æ•—: {e}")
        import traceback
        traceback.print_exc()
        return JSONResponse(
            status_code=500,
            content={"error": f"HybridRAG(RRF) retrieval failed: {str(e)}"}
        )


@app.post("/api/enhanced-multi-level-hybrid-retrieve")
def enhanced_multi_level_hybrid_retrieve(req: MultiLevelFusionRequest):
    """ä½¿ç”¨å¢å¼·ç‰ˆHybridRAGé€²è¡Œå¤šå±¤æ¬¡æª¢ç´¢"""
    print(f"ğŸš€ å¢å¼·ç‰ˆå¤šå±¤æ¬¡HybridRAGæª¢ç´¢è«‹æ±‚: {req.query}, k={req.k}")
    
    # æª¢æŸ¥æ˜¯å¦æœ‰å¤šå±¤æ¬¡ç´¢å¼•
    if not faiss_store.has_multi_level_vectors() and not bm25_index.has_multi_level_index():
        return JSONResponse(
            status_code=400,
            content={"error": "No multi-level indices available. Please run /api/multi-level-embed first."}
        )
    
    try:
        # é…ç½®å¢å¼·ç‰ˆHybridRAG
        config = EnhancedHybridConfig(
            vector_weight=0.6,
            bm25_weight=0.25,
            metadata_weight=0.15,
            w_law_match=0.15,
            w_article_match=0.15,
            w_concept_match=0.1,
            w_keyword_hit=0.05,
            w_domain_match=0.05,
            w_title_match=0.1,
            w_category_match=0.05,
            max_bonus=0.4,
            title_boost_factor=1.5,
            category_boost_factor=1.3,
            # Metadataå‘ä¸‹ç¹¼æ‰¿é…ç½®
            enable_inheritance_strategy=True,
            metadata_match_threshold=0.3,
            inheritance_bonus=0.1,
            inheritance_boost_factor=1.2
        )
        
        # åŸ·è¡Œå¤šå±¤æ¬¡æª¢ç´¢
        level_results = {}
        available_levels = faiss_store.get_available_levels()
        
        for level_name in available_levels:
            try:
                level_results[level_name] = enhanced_hybrid_rag.retrieve_multi_level(
                    req.query, level_name, req.k, config
                )
                print(f"âœ… å±¤æ¬¡ '{level_name}' æª¢ç´¢å®Œæˆï¼Œè¿”å› {len(level_results[level_name])} å€‹çµæœ")
            except Exception as e:
                print(f"âš ï¸ å±¤æ¬¡ '{level_name}' æª¢ç´¢å¤±æ•—: {e}")
                level_results[level_name] = []
        
        # ä½¿ç”¨èåˆç­–ç•¥åˆä½µçµæœ
        fusion_config = FusionConfig(
            strategy=req.fusion_strategy,
            level_weights=req.level_weights,
            similarity_threshold=req.similarity_threshold,
            max_results=req.max_results,
            normalize_scores=req.normalize_scores
        )
        
        # è½‰æ›ç‚ºèåˆå™¨æœŸæœ›çš„æ ¼å¼
        formatted_level_results = {}
        for level_name, results in level_results.items():
            formatted_level_results[level_name] = []
            for result in results:
                formatted_result = {
                    "content": result.get("content", ""),
                    "similarity": result.get("hybrid_score", 0.0),
                    "metadata": result.get("enhanced_metadata", {}),
                    "hierarchical_description": result.get("hierarchical_description", "")
                }
                formatted_level_results[level_name].append(formatted_result)
        
        # åŸ·è¡Œèåˆ
        fusion = MultiLevelResultFusion(fusion_config)
        fused_results = fusion.fuse_results(formatted_level_results)
        
        print(f"âœ… å¢å¼·ç‰ˆå¤šå±¤æ¬¡HybridRAGæª¢ç´¢å®Œæˆï¼Œèåˆå¾Œè¿”å› {len(fused_results)} å€‹çµæœ")
        
        return {
            "results": fused_results,
            "query": req.query,
            "level_results": {k: len(v) for k, v in level_results.items()},
            "final_results": len(fused_results),
            "fusion_config": fusion_config.__dict__,
            "retrieval_stats": enhanced_hybrid_rag.get_retrieval_stats()
        }
        
    except Exception as e:
        print(f"âŒ å¢å¼·ç‰ˆå¤šå±¤æ¬¡HybridRAGæª¢ç´¢å¤±æ•—: {e}")
        return JSONResponse(
            status_code=500,
            content={"error": f"Enhanced multi-level HybridRAG retrieval failed: {str(e)}"}
        )


async def gemini_chat(messages: List[Dict[str, str]]) -> str:
    if not httpx:
        raise RuntimeError("httpx not available")
    
    # å„ªå…ˆä½¿ç”¨ GOOGLE_API_KEYï¼Œå¦‚æœæ²’æœ‰å‰‡ä½¿ç”¨ GEMINI_API_KEY
    api_key = GOOGLE_API_KEY or os.getenv("GEMINI_API_KEY")
    if not api_key:
        raise RuntimeError("GOOGLE_API_KEY or GEMINI_API_KEY not set")
    
    model = os.getenv("GOOGLE_CHAT_MODEL", "gemini-1.5-flash")
    # Use Generative Language API: models/{model}:generateContent
    url = f"https://generativelanguage.googleapis.com/v1beta/models/{model}:generateContent"
    # Convert messages to Gemini format
    contents = []
    for message in messages:
        contents.append({
            "parts": [{"text": message.get("content", "")}],
            "role": "user" if message.get("role") == "user" else "model"
        })
    
    payload = {
        "contents": contents,
        "generationConfig": {
            "temperature": 0.2,
            "maxOutputTokens": 2048
        }
    }
    
    headers = {
        "x-goog-api-key": api_key,
        "Content-Type": "application/json"
    }
    
    async with httpx.AsyncClient(timeout=60) as client:
        r = await client.post(url, headers=headers, json=payload)
        r.raise_for_status()
        data = r.json()
        # Extract response from new format
        if "candidates" in data and data["candidates"]:
            candidate = data["candidates"][0]
            if "content" in candidate and "parts" in candidate["content"]:
                return candidate["content"]["parts"][0].get("text", "").strip()
        return "No response generated"


def simple_extractive_answer(query: str, contexts: List[str]) -> str:
    """é‡å°ä¸­è‹±æ–‡æ”¹é€²çš„æ¥µç°¡æŠ½å–å¼å›ç­”ï¼š
    - æ”¯æ´ä¸­æ–‡æ–·å¥ï¼ˆã€‚ï¼ï¼Ÿï¼›ï¼‰èˆ‡æ›è¡Œ
    - åˆ†è©åŒæ™‚è€ƒæ…®è‹±æ–‡/æ•¸å­—è©èˆ‡ä¸­æ–‡å–®å­—
    - è‹¥ç„¡æ˜é¡¯é‡ç–Šï¼Œå›é€€è¼¸å‡ºå‰å¹¾å¥æœ€å‰é¢çš„å…§å®¹
    """
    import re
    from collections import Counter

    # 1) æ–·å¥ï¼ˆåŒæ™‚æ”¯æ´ä¸­è‹±æ¨™é»èˆ‡æ›è¡Œï¼‰
    def split_sentences(text: str) -> List[str]:
        # ä¿ç•™åŸæ–‡ç‰‡æ®µï¼Œé¿å…éåº¦åˆ‡ç¢
        # å…ˆæŒ‰æ›è¡Œæ‹†ï¼Œå†æŒ‰ä¸­æ–‡/è‹±æ–‡å¥æœ«æ¨™é»ç´°åˆ†
        parts: List[str] = []
        for seg in re.split(r"[\n\r]+", text):
            seg = seg.strip()
            if not seg:
                continue
            parts.extend([s.strip() for s in re.split(r"(?<=[ã€‚ï¼ï¼Ÿ!?ï¼›;])\s+", seg) if s.strip()])
        return parts

    # 2) ç°¡å–®åˆ†è©ï¼šè‹±æ–‡/æ•¸å­—è© + ä¸­æ–‡å–®å­—
    def tokenize(text: str) -> List[str]:
        text_norm = text.lower()
        en = re.findall(r"[a-z0-9_]+", text_norm)
        zh = re.findall(r"[\u4e00-\u9fff]", text_norm)
        return en + zh

    q_tokens = set(tokenize(query))
    if not q_tokens:
        q_tokens = set(query.lower())  # é€€åŒ–ç‚ºå­—ç¬¦é›†åˆ

    # 3) èšåˆæ‰€æœ‰ä¸Šä¸‹æ–‡çš„å¥å­
    sents: List[str] = []
    for ctx in contexts:
        sents.extend(split_sentences(ctx))

    # 4) è¨ˆåˆ†ï¼šé‡ç–Š token æ•¸é‡ + è¼•åº¦é•·åº¦å¹³è¡¡
    counts = Counter()
    for s in sents:
        t = tokenize(s)
        if not t:
            continue
        overlap = len(set(t) & q_tokens)
        if overlap > 0:
            # è¼•åº¦é¼“å‹µè¼ƒå®Œæ•´å¥å­
            counts[s] = overlap + min(len(s) / 200.0, 1.0)

    # 5) å›å‚³ï¼šæœ‰åŒ¹é…å‰‡å–å‰5å¥ï¼Œå¦å‰‡å›é€€å–æœ€å‰é¢å…§å®¹
    if counts:
        best = [s for s, _ in counts.most_common(5)]
        return " \n".join(best)

    # å›é€€ï¼šå–å‰å…©æ®µçš„å‰å…©å¥
    fallback: List[str] = []
    for ctx in contexts[:2]:
        ss = split_sentences(ctx)
        fallback.extend(ss[:2])
        if len(fallback) >= 4:
            break
    if fallback:
        return " \n".join(fallback[:4])
    return "No relevant answer found in context."


@app.post("/api/generate")
def generate(req: GenerateRequest):
    # ä½¿ç”¨ HybridRAGï¼ˆå‘é‡æª¢ç´¢ + metadata é—œéµå­—åŠ åˆ†ï¼‰å–å¾—ç”Ÿæˆä¸Šä¸‹æ–‡
    if store.embeddings is None:
        return JSONResponse(status_code=400, content={"error": "run /embed first"})

    # æ§‹å»º nodesï¼ˆèˆ‡ /api/hybrid-retrieve ä¿æŒä¸€è‡´ï¼‰
    chunks_flat = store.chunks_flat
    mapping_doc_ids = store.chunk_doc_ids
    if not chunks_flat:
        return JSONResponse(status_code=400, content={"error": "no chunks available"})

    nodes = []
    for i, (chunk, doc_id) in enumerate(zip(chunks_flat, mapping_doc_ids)):
        doc = store.docs.get(doc_id)
        metadata = {}
        if doc and hasattr(doc, 'structured_chunks') and doc.structured_chunks and i < len(doc.structured_chunks):
            structured_chunk = doc.structured_chunks[i]
            metadata = structured_chunk.get("metadata", {})
        nodes.append({
            "content": chunk,
            "metadata": metadata,
            "doc_id": doc_id,
            "chunk_index": i
        })

    # å…ˆç”¨å¯†é›†å‘é‡è¨ˆç®—æ‰€æœ‰ç¯€é»çš„ç›¸ä¼¼åº¦ï¼Œå–å‰ N åš Hybrid å€™é¸
    dense_top_k = min(len(nodes), max(req.top_k * 4, req.top_k))
    all_vec_idxs, all_vec_sims = rank_with_dense_vectors(req.query, k=len(nodes))
    node_vector_scores = [0.0] * len(nodes)
    for rank_idx, node_idx in enumerate(all_vec_idxs):
        node_vector_scores[node_idx] = float(all_vec_sims[rank_idx])
    top_vec_pairs = sorted(
        [(i, s) for i, s in enumerate(node_vector_scores)], key=lambda x: x[1], reverse=True
    )[:dense_top_k]
    candidate_nodes = [nodes[i] for i, _ in top_vec_pairs]
    candidate_scores = [s for _, s in top_vec_pairs]

    config = HybridConfig(
        alpha=0.8,
        w_law_match=0.15,
        w_article_match=0.15,
        w_keyword_hit=0.05,
        max_bonus=0.4,
    )
    hybrid_results = hybrid_rank(req.query, candidate_nodes, k=req.top_k, config=config, vector_scores=candidate_scores)

    # ç”Ÿæˆä½¿ç”¨çš„çµæœ
    results = []
    for rank, item in enumerate(hybrid_results, start=1):
        result = {
            "rank": rank,
            "score": item.get("score"),
            "vector_score": item.get("vector_score"),
            "bonus": item.get("bonus"),
            "doc_id": item.get("doc_id"),
            "chunk_index": item.get("chunk_index"),
            "content": item.get("content"),
        }
        md = (item.get("metadata") or {})
        if md:
            result["legal_structure"] = {
                "id": md.get("id", ""),
                "category": md.get("category", ""),
                "article_label": md.get("article_label", ""),
                "article_number": md.get("article_number"),
                "article_suffix": md.get("article_suffix"),
                "spans": md.get("spans", {}),
                "page_range": md.get("page_range", {}),
            }
        results.append(result)
    contexts = [item["content"] for item in results]

    # æ§‹å»ºçµæ§‹åŒ–ä¸Šä¸‹æ–‡ä¿¡æ¯
    structured_context = []
    legal_references = []
    
    for item in results:
        context_text = item["content"]
        
        # å¦‚æœæœ‰æ³•å¾‹çµæ§‹ä¿¡æ¯ï¼Œæ·»åŠ åˆ°ä¸Šä¸‹æ–‡ä¸­
        if "legal_structure" in item:
            legal_info = item["legal_structure"]
            law_name = legal_info.get("law_name", "")
            article = legal_info.get("article", "")
            item_ref = legal_info.get("item", "")
            sub_item = legal_info.get("sub_item", "")
            chunk_type = legal_info.get("chunk_type", "")
            
            # æ§‹å»ºæ³•å¾‹å¼•ç”¨
            legal_ref = f"{law_name}"
            if article:
                legal_ref += f" {article}"
            if item_ref:
                legal_ref += f" {item_ref}"
            if sub_item:
                legal_ref += f" {sub_item}"
            
            if legal_ref not in legal_references:
                legal_references.append(legal_ref)
            
            # æ·»åŠ çµæ§‹åŒ–ä¸Šä¸‹æ–‡
            structured_context.append(f"[{legal_ref}] {context_text}")
        else:
            structured_context.append(context_text)

    reasoning_steps = [
        {"type": "plan", "text": "Read query, identify entities and constraints."},
        {"type": "gather", "text": f"Collect top-{req.top_k} chunks as context."},
        {"type": "analyze", "text": f"Analyze legal structure: {', '.join(legal_references[:3])}."},
        {"type": "synthesize", "text": "Synthesize answer grounded in retrieved text with legal references."},
    ]

    if USE_GEMINI_COMPLETION:
        # æ§‹å»ºåŒ…å«æ³•å¾‹çµæ§‹ä¿¡æ¯çš„prompt
        system_prompt = """ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„æ³•å¾‹åŠ©æ‰‹ã€‚è«‹åŸºæ–¼æä¾›çš„æ³•å¾‹æ–‡æª”å…§å®¹å›ç­”å•é¡Œã€‚

é‡è¦è¦æ±‚ï¼š
1. åªä½¿ç”¨æä¾›çš„ä¸Šä¸‹æ–‡å…§å®¹å›ç­”å•é¡Œ
2. å¦‚æœç­”æ¡ˆæ¶‰åŠå…·é«”æ³•å¾‹æ¢æ–‡ï¼Œè«‹å¼•ç”¨ç›¸é—œçš„æ³•è¦åç¨±å’Œæ¢æ–‡è™Ÿç¢¼
3. å¦‚æœä¿¡æ¯ä¸è¶³ï¼Œè«‹æ˜ç¢ºèªªæ˜ä½ ä¸çŸ¥é“
4. å›ç­”è¦æº–ç¢ºã€å°ˆæ¥­ï¼Œç¬¦åˆæ³•å¾‹æ–‡æª”çš„è¡¨è¿°æ–¹å¼"""

        user_content = f"å•é¡Œ: {req.query}\n\n"
        
        if legal_references:
            user_content += f"ç›¸é—œæ³•è¦: {', '.join(legal_references)}\n\n"
        
        user_content += "æ³•å¾‹æ–‡æª”å…§å®¹:\n" + "\n---\n".join(structured_context)
        
        prompt = [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_content},
        ]
        
        try:
            answer = asyncio_run(gemini_chat(prompt))
        except Exception as e:
            answer = f"Geminièª¿ç”¨å¤±æ•—: {e}. å›é€€åˆ°æå–å¼å›ç­”ã€‚\n" + simple_extractive_answer(req.query, contexts)
    else:
        answer = simple_extractive_answer(req.query, contexts)

    return {
        "query": req.query,
        "answer": answer,
        "contexts": results,
        "legal_references": legal_references,
        "steps": reasoning_steps,
    }


def merge_law_documents(law_documents: List[Dict[str, Any]]) -> Dict[str, Any]:
    """
    æ•´åˆå¤šå€‹æ³•å¾‹æ–‡æª”æˆä¸€å€‹çµ±ä¸€çš„JSONçµæ§‹
    å®Œå…¨æŒ‰ç…§å–®å€‹PDFè½‰æ›çš„æ–¹å¼ä¾†æ§‹å»ºçµæ§‹
    
    åƒæ•¸:
    - law_documents: å¤šå€‹æ³•å¾‹æ–‡æª”çš„åˆ—è¡¨
    
    è¿”å›:
    - æ•´åˆå¾Œçš„æ³•å¾‹æ–‡æª”ï¼Œæ ¼å¼ç‚º {"laws": [...]}
    """
    if not law_documents:
        return {"laws": []}
    
    merged_laws = []
    
    for doc in law_documents:
        if not doc or "law_name" not in doc:
            continue
            
        # ç›´æ¥ä½¿ç”¨åŸå§‹æ–‡æª”çµæ§‹ï¼Œç¢ºä¿å®Œå…¨ä¸€è‡´
        merged_law = {
            "law_name": doc["law_name"],
            "chapters": []
        }
        
        # è™•ç†ç« ç¯€
        chapters = doc.get("chapters", [])
        for chapter in chapters:
            merged_chapter = {
                "chapter": chapter.get("chapter", ""),
                "chapter_no": chapter.get("chapter_no", ""),
                "type_en": chapter.get("type_en", "Chapter"),
                "sections": []
            }
            
            # è™•ç†ç¯€
            sections = chapter.get("sections", [])
            for section in sections:
                merged_section = {
                    "section": section.get("section", ""),
                    "section_no": section.get("section_no", ""),
                    "type_en": section.get("type_en", "Section"),
                    "articles": []
                }
                
                # è™•ç†æ¢æ–‡
                articles = section.get("articles", [])
                for article in articles:
                    # æŒ‰ç…§å–®å€‹PDFè½‰æ›çš„æ–¹å¼æ§‹å»ºæ¢æ–‡çµæ§‹
                    merged_article = {
                        "article": article.get("article", ""),
                        "article_no": article.get("article_no", ""),
                        "type_en": article.get("type_en", "Article"),
                        "content": article.get("content", ""),
                        "paragraphs": [],
                        "metadata": article.get("metadata", {})
                    }
                    
                    # è™•ç†æ®µè½ - æ”¯æ´æ–°çµæ§‹ (paragraphs) å’ŒèˆŠçµæ§‹ (items)
                    paragraphs = article.get("paragraphs", [])
                    items = article.get("items", [])
                    
                    # ä½¿ç”¨ paragraphs å¦‚æœå­˜åœ¨ï¼Œå¦å‰‡ä½¿ç”¨ items
                    items_to_process = paragraphs if paragraphs else items
                    
                    for item in items_to_process:
                        # æŒ‰ç…§å–®å€‹PDFè½‰æ›çš„æ–¹å¼æ§‹å»ºæ®µè½çµæ§‹
                        merged_paragraph = {
                            "paragraph": item.get("paragraph", item.get("item", "")),
                            "paragraph_no": item.get("paragraph_no", ""),
                            "type_en": item.get("type_en", "Paragraph"),
                            "content": item.get("content", ""),
                            "subparagraphs": [],
                            "metadata": item.get("metadata", {})
                        }
                        
                        # è™•ç†å­æ®µè½ - æ”¯æ´æ–°çµæ§‹ (subparagraphs) å’ŒèˆŠçµæ§‹ (sub_items)
                        subparagraphs = item.get("subparagraphs", [])
                        sub_items = item.get("sub_items", [])
                        
                        # ä½¿ç”¨ subparagraphs å¦‚æœå­˜åœ¨ï¼Œå¦å‰‡ä½¿ç”¨ sub_items
                        sub_items_to_process = subparagraphs if subparagraphs else sub_items
                        
                        for sub_item in sub_items_to_process:
                            # æŒ‰ç…§å–®å€‹PDFè½‰æ›çš„æ–¹å¼æ§‹å»ºå­æ®µè½çµæ§‹
                            merged_subparagraph = {
                                "subparagraph": sub_item.get("subparagraph", sub_item.get("sub_item", "")),
                                "subparagraph_no": sub_item.get("subparagraph_no", ""),
                                "type_en": sub_item.get("type_en", "Subparagraph"),
                                "content": sub_item.get("content", ""),
                                "items": [],
                                "metadata": sub_item.get("metadata", {})
                            }
                            
                            # è™•ç†ç¬¬ä¸‰å±¤é …ç›® (items/ç›®)
                            third_level_items = sub_item.get("items", [])
                            for third_item in third_level_items:
                                merged_third_item = {
                                    "item": third_item.get("item", ""),
                                    "item_no": third_item.get("item_no", ""),
                                    "type_en": third_item.get("type_en", "Item"),
                                    "content": third_item.get("content", ""),
                                    "metadata": third_item.get("metadata", {})
                                }
                                merged_subparagraph["items"].append(merged_third_item)
                            
                            merged_paragraph["subparagraphs"].append(merged_subparagraph)
                        
                        merged_article["paragraphs"].append(merged_paragraph)
                    
                    merged_section["articles"].append(merged_article)
                
                merged_chapter["sections"].append(merged_section)
            
            merged_law["chapters"].append(merged_chapter)
        
        merged_laws.append(merged_law)
    
    return {"laws": merged_laws}


def clean_legal_amendments_and_effective_status(text: str) -> str:
    """
    æ¸…ç†æ³•è¦ä¸­çš„ä¿®æ­£æ—¥æœŸå’Œç”Ÿæ•ˆç‹€æ…‹ä¿¡æ¯
    
    ç§»é™¤æ¨¡å¼ï¼š
    1. ä¿®æ­£æ—¥æœŸï¼šæ°‘åœ‹ XXX å¹´ XX æœˆ XX æ—¥
    2. ç”Ÿæ•ˆç‹€æ…‹ï¼šâ€»æœ¬æ³•è¦éƒ¨åˆ†æˆ–å…¨éƒ¨æ¢æ–‡å°šæœªç”Ÿæ•ˆ
    3. ç›¸é—œçš„æ–½è¡Œæ—¥æœŸèªªæ˜
    4. æ³•è¦åç¨±è¡Œï¼ˆé¿å…ç”¢ç”Ÿæœªåˆ†é¡ç« ç¯€ï¼‰
    """
    import re
    
    lines = text.split('\n')
    cleaned_lines = []
    
    i = 0
    while i < len(lines):
        line = lines[i].strip()
        
        # è·³éä¿®æ­£æ—¥æœŸè¡Œ
        if re.match(r'^ä¿®æ­£æ—¥æœŸï¼šæ°‘åœ‹\s*\d+\s*å¹´\s*\d+\s*æœˆ\s*\d+\s*æ—¥', line):
            i += 1
            continue
            
        # è·³éç”Ÿæ•ˆç‹€æ…‹è¡Œ
        if 'â€»æœ¬æ³•è¦éƒ¨åˆ†æˆ–å…¨éƒ¨æ¢æ–‡å°šæœªç”Ÿæ•ˆ' in line or 'ç”Ÿæ•ˆç‹€æ…‹ï¼š' in line:
            i += 1
            continue
            
        # è·³éæ³•è¦åç¨±è¡Œï¼ˆé¿å…ç”¢ç”Ÿæœªåˆ†é¡ç« ç¯€ï¼‰
        if re.match(r'^æ³•è¦åç¨±ï¼š', line):
            i += 1
            continue
            
        # è·³éæ–½è¡Œæ—¥æœŸèªªæ˜æ®µè½ï¼ˆé€šå¸¸ä»¥æ•¸å­—é–‹é ­çš„åˆ—è¡¨é …ï¼‰
        if re.match(r'^\d+\.', line) and any(keyword in line for keyword in ['ä¿®æ­£', 'æ–½è¡Œ', 'ç”Ÿæ•ˆ', 'æ°‘åœ‹', 'å¹´', 'æœˆ', 'æ—¥']):
            # æª¢æŸ¥å¾ŒçºŒè¡Œæ˜¯å¦ä¹Ÿæ˜¯æ–½è¡Œæ—¥æœŸèªªæ˜ï¼ˆåŒ…æ‹¬ç¸®é€²è¡Œï¼‰
            j = i + 1
            while j < len(lines):
                next_line = lines[j].strip()
                # å¦‚æœæ˜¯ç©ºè¡Œï¼Œè·³é
                if not next_line:
                    j += 1
                    continue
                # å¦‚æœæ˜¯ä»¥ç©ºæ ¼/å…¨å½¢ç©ºæ ¼é–‹é ­çš„ç¸®é€²è¡Œï¼Œæˆ–è€…æ˜¯æ•¸å­—é–‹é ­çš„åˆ—è¡¨é …ï¼Œæˆ–è€…æ˜¯åŒ…å«é—œéµè©çš„è¡Œ
                if (next_line.startswith((' ', 'ã€€', '\t')) or 
                    re.match(r'^\d+\.', next_line) or
                    any(keyword in next_line for keyword in ['ä¿®æ­£', 'æ–½è¡Œ', 'ç”Ÿæ•ˆ', 'æ°‘åœ‹', 'å¹´', 'æœˆ', 'æ—¥', 'æ”¿é™¢', 'æ¢æ–‡', 'å¢è¨‚', 'åˆªé™¤'])):
                    j += 1
                    continue
                else:
                    break
            i = j
            continue
            
        # è·³éä»¥æ•¸å­—é–‹é ­ä¸”åŒ…å«ä¿®æ­£/æ–½è¡Œé—œéµè©çš„é€£çºŒè¡Œ
        if re.match(r'^\d+\.', line) and any(keyword in line for keyword in ['ä¿®æ­£', 'æ–½è¡Œ', 'ç”Ÿæ•ˆ']):
            # æª¢æŸ¥æ˜¯å¦ç‚ºä¿®æ­£æ¢æ–‡èªªæ˜
            if any(keyword in line for keyword in ['æ¢æ–‡', 'å¢è¨‚', 'åˆªé™¤']):
                i += 1
                continue
        
        # è·³éåŒ…å«æ–½è¡Œæ—¥æœŸç›¸é—œé—œéµè©çš„å­¤ç«‹è¡Œ
        if (any(keyword in line for keyword in ['æ”¿é™¢', 'æ–½è¡Œæ—¥æœŸ', 'å®šä¹‹', 'ä¿®æ­£ä¹‹ç¬¬']) and 
            not any(keyword in line for keyword in ['ç¬¬', 'æ¢', 'ç« ', 'ç¯€'])):
            i += 1
            continue
        
        # ä¿ç•™å…¶ä»–è¡Œ
        cleaned_lines.append(lines[i])
        i += 1
    
    # é€²ä¸€æ­¥æ¸…ç†ï¼šç§»é™¤é–‹é ­çš„ç©ºç™½è¡Œï¼Œç¢ºä¿å¾çœŸæ­£çš„ç« ç¯€é–‹å§‹
    while cleaned_lines and not cleaned_lines[0].strip():
        cleaned_lines.pop(0)
    
    return '\n'.join(cleaned_lines)


def convert_pdf_structured(file_content: bytes, filename: str, options: MetadataOptions) -> Dict[str, Any]:
    """å°‡PDFè½‰æ›ç‚ºçµæ§‹åŒ–æ ¼å¼"""
    import time
    start_time = time.time()
    
    try:
        # Read PDF content safely; skip pages with no text
        try:
            reader = PdfReader(io.BytesIO(file_content))
        except Exception as e:
            raise Exception(f"ç„¡æ³•è®€å–PDFæ–‡ä»¶: {str(e)}")
        
        # æ‰¹é‡æå–æ–‡æœ¬ï¼Œé¡¯ç¤ºé€²åº¦
        texts = []
        total_pages = len(reader.pages)
        print(f"ç¸½é æ•¸: {total_pages}")
        
        for i, page in enumerate(reader.pages):
            try:
                t = page.extract_text() or ""
                texts.append(t)
            except Exception:
                texts.append("")
            
            # æ¯è™•ç†10é é¡¯ç¤ºé€²åº¦
            if (i + 1) % 10 == 0 or (i + 1) == total_pages:
                print(f"å·²è™•ç† {i + 1}/{total_pages} é ")
        
        if not any(texts):  # No text extracted
            raise Exception("PDFæ–‡ä»¶ä¸­æ²¡æœ‰æ‰¾åˆ°å¯æå–çš„æ–‡æœ¬å†…å®¹")
            
        full_text = "\n".join(texts)
        print(f"æ–‡æœ¬æå–å®Œæˆï¼Œç¸½é•·åº¦: {len(full_text)} å­—ç¬¦")
        
        # æ¸…ç†ä¿®æ­£æ—¥æœŸå’Œç”Ÿæ•ˆç‹€æ…‹ä¿¡æ¯
        cleaned_text = clean_legal_amendments_and_effective_status(full_text)
        print(f"æ¸…ç†å®Œæˆï¼Œæ¸…ç†å¾Œé•·åº¦: {len(cleaned_text)} å­—ç¬¦")
        
        # ä½¿ç”¨æ¸…ç†å¾Œçš„æ–‡æœ¬é€²è¡Œå¾ŒçºŒè™•ç†
        full_text = cleaned_text

        def normalize_digits(s: str) -> str:
            # Convert fullwidth digits to ASCII for simpler matching
            fw = "ï¼ï¼‘ï¼’ï¼“ï¼”ï¼•ï¼–ï¼—ï¼˜ï¼™"
            hw = "0123456789"
            return s.translate(str.maketrans(fw, hw))

        # Determine law name: å¾åŸå§‹æ–‡æœ¬ä¸­æå–æ³•è¦åç¨±ï¼Œä½†ä½¿ç”¨æ¸…ç†å¾Œçš„æ–‡æœ¬é€²è¡Œçµæ§‹åŒ–
        original_text = "\n".join(texts)  # åŸå§‹æœªæ¸…ç†çš„æ–‡æœ¬
        original_lines = [normalize_digits((ln or "").strip()) for ln in original_text.splitlines()]
        law_name = None
        for ln in original_lines:
            if not ln:
                continue
            if any(key in ln for key in ["æ³•", "æ¢ä¾‹", "æ³•è¦", "æ³•å¾‹"]):
                law_name = ln
                break
        if not law_name:
            base = os.path.splitext(filename or "document")[0]
            law_name = base or "æœªå‘½åæ³•è¦"

        chapter_re = re.compile(r"^ç¬¬\s*([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒ0-9]+)\s*ç« [\u3000\s]*(.*)$")
        section_re = re.compile(r"^ç¬¬\s*([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒ0-9]+)\s*ç¯€[\u3000\s]*(.*)$")
        article_re = re.compile(r"^ç¬¬\s*([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾åƒ0-9]+(?:ä¹‹[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å0-9]+)?)\s*æ¢[\u3000\s]*(.*)$")

        def parse_item_line(ln: str):
            # Match common item markers like ã€Œä¸€ã€ã€ã€Œ1.ã€ã€Œï¼ˆä¸€ï¼‰ã€ã€Œ(1)ã€ã€Œ1ï¼‰ã€ etc.
            ln = ln.lstrip()
            # ï¼ˆä¸€ï¼‰ or (1)
            m = re.match(r"^[ï¼ˆ(]([0-9ï¼-ï¼™ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+)[ï¼‰)]\s*(.*)$", ln)
            if m:
                return m.group(1), m.group(2), "parentheses"
            # ä¸€ã€ äºŒã€ åã€ style (Chinese numerals with punctuation)
            m = re.match(r"^([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+)[ã€ï¼.ï¼‰)]\s*(.*)$", ln)
            if m:
                return m.group(1), m.group(2), "chinese_with_punct"
            # 1. 1ã€ 1) styles (Arabic numbers with punctuation)
            m = re.match(r"^([0-9ï¼-ï¼™]+)[ã€ï¼.ï¼‰)]\s*(.*)$", ln)
            if m:
                return m.group(1), m.group(2), "arabic_with_punct"
            # 1 2 3 styles (Arabic numbers followed by space, common in ROC legal documents)
            m = re.match(r"^([0-9ï¼-ï¼™]+)\s+(.*)$", ln)
            if m:
                return m.group(1), m.group(2), "arabic_space"
            # ä¸€ äºŒ ä¸‰ styles (Chinese numerals followed by space, sub-items)
            m = re.match(r"^([ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+)\s+(.*)$", ln)
            if m:
                return m.group(1), m.group(2), "chinese_space"
            return None, None, None

        structure: Dict[str, Any] = {"law_name": law_name, "chapters": []}
        current_chapter: Optional[Dict[str, Any]] = None
        current_section: Optional[Dict[str, Any]] = None
        current_article: Optional[Dict[str, Any]] = None
        # ä¾æ“šå°ç£æ³•å¾‹å±¤æ¬¡ï¼šæ¢ â†’ é …(Paragraph) â†’ æ¬¾(Subparagraph) â†’ ç›®(Item)
        current_paragraph: Optional[Dict[str, Any]] = None
        current_subparagraph: Optional[Dict[str, Any]] = None
        current_item_lvl3: Optional[Dict[str, Any]] = None

        def ensure_chapter():
            nonlocal current_chapter
            if current_chapter is None:
                current_chapter = {"chapter": "æœªåˆ†é¡ç« ", "sections": []}
                structure["chapters"].append(current_chapter)

        def ensure_section():
            nonlocal current_section
            ensure_chapter()
            if current_section is None:
                current_section = {"section": "æœªåˆ†é¡ç¯€", "articles": []}
                current_chapter["sections"].append(current_section)

        # ä½¿ç”¨æ¸…ç†å¾Œçš„æ–‡æœ¬é€²è¡Œçµæ§‹åŒ–è§£æ
        lines = [normalize_digits((ln or "").strip()) for ln in full_text.splitlines()]
        
        for raw in lines:
            ln = raw.strip()
            if not ln:
                continue

            # Headings
            m = chapter_re.match(ln)
            if m:
                num_raw = m.group(1)
                title = f"ç¬¬{num_raw}ç« " + (f" {m.group(2).strip()}" if m.group(2) else "")
                current_chapter = {"chapter": title, "chapter_no": normalize_digits(num_raw), "type_en": "Chapter", "sections": []}
                structure["chapters"].append(current_chapter)
                current_section = None
                current_article = None
                current_paragraph = None
                current_subparagraph = None
                current_item_lvl3 = None
                continue

            m = section_re.match(ln)
            if m:
                ensure_chapter()
                num_raw = m.group(1)
                title = f"ç¬¬{num_raw}ç¯€" + (f" {m.group(2).strip()}" if m.group(2) else "")
                current_section = {"section": title, "section_no": normalize_digits(num_raw), "type_en": "Section", "articles": []}
                current_chapter["sections"].append(current_section)
                current_article = None
                current_paragraph = None
                current_subparagraph = None
                current_item_lvl3 = None
                continue

            m = article_re.match(ln)
            if m:
                ensure_section()
                num_raw = m.group(1)
                title = f"ç¬¬{num_raw}æ¢"
                rest = m.group(2).strip() if m.group(2) else ""
                # å»ºç«‹æ¢æ–‡ï¼Œæ–°å¢ paragraphs æ¸…å–®ä¸¦ä¿ç•™ç›¸å®¹çš„ items æ¬„ä½
                current_article = {"article": title, "article_no": normalize_digits(num_raw), "type_en": "Article", "content": rest, "paragraphs": []}
                # ç›¸å®¹èˆŠæ¬„ä½ï¼ˆå°‡æŒ‡å‘åŒä¸€å€‹åˆ—è¡¨ï¼‰
                current_article["items"] = current_article["paragraphs"]
                current_section["articles"].append(current_article)
                current_paragraph = None
                current_subparagraph = None
                current_item_lvl3 = None
                continue

            # æ¢æ–‡å…§å±¤ç´šè§£æï¼šé …(é˜¿æ‹‰ä¼¯æ•¸å­—) â†’ æ¬¾(ä¸­æ–‡æ•¸å­—) â†’ ç›®ï¼ˆæ‹¬è™Ÿä¸­æ–‡æ•¸å­—ï¼‰
            if current_article is not None:
                num, content, item_type = parse_item_line(ln)
                if num is not None:
                    num = normalize_digits(num)
                    # 1) é … Paragraph: é˜¿æ‹‰ä¼¯æ•¸å­—ï¼ˆå« 1. 1ã€ 1) æˆ–æ•¸å­—+ç©ºç™½ï¼‰
                    if item_type in ("arabic_with_punct", "arabic_space"):
                        current_paragraph = {"paragraph": str(num), "paragraph_no": str(num), "type_en": "Paragraph", "content": content or "", "subparagraphs": []}
                        # ç›¸å®¹æ¬„ä½
                        current_paragraph["sub_items"] = current_paragraph["subparagraphs"]
                        current_article["paragraphs"].append(current_paragraph)
                        current_item_lvl3 = None
                        current_subparagraph = None
                    # 2) æ¬¾ Subparagraph: ä¸­æ–‡æ•¸å­—ï¼ˆå« ä¸€ã€ æˆ– ä¸­æ–‡æ•¸å­—+ç©ºç™½ï¼‰
                    elif item_type in ("chinese_with_punct", "chinese_space") and current_paragraph is not None:
                        if "subparagraphs" not in current_paragraph:
                            current_paragraph["subparagraphs"] = []
                            current_paragraph["sub_items"] = current_paragraph["subparagraphs"]
                        current_subparagraph = {"subparagraph": str(num), "subparagraph_no": str(num), "type_en": "Subparagraph", "content": content or "", "items": []}
                        # ç¬¬ä¸‰ç´šç›¸å®¹éµå
                        current_subparagraph["sub_sub_items"] = current_subparagraph["items"]
                        current_paragraph["subparagraphs"].append(current_subparagraph)
                        current_item_lvl3 = None
                    # 3) ç›® Item: æ‹¬è™Ÿä¸­æ–‡æˆ–æ•¸å­—ï¼ˆï¼ˆä¸€ï¼‰ã€(1)ï¼‰å‡ºç¾åœ¨æ¬¾å…§
                    elif item_type == "parentheses" and current_subparagraph is not None:
                        if "items" not in current_subparagraph:
                            current_subparagraph["items"] = []
                            current_subparagraph["sub_sub_items"] = current_subparagraph["items"]
                        current_item_lvl3 = {"item": str(num), "item_no": str(num), "type_en": "Item", "content": content or ""}
                        current_subparagraph["items"].append(current_item_lvl3)
                    else:
                        # è‹¥ç„¡æ³•åˆ¤åˆ¥å±¤ç´šï¼Œè¦–ç‚ºç•¶å‰æœ€æ·±å±¤çš„çºŒè¡Œæ–‡å­—
                        pass
                else:
                    # çºŒè¡Œæ–‡å­—ï¼šé™„åŠ åˆ°æœ€æ·±å±¤ï¼ˆç›® â†’ æ¬¾ â†’ é … â†’ æ¢ï¼‰
                    if current_item_lvl3 is not None:
                        sep = "\n" if current_item_lvl3.get("content") else ""
                        current_item_lvl3["content"] = f"{current_item_lvl3.get('content','')}{sep}{ln}"
                    elif current_subparagraph is not None:
                        sep = "\n" if current_subparagraph.get("content") else ""
                        current_subparagraph["content"] = f"{current_subparagraph.get('content','')}{sep}{ln}"
                    elif current_paragraph is not None:
                        sep = "\n" if current_paragraph.get("content") else ""
                        current_paragraph["content"] = f"{current_paragraph.get('content','')}{sep}{ln}"
                    else:
                        # accumulate into article content
                        if "content" not in current_article or current_article["content"] is None:
                            current_article["content"] = ln
                        else:
                            current_article["content"] = (current_article["content"] + "\n" + ln).strip()
                continue

            # If no article yet, but we have text, place it under a default article
            if current_section is not None and current_article is None:
                current_article = {"article": "æœªæ¨™ç¤ºæ¢æ–‡", "content": ln, "paragraphs": []}
                current_article["items"] = current_article["paragraphs"]
                current_section["articles"].append(current_article)
                current_paragraph = None
                current_subparagraph = None
                current_item_lvl3 = None
            elif current_article is None:
                ensure_section()
                current_article = {"article": "æœªæ¨™ç¤ºæ¢æ–‡", "content": ln, "paragraphs": []}
                current_article["items"] = current_article["paragraphs"]
                current_section["articles"].append(current_article)
                current_paragraph = None
                current_subparagraph = None
                current_item_lvl3 = None
            else:
                # fallback append
                current_article["content"] = (current_article.get("content", "") + "\n" + ln).strip()

        # å„ªåŒ–ç‰ˆæœ¬çš„metadataæ·»åŠ 
        def add_metadata_to_structure_optimized(structure, options, full_text):
            """å„ªåŒ–ç‰ˆæœ¬çš„metadataæ·»åŠ ï¼Œå¤§å¹…æå‡æ€§èƒ½"""
            print("é–‹å§‹æ·»åŠ metadata...")
            metadata_start = time.time()
            
            # é è¨ˆç®—æ‰€æœ‰æ¢æ–‡ï¼ˆé¿å…é‡è¤‡è¨ˆç®—ï¼‰
            all_articles = []
            for chapter in structure["chapters"]:
                for section in chapter["sections"]:
                    for article in section["articles"]:
                        all_articles.append({
                            "article": article["article"],
                            "content": article["content"],
                            "chapter": chapter["chapter"],
                            "section": section["section"]
                        })
            
            print(f"æ‰¾åˆ° {len(all_articles)} å€‹æ¢æ–‡")
            
            # æ‰¹é‡è™•ç†metadataï¼ˆå¦‚æœå•Ÿç”¨ï¼‰
            if options.include_id:
                print("æ‰¹é‡è™•ç†metadata...")
            
            processed_count = 0
            for chapter in structure["chapters"]:
                chapter_name = chapter["chapter"]
                for section in chapter["sections"]:
                    section_name = section["section"]
                    for article in section["articles"]:
                        article_name = article["article"]
                        
                        # ç°¡åŒ–çš„metadataè™•ç†
                        article_metadata = {}
                        if options.include_id:
                            article_metadata["id"] = f"{structure['law_name']}_{chapter_name}_{section_name}_{article_name}".replace(" ", "_")
                        
                        article["metadata"] = article_metadata
                        
                        # ç‚ºé …ç›®æ·»åŠ ç°¡åŒ–metadata - æ”¯æ´æ–°çµæ§‹ (paragraphs) å’ŒèˆŠçµæ§‹ (items)
                        paragraphs = article.get("paragraphs", [])
                        items = article.get("items", [])
                        items_to_process = paragraphs if paragraphs else items
                        
                        for item in items_to_process:
                            # æ”¯æ´æ–°çµæ§‹çš„éµå
                            item_name = item.get("paragraph", item.get("item", ""))
                            item_metadata = {}
                            if options.include_id:
                                item_metadata["id"] = f"{structure['law_name']}_{chapter_name}_{section_name}_{article_name}_{item_name}".replace(" ", "_")
                            
                            item["metadata"] = item_metadata
                            
                            # ç‚ºå­é …ç›®æ·»åŠ ç°¡åŒ–metadata - æ”¯æ´æ–°çµæ§‹ (subparagraphs) å’ŒèˆŠçµæ§‹ (sub_items)
                            subparagraphs = item.get("subparagraphs", [])
                            sub_items = item.get("sub_items", [])
                            sub_items_to_process = subparagraphs if subparagraphs else sub_items
                            
                            for sub_item in sub_items_to_process:
                                # æ”¯æ´æ–°çµæ§‹çš„éµå
                                sub_item_name = sub_item.get("subparagraph", sub_item.get("sub_item", ""))
                                sub_item_metadata = {}
                                if options.include_id:
                                    sub_item_metadata["id"] = f"{structure['law_name']}_{chapter_name}_{section_name}_{article_name}_{item_name}_{sub_item_name}".replace(" ", "_")
                                
                                sub_item["metadata"] = sub_item_metadata
                                
                                # è™•ç†ç¬¬ä¸‰å±¤é …ç›® (items)
                                third_level_items = sub_item.get("items", [])
                                for third_item in third_level_items:
                                    third_item_name = third_item.get("item", "")
                                    third_item_metadata = {}
                                    if options.include_id:
                                        third_item_metadata["id"] = f"{structure['law_name']}_{chapter_name}_{section_name}_{article_name}_{item_name}_{sub_item_name}_{third_item_name}".replace(" ", "_")
                                    
                                    third_item["metadata"] = third_item_metadata
                        
                        processed_count += 1
                        if processed_count % 10 == 0:
                            print(f"å·²è™•ç† {processed_count} å€‹æ¢æ–‡")
            
            metadata_time = time.time() - metadata_start
            print(f"Metadataè™•ç†å®Œæˆï¼Œè€—æ™‚: {metadata_time:.2f}ç§’")
        
        # æ·»åŠ metadataï¼ˆä½¿ç”¨å„ªåŒ–ç‰ˆæœ¬ï¼‰
        if options.include_id:
            add_metadata_to_structure_optimized(structure, options, full_text)
        else:
            print("è·³émetadataè™•ç†ï¼ˆæœªå•Ÿç”¨ï¼‰")

        total_time = time.time() - start_time
        print(f"ç¸½è½‰æ›æ™‚é–“: {total_time:.2f}ç§’")
        
        return {
            "text": full_text,
            "metadata": structure,
            "processing_time": total_time,
            "success": True
        }
        
    except Exception as e:
        return {
            "text": "",
            "metadata": {"error": str(e)},
            "processing_time": time.time() - start_time,
            "success": False,
            "error": str(e)
        }


# ç•°æ­¥ä»»å‹™å­˜å„²
conversion_tasks = {}

# PDFç·©å­˜å­˜å„² (åŸºæ–¼æ–‡ä»¶å…§å®¹å“ˆå¸Œ)
pdf_cache = {}

# æ¸…ç†èˆŠä»»å‹™çš„å¾Œå°ä»»å‹™
async def cleanup_old_tasks():
    """æ¸…ç†è¶…é1å°æ™‚çš„èˆŠä»»å‹™"""
    while True:
        try:
            current_time = time.time()
            expired_tasks = []
            
            for task_id, task in conversion_tasks.items():
                if current_time - task["created_at"] > 3600:  # 1å°æ™‚
                    expired_tasks.append(task_id)
            
            for task_id in expired_tasks:
                del conversion_tasks[task_id]
                print(f"æ¸…ç†éæœŸä»»å‹™: {task_id}")
            
            # æ¯5åˆ†é˜æ¸…ç†ä¸€æ¬¡
            await asyncio.sleep(300)
        except Exception as e:
            print(f"æ¸…ç†ä»»å‹™æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
            await asyncio.sleep(60)  # å‡ºéŒ¯æ™‚ç­‰å¾…1åˆ†é˜å†é‡è©¦

# æ¸…ç†ä»»å‹™å°‡åœ¨æ‡‰ç”¨å•Ÿå‹•æ™‚å•Ÿå‹•
@app.on_event("startup")
async def startup_event():
    """æ‡‰ç”¨å•Ÿå‹•æ™‚çš„äº‹ä»¶"""
    import asyncio
    asyncio.create_task(cleanup_old_tasks())

@app.post("/api/convert")
async def convert(file: UploadFile = File(...), metadata_options: str = Form("{}")):
    """å•Ÿå‹•PDFè½‰æ›ä»»å‹™"""
    try:
        # Parse metadata options
        try:
            metadata_config = json.loads(metadata_options)
            options = MetadataOptions(**metadata_config)
        except:
            options = MetadataOptions()  # ä½¿ç”¨é»˜èªé¸é …
        
        # Validate file type
        if not file.filename or not file.filename.lower().endswith('.pdf'):
            return JSONResponse(
                status_code=400, 
                content={"error": "åªæ”¯æŒPDFæ–‡ä»¶æ ¼å¼", "detail": "Invalid file type"}
            )
        
        # Reset file pointer to beginning
        await file.seek(0)
        
        # ç”Ÿæˆä»»å‹™ID
        task_id = f"convert_{int(time.time() * 1000)}_{hash(file.filename) % 10000}"
        
        # è®€å–æ–‡ä»¶å…§å®¹
        file_content = await file.read()
        
        # æª¢æŸ¥ç·©å­˜ï¼ˆåŸºæ–¼æ–‡ä»¶å…§å®¹å“ˆå¸Œï¼‰
        import hashlib
        file_hash = hashlib.md5(file_content).hexdigest()
        
        # æª¢æŸ¥æ˜¯å¦å·²ç·©å­˜
        cache_key = f"{file_hash}_{json.dumps(options.__dict__, sort_keys=True)}"
        if cache_key in pdf_cache:
            cached_result = pdf_cache[cache_key]
            print(f"ä½¿ç”¨ç·©å­˜çš„PDFè½‰æ›çµæœ: {file.filename}")
            
            # ç”Ÿæˆæ–°çš„doc_id
            doc_id = f"doc_{int(time.time() * 1000)}_{hash(file.filename) % 10000}"
            
            # å°‡æ–‡æª”å­˜å„²åˆ°storeä¸­
            store.docs[doc_id] = DocRecord(
                id=doc_id,
                filename=file.filename,
                text=cached_result["text"],
                json_data=cached_result["metadata"],
                chunks=[],
                chunk_size=0,
                overlap=0,
            )
            
            return {
                "doc_id": doc_id,
                "filename": file.filename,
                "text_length": cached_result["text_length"],
                "metadata": cached_result["metadata"],
                "processing_time": 0.1,  # ç·©å­˜å‘½ä¸­ï¼Œå¹¾ä¹ç¬é–“å®Œæˆ
                "cached": True
            }
        
        # å‰µå»ºä»»å‹™
        conversion_tasks[task_id] = {
            "status": "pending",
            "progress": 0,
            "filename": file.filename,
            "created_at": time.time(),
            "result": None,
            "error": None,
            "file_hash": file_hash,
            "cache_key": cache_key
        }
        
        # å•Ÿå‹•å¾Œå°ä»»å‹™
        import asyncio
        asyncio.create_task(process_pdf_conversion(task_id, file_content, options))
        
        return {
            "task_id": task_id,
            "status": "pending",
            "message": "PDFè½‰æ›ä»»å‹™å·²å•Ÿå‹•ï¼Œè«‹ä½¿ç”¨task_idæŸ¥è©¢é€²åº¦"
        }
        
    except Exception as e:
        print(f"Convert endpoint error: {e}")
        return JSONResponse(
            status_code=500, 
            content={"error": "å•Ÿå‹•PDFè½‰æ›ä»»å‹™å¤±æ•—", "detail": str(e)}
        )


async def process_pdf_conversion(task_id: str, file_content: bytes, options: MetadataOptions):
    """å¾Œå°è™•ç†PDFè½‰æ›"""
    import time
    start_time = time.time()
    
    try:
        # æ›´æ–°ä»»å‹™ç‹€æ…‹
        conversion_tasks[task_id]["status"] = "processing"
        conversion_tasks[task_id]["progress"] = 10
        
        print(f"é–‹å§‹è½‰æ›PDF: {conversion_tasks[task_id]['filename']}")
        
        # ç›´æ¥èª¿ç”¨convert_pdf_structuredå‡½æ•¸
        conversion_tasks[task_id]["progress"] = 20
        result = convert_pdf_structured(file_content, conversion_tasks[task_id]['filename'], options)
        
        if not result["success"]:
            conversion_tasks[task_id]["status"] = "failed"
            conversion_tasks[task_id]["error"] = result.get("error", "PDFè½‰æ›å¤±æ•—")
            return
        
        # æå–çµæœ
        full_text = result["text"]
        structure = result["metadata"]
        total_time = result["processing_time"]
        
        conversion_tasks[task_id]["progress"] = 80
        print(f"PDFè½‰æ›å®Œæˆï¼Œæ–‡æœ¬é•·åº¦: {len(full_text)} å­—ç¬¦")
        
        # ç”Ÿæˆæ–‡æª”ID
        doc_id = f"doc_{int(time.time() * 1000)}_{hash(conversion_tasks[task_id]['filename']) % 10000}"
        
        # å°‡æ–‡æª”å­˜å„²åˆ°storeä¸­
        store.docs[doc_id] = DocRecord(
            id=doc_id,
            filename=conversion_tasks[task_id]['filename'],
            text=full_text,
            json_data=structure,
            chunks=[],
            chunk_size=0,
            overlap=0,
        )
        
        # é‡ç½®åµŒå…¥ç‹€æ…‹
        store.reset_embeddings()
        store.save_data()
        
        # ä¿å­˜åˆ°ç·©å­˜
        cache_data = {
            "text": full_text,
            "metadata": structure,
            "text_length": len(full_text),
            "processing_time": total_time
        }
        pdf_cache[conversion_tasks[task_id]["cache_key"]] = cache_data
        
        # é™åˆ¶ç·©å­˜å¤§å°ï¼ˆæœ€å¤šä¿å­˜100å€‹è½‰æ›çµæœï¼‰
        if len(pdf_cache) > 100:
            # åˆªé™¤æœ€èˆŠçš„ç·©å­˜é …
            oldest_key = next(iter(pdf_cache))
            del pdf_cache[oldest_key]
        
        # æ›´æ–°ä»»å‹™ç‹€æ…‹ç‚ºå®Œæˆ
        conversion_tasks[task_id]["status"] = "completed"
        conversion_tasks[task_id]["progress"] = 100
        conversion_tasks[task_id]["result"] = {
            "doc_id": doc_id,
            "filename": conversion_tasks[task_id]['filename'],
            "text_length": len(full_text),
            "metadata": structure,
            "processing_time": total_time
        }
        
    except Exception as e:
        # æ›´æ–°ä»»å‹™ç‹€æ…‹ç‚ºå¤±æ•—
        conversion_tasks[task_id]["status"] = "failed"
        conversion_tasks[task_id]["error"] = str(e)
        print(f"PDFè½‰æ›å¤±æ•—: {str(e)}")


@app.get("/api/convert/status/{task_id}")
async def get_convert_status(task_id: str):
    """æŸ¥è©¢PDFè½‰æ›ä»»å‹™ç‹€æ…‹"""
    if task_id not in conversion_tasks:
        raise HTTPException(status_code=404, detail="ä»»å‹™ä¸å­˜åœ¨")
    
    task = conversion_tasks[task_id]
    
    # æ¸…ç†è¶…é1å°æ™‚çš„èˆŠä»»å‹™
    if time.time() - task["created_at"] > 3600:
        del conversion_tasks[task_id]
        raise HTTPException(status_code=404, detail="ä»»å‹™å·²éæœŸ")
    
    return {
        "task_id": task_id,
        "status": task["status"],
        "progress": task["progress"],
        "filename": task["filename"],
        "result": task.get("result"),
        "error": task.get("error")
    }


def run_evaluation_task(task_id: str):
    """
    åœ¨å¾Œå°é‹è¡Œè©•æ¸¬ä»»å‹™
    """
    task = eval_store.get_task(task_id)
    if not task:
        return
    
    try:
        eval_store.update_task_status(task_id, "running")
        
        doc = store.docs.get(task.doc_id)
        if not doc:
            eval_store.update_task_status(task_id, "failed", error_message="Document not found")
            return
        
        results = []
        total_configs = len(task.configs)
        
        for i, config in enumerate(task.configs):
            result = evaluate_chunk_config(doc, config, task.test_queries, task.k_values, task.strategy)
            results.append(result)
            
            # æ›´æ–°é€²åº¦
            progress = (i + 1) / total_configs
            eval_store.update_task_status(task_id, "running", progress=progress)
        
        eval_store.update_task_status(task_id, "completed", results=results)
        
    except Exception as e:
        eval_store.update_task_status(task_id, "failed", error_message=str(e))


@app.post("/api/evaluate/fixed-size")
def start_fixed_size_evaluation(req: FixedSizeEvaluationRequest, background_tasks: BackgroundTasks):
    """
    é–‹å§‹å›ºå®šå¤§å°åˆ†å‰²ç­–ç•¥è©•æ¸¬
    """
    doc = store.docs.get(req.doc_id)
    if not doc:
        return JSONResponse(status_code=404, content={"error": "Document not found"})
    
    # æª¢æŸ¥æ˜¯å¦å·²æœ‰ç”Ÿæˆçš„å•é¡Œ
    if not hasattr(doc, 'generated_questions') or not doc.generated_questions:
        return JSONResponse(
            status_code=400, 
            content={"error": "è«‹å…ˆä½¿ç”¨ã€Œç”Ÿæˆå•é¡Œã€åŠŸèƒ½ç‚ºæ–‡æª”ç”Ÿæˆæ¸¬è©¦å•é¡Œï¼Œç„¶å¾Œå†é€²è¡Œè©•æ¸¬"}
        )
    
    # ä½¿ç”¨æ–‡æª”ä¸­å­˜å„²çš„å•é¡Œè€Œä¸æ˜¯é è¨­å•é¡Œ
    req.test_queries = doc.generated_questions
    
    # ç”Ÿæˆæ‰€æœ‰é…ç½®çµ„åˆï¼ŒåŒ…æ‹¬ç­–ç•¥ç‰¹å®šåƒæ•¸
    configs = []
    for chunk_size in req.chunk_sizes:
        for overlap_ratio in req.overlap_ratios:
            overlap = int(chunk_size * overlap_ratio)
            
            # æ ¹æ“šç­–ç•¥ç”Ÿæˆä¸åŒçš„åƒæ•¸çµ„åˆ
            if req.strategy == "structured_hierarchical":
                for chunk_by in req.chunk_by_options:
                    config = ChunkConfig(
                        chunk_size=chunk_size,
                        overlap=overlap,
                        overlap_ratio=overlap_ratio,
                        chunk_by=chunk_by
                    )
                    configs.append(config)
            elif req.strategy == "rcts_hierarchical":
                for preserve_structure in req.preserve_structure_options:
                    config = ChunkConfig(
                        chunk_size=chunk_size,
                        overlap=overlap,
                        overlap_ratio=overlap_ratio,
                        preserve_structure=preserve_structure,
                        chunk_by="article"  # é»˜èªå€¼
                    )
                    configs.append(config)
            elif req.strategy == "hierarchical":
                for level_depth in req.level_depth_options:
                    for min_chunk_size in req.min_chunk_size_options:
                        config = ChunkConfig(
                            chunk_size=chunk_size,
                            overlap=overlap,
                            overlap_ratio=overlap_ratio,
                            level_depth=level_depth,
                            min_chunk_size=min_chunk_size,
                            chunk_by="article"  # é»˜èªå€¼
                        )
                        configs.append(config)
            elif req.strategy == "semantic":
                for similarity_threshold in req.similarity_threshold_options:
                    for context_window in req.context_window_options:
                        config = ChunkConfig(
                            chunk_size=chunk_size,
                            overlap=overlap,
                            overlap_ratio=overlap_ratio,
                            similarity_threshold=similarity_threshold,
                            context_window=context_window,
                            chunk_by="article"  # é»˜èªå€¼
                        )
                        configs.append(config)
            elif req.strategy == "llm_semantic":
                for semantic_threshold in req.semantic_threshold_options:
                    for context_window in req.context_window_options:
                        config = ChunkConfig(
                            chunk_size=chunk_size,
                            overlap=overlap,
                            overlap_ratio=overlap_ratio,
                            semantic_threshold=semantic_threshold,
                            context_window=context_window,
                            chunk_by="article"  # é»˜èªå€¼
                        )
                        configs.append(config)
            elif req.strategy == "sliding_window":
                for window_size in req.window_size_options:
                    for step_size in req.step_size_options:
                        for boundary_aware in req.boundary_aware_options:
                            for preserve_sentences in req.preserve_sentences_options:
                                for min_chunk_size_sw in req.min_chunk_size_options_sw:
                                    for max_chunk_size_sw in req.max_chunk_size_options_sw:
                                        config = ChunkConfig(
                                            chunk_size=window_size,  # ä½¿ç”¨window_sizeä½œç‚ºchunk_size
                                            overlap=overlap,
                                            overlap_ratio=overlap_ratio,
                                            strategy="sliding_window",
                                            step_size=step_size,
                                            window_size=window_size,
                                            boundary_aware=boundary_aware,
                                            preserve_sentences=preserve_sentences,
                                            min_chunk_size_sw=min_chunk_size_sw,
                                            max_chunk_size_sw=max_chunk_size_sw,
                                            chunk_by="article"  # é»˜èªå€¼
                                        )
                                        configs.append(config)
            elif req.strategy == "hybrid":
                for switch_threshold in req.switch_threshold_options:
                    for secondary_size in req.secondary_size_options:
                        config = ChunkConfig(
                            chunk_size=chunk_size,
                            overlap=overlap,
                            overlap_ratio=overlap_ratio,
                            switch_threshold=switch_threshold,
                            secondary_size=secondary_size,
                            chunk_by="article"  # é»˜èªå€¼
                        )
                        configs.append(config)
            else:
                # é»˜èªé…ç½®ï¼ˆfixed_sizeç­‰ï¼‰
                config = ChunkConfig(
                    chunk_size=chunk_size,
                    overlap=overlap,
                    overlap_ratio=overlap_ratio,
                    chunk_by="article"  # é»˜èªå€¼
                )
                configs.append(config)
    
    # ç²å–åˆ†å‰²ç­–ç•¥ï¼ˆå¾è«‹æ±‚ä¸­ç²å–ï¼Œé»˜èªç‚ºfixed_sizeï¼‰
    strategy = getattr(req, 'strategy', 'fixed_size')
    
    # å‰µå»ºè©•æ¸¬ä»»å‹™
    task_id = eval_store.create_task(
        doc_id=req.doc_id,
        configs=configs,
        test_queries=req.test_queries,
        k_values=req.k_values,
        strategy=strategy
    )
    
    # åœ¨å¾Œå°é‹è¡Œè©•æ¸¬
    background_tasks.add_task(run_evaluation_task, task_id)
    
    return {
        "task_id": task_id,
        "status": "started",
        "total_configs": len(configs),
        "message": "è©•æ¸¬ä»»å‹™å·²é–‹å§‹ï¼Œè«‹ä½¿ç”¨task_idæŸ¥è©¢é€²åº¦"
    }


@app.get("/api/evaluate/status/{task_id}")
def get_evaluation_status(task_id: str):
    """
    ç²å–è©•æ¸¬ä»»å‹™ç‹€æ…‹
    """
    task = eval_store.get_task(task_id)
    if not task:
        return JSONResponse(status_code=404, content={"error": "Task not found"})
    
    total_configs = len(task.configs)
    completed_configs = int(task.progress * total_configs) if task.progress > 0 else 0
    
    return {
        "task_id": task_id,
        "status": task.status,
        "created_at": task.created_at.isoformat(),
        "completed_at": task.completed_at.isoformat() if task.completed_at else None,
        "error_message": task.error_message,
        "total_configs": total_configs,
        "completed_configs": completed_configs,
        "progress": task.progress
    }


@app.get("/api/evaluate/results/{task_id}")
def get_evaluation_results(task_id: str):
    """
    ç²å–è©•æ¸¬çµæœ
    """
    task = eval_store.get_task(task_id)
    if not task:
        return JSONResponse(status_code=404, content={"error": "Task not found"})
    
    if task.status != "completed":
        return JSONResponse(status_code=400, content={"error": "Task not completed yet"})
    
    # è½‰æ›çµæœç‚ºå¯åºåˆ—åŒ–çš„æ ¼å¼
    results = []
    for result in task.results:
        result_dict = {
            "config": result.config,  # ç¾åœ¨ config å·²ç¶“æ˜¯å­—å…¸äº†
            "metrics": {
                "precision_omega": result.metrics.precision_omega,
                "precision_at_k": result.metrics.precision_at_k,
                "recall_at_k": result.metrics.recall_at_k,
                "chunk_count": result.metrics.chunk_count,
                "avg_chunk_length": result.metrics.avg_chunk_length,
                "length_variance": result.metrics.length_variance
            },
            "test_queries": result.test_queries,
            "retrieval_results": result.retrieval_results,
            "timestamp": result.timestamp.isoformat()
        }
        results.append(result_dict)
    
    return {
        "task_id": task_id,
        "status": task.status,
        "results": results,
        "summary": {
            "total_configs": len(results),
            "best_precision_omega": max(r["metrics"]["precision_omega"] for r in results),
            "best_precision_at_5": max(r["metrics"]["precision_at_k"].get(5, 0) for r in results),
            "best_recall_at_5": max(r["metrics"]["recall_at_k"].get(5, 0) for r in results),
            "avg_chunk_count": sum(r["metrics"]["chunk_count"] for r in results) / len(results),
            "avg_chunk_length": sum(r["metrics"]["avg_chunk_length"] for r in results) / len(results)
        }
    }


@app.get("/api/evaluate/comparison/{task_id}")
def get_evaluation_comparison(task_id: str):
    """
    ç²å–è©•æ¸¬çµæœå°æ¯”åˆ†æ
    """
    task = eval_store.get_task(task_id)
    if not task:
        return JSONResponse(status_code=404, content={"error": "Task not found"})
    
    if task.status != "completed":
        return JSONResponse(status_code=400, content={"error": "Task not completed yet"})
    
    # ç”Ÿæˆå°æ¯”åˆ†æ
    comparison = {
        "chunk_size_analysis": {},
        "overlap_analysis": {},
        "strategy_specific_analysis": {},
        "recommendations": []
    }
    
    # æŒ‰chunk sizeåˆ†çµ„åˆ†æ
    chunk_size_groups = {}
    for result in task.results:
        size = result.config["chunk_size"]
        if size not in chunk_size_groups:
            chunk_size_groups[size] = []
        chunk_size_groups[size].append(result)
    
    for size, results in chunk_size_groups.items():
        avg_metrics = {
            "precision_omega": sum(r.metrics.precision_omega for r in results) / len(results),
            "precision_at_k": {
                "1": sum(r.metrics.precision_at_k.get(1, 0) for r in results) / len(results),
                "3": sum(r.metrics.precision_at_k.get(3, 0) for r in results) / len(results),
                "5": sum(r.metrics.precision_at_k.get(5, 0) for r in results) / len(results),
                "10": sum(r.metrics.precision_at_k.get(10, 0) for r in results) / len(results)
            },
            "recall_at_k": {
                "1": sum(r.metrics.recall_at_k.get(1, 0) for r in results) / len(results),
                "3": sum(r.metrics.recall_at_k.get(3, 0) for r in results) / len(results),
                "5": sum(r.metrics.recall_at_k.get(5, 0) for r in results) / len(results),
                "10": sum(r.metrics.recall_at_k.get(10, 0) for r in results) / len(results)
            },
            "avg_chunk_count": sum(r.metrics.chunk_count for r in results) / len(results),
            "avg_chunk_length": sum(r.metrics.avg_chunk_length for r in results) / len(results),
            "length_variance": sum(r.metrics.length_variance for r in results) / len(results)
        }
        comparison["chunk_size_analysis"][size] = avg_metrics
    
    # æŒ‰overlap ratioåˆ†çµ„åˆ†æ
    overlap_groups = {}
    for result in task.results:
        ratio = result.config["overlap_ratio"]
        if ratio not in overlap_groups:
            overlap_groups[ratio] = []
        overlap_groups[ratio].append(result)
    
    for ratio, results in overlap_groups.items():
        avg_metrics = {
            "precision_omega": sum(r.metrics.precision_omega for r in results) / len(results),
            "precision_at_k": {
                "1": sum(r.metrics.precision_at_k.get(1, 0) for r in results) / len(results),
                "3": sum(r.metrics.precision_at_k.get(3, 0) for r in results) / len(results),
                "5": sum(r.metrics.precision_at_k.get(5, 0) for r in results) / len(results),
                "10": sum(r.metrics.precision_at_k.get(10, 0) for r in results) / len(results)
            },
            "recall_at_k": {
                "1": sum(r.metrics.recall_at_k.get(1, 0) for r in results) / len(results),
                "3": sum(r.metrics.recall_at_k.get(3, 0) for r in results) / len(results),
                "5": sum(r.metrics.recall_at_k.get(5, 0) for r in results) / len(results),
                "10": sum(r.metrics.recall_at_k.get(10, 0) for r in results) / len(results)
            },
            "avg_chunk_count": sum(r.metrics.chunk_count for r in results) / len(results),
            "avg_chunk_length": sum(r.metrics.avg_chunk_length for r in results) / len(results),
            "length_variance": sum(r.metrics.length_variance for r in results) / len(results)
        }
        comparison["overlap_analysis"][ratio] = avg_metrics
    
    # æŒ‰ç­–ç•¥ç‰¹å®šåƒæ•¸åˆ†çµ„åˆ†æ
    if task.results:
        strategy = task.results[0].config.get("strategy", "fixed_size")
        
        if strategy == "structured_hierarchical":
            # æŒ‰åˆ†å‰²å–®ä½åˆ†çµ„
            chunk_by_groups = {}
            for result in task.results:
                chunk_by = result.config.get("chunk_by", "article")
                if chunk_by not in chunk_by_groups:
                    chunk_by_groups[chunk_by] = []
                chunk_by_groups[chunk_by].append(result)
            
            for chunk_by, results in chunk_by_groups.items():
                avg_metrics = {
                    "precision_omega": sum(r.metrics.precision_omega for r in results) / len(results),
                    "precision_at_k": {
                        "1": sum(r.metrics.precision_at_k.get(1, 0) for r in results) / len(results),
                        "3": sum(r.metrics.precision_at_k.get(3, 0) for r in results) / len(results),
                        "5": sum(r.metrics.precision_at_k.get(5, 0) for r in results) / len(results),
                        "10": sum(r.metrics.precision_at_k.get(10, 0) for r in results) / len(results)
                    },
                    "recall_at_k": {
                        "1": sum(r.metrics.recall_at_k.get(1, 0) for r in results) / len(results),
                        "3": sum(r.metrics.recall_at_k.get(3, 0) for r in results) / len(results),
                        "5": sum(r.metrics.recall_at_k.get(5, 0) for r in results) / len(results),
                        "10": sum(r.metrics.recall_at_k.get(10, 0) for r in results) / len(results)
                    },
                    "avg_chunk_count": sum(r.metrics.chunk_count for r in results) / len(results),
                    "avg_chunk_length": sum(r.metrics.avg_chunk_length for r in results) / len(results),
                    "length_variance": sum(r.metrics.length_variance for r in results) / len(results)
                }
                comparison["strategy_specific_analysis"][f"chunk_by_{chunk_by}"] = avg_metrics
        
        elif strategy == "rcts_hierarchical":
            # æŒ‰ä¿æŒçµæ§‹åˆ†çµ„
            preserve_groups = {}
            for result in task.results:
                preserve = result.config.get("preserve_structure", True)
                key = "preserve_structure" if preserve else "no_preserve_structure"
                if key not in preserve_groups:
                    preserve_groups[key] = []
                preserve_groups[key].append(result)
            
            for key, results in preserve_groups.items():
                avg_metrics = {
                    "precision_omega": sum(r.metrics.precision_omega for r in results) / len(results),
                    "precision_at_k": {
                        "1": sum(r.metrics.precision_at_k.get(1, 0) for r in results) / len(results),
                        "3": sum(r.metrics.precision_at_k.get(3, 0) for r in results) / len(results),
                        "5": sum(r.metrics.precision_at_k.get(5, 0) for r in results) / len(results),
                        "10": sum(r.metrics.precision_at_k.get(10, 0) for r in results) / len(results)
                    },
                    "recall_at_k": {
                        "1": sum(r.metrics.recall_at_k.get(1, 0) for r in results) / len(results),
                        "3": sum(r.metrics.recall_at_k.get(3, 0) for r in results) / len(results),
                        "5": sum(r.metrics.recall_at_k.get(5, 0) for r in results) / len(results),
                        "10": sum(r.metrics.recall_at_k.get(10, 0) for r in results) / len(results)
                    },
                    "avg_chunk_count": sum(r.metrics.chunk_count for r in results) / len(results),
                    "avg_chunk_length": sum(r.metrics.avg_chunk_length for r in results) / len(results),
                    "length_variance": sum(r.metrics.length_variance for r in results) / len(results)
                }
                comparison["strategy_specific_analysis"][key] = avg_metrics
        
        elif strategy == "hierarchical":
            # æŒ‰å±¤æ¬¡æ·±åº¦åˆ†çµ„
            level_groups = {}
            for result in task.results:
                level = result.config.get("level_depth", 3)
                if level not in level_groups:
                    level_groups[level] = []
                level_groups[level].append(result)
            
            for level, results in level_groups.items():
                avg_metrics = {
                    "precision_omega": sum(r.metrics.precision_omega for r in results) / len(results),
                    "precision_at_k": {
                        "1": sum(r.metrics.precision_at_k.get(1, 0) for r in results) / len(results),
                        "3": sum(r.metrics.precision_at_k.get(3, 0) for r in results) / len(results),
                        "5": sum(r.metrics.precision_at_k.get(5, 0) for r in results) / len(results),
                        "10": sum(r.metrics.precision_at_k.get(10, 0) for r in results) / len(results)
                    },
                    "recall_at_k": {
                        "1": sum(r.metrics.recall_at_k.get(1, 0) for r in results) / len(results),
                        "3": sum(r.metrics.recall_at_k.get(3, 0) for r in results) / len(results),
                        "5": sum(r.metrics.recall_at_k.get(5, 0) for r in results) / len(results),
                        "10": sum(r.metrics.recall_at_k.get(10, 0) for r in results) / len(results)
                    },
                    "avg_chunk_count": sum(r.metrics.chunk_count for r in results) / len(results),
                    "avg_chunk_length": sum(r.metrics.avg_chunk_length for r in results) / len(results),
                    "length_variance": sum(r.metrics.length_variance for r in results) / len(results)
                }
                comparison["strategy_specific_analysis"][f"level_depth_{level}"] = avg_metrics
    
    # ç”Ÿæˆæ¨è–¦
    best_overall = max(task.results, key=lambda r: (
        r.metrics.precision_omega * 0.4 + 
        r.metrics.precision_at_k.get(5, 0) * 0.3 + 
        r.metrics.recall_at_k.get(5, 0) * 0.3
    ))
    
    # ç”Ÿæˆè©³ç´°çš„æ¨è–¦é…ç½®
    config_parts = []
    config_parts.append(f"chunk_size={best_overall.config['chunk_size']}")
    config_parts.append(f"overlap_ratio={best_overall.config['overlap_ratio']}")
    
    # æ·»åŠ ç­–ç•¥ç‰¹å®šåƒæ•¸
    strategy = best_overall.config.get("strategy", "fixed_size")
    if strategy == "structured_hierarchical":
        chunk_by = best_overall.config.get("chunk_by", "article")
        chunk_by_label = {"article": "æŒ‰æ¢æ–‡åˆ†å‰²", "item": "æŒ‰é …åˆ†å‰²", "section": "æŒ‰ç¯€åˆ†å‰²", "chapter": "æŒ‰ç« åˆ†å‰²"}.get(chunk_by, chunk_by)
        config_parts.append(f"chunk_by={chunk_by}({chunk_by_label})")
    elif strategy == "rcts_hierarchical":
        preserve = best_overall.config.get("preserve_structure", True)
        config_parts.append(f"preserve_structure={preserve}({'ä¿æŒçµæ§‹' if preserve else 'ä¸ä¿æŒçµæ§‹'})")
    elif strategy == "hierarchical":
        level = best_overall.config.get("level_depth", 3)
        min_size = best_overall.config.get("min_chunk_size", 200)
        config_parts.append(f"level_depth={level}")
        config_parts.append(f"min_chunk_size={min_size}")
    elif strategy == "semantic":
        threshold = best_overall.config.get("similarity_threshold", 0.6)
        context = best_overall.config.get("context_window", 100)
        config_parts.append(f"similarity_threshold={threshold}")
        config_parts.append(f"context_window={context}")
    elif strategy == "llm_semantic":
        threshold = best_overall.config.get("semantic_threshold", 0.7)
        context = best_overall.config.get("context_window", 100)
        config_parts.append(f"semantic_threshold={threshold}")
        config_parts.append(f"context_window={context}")
    elif strategy == "sliding_window":
        step = best_overall.config.get("step_size", 250)
        config_parts.append(f"step_size={step}")
    elif strategy == "hybrid":
        switch = best_overall.config.get("switch_threshold", 0.5)
        secondary = best_overall.config.get("secondary_size", 400)
        config_parts.append(f"switch_threshold={switch}")
        config_parts.append(f"secondary_size={secondary}")
    
    comparison["recommendations"] = [
        f"æœ€ä½³é…ç½®ï¼š{', '.join(config_parts)}",
        f"è©²é…ç½®çš„precision omega: {best_overall.metrics.precision_omega:.3f}",
        f"è©²é…ç½®çš„precision@5: {best_overall.metrics.precision_at_k.get(5, 0):.3f}",
        f"è©²é…ç½®çš„recall@5: {best_overall.metrics.recall_at_k.get(5, 0):.3f}",
        f"è©²é…ç½®çš„chunk count: {best_overall.metrics.chunk_count}",
        f"è©²é…ç½®çš„å¹³å‡chunké•·åº¦: {best_overall.metrics.avg_chunk_length:.1f}"
    ]
    
    return comparison


@app.post("/api/generate-questions")
def generate_questions(req: GenerateQuestionsRequest):
    """
    ç”Ÿæˆç¹é«”ä¸­æ–‡æ³•å¾‹è€ƒå¤é¡Œå¾æ³•å¾‹æ–‡æœ¬ä¸­ç”Ÿæˆå•é¡Œ
    """
    doc = store.docs.get(req.doc_id)
    if not doc:
        return JSONResponse(status_code=404, content={"error": "Document not found"})
    
    start_time = time.time()
    
    try:
        # ä½¿ç”¨Geminiç”Ÿæˆå•é¡Œ
        questions = generate_questions_with_gemini(
            doc.text, 
            req.num_questions, 
            req.question_types, 
            req.difficulty_levels
        )
        
        generation_time = time.time() - start_time
        
        # å°‡ç”Ÿæˆçš„å•é¡Œå­˜å„²åˆ°æ–‡æª”è¨˜éŒ„ä¸­
        question_texts = [q.question for q in questions]
        doc.generated_questions = question_texts
        store.docs[req.doc_id] = doc  # æ›´æ–°æ–‡æª”è¨˜éŒ„
        
        # æª¢æŸ¥æ˜¯å¦ç”Ÿæˆäº†å•é¡Œ
        if not questions:
            print("è­¦å‘Šï¼šæ²’æœ‰ç”Ÿæˆä»»ä½•å•é¡Œ")
            return JSONResponse(
                status_code=400,
                content={"error": "ç„¡æ³•å¾æ–‡æª”ä¸­ç”Ÿæˆå•é¡Œï¼Œè«‹æª¢æŸ¥æ–‡æª”å…§å®¹æ˜¯å¦åŒ…å«æ³•å¾‹æ¢æ–‡"}
            )
        
        result = QuestionGenerationResult(
            doc_id=req.doc_id,
            total_questions=len(questions),
            questions=questions,
            generation_time=generation_time,
            timestamp=datetime.now()
        )
        
        response_data = {
            "success": True,
            "result": {
                "doc_id": result.doc_id,
                "total_questions": result.total_questions,
                "generation_time": result.generation_time,
                "timestamp": result.timestamp.isoformat(),
                "questions": [
                    {
                        "question": q.question,
                        "references": q.references,
                        "question_type": q.question_type,
                        "difficulty": q.difficulty,
                        "keywords": q.keywords,
                        "estimated_tokens": q.estimated_tokens
                    }
                    for q in result.questions
                ]
            }
        }
        
        print(f"è¿”å›éŸ¿æ‡‰æ•¸æ“š: success={response_data['success']}, questions_count={len(response_data['result']['questions'])}")
        return response_data
        
    except Exception as e:
        print(f"å•é¡Œç”Ÿæˆç•°å¸¸: {str(e)}")  # æ·»åŠ æ—¥èªŒ
        return JSONResponse(
            status_code=500, 
            content={"error": f"å•é¡Œç”Ÿæˆå¤±æ•—: {str(e)}"}
        )


@app.get("/docs/schema")
def schema():
    # Minimal shape for frontend wiring/testing
    return {
        "upload": {"POST": {"multipart": True}},
        "chunk": {"POST": {"json": {"doc_id": "str", "chunk_size": "int", "overlap": "int"}}},
        "embed": {"POST": {"json": {"doc_ids": "List[str]|None"}}},
        "retrieve": {"POST": {"json": {"query": "str", "k": "int"}}},
        "generate": {"POST": {"json": {"query": "str", "top_k": "int"}}},
        "evaluate/fixed-size": {"POST": {"json": "FixedSizeEvaluationRequest"}},
        "evaluate/status/{task_id}": {"GET": {}},
        "evaluate/results/{task_id}": {"GET": {}},
        "evaluate/comparison/{task_id}": {"GET": {}},
        "generate-questions": {"POST": {"json": "GenerateQuestionsRequest"}},
        # æ–°å¢çš„å¢å¼·ç‰ˆAPIç«¯é»
        "legal-semantic-chunk": {"POST": {"json": "ChunkConfig"}},
        "multi-level-semantic-chunk": {"POST": {"json": "ChunkConfig"}},
        "build-concept-graph": {"POST": {}},
        "concept-graph-retrieve": {"POST": {"json": "RetrieveRequest"}},
        "adaptive-retrieve": {"POST": {"json": "RetrieveRequest"}},
        "strategy-performance": {"GET": {}},
        "concept-graph-info": {"GET": {}},
    }


# ============================================================================
# æ–°å¢çš„å¢å¼·ç‰ˆåŠŸèƒ½ - æ³•å¾‹èªç¾©æª¢ç´¢æ”¹é€²
# ============================================================================

# å°å…¥æ–°çš„æ¨¡çµ„
try:
    from .legal_semantic_chunking import LegalSemanticIntegrityChunking, MultiLevelSemanticChunking
    from .legal_concept_graph import LegalConceptGraph, LegalConceptGraphRetrieval
    from .adaptive_legal_rag import AdaptiveLegalRAG, QueryAnalyzer
    from .legal_reasoning_engine import legal_reasoning_engine
    from .intelligent_legal_concept_extractor import intelligent_extractor
    from .dynamic_concept_learning import dynamic_learning_system
    
    # åˆå§‹åŒ–å¢å¼·ç‰ˆçµ„ä»¶
    legal_semantic_chunker = LegalSemanticIntegrityChunking()
    multi_level_chunker = MultiLevelSemanticChunking()
    concept_graph = LegalConceptGraph()
    concept_graph_retrieval = None
    adaptive_rag = AdaptiveLegalRAG()
    
    print("âœ… å¢å¼·ç‰ˆåŠŸèƒ½æ¨¡çµ„è¼‰å…¥æˆåŠŸ")
    
except ImportError as e:
    print(f"âš ï¸  å¢å¼·ç‰ˆåŠŸèƒ½æ¨¡çµ„è¼‰å…¥å¤±æ•—: {e}")
    print("   è«‹ç¢ºä¿æ‰€æœ‰æ–°å¢æ–‡ä»¶éƒ½å­˜åœ¨")
    legal_semantic_chunker = None
    multi_level_chunker = None
    concept_graph = None
    concept_graph_retrieval = None
    adaptive_rag = None
    legal_reasoning_engine = None
    intelligent_extractor = None
    dynamic_learning_system = None


@app.post("/api/legal-semantic-chunk")
def legal_semantic_chunk(req: ChunkConfig):
    """æ³•å¾‹èªç¾©å®Œæ•´æ€§åˆ†å¡Š"""
    if not legal_semantic_chunker:
        return JSONResponse(status_code=503, content={"error": "æ³•å¾‹èªç¾©åˆ†å¡ŠåŠŸèƒ½æœªå•Ÿç”¨"})
    
    try:
        doc = store.get_doc(req.doc_id)
        if not doc:
            return JSONResponse(status_code=404, content={"error": f"æ–‡æª” {req.doc_id} ä¸å­˜åœ¨"})
        
        print(f"ğŸ” é–‹å§‹æ³•å¾‹èªç¾©å®Œæ•´æ€§åˆ†å¡Šï¼Œæ–‡æª”: {doc.filename}")
        
        # ä½¿ç”¨æ³•å¾‹èªç¾©å®Œæ•´æ€§åˆ†å¡Š
        chunks_with_span = legal_semantic_chunker.chunk(
            doc.text,
            max_chunk_size=req.chunk_size,
            overlap_ratio=req.overlap_ratio,
            preserve_concepts=True
        )
        
        # æå–ç´”æ–‡æœ¬chunks
        chunks = [chunk["content"] for chunk in chunks_with_span]
        
        # æ›´æ–°æ–‡æª”è¨˜éŒ„
        doc.chunks = chunks
        doc.chunk_size = req.chunk_size
        doc.overlap = int(req.chunk_size * req.overlap_ratio)
        doc.structured_chunks = chunks_with_span
        doc.chunking_strategy = "legal_semantic_integrity"
        store.add_doc(doc)
        
        store.reset_embeddings()
        store.save_data()
        
        # è¨ˆç®—çµ±è¨ˆä¿¡æ¯
        chunk_lengths = [len(chunk) for chunk in chunks] if chunks else []
        avg_chunk_length = sum(chunk_lengths) / len(chunk_lengths) if chunk_lengths else 0
        min_length = min(chunk_lengths) if chunk_lengths else 0
        max_length = max(chunk_lengths) if chunk_lengths else 0
        
        if chunk_lengths:
            variance = sum((length - avg_chunk_length) ** 2 for length in chunk_lengths) / len(chunk_lengths)
        else:
            variance = 0
        
        # è¨ˆç®—æ¦‚å¿µå®Œæ•´æ€§çµ±è¨ˆ
        concept_stats = _calculate_concept_statistics(chunks_with_span)
        
        return {
            "doc_id": req.doc_id,
            "chunk_count": len(chunks),
            "avg_chunk_length": avg_chunk_length,
            "min_chunk_length": min_length,
            "max_chunk_length": max_length,
            "length_variance": variance,
            "strategy": "legal_semantic_integrity",
            "config": req.dict(),
            "chunks_with_span": chunks_with_span,
            "concept_statistics": concept_stats
        }
        
    except Exception as e:
        print(f"âŒ æ³•å¾‹èªç¾©åˆ†å¡ŠéŒ¯èª¤: {e}")
        return JSONResponse(status_code=500, content={"error": f"åˆ†å¡ŠéŒ¯èª¤: {str(e)}"})


@app.post("/api/multi-level-semantic-chunk")
def multi_level_semantic_chunk(req: ChunkConfig):
    """å¤šå±¤æ¬¡èªç¾©åˆ†å¡Š"""
    if not multi_level_chunker:
        return JSONResponse(status_code=503, content={"error": "å¤šå±¤æ¬¡èªç¾©åˆ†å¡ŠåŠŸèƒ½æœªå•Ÿç”¨"})
    
    try:
        doc = store.get_doc(req.doc_id)
        if not doc:
            return JSONResponse(status_code=404, content={"error": f"æ–‡æª” {req.doc_id} ä¸å­˜åœ¨"})
        
        print(f"ğŸ” é–‹å§‹å¤šå±¤æ¬¡èªç¾©åˆ†å¡Šï¼Œæ–‡æª”: {doc.filename}")
        
        # ä½¿ç”¨å¤šå±¤æ¬¡èªç¾©åˆ†å¡Š
        multi_level_chunks = multi_level_chunker.chunk(
            doc.text,
            max_chunk_size=req.chunk_size,
            overlap_ratio=req.overlap_ratio
        )
        
        # ä¿å­˜å¤šå±¤æ¬¡åˆ†å¡Šçµæœ
        doc.multi_level_chunks = multi_level_chunks
        doc.chunking_strategy = "multi_level_semantic"
        store.add_doc(doc)
        
        store.reset_embeddings()
        store.save_data()
        
        # è¨ˆç®—å„å±¤æ¬¡çµ±è¨ˆ
        level_statistics = {}
        for level_name, level_chunks in multi_level_chunks.items():
            chunk_lengths = [len(chunk["content"]) for chunk in level_chunks]
            level_statistics[level_name] = {
                "chunk_count": len(level_chunks),
                "avg_length": sum(chunk_lengths) / len(chunk_lengths) if chunk_lengths else 0,
                "min_length": min(chunk_lengths) if chunk_lengths else 0,
                "max_length": max(chunk_lengths) if chunk_lengths else 0
            }
        
        return {
            "doc_id": req.doc_id,
            "strategy": "multi_level_semantic",
            "config": req.dict(),
            "multi_level_chunks": multi_level_chunks,
            "level_statistics": level_statistics
        }
        
    except Exception as e:
        print(f"âŒ å¤šå±¤æ¬¡èªç¾©åˆ†å¡ŠéŒ¯èª¤: {e}")
        return JSONResponse(status_code=500, content={"error": f"åˆ†å¡ŠéŒ¯èª¤: {str(e)}"})


@app.post("/api/build-concept-graph")
def build_concept_graph():
    """æ§‹å»ºæ³•å¾‹æ¦‚å¿µåœ–"""
    if not concept_graph:
        return JSONResponse(status_code=503, content={"error": "æ¦‚å¿µåœ–åŠŸèƒ½æœªå•Ÿç”¨"})
    
    try:
        print("ğŸ”¨ é–‹å§‹æ§‹å»ºæ³•å¾‹æ¦‚å¿µåœ–...")
        
        # ç²å–æ‰€æœ‰æ–‡æª”
        docs = store.list_docs()
        if not docs:
            return JSONResponse(status_code=400, content={"error": "æ²’æœ‰æ–‡æª”å¯ç”¨"})
        
        # æº–å‚™æ–‡æª”æ•¸æ“š
        documents = []
        for doc in docs:
            if doc.chunks:
                for i, chunk in enumerate(doc.chunks):
                    documents.append({
                        'content': chunk,
                        'doc_id': doc.id,
                        'chunk_index': i,
                        'filename': doc.filename
                    })
        
        if not documents:
            return JSONResponse(status_code=400, content={"error": "æ²’æœ‰å¯ç”¨çš„æ–‡æª”å…§å®¹"})
        
        # æ§‹å»ºæ¦‚å¿µåœ–
        concept_graph.build_graph(documents)
        
        # åˆå§‹åŒ–æ¦‚å¿µåœ–æª¢ç´¢
        global concept_graph_retrieval
        concept_graph_retrieval = LegalConceptGraphRetrieval(concept_graph)
        
        # è¨»å†Šåˆ°è‡ªé©æ‡‰RAG
        if adaptive_rag:
            adaptive_rag.register_strategy('concept_graph', concept_graph_retrieval)
        
        # ç²å–æ¦‚å¿µåœ–çµ±è¨ˆ
        graph_stats = {
            'node_count': concept_graph.graph.number_of_nodes(),
            'edge_count': concept_graph.graph.number_of_edges(),
            'concept_count': len(concept_graph.concepts),
            'relation_count': len(concept_graph.relations)
        }
        
        print(f"âœ… æ¦‚å¿µåœ–æ§‹å»ºå®Œæˆ: {graph_stats}")
        
        return {
            "status": "success",
            "message": "æ¦‚å¿µåœ–æ§‹å»ºå®Œæˆ",
            "statistics": graph_stats
        }
        
    except Exception as e:
        print(f"âŒ æ¦‚å¿µåœ–æ§‹å»ºéŒ¯èª¤: {e}")
        return JSONResponse(status_code=500, content={"error": f"æ§‹å»ºéŒ¯èª¤: {str(e)}"})


@app.post("/api/concept-graph-retrieve")
def concept_graph_retrieve(req: RetrieveRequest):
    """æ¦‚å¿µåœ–æª¢ç´¢"""
    if not concept_graph_retrieval:
        return JSONResponse(status_code=400, content={"error": "æ¦‚å¿µåœ–æœªæ§‹å»ºï¼Œè«‹å…ˆèª¿ç”¨ /api/build-concept-graph"})
    
    try:
        print(f"ğŸ” é–‹å§‹æ¦‚å¿µåœ–æª¢ç´¢ï¼ŒæŸ¥è©¢: '{req.query}'")
        
        # åŸ·è¡Œæ¦‚å¿µåœ–æª¢ç´¢
        results = concept_graph_retrieval.retrieve(req.query, req.k)
        
        # è¨ˆç®—æª¢ç´¢æŒ‡æ¨™
        metrics = calculate_retrieval_metrics(req.query, results, req.k)
        
        # æ·»åŠ æ¦‚å¿µåœ–ç‰¹å®šä¿¡æ¯
        metrics["concept_graph_analysis"] = {
            "reasoning_paths_used": len(set(r.get('reasoning_path', []) for r in results)),
            "concept_matches": len([r for r in results if r.get('concept_based', False)]),
            "avg_reasoning_score": sum(r.get('reasoning_score', 0) for r in results) / len(results) if results else 0
        }
        
        metrics["note"] = f"æ¦‚å¿µåœ–æª¢ç´¢: ä½¿ç”¨{metrics['concept_graph_analysis']['reasoning_paths_used']}æ¢æ¨ç†è·¯å¾‘"
        
        return {
            "results": results,
            "metrics": metrics,
            "embedding_provider": "concept_graph",
            "embedding_model": "legal_concept_reasoning"
        }
        
    except Exception as e:
        print(f"âŒ æ¦‚å¿µåœ–æª¢ç´¢éŒ¯èª¤: {e}")
        return JSONResponse(status_code=500, content={"error": f"æª¢ç´¢éŒ¯èª¤: {str(e)}"})


@app.post("/api/adaptive-retrieve")
def adaptive_retrieve(req: RetrieveRequest):
    """è‡ªé©æ‡‰æª¢ç´¢"""
    if not adaptive_rag:
        return JSONResponse(status_code=503, content={"error": "è‡ªé©æ‡‰æª¢ç´¢åŠŸèƒ½æœªå•Ÿç”¨"})
    
    try:
        print(f"ğŸš€ é–‹å§‹è‡ªé©æ‡‰æª¢ç´¢ï¼ŒæŸ¥è©¢: '{req.query}'")
        
        # ç¢ºä¿æª¢ç´¢ç­–ç•¥å·²è¨»å†Š
        if not adaptive_rag.retrieval_strategies:
            _register_default_strategies()
        
        # åŸ·è¡Œè‡ªé©æ‡‰æª¢ç´¢
        results = adaptive_rag.retrieve(req.query, req.k)
        
        # è¨ˆç®—æª¢ç´¢æŒ‡æ¨™
        metrics = calculate_retrieval_metrics(req.query, results, req.k)
        
        # æ·»åŠ è‡ªé©æ‡‰æª¢ç´¢ç‰¹å®šä¿¡æ¯
        if results:
            first_result = results[0]
            contributing_strategies = first_result.get('contributing_strategies', [])
            strategy_count = first_result.get('strategy_count', 0)
            
            metrics["adaptive_analysis"] = {
                "strategies_used": list(set(contributing_strategies)),
                "strategy_count": strategy_count,
                "fusion_performed": first_result.get('metadata', {}).get('adaptive_fusion', False),
                "avg_fused_score": sum(r.get('fused_score', 0) for r in results) / len(results)
            }
            
            metrics["note"] = f"è‡ªé©æ‡‰æª¢ç´¢: èåˆ{strategy_count}å€‹ç­–ç•¥"
        
        return {
            "results": results,
            "metrics": metrics,
            "embedding_provider": "adaptive_rag",
            "embedding_model": "multi_strategy_fusion"
        }
        
    except Exception as e:
        print(f"âŒ è‡ªé©æ‡‰æª¢ç´¢éŒ¯èª¤: {e}")
        return JSONResponse(status_code=500, content={"error": f"æª¢ç´¢éŒ¯èª¤: {str(e)}"})


@app.get("/api/strategy-performance")
def get_strategy_performance():
    """ç²å–ç­–ç•¥æ€§èƒ½çµ±è¨ˆ"""
    if not adaptive_rag:
        return JSONResponse(status_code=503, content={"error": "è‡ªé©æ‡‰æª¢ç´¢åŠŸèƒ½æœªå•Ÿç”¨"})
    
    try:
        performance = adaptive_rag.performance_monitor.get_strategy_performance()
        
        return {
            "strategy_performance": performance,
            "total_retrievals": len(adaptive_rag.performance_monitor.retrieval_history)
        }
        
    except Exception as e:
        print(f"âŒ ç²å–ç­–ç•¥æ€§èƒ½éŒ¯èª¤: {e}")
        return JSONResponse(status_code=500, content={"error": f"ç²å–æ€§èƒ½éŒ¯èª¤: {str(e)}"})


@app.get("/api/concept-graph-info")
def get_concept_graph_info():
    """ç²å–æ¦‚å¿µåœ–ä¿¡æ¯"""
    if not concept_graph:
        return JSONResponse(status_code=503, content={"error": "æ¦‚å¿µåœ–åŠŸèƒ½æœªå•Ÿç”¨"})
    
    try:
        # ç²å–æ¦‚å¿µåˆ—è¡¨
        concepts_info = []
        for concept_id, concept in concept_graph.concepts.items():
            concepts_info.append({
                "concept_id": concept_id,
                "concept_name": concept.concept_name,
                "content": concept.content[:200] + "..." if len(concept.content) > 200 else concept.content,
                "importance_score": concept.importance_score,
                "frequency": concept.frequency
            })
        
        # ç²å–é—œä¿‚åˆ—è¡¨
        relations_info = []
        for relation in concept_graph.relations:
            relations_info.append({
                "source": relation.source_concept,
                "target": relation.target_concept,
                "relation_type": relation.relation_type,
                "confidence": relation.confidence
            })
        
        # ç²å–åœ–çµ±è¨ˆ
        graph_stats = {
            "node_count": concept_graph.graph.number_of_nodes(),
            "edge_count": concept_graph.graph.number_of_edges(),
            "concept_count": len(concept_graph.concepts),
            "relation_count": len(concept_graph.relations)
        }
        
        # ç²å–åº¦ä¸­å¿ƒæ€§æœ€é«˜çš„æ¦‚å¿µï¼ˆå‰10å€‹ï¼‰
        if concept_graph.graph.number_of_nodes() > 0:
            import networkx as nx
            centrality = nx.degree_centrality(concept_graph.graph)
            top_concepts = sorted(centrality.items(), key=lambda x: x[1], reverse=True)[:10]
            top_concepts_info = [
                {
                    "concept_id": concept_id,
                    "concept_name": concept_graph.concepts[concept_id].concept_name,
                    "centrality": centrality_score
                }
                for concept_id, centrality_score in top_concepts
            ]
        else:
            top_concepts_info = []
        
        return {
            "graph_statistics": graph_stats,
            "top_concepts": top_concepts_info,
            "concepts": concepts_info[:20],  # åªè¿”å›å‰20å€‹æ¦‚å¿µ
            "relations": relations_info[:20],  # åªè¿”å›å‰20å€‹é—œä¿‚
            "total_concepts": len(concepts_info),
            "total_relations": len(relations_info)
        }
        
    except Exception as e:
        print(f"âŒ ç²å–æ¦‚å¿µåœ–ä¿¡æ¯éŒ¯èª¤: {e}")
        return JSONResponse(status_code=500, content={"error": f"ç²å–æ¦‚å¿µåœ–ä¿¡æ¯éŒ¯èª¤: {str(e)}"})


@app.post("/api/legal-reasoning")
def analyze_legal_query(request: Dict[str, str]):
    """æ³•å¾‹æ¨ç†åˆ†æ"""
    if not legal_reasoning_engine:
        return JSONResponse(status_code=503, content={"error": "æ³•å¾‹æ¨ç†å¼•æ“æœªå•Ÿç”¨"})
    
    try:
        query = request.get("query", "")
        if not query:
            return JSONResponse(status_code=400, content={"error": "æŸ¥è©¢ä¸èƒ½ç‚ºç©º"})
        
        print(f"ğŸ” é–‹å§‹æ³•å¾‹æ¨ç†åˆ†æï¼ŒæŸ¥è©¢: '{query}'")
        
        # åŸ·è¡Œæ¨ç†åˆ†æ
        analysis = legal_reasoning_engine.analyze_query(query)
        
        return {
            "analysis_result": analysis,
            "status": "success"
        }
        
    except Exception as e:
        print(f"âŒ æ³•å¾‹æ¨ç†åˆ†æéŒ¯èª¤: {e}")
        return JSONResponse(status_code=500, content={"error": f"æ¨ç†åˆ†æéŒ¯èª¤: {str(e)}"})


@app.post("/api/extract-legal-concepts")
def extract_legal_concepts():
    """æ™ºèƒ½æå–æ³•å¾‹æ¦‚å¿µ"""
    if not intelligent_extractor:
        return JSONResponse(status_code=503, content={"error": "æ™ºèƒ½æ¦‚å¿µæå–å™¨æœªå•Ÿç”¨"})
    
    try:
        print("ğŸ” é–‹å§‹æ™ºèƒ½æ³•å¾‹æ¦‚å¿µæå–...")
        
        # ç²å–æ‰€æœ‰æ–‡æª”
        docs = store.list_docs()
        if not docs:
            return JSONResponse(status_code=400, content={"error": "æ²’æœ‰æ–‡æª”å¯ç”¨"})
        
        # æº–å‚™æ–‡æª”æ•¸æ“š
        documents = []
        for doc in docs:
            if hasattr(doc, 'structured_chunks') and doc.structured_chunks:
                documents.append({
                    'filename': doc.filename,
                    'structured_chunks': doc.structured_chunks
                })
        
        if not documents:
            return JSONResponse(status_code=400, content={"error": "æ²’æœ‰çµæ§‹åŒ–åˆ†å¡Šæ•¸æ“š"})
        
        # åŸ·è¡Œæ¦‚å¿µæå–
        extraction_result = intelligent_extractor.extract_concepts_from_documents(documents)
        
        # ä¿å­˜æå–çµæœåˆ°å…¨å±€è®Šé‡
        global extracted_legal_concepts
        extracted_legal_concepts = extraction_result
        
        return {
            "extraction_result": extraction_result,
            "status": "success"
        }
        
    except Exception as e:
        print(f"âŒ æ¦‚å¿µæå–éŒ¯èª¤: {e}")
        return JSONResponse(status_code=500, content={"error": f"æ¦‚å¿µæå–éŒ¯èª¤: {str(e)}"})


@app.post("/api/learn-from-feedback")
def learn_from_feedback(request: Dict[str, Any]):
    """å¾ç”¨æˆ¶åé¥‹ä¸­å­¸ç¿’"""
    if not dynamic_learning_system:
        return JSONResponse(status_code=503, content={"error": "å‹•æ…‹å­¸ç¿’ç³»çµ±æœªå•Ÿç”¨"})
    
    try:
        query = request.get("query", "")
        retrieved_results = request.get("retrieved_results", [])
        user_feedback = request.get("user_feedback", {})
        
        if not query:
            return JSONResponse(status_code=400, content={"error": "æŸ¥è©¢ä¸èƒ½ç‚ºç©º"})
        
        print(f"ğŸ§  é–‹å§‹å¾åé¥‹ä¸­å­¸ç¿’: '{query}'")
        
        # åŸ·è¡Œå­¸ç¿’
        learning_result = dynamic_learning_system.learn_from_query_feedback(
            query, retrieved_results, user_feedback
        )
        
        return {
            "learning_result": learning_result,
            "status": "success"
        }
        
    except Exception as e:
        print(f"âŒ å­¸ç¿’éŒ¯èª¤: {e}")
        return JSONResponse(status_code=500, content={"error": f"å­¸ç¿’éŒ¯èª¤: {str(e)}"})


@app.get("/api/learning-statistics")
def get_learning_statistics():
    """ç²å–å­¸ç¿’çµ±è¨ˆ"""
    if not dynamic_learning_system:
        return JSONResponse(status_code=503, content={"error": "å‹•æ…‹å­¸ç¿’ç³»çµ±æœªå•Ÿç”¨"})
    
    try:
        statistics = dynamic_learning_system.get_learning_statistics()
        
        return {
            "statistics": statistics,
            "status": "success"
        }
        
    except Exception as e:
        print(f"âŒ ç²å–å­¸ç¿’çµ±è¨ˆéŒ¯èª¤: {e}")
        return JSONResponse(status_code=500, content={"error": f"ç²å–å­¸ç¿’çµ±è¨ˆéŒ¯èª¤: {str(e)}"})


@app.post("/api/enhanced-query-expansion")
def enhanced_query_expansion(request: Dict[str, str]):
    """å¢å¼·æŸ¥è©¢æ“´å±•"""
    if not dynamic_learning_system:
        return JSONResponse(status_code=503, content={"error": "å‹•æ…‹å­¸ç¿’ç³»çµ±æœªå•Ÿç”¨"})
    
    try:
        query = request.get("query", "")
        if not query:
            return JSONResponse(status_code=400, content={"error": "æŸ¥è©¢ä¸èƒ½ç‚ºç©º"})
        
        print(f"ğŸ” é–‹å§‹å¢å¼·æŸ¥è©¢æ“´å±•: '{query}'")
        
        # åŸ·è¡Œå¢å¼·æŸ¥è©¢æ“´å±•
        expansion_result = dynamic_learning_system.generate_enhanced_query_expansion(query)
        
        return {
            "expansion_result": expansion_result,
            "status": "success"
        }
        
    except Exception as e:
        print(f"âŒ å¢å¼·æŸ¥è©¢æ“´å±•éŒ¯èª¤: {e}")
        return JSONResponse(status_code=500, content={"error": f"å¢å¼·æŸ¥è©¢æ“´å±•éŒ¯èª¤: {str(e)}"})


def _calculate_concept_statistics(chunks_with_span: List[Dict[str, Any]]) -> Dict[str, Any]:
    """è¨ˆç®—æ¦‚å¿µçµ±è¨ˆä¿¡æ¯"""
    stats = {
        "total_chunks": len(chunks_with_span),
        "concept_chunks": 0,
        "definition_chunks": 0,
        "exception_chunks": 0,
        "condition_chunks": 0,
        "avg_importance_score": 0.0,
        "concept_density": 0.0
    }
    
    total_importance = 0.0
    total_concept_count = 0
    
    for chunk in chunks_with_span:
        metadata = chunk.get("metadata", {})
        semantic_features = metadata.get("semantic_features", {})
        
        concept_count = semantic_features.get("concept_count", 0)
        importance_score = semantic_features.get("importance_score", 0.0)
        
        if concept_count > 0:
            stats["concept_chunks"] += 1
            total_importance += importance_score
            total_concept_count += concept_count
        
        if semantic_features.get("has_definition", False):
            stats["definition_chunks"] += 1
        
        if semantic_features.get("has_exception", False):
            stats["exception_chunks"] += 1
        
        if semantic_features.get("has_condition", False):
            stats["condition_chunks"] += 1
    
    if stats["concept_chunks"] > 0:
        stats["avg_importance_score"] = total_importance / stats["concept_chunks"]
    
    if len(chunks_with_span) > 0:
        stats["concept_density"] = total_concept_count / len(chunks_with_span)
    
    return stats


def _register_default_strategies():
    """è¨»å†Šé»˜èªæª¢ç´¢ç­–ç•¥"""
    if not adaptive_rag:
        return
        
    # è¨»å†Šå‘é‡æª¢ç´¢
    adaptive_rag.register_strategy('vector_search', {
        'retrieve': lambda query, **kwargs: retrieve_original(query, kwargs.get('k', 5))
    })
    
    # è¨»å†ŠHybridRAG
    adaptive_rag.register_strategy('hybrid_rag', {
        'retrieve': lambda query, **kwargs: hybrid_retrieve_original(query, kwargs.get('k', 5))
    })
    
    # è¨»å†Šå¤šå±¤æ¬¡æª¢ç´¢
    adaptive_rag.register_strategy('hierarchical', {
        'retrieve': lambda query, **kwargs: []  # æš«æ™‚è¿”å›ç©ºåˆ—è¡¨
    })


def retrieve_original(query: str, k: int):
    """åŸå§‹å‘é‡æª¢ç´¢"""
    # é€™è£¡èª¿ç”¨åŸæœ‰çš„æª¢ç´¢é‚è¼¯
    pass


async def hybrid_retrieve_original(query: str, k: int):
    """åŸå§‹HybridRAGæª¢ç´¢"""
    # æš«æ™‚è¿”å›ç©ºåˆ—è¡¨ï¼ŒHybridRAGåŠŸèƒ½å¾…å¯¦ç¾
    return []


@app.get("/api/embedding-databases")
async def list_embedding_databases():
    """åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„embeddingè³‡æ–™åº«"""
    databases = []
    print(f"ğŸ” APIèª¿ç”¨é–‹å§‹: has_multi_level_embeddings={store.has_multi_level_embeddings()}")
    if store.has_multi_level_embeddings():
        print(f"ğŸ” å¯ç”¨å±¤æ¬¡: {store.get_available_levels()}")
    
    # å¼·åˆ¶æ¸¬è©¦å¤šå±¤æ¬¡embedding
    print(f"ğŸ” å¼·åˆ¶æª¢æŸ¥: multi_level_embeddings keys = {list(store.multi_level_embeddings.keys())}")
    
    # æ‰‹å‹•æª¢æŸ¥ä¸¦å‰µå»ºæ¸¬è©¦æ•¸æ“š
    if len(store.multi_level_embeddings) == 0:
        print("ğŸ” æ²’æœ‰å¤šå±¤æ¬¡embeddingï¼Œè·³éåˆä½µé‚è¼¯")
    else:
        print(f"ğŸ” æ‰¾åˆ°å¤šå±¤æ¬¡embedding: {list(store.multi_level_embeddings.keys())}")
    
    # ç‚ºäº†æ¼”ç¤ºç›®çš„ï¼Œå¦‚æœæ²’æœ‰embeddingè³‡æ–™ä¸”æ²’æœ‰è¢«æ¨™è¨˜ç‚ºå·²åˆªé™¤ï¼Œå‰‡å‰µå»ºä¸€äº›æ¸¬è©¦embeddingè³‡æ–™
    # ç§»é™¤è‡ªå‹•å‰µå»ºæ¼”ç¤ºembeddingè³‡æ–™åº«çš„é‚è¼¯
    # ç¾åœ¨åªæœ‰åœ¨ç”¨æˆ¶å¯¦éš›å‰µå»ºembeddingæ™‚æ‰æœƒé¡¯ç¤ºè³‡æ–™åº«
    
    # æª¢æŸ¥æ¨™æº–embedding
    if store.embeddings is not None and store.chunks_flat:
        # ç²å–ç›¸é—œæ–‡æª”ä¿¡æ¯
        doc_info = {}
        for doc_id in set(store.chunk_doc_ids):
            doc = store.get_doc(doc_id)
            if doc:
                doc_info[doc_id] = {
                    "filename": doc.filename,
                    "json_data": doc.json_data is not None
                }
        
        databases.append({
            "id": "standard_embedding",
            "type": "standard",
            "name": "æ¨™æº–Embedding",
            "provider": "gemini",  # å¾é…ç½®æ¨æ–·
            "model": "gemini-embedding-001",
            "num_vectors": len(store.embeddings),
            "dimension": len(store.embeddings[0]) if store.embeddings else 0,
            "chunking_strategy": "basic",  # éœ€è¦å¾é…ç½®æ¨æ–·
            "documents": list(doc_info.values()),
            "created_at": datetime.now().isoformat()
        })
    
    # æª¢æŸ¥å¤šå±¤æ¬¡embedding - åˆä½µç‚ºä¸€å€‹è³‡æ–™åº«é¡¯ç¤º
    print(f"ğŸ” æª¢æŸ¥å¤šå±¤æ¬¡embedding: has_multi_level_embeddings={store.has_multi_level_embeddings()}")
    if store.has_multi_level_embeddings():
        available_levels = store.get_available_levels()
        
        # æ”¶é›†æ‰€æœ‰å±¤æ¬¡çš„ä¿¡æ¯
        all_doc_info = {}
        total_vectors = 0
        providers = set()
        models = set()
        dimensions = set()
        levels_info = []
        
        for level in available_levels:
            level_data = store.get_multi_level_embeddings(level)
            if level_data:
                # æ”¶é›†æ–‡æª”ä¿¡æ¯
                for doc_id in set(level_data.get('doc_ids', [])):
                    doc = store.get_doc(doc_id)
                    if doc:
                        all_doc_info[doc_id] = {
                            "filename": doc.filename,
                            "json_data": doc.json_data is not None
                        }
                
                # çµ±è¨ˆä¿¡æ¯
                level_vectors = len(level_data.get('embeddings', []))
                total_vectors += level_vectors
                providers.add(level_data.get('metadata', {}).get('provider', 'unknown'))
                models.add(level_data.get('metadata', {}).get('model', 'unknown'))
                dimensions.add(level_data.get('metadata', {}).get('dimension', 0))
                
                levels_info.append({
                    "level": level,
                    "description": get_level_description(level),
                    "num_vectors": level_vectors
                })
        
        if total_vectors > 0:
            # æ ¹æ“šå±¤æ¬¡çµ„åˆç¢ºå®šå¯¦é©—çµ„
            level_names = [level["level"] for level in levels_info]
            group_name = "æœªçŸ¥å¯¦é©—çµ„"
            
            if level_names == ["basic_unit"]:
                group_name = "Açµ„ï¼šåƒ…æ¢æ–‡å±¤ (Baseline)"
            elif set(level_names) == {"basic_unit_hierarchy", "basic_unit"}:
                group_name = "Bçµ„ï¼šæ¢æ–‡+ç« ç¯€çµæ§‹"
            elif set(level_names) == {"basic_unit", "basic_unit_component", "enumeration"}:
                group_name = "Cçµ„ï¼šæ¢æ–‡+ç´°ç¯€å±¤æ¬¡"
            elif len(level_names) == 6:
                group_name = "Dçµ„ï¼šå®Œæ•´å¤šå±¤æ¬¡ML-RAG"
            
            databases.append({
                "id": "multi_level_combined",
                "type": "multi_level",
                "name": f"å¯¦é©—çµ„Embedding - {group_name}",
                "provider": list(providers)[0] if providers else "unknown",
                "model": list(models)[0] if models else "unknown",
                "num_vectors": total_vectors,
                "dimension": list(dimensions)[0] if dimensions else 0,
                "chunking_strategy": "hierarchical",
                "documents": list(all_doc_info.values()),
                "levels": levels_info,
                "experimental_group": group_name,
                "created_at": datetime.now().isoformat()
            })
    
    return databases


@app.post("/api/embedding-databases/{database_id}/activate")
async def activate_embedding_database(database_id: str):
    """æ¿€æ´»æŒ‡å®šçš„embeddingè³‡æ–™åº«ï¼ŒåŠ è¼‰å°æ‡‰çš„FAISSå’ŒBM25ç´¢å¼•"""
    try:
        print(f"ğŸ”„ æ¿€æ´»embeddingè³‡æ–™åº«: {database_id}")
        
        if database_id == "standard_embedding":
            # æª¢æŸ¥æ¨™æº–embeddingæ˜¯å¦å­˜åœ¨
            if store.embeddings is None or not store.chunks_flat:
                return JSONResponse(
                    status_code=404,
                    content={"error": "æ¨™æº–embeddingè³‡æ–™ä¸å­˜åœ¨ï¼Œè«‹å…ˆåŸ·è¡Œembedding"}
                )
            
            # é‡æ–°åŠ è¼‰FAISSå’ŒBM25ç´¢å¼•
            print("ğŸ“Š é‡æ–°åŠ è¼‰FAISSå’ŒBM25ç´¢å¼•...")
            faiss_store.load_data()
            bm25_index.load_data()
            
            # é©—è­‰ç´¢å¼•æ˜¯å¦æˆåŠŸåŠ è¼‰
            faiss_loaded = faiss_store.has_vectors()
            bm25_loaded = bm25_index.has_index()
            print(f"ğŸ“Š ç´¢å¼•åŠ è¼‰ç‹€æ…‹: FAISS={faiss_loaded}, BM25={bm25_loaded}")
            
            # å¦‚æœä»»ä¸€ç´¢å¼•æœªåŠ è¼‰ï¼Œå˜—è©¦å¾storeé‡å»º
            if not faiss_loaded or not bm25_loaded:
                print("âš ï¸ éƒ¨åˆ†æˆ–å…¨éƒ¨ç´¢å¼•æœªæ‰¾åˆ°ï¼Œå˜—è©¦å¾storeé‡å»ºç´¢å¼•...")
                vectors = store.embeddings
                chunks = store.chunks_flat
                
                if not vectors or not chunks:
                    return JSONResponse(
                        status_code=404,
                        content={
                            "error": "ç„¡æ³•é‡å»ºç´¢å¼•ï¼šstoreä¸­æ²’æœ‰embeddingæ•¸æ“š",
                            "faiss_available": faiss_loaded,
                            "bm25_available": bm25_loaded
                        }
                    )
                
                chunk_ids = [f"{doc_id}_{i}" for i, doc_id in enumerate(store.chunk_doc_ids)]
                
                # é‡å»ºFAISSç´¢å¼•ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
                if not faiss_loaded:
                    print("ğŸ”„ é‡å»ºFAISSç´¢å¼•...")
                    dimension = len(vectors[0]) if vectors else EMBEDDING_DIMENSION
                    faiss_store.create_index(dimension, "flat")
                    faiss_store.add_vectors(vectors, chunk_ids, store.chunk_doc_ids, chunks)
                    print(f"âœ… FAISSç´¢å¼•å·²é‡å»º: {len(vectors)} å€‹å‘é‡")
                
                # é‡å»ºBM25ç´¢å¼•ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
                if not bm25_loaded:
                    print("ğŸ”„ é‡å»ºBM25ç´¢å¼•...")
                    bm25_index.build_index(chunks, chunk_ids, store.chunk_doc_ids)
                    print(f"âœ… BM25ç´¢å¼•å·²é‡å»º: {len(chunks)} å€‹æ–‡æª”")
                
                # å¦‚æœæœ‰enhanced metadataï¼Œä¹Ÿéœ€è¦æ¢å¾©
                if hasattr(store, 'enhanced_metadata') and store.enhanced_metadata:
                    for chunk_id, metadata in store.enhanced_metadata.items():
                        faiss_store.set_enhanced_metadata(chunk_id, metadata)
                    print(f"âœ… å·²æ¢å¾© {len(store.enhanced_metadata)} å€‹enhanced metadata")
                
                # ä¿å­˜ç´¢å¼•
                faiss_store.save_data()
                bm25_index.save_data()
                print("âœ… ç´¢å¼•å·²ä¿å­˜åˆ°ç£ç›¤")
                
                # å†æ¬¡é©—è­‰
                faiss_loaded = faiss_store.has_vectors()
                bm25_loaded = bm25_index.has_index()
                print(f"ğŸ“Š é‡å»ºå¾Œç´¢å¼•ç‹€æ…‹: FAISS={faiss_loaded}, BM25={bm25_loaded}")
            
            print(f"âœ… æ¨™æº–embeddingè³‡æ–™åº«å·²æ¿€æ´»")
            return {
                "message": "æ¨™æº–embeddingè³‡æ–™åº«å·²æ¿€æ´»",
                "database_id": database_id,
                "faiss_available": faiss_store.has_vectors(),
                "bm25_available": bm25_index.has_index(),
                "num_vectors": len(store.embeddings) if store.embeddings else 0,
                "success": True
            }
            
        elif database_id == "multi_level_combined":
            # æª¢æŸ¥å¤šå±¤æ¬¡embeddingæ˜¯å¦å­˜åœ¨
            if not store.has_multi_level_embeddings():
                return JSONResponse(
                    status_code=404,
                    content={"error": "å¤šå±¤æ¬¡embeddingè³‡æ–™ä¸å­˜åœ¨ï¼Œè«‹å…ˆåŸ·è¡Œmulti-level-embed"}
                )
            
            # é‡æ–°åŠ è¼‰FAISSå’ŒBM25ç´¢å¼•
            print("ğŸ“Š é‡æ–°åŠ è¼‰å¤šå±¤æ¬¡FAISSå’ŒBM25ç´¢å¼•...")
            faiss_store.load_data()
            bm25_index.load_data()
            
            # é©—è­‰å¤šå±¤æ¬¡ç´¢å¼•æ˜¯å¦æˆåŠŸåŠ è¼‰
            available_levels = faiss_store.get_available_levels()
            if not available_levels:
                # å¦‚æœåŠ è¼‰å¤±æ•—ï¼Œå˜—è©¦å¾storeé‡å»ºç´¢å¼•
                print("âš ï¸ å¤šå±¤æ¬¡ç´¢å¼•æœªæ‰¾åˆ°ï¼Œå˜—è©¦å¾storeé‡å»ºç´¢å¼•...")
                available_levels = store.get_available_levels()
                
                for level_name in available_levels:
                    level_data = store.get_multi_level_embeddings(level_name)
                    if level_data:
                        vectors = level_data.get('embeddings', [])
                        chunks = level_data.get('chunks', [])
                        doc_ids = level_data.get('doc_ids', [])
                        chunk_ids = [f"{doc_id}_{i}" for i, doc_id in enumerate(doc_ids)]
                        
                        if vectors and chunks:
                            faiss_store.add_multi_level_vectors(level_name, vectors, chunk_ids, doc_ids, chunks)
                            bm25_index.build_multi_level_index(level_name, chunks, chunk_ids, doc_ids)
                
                # ä¿å­˜ç´¢å¼•
                faiss_store.save_data()
                bm25_index.save_data()
                print("âœ… å·²å¾storeé‡å»ºå¤šå±¤æ¬¡ç´¢å¼•ä¸¦ä¿å­˜")
            
            print(f"âœ… å¤šå±¤æ¬¡embeddingè³‡æ–™åº«å·²æ¿€æ´»ï¼Œå¯ç”¨å±¤æ¬¡: {available_levels}")
            return {
                "message": "å¤šå±¤æ¬¡embeddingè³‡æ–™åº«å·²æ¿€æ´»",
                "database_id": database_id,
                "faiss_available": faiss_store.has_multi_level_vectors(),
                "bm25_available": bm25_index.has_multi_level_index(),
                "available_levels": available_levels,
                "success": True
            }
        else:
            return JSONResponse(
                status_code=404,
                content={"error": f"æœªçŸ¥çš„embeddingè³‡æ–™åº«ID: {database_id}"}
            )
            
    except Exception as e:
        print(f"âŒ æ¿€æ´»embeddingè³‡æ–™åº«å¤±æ•—: {e}")
        import traceback
        traceback.print_exc()
        return JSONResponse(
            status_code=500,
            content={"error": f"æ¿€æ´»embeddingè³‡æ–™åº«å¤±æ•—: {str(e)}"}
        )


@app.delete("/api/embedding-databases/{database_id}")
async def delete_embedding_database(database_id: str):
    """åˆªé™¤æŒ‡å®šçš„embeddingè³‡æ–™åº«"""
    try:
        if database_id == "standard_embedding":
            # åˆªé™¤æ¨™æº–embedding
            if store.embeddings is not None:
                store.reset_embeddings()
                store.save_data()
                # æ¨™è¨˜æ¼”ç¤ºè³‡æ–™å·²è¢«åˆªé™¤ï¼Œé˜²æ­¢é‡æ–°å‰µå»º
                store.demo_data_deleted = True
                print(f"âœ… å·²åˆªé™¤æ¨™æº–embeddingè³‡æ–™åº«")
                return {"message": "æ¨™æº–embeddingè³‡æ–™åº«å·²åˆªé™¤", "success": True}
            else:
                return JSONResponse(
                    status_code=404, 
                    content={"error": "æ¨™æº–embeddingè³‡æ–™åº«ä¸å­˜åœ¨"}
                )
        elif database_id == "multi_level_combined":
            # åˆªé™¤æ•´å€‹å¤šå±¤æ¬¡embeddingè³‡æ–™åº«
            if store.has_multi_level_embeddings() or faiss_store.has_multi_level_vectors() or bm25_index.has_multi_level_index():
                # æ¸…é™¤æ‰€æœ‰å¤šå±¤æ¬¡embeddingæ•¸æ“š
                store.multi_level_embeddings = {}
                store.multi_level_chunk_doc_ids = {}
                store.multi_level_chunks_flat = {}
                store.multi_level_metadata = {}
                store.save_data()
                
                # æ¸…é™¤FAISSå’ŒBM25å¤šå±¤æ¬¡ç´¢å¼•
                faiss_store.reset_vectors()
                bm25_index.reset_index()
                
                # åˆªé™¤ç£ç›¤ä¸Šçš„ç´¢å¼•æ–‡ä»¶
                import os
                data_dir = "data"
                for level_name in ["document", "document_component", "basic_unit_hierarchy", "basic_unit", "basic_unit_component", "enumeration"]:
                    faiss_file = os.path.join(data_dir, f"faiss_index_{level_name}.bin")
                    bm25_file = os.path.join(data_dir, f"bm25_index_{level_name}.pkl")
                    if os.path.exists(faiss_file):
                        os.remove(faiss_file)
                        print(f"ğŸ—‘ï¸ åˆªé™¤FAISSæ–‡ä»¶: {faiss_file}")
                    if os.path.exists(bm25_file):
                        os.remove(bm25_file)
                        print(f"ğŸ—‘ï¸ åˆªé™¤BM25æ–‡ä»¶: {bm25_file}")
                
                # é‡æ–°ä¿å­˜ç©ºçš„metadata
                faiss_store.save_data()
                bm25_index.save_data()
                
                print(f"âœ… å·²åˆªé™¤æ•´å€‹å¤šå±¤æ¬¡embeddingè³‡æ–™åº«ï¼ˆåŒ…æ‹¬ç£ç›¤æ–‡ä»¶ï¼‰")
                return {"message": "å¤šå±¤æ¬¡embeddingè³‡æ–™åº«å·²åˆªé™¤ï¼ˆåŒ…æ‹¬ç£ç›¤æ–‡ä»¶ï¼‰", "success": True}
            else:
                return JSONResponse(
                    status_code=404, 
                    content={"error": "å¤šå±¤æ¬¡embeddingè³‡æ–™åº«ä¸å­˜åœ¨"}
                )
        elif database_id.startswith("multi_level_"):
            # åˆªé™¤ç‰¹å®šå±¤æ¬¡çš„å¤šå±¤æ¬¡embeddingï¼ˆä¿ç•™å‘å¾Œå…¼å®¹æ€§ï¼‰
            level_name = database_id.replace("multi_level_", "")
            if store.has_multi_level_embeddings():
                available_levels = store.get_available_levels()
                if level_name in available_levels:
                    # åˆªé™¤ç‰¹å®šå±¤æ¬¡
                    if level_name in store.multi_level_embeddings:
                        del store.multi_level_embeddings[level_name]
                    if level_name in store.multi_level_chunk_doc_ids:
                        del store.multi_level_chunk_doc_ids[level_name]
                    if level_name in store.multi_level_chunks_flat:
                        del store.multi_level_chunks_flat[level_name]
                    if level_name in store.multi_level_metadata:
                        del store.multi_level_metadata[level_name]
                    
                    print(f"âœ… å·²åˆªé™¤å¤šå±¤æ¬¡embeddingå±¤æ¬¡: {level_name}")
                    return {"message": f"å¤šå±¤æ¬¡embeddingå±¤æ¬¡ '{level_name}' å·²åˆªé™¤", "success": True}
                else:
                    return JSONResponse(
                        status_code=404, 
                        content={"error": f"å¤šå±¤æ¬¡embeddingå±¤æ¬¡ '{level_name}' ä¸å­˜åœ¨"}
                    )
            else:
                return JSONResponse(
                    status_code=404, 
                    content={"error": "å¤šå±¤æ¬¡embeddingè³‡æ–™åº«ä¸å­˜åœ¨"}
                )
        else:
            return JSONResponse(
                status_code=400, 
                content={"error": f"ä¸æ”¯æŒçš„embeddingè³‡æ–™åº«é¡å‹: {database_id}"}
            )
    except Exception as e:
        print(f"âŒ åˆªé™¤embeddingè³‡æ–™åº«å¤±æ•—: {e}")
        return JSONResponse(
            status_code=500, 
            content={"error": f"åˆªé™¤embeddingè³‡æ–™åº«å¤±æ•—: {str(e)}"}
        )


@app.post("/api/reset-demo-data")
async def reset_demo_data():
    """é‡ç½®æ¼”ç¤ºè³‡æ–™ç‹€æ…‹ï¼Œç”¨æ–¼æ¸¬è©¦ç›®çš„"""
    try:
        store.demo_data_deleted = False
        store.reset_embeddings()
        store.save_data()
        print("âœ… å·²é‡ç½®æ¼”ç¤ºè³‡æ–™ç‹€æ…‹")
        return {"message": "æ¼”ç¤ºè³‡æ–™ç‹€æ…‹å·²é‡ç½®", "success": True}
    except Exception as e:
        print(f"âŒ é‡ç½®æ¼”ç¤ºè³‡æ–™å¤±æ•—: {e}")
        return JSONResponse(
            status_code=500, 
            content={"error": f"é‡ç½®æ¼”ç¤ºè³‡æ–™å¤±æ•—: {str(e)}"}
        )


@app.post("/api/clear-all-data")
async def clear_all_data():
    """æ¸…é™¤æ‰€æœ‰æ•¸æ“šï¼ˆç”¨æ–¼æ¸¬è©¦ï¼‰"""
    try:
        store.clear_all_data()
        print("ğŸ—‘ï¸ æ‰€æœ‰æ•¸æ“šå·²æ¸…é™¤")
        return {"message": "All data cleared successfully", "success": True}
    except Exception as e:
        print(f"âŒ æ¸…é™¤æ•¸æ“šå¤±æ•—: {e}")
        return JSONResponse(
            status_code=500, 
            content={"error": f"æ¸…é™¤æ•¸æ“šå¤±æ•—: {str(e)}"}
        )


@app.get("/api/debug-store")
async def debug_store():
    """èª¿è©¦storeç‹€æ…‹"""
    return {
        "has_standard_embeddings": store.embeddings is not None,
        "has_multi_level_embeddings": store.has_multi_level_embeddings(),
        "available_levels": store.get_available_levels(),
        "multi_level_embeddings_keys": list(store.multi_level_embeddings.keys()),
        "multi_level_embeddings_details": {
            level: {
                "num_vectors": len(store.multi_level_embeddings.get(level, {}).get('embeddings', [])),
                "metadata": store.multi_level_embeddings.get(level, {}).get('metadata', {})
            }
            for level in store.get_available_levels()
        },
        "demo_data_deleted": getattr(store, 'demo_data_deleted', False),
        "docs_count": len(store.docs)
    }


@app.get("/api/docs")
async def list_docs():
    """åˆ—å‡ºæ‰€æœ‰æ–‡æª”"""
    docs = store.list_docs()
    return [{"id": d.id, "filename": d.filename, "num_chars": len(d.text)} for d in docs]


# å®šç¾©ç²’åº¦çµ„åˆé…ç½® - å°æ‡‰è«–æ–‡çš„å…­å€‹å±¤æ¬¡
GRANULARITY_COMBINATIONS = {
    # Açµ„ï¼šåƒ…å±¤æ¬¡ 4 (åŸºæœ¬å–®å…ƒå±¤ - æ¢æ–‡)
    "group_a": {
        "name": "Açµ„ï¼šåƒ…æ¢æ–‡å±¤ (Baseline)",
        "description": "å‚³çµ±å¹³é¢æ³•çš„è¡¨ç¾ - åƒ…ä½¿ç”¨åŸºæœ¬å–®å…ƒå±¤ï¼ˆæ¢æ–‡ï¼‰",
        "levels": ["basic_unit"],
        "research_purpose": "åŸºç·šå°ç…§çµ„ï¼Œè©•ä¼°å‚³çµ±å¹³é¢æª¢ç´¢çš„è¡¨ç¾"
    },
    
    # Bçµ„ï¼šå±¤æ¬¡ 3 + 4 (åŸºæœ¬å–®å…ƒå±¤ç´šå±¤ + åŸºæœ¬å–®å…ƒå±¤)
    "group_b": {
        "name": "Bçµ„ï¼šæ¢æ–‡+ç« ç¯€çµæ§‹",
        "description": "åŸºæœ¬å–®å…ƒå±¤ + åŸºæœ¬å–®å…ƒå±¤ç´šå±¤ï¼ˆç« ã€ç¯€ã€ç·¨ï¼‰",
        "levels": ["basic_unit_hierarchy", "basic_unit"],
        "research_purpose": "è©•ä¼°çµæ§‹åˆ†çµ„ï¼ˆå¦‚ï¼šã€Šå•†æ¨™æ³•ã€‹çš„ã€Œç« ã€ç¯€ã€ï¼‰çš„åµŒå…¥æ˜¯å¦èƒ½æ›´å¥½åœ°æ•æ‰å»£æ³›ä¸»é¡Œ(aboutness)"
    },
    
    # Cçµ„ï¼šå±¤æ¬¡ 4 + 5 + 6 (åŸºæœ¬å–®å…ƒå±¤ + åŸºæœ¬å–®å…ƒçµ„æˆå±¤ + åˆ—èˆ‰å±¤)
    "group_c": {
        "name": "Cçµ„ï¼šæ¢æ–‡+ç´°ç¯€å±¤æ¬¡",
        "description": "åŸºæœ¬å–®å…ƒå±¤ + åŸºæœ¬å–®å…ƒçµ„æˆå±¤ï¼ˆé …ï¼‰+ åˆ—èˆ‰å±¤ï¼ˆæ¬¾ã€ç›®ï¼‰",
        "levels": ["basic_unit", "basic_unit_component", "enumeration"],
        "research_purpose": "è©•ä¼°ç´°ç¯€åŒ–å±¤æ¬¡å°æ–¼è™•ç†è‡ºç£æ³•å¾‹ä¸­å¸¸è¦‹çš„åˆ—èˆ‰å¼è¦å®šï¼ˆå¦‚ï¼šã€Šå•†æ¨™æ³•ã€‹ç¬¬30æ¢çš„15æ¬¾ä¸å¾—è¨»å†Šæƒ…å½¢ï¼‰æ‰€å¸¶ä¾†çš„ç²¾ç¢ºåº¦å¢ç›Š"
    },
    
    # Dçµ„ï¼šå±¤æ¬¡ 1 + 2 + 3 + 4 + 5 + 6 (å®Œæ•´å¤šå±¤æ¬¡)
    "group_d": {
        "name": "Dçµ„ï¼šå®Œæ•´å¤šå±¤æ¬¡ML-RAG",
        "description": "åŒ…å«æ‰€æœ‰å…­å€‹ç²’åº¦å±¤æ¬¡",
        "levels": ["document", "document_component", "basic_unit_hierarchy", 
                   "basic_unit", "basic_unit_component", "enumeration"],
        "research_purpose": "ä½œç‚ºæœ€ä½³æ•ˆèƒ½çš„å°æ¯”çµ„ï¼Œè©•ä¼°å®Œæ•´å¤šå±¤æ¬¡æ–¹æ³•çš„ç¶œåˆè¡¨ç¾"
    },
    
    # é¡å¤–çš„å°æ¯”çµ„åˆï¼Œç”¨æ–¼æ›´ç´°ç·»çš„åˆ†æ
    "document_only": {
        "name": "åƒ…æ–‡ä»¶å±¤",
        "description": "åƒ…ä½¿ç”¨æ–‡ä»¶å±¤ç´šembedding",
        "levels": ["document"],
        "research_purpose": "è©•ä¼°æœ€é«˜å±¤ç´šçµæ§‹çš„ç¨ç«‹è²¢ç»"
    },
    
    "structure_only": {
        "name": "åƒ…çµæ§‹å±¤",
        "description": "åƒ…ä½¿ç”¨çµæ§‹å±¤æ¬¡ï¼ˆæ–‡ä»¶ã€æ–‡ä»¶çµ„ä»¶ã€åŸºæœ¬å–®å…ƒå±¤ç´šï¼‰",
        "levels": ["document", "document_component", "basic_unit_hierarchy"],
        "research_purpose": "è©•ä¼°ç´”çµæ§‹å±¤æ¬¡çš„è²¢ç»ï¼Œä¸åŒ…å«å…·é«”å…§å®¹"
    },
    
    "content_only": {
        "name": "åƒ…å…§å®¹å±¤",
        "description": "åƒ…ä½¿ç”¨å…§å®¹å±¤æ¬¡ï¼ˆæ¢æ–‡ã€é …ã€æ¬¾ç›®ï¼‰",
        "levels": ["basic_unit", "basic_unit_component", "enumeration"],
        "research_purpose": "è©•ä¼°ç´”å…§å®¹å±¤æ¬¡çš„è²¢ç»ï¼Œä¸åŒ…å«é«˜å±¤çµæ§‹"
    }
}


@app.get("/api/granularity-combinations")
def get_granularity_combinations():
    """ç²å–å¯ç”¨çš„ç²’åº¦çµ„åˆé…ç½®"""
    return {"combinations": GRANULARITY_COMBINATIONS}


@app.post("/api/test-experimental-groups")
async def test_experimental_groups(req: Dict[str, Any]):
    """æ¸¬è©¦å¯¦é©—çµ„å±¤æ¬¡é¸æ“‡é‚è¼¯"""
    experimental_groups = req.get("experimental_groups", [])
    
    if not experimental_groups:
        return {"message": "è«‹æä¾›experimental_groupsåƒæ•¸"}
    
    # æ¨¡æ“¬å¯¦é©—çµ„é¸æ“‡é‚è¼¯
    six_levels = [
        'document', 'document_component', 'basic_unit_hierarchy', 
        'basic_unit', 'basic_unit_component', 'enumeration'
    ]
    
    print(f"ğŸ§ª æ¸¬è©¦å¯¦é©—çµ„é¸æ“‡: {experimental_groups}")
    
    # æ”¶é›†æ‰€æœ‰éœ€è¦çš„å±¤æ¬¡
    required_levels = set()
    group_details = {}
    
    for group_key in experimental_groups:
        if group_key in GRANULARITY_COMBINATIONS:
            group_info = GRANULARITY_COMBINATIONS[group_key]
            group_levels = group_info["levels"]
            required_levels.update(group_levels)
            
            group_details[group_key] = {
                "name": group_info["name"],
                "description": group_info["description"],
                "levels": group_levels,
                "research_purpose": group_info["research_purpose"]
            }
        else:
            group_details[group_key] = {"error": "æœªçŸ¥çš„å¯¦é©—çµ„"}
    
    # ç¢ºå®šè¦è™•ç†çš„å±¤æ¬¡
    selected_levels = [level for level in six_levels if level in required_levels]
    skipped_levels = [level for level in six_levels if level not in required_levels]
    
    return {
        "experimental_groups": experimental_groups,
        "group_details": group_details,
        "all_levels": six_levels,
        "selected_levels": selected_levels,
        "skipped_levels": skipped_levels,
        "total_selected": len(selected_levels),
        "total_skipped": len(skipped_levels)
    }


@app.post("/api/granularity-comparison-retrieve")
async def granularity_comparison_retrieve(req: Dict[str, Any]):
    """
    ä½¿ç”¨æŒ‡å®šç²’åº¦çµ„åˆé€²è¡Œæª¢ç´¢
    req = {query, k, granularity_combination}
    """
    query = req.get("query")
    k = req.get("k", 10)
    combination_key = req.get("granularity_combination", "full_ml")
    
    # ç²å–å±¤æ¬¡çµ„åˆé…ç½®
    combination = GRANULARITY_COMBINATIONS.get(combination_key)
    if not combination:
        return JSONResponse(
            status_code=400,
            content={"error": f"Unknown combination: {combination_key}"}
        )
    
    selected_levels = combination["levels"]
    
    # ç”ŸæˆæŸ¥è©¢å‘é‡
    if USE_GEMINI_EMBEDDING and GOOGLE_API_KEY:
        query_vector = (await embed_gemini([query]))[0]
    elif USE_BGE_M3_EMBEDDING:
        query_vector = embed_bge_m3([query])[0]
    else:
        return JSONResponse(status_code=400, content={"error": "No embedding method available"})
    
    # å¾é¸å®šçš„å±¤æ¬¡ä¸­æª¢ç´¢ä¸¦èåˆçµæœ
    all_results = []
    level_contributions = {}
    
    for level_name in selected_levels:
        level_data = store.get_multi_level_embeddings(level_name)
        if not level_data:
            continue
        
        vectors = np.array(level_data['embeddings'])
        chunks = level_data['chunks']
        doc_ids = level_data['doc_ids']
        
        # è¨ˆç®—ç›¸ä¼¼åº¦
        similarities = cosine_similarity([query_vector], vectors)[0]
        
        # ç²å–è©²å±¤æ¬¡çš„top-kçµæœ
        top_indices = np.argsort(similarities)[::-1][:k]
        
        level_results = []
        for idx in top_indices:
            result = {
                "content": chunks[idx],
                "similarity": float(similarities[idx]),
                "level": level_name,
                "doc_id": doc_ids[idx],
                "chunk_index": int(idx)
            }
            level_results.append(result)
            all_results.append(result)
        
        level_contributions[level_name] = {
            "results": level_results,
            "total_chunks": len(chunks),
            "avg_similarity": float(np.mean([r["similarity"] for r in level_results]))
        }
    
    # èåˆçµæœï¼ˆæŒ‰ç›¸ä¼¼åº¦æ’åºï¼‰
    fused_results = sorted(all_results, key=lambda x: x["similarity"], reverse=True)
    
    return {
        "query": query,
        "combination": combination,
        "level_contributions": level_contributions,
        "fused_results": fused_results[:k],
        "total_results": len(all_results)
    }


@app.post("/api/annotations/save")
async def save_annotations(req: AnnotationBatchRequest):
    """ä¿å­˜E/C/Uæ¨™è¨»"""
    saved_annotations = []
    
    for idx_str, label in req.annotations.items():
        idx = int(idx_str)
        if idx >= len(req.results):
            continue
            
        result = req.results[idx]
        annotation = ECUAnnotation(
            annotation_id=str(uuid.uuid4()),
            query=req.query,
            chunk_content=result["content"],
            chunk_index=idx,
            level=result.get("level", "unknown"),
            doc_id=result.get("doc_id", ""),
            relevance_label=label,
            annotator="user",
            timestamp=datetime.now().isoformat()
        )
        store.save_annotation(annotation)
        saved_annotations.append(annotation)
    
    return {"saved": len(saved_annotations), "annotations": saved_annotations}


@app.get("/api/annotations/stats")
def get_annotation_stats(query: Optional[str] = None):
    """ç²å–æ¨™è¨»çµ±è¨ˆ"""
    if query:
        annotations = store.get_annotations_for_query(query)
    else:
        annotations = store.get_all_annotations()
    
    stats = {
        "total": len(annotations),
        "by_label": {
            "E": sum(1 for a in annotations if a.relevance_label == 'E'),
            "C": sum(1 for a in annotations if a.relevance_label == 'C'),
            "U": sum(1 for a in annotations if a.relevance_label == 'U')
        },
        "by_level": {}
    }
    
    for annotation in annotations:
        level = annotation.level
        if level not in stats["by_level"]:
            stats["by_level"][level] = {"E": 0, "C": 0, "U": 0}
        stats["by_level"][level][annotation.relevance_label] += 1
    
    return stats


@app.get("/api/annotations/query/{query}")
def get_annotations_for_query(query: str):
    """ç²å–ç‰¹å®šæŸ¥è©¢çš„æ‰€æœ‰æ¨™è¨»"""
    annotations = store.get_annotations_for_query(query)
    return {"query": query, "annotations": annotations}


@app.delete("/api/annotations/query/{query}")
def delete_annotations_for_query(query: str):
    """åˆªé™¤ç‰¹å®šæŸ¥è©¢çš„æ‰€æœ‰æ¨™è¨»"""
    store.delete_annotations_for_query(query)
    return {"message": f"Deleted annotations for query: {query}"}


def calculate_ecu_metrics(annotations: List[ECUAnnotation], k_values: List[int]) -> Dict:
    """åŸºæ–¼æ¨™è¨»è¨ˆç®—E/C/UæŒ‡æ¨™"""
    metrics = {}
    
    # æŒ‰ç›¸ä¼¼åº¦æ’åºï¼ˆå‡è¨­æœ‰similarityå­—æ®µï¼‰
    sorted_annotations = sorted(annotations, key=lambda x: getattr(x, 'similarity', 0), reverse=True)
    
    for k in k_values:
        top_k = sorted_annotations[:k]
        e_count = sum(1 for a in top_k if a.relevance_label == 'E')
        c_count = sum(1 for a in top_k if a.relevance_label == 'C')
        u_count = sum(1 for a in top_k if a.relevance_label == 'U')
        
        metrics[f"E@{k}"] = (e_count / k) * 100 if k > 0 else 0
        metrics[f"C@{k}"] = (c_count / k) * 100 if k > 0 else 0
        metrics[f"U@{k}"] = (u_count / k) * 100 if k > 0 else 0
        metrics[f"E+C@{k}"] = ((e_count + c_count) / k) * 100 if k > 0 else 0
    
    return metrics


@app.post("/api/experimental-groups-generate-embeddings")
async def experimental_groups_generate_embeddings(req: Dict[str, Any]):
    """
    ç‚ºä¸åŒå¯¦é©—çµ„ç”Ÿæˆå°æ‡‰å±¤æ¬¡çš„embedding
    req = {
        "doc_id": str,
        "groups_to_embed": List[str]  # ["group_a", "group_b", "group_c", "group_d"]
    }
    """
    doc_id = req.get("doc_id")
    groups_to_embed = req.get("groups_to_embed", ["group_a", "group_b", "group_c", "group_d"])
    
    if not doc_id:
        return JSONResponse(status_code=400, content={"error": "Document ID is required"})
    
    doc = store.get_doc(doc_id)
    if not doc:
        return JSONResponse(status_code=404, content={"error": "Document not found"})
    
    results = {}
    
    for group_key in groups_to_embed:
        if group_key not in GRANULARITY_COMBINATIONS:
            continue
            
        combination = GRANULARITY_COMBINATIONS[group_key]
        selected_levels = combination["levels"]
        
        # ç‚ºè©²å¯¦é©—çµ„ç”Ÿæˆembedding
        group_results = {
            "group_info": combination,
            "levels_processed": [],
            "total_chunks": 0,
            "embedding_status": "processing"
        }
        
        try:
            # ç²å–è©²çµ„éœ€è¦çš„å±¤æ¬¡æ•¸æ“š
            for level_name in selected_levels:
                # æª¢æŸ¥æ˜¯å¦å·²æœ‰è©²å±¤æ¬¡çš„embedding
                existing_data = store.get_multi_level_embeddings(level_name)
                if existing_data and len(existing_data['embeddings']) > 0:
                    group_results["levels_processed"].append({
                        "level": level_name,
                        "status": "existing",
                        "chunk_count": len(existing_data['chunks'])
                    })
                    group_results["total_chunks"] += len(existing_data['chunks'])
                else:
                    # éœ€è¦ç”Ÿæˆè©²å±¤æ¬¡çš„embedding
                    group_results["levels_processed"].append({
                        "level": level_name,
                        "status": "missing",
                        "chunk_count": 0
                    })
            
            results[group_key] = group_results
            
        except Exception as e:
            results[group_key] = {
                "group_info": combination,
                "error": str(e),
                "embedding_status": "error"
            }
    
    return {
        "doc_id": doc_id,
        "groups_processed": list(results.keys()),
        "results": results,
        "message": "è«‹å…ˆç‚ºéœ€è¦çš„å±¤æ¬¡ç”Ÿæˆembeddingï¼Œç„¶å¾Œå†é€²è¡Œå¯¦é©—çµ„å°æ¯”"
    }


@app.post("/api/experimental-groups-batch-retrieve")
async def experimental_groups_batch_retrieve(req: Dict[str, Any]):
    """
    æ‰¹é‡æª¢ç´¢ä¸åŒå¯¦é©—çµ„çš„çµæœï¼Œç”¨æ–¼å°æ¯”å¯¦é©—
    æ³¨æ„ï¼šéœ€è¦å…ˆç‚ºå„å¯¦é©—çµ„ç”Ÿæˆå°æ‡‰çš„embedding
    req = {
        "query": str,
        "k": int,
        "groups_to_test": List[str]  # ["group_a", "group_b", "group_c", "group_d"]
    }
    """
    query = req.get("query")
    k = req.get("k", 10)
    groups_to_test = req.get("groups_to_test", ["group_a", "group_b", "group_c", "group_d"])
    
    if not query:
        return JSONResponse(status_code=400, content={"error": "Query is required"})
    
    # æª¢æŸ¥å„å¯¦é©—çµ„æ˜¯å¦æœ‰å°æ‡‰çš„embedding
    missing_embeddings = []
    for group_key in groups_to_test:
        if group_key not in GRANULARITY_COMBINATIONS:
            continue
        combination = GRANULARITY_COMBINATIONS[group_key]
        for level_name in combination["levels"]:
            level_data = store.get_multi_level_embeddings(level_name)
            if not level_data or len(level_data['embeddings']) == 0:
                missing_embeddings.append(f"{group_key}: {level_name}")
    
    if missing_embeddings:
        return JSONResponse(
            status_code=400, 
            content={
                "error": "Missing embeddings for experimental groups",
                "missing": missing_embeddings,
                "message": "è«‹å…ˆç‚ºé€™äº›å±¤æ¬¡ç”Ÿæˆembeddingï¼š\n" + "\n".join(missing_embeddings)
            }
        )
    
    # ç”ŸæˆæŸ¥è©¢å‘é‡
    if USE_GEMINI_EMBEDDING and GOOGLE_API_KEY:
        query_vector = (await embed_gemini([query]))[0]
    elif USE_BGE_M3_EMBEDDING:
        query_vector = embed_bge_m3([query])[0]
    else:
        return JSONResponse(status_code=400, content={"error": "No embedding method available"})
    
    results = {}
    
    for group_key in groups_to_test:
        if group_key not in GRANULARITY_COMBINATIONS:
            continue
            
        combination = GRANULARITY_COMBINATIONS[group_key]
        selected_levels = combination["levels"]
        
        # å¾é¸å®šçš„å±¤æ¬¡ä¸­æª¢ç´¢ä¸¦èåˆçµæœ
        all_results = []
        level_contributions = {}
        
        for level_name in selected_levels:
            level_data = store.get_multi_level_embeddings(level_name)
            if not level_data:
                continue
            
            vectors = np.array(level_data['embeddings'])
            chunks = level_data['chunks']
            doc_ids = level_data['doc_ids']
            
            # è¨ˆç®—ç›¸ä¼¼åº¦
            similarities = cosine_similarity([query_vector], vectors)[0]
            
            # ç²å–è©²å±¤æ¬¡çš„top-kçµæœ
            top_indices = np.argsort(similarities)[::-1][:k]
            
            level_results = []
            for idx in top_indices:
                result = {
                    "content": chunks[idx],
                    "similarity": float(similarities[idx]),
                    "level": level_name,
                    "doc_id": doc_ids[idx],
                    "chunk_index": int(idx)
                }
                level_results.append(result)
                all_results.append(result)
            
            level_contributions[level_name] = {
                "results": level_results,
                "total_chunks": len(chunks),
                "avg_similarity": float(np.mean([r["similarity"] for r in level_results])) if level_results else 0
            }
        
        # èåˆçµæœï¼ˆæŒ‰ç›¸ä¼¼åº¦æ’åºï¼‰
        fused_results = sorted(all_results, key=lambda x: x["similarity"], reverse=True)
        
        results[group_key] = {
            "group_info": combination,
            "level_contributions": level_contributions,
            "fused_results": fused_results[:k],
            "total_results": len(all_results)
        }
    
    return {
        "query": query,
        "k": k,
        "groups_tested": list(results.keys()),
        "results": results
    }


@app.get("/api/granularity-comparison-report")
def generate_comparison_report():
    """ç”Ÿæˆç²’åº¦å°æ¯”å ±å‘Š"""
    all_annotations = store.get_all_annotations()
    
    if not all_annotations:
        return {"message": "No annotations available for comparison"}
    
    # æŒ‰æŸ¥è©¢å’Œå¯¦é©—çµ„åˆ†çµ„
    query_group_data = {}
    for annotation in all_annotations:
        # å¾annotationä¸­æå–å¯¦é©—çµ„ä¿¡æ¯ï¼ˆéœ€è¦åœ¨å‰ç«¯æ¨™è¨»æ™‚è¨˜éŒ„ï¼‰
        query = annotation.query
        group_info = getattr(annotation, 'experimental_group', 'unknown')
        
        if query not in query_group_data:
            query_group_data[query] = {}
        if group_info not in query_group_data[query]:
            query_group_data[query][group_info] = []
        
        query_group_data[query][group_info].append(annotation)
    
    # è¨ˆç®—å„æŸ¥è©¢å„çµ„çš„æŒ‡æ¨™
    report = {
        "total_queries": len(query_group_data),
        "total_annotations": len(all_annotations),
        "experimental_groups": ["group_a", "group_b", "group_c", "group_d"],
        "per_query_results": {},
        "group_comparison": {},
        "marginal_benefit_analysis": {}
    }
    
    k_values = [1, 3, 5, 10]
    
    # è¨ˆç®—å„æŸ¥è©¢çš„çµæœ
    for query, group_annotations in query_group_data.items():
        report["per_query_results"][query] = {}
        
        for group, annotations in group_annotations.items():
            metrics = calculate_ecu_metrics(annotations, k_values)
            report["per_query_results"][query][group] = {
                "total_annotations": len(annotations),
                "metrics": metrics,
                "label_distribution": {
                    "E": sum(1 for a in annotations if a.relevance_label == 'E'),
                    "C": sum(1 for a in annotations if a.relevance_label == 'C'),
                    "U": sum(1 for a in annotations if a.relevance_label == 'U')
                }
            }
    
    # è¨ˆç®—å„å¯¦é©—çµ„çš„èšåˆæŒ‡æ¨™
    for group in report["experimental_groups"]:
        group_metrics = []
        for query_data in report["per_query_results"].values():
            if group in query_data:
                group_metrics.append(query_data[group]["metrics"])
        
        if group_metrics:
            report["group_comparison"][group] = {}
            for k in k_values:
                report["group_comparison"][group][f"avg_E@{k}"] = np.mean([m[f"E@{k}"] for m in group_metrics])
                report["group_comparison"][group][f"avg_C@{k}"] = np.mean([m[f"C@{k}"] for m in group_metrics])
                report["group_comparison"][group][f"avg_U@{k}"] = np.mean([m[f"U@{k}"] for m in group_metrics])
                report["group_comparison"][group][f"avg_E+C@{k}"] = np.mean([m[f"E+C@{k}"] for m in group_metrics])
    
    # è¨ˆç®—é‚Šéš›æ•ˆç›Šåˆ†æ
    if "group_a" in report["group_comparison"]:
        baseline = report["group_comparison"]["group_a"]
        for group in ["group_b", "group_c", "group_d"]:
            if group in report["group_comparison"]:
                comparison = report["group_comparison"][group]
                report["marginal_benefit_analysis"][f"{group}_vs_group_a"] = {}
                
                for k in k_values:
                    report["marginal_benefit_analysis"][f"{group}_vs_group_a"][f"E@{k}_improvement"] = (
                        comparison[f"avg_E@{k}"] - baseline[f"avg_E@{k}"]
                    )
                    report["marginal_benefit_analysis"][f"{group}_vs_group_a"][f"E+C@{k}_improvement"] = (
                        comparison[f"avg_E+C@{k}"] - baseline[f"avg_E+C@{k}"]
                    )
                    report["marginal_benefit_analysis"][f"{group}_vs_group_a"][f"U@{k}_reduction"] = (
                        baseline[f"avg_U@{k}"] - comparison[f"avg_U@{k}"]
                    )
    
    return report


# ============================================
