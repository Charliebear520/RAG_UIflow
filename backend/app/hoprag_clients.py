"""
HopRAGå®¢æˆ¶ç«¯é©é…å™¨
æ•´åˆç¾æœ‰çš„LLMå’ŒEmbeddingç³»çµ±
"""

import asyncio
import json
from typing import List, Dict, Any, Optional
import numpy as np

class LLMClientAdapter:
    """LLMå®¢æˆ¶ç«¯é©é…å™¨ - ä½¿ç”¨Gemini"""
    
    def __init__(self, llm_client=None):
        self.llm_client = llm_client
        self.use_gemini = False
        
        # å˜—è©¦æª¢æ¸¬Gemini
        self._detect_available_llm()
    
    def _detect_available_llm(self):
        """æª¢æ¸¬å¯ç”¨çš„LLMæœå‹™"""
        try:
            # æª¢æŸ¥Gemini
            import google.generativeai as genai
            import os
            if os.getenv('GOOGLE_API_KEY'):
                self.use_gemini = True
                print("âœ… æª¢æ¸¬åˆ°Gemini APIå¯ç”¨")
        except ImportError:
            pass
    
    async def generate_async(self, prompt: str, max_retries: int = 3) -> str:
        """ç•°æ­¥ç”Ÿæˆæ–‡æœ¬"""
        for attempt in range(max_retries):
            try:
                if self.use_gemini:
                    return await self._generate_with_gemini(prompt)
                else:
                    # ä½¿ç”¨æ¨¡æ“¬éŸ¿æ‡‰
                    return self._generate_mock_response(prompt)
                    
            except Exception as e:
                print(f"âŒ LLMç”Ÿæˆå¤±æ•— (å˜—è©¦ {attempt + 1}/{max_retries}): {e}")
                if attempt == max_retries - 1:
                    # æœ€å¾Œä¸€æ¬¡å˜—è©¦å¤±æ•—ï¼Œè¿”å›æ¨¡æ“¬éŸ¿æ‡‰
                    return self._generate_mock_response(prompt)
                await asyncio.sleep(1)  # ç­‰å¾…1ç§’å¾Œé‡è©¦
    
    async def _generate_with_gemini(self, prompt: str) -> str:
        """ä½¿ç”¨Geminiç”Ÿæˆ"""
        import google.generativeai as genai
        import os
        
        # é…ç½®Gemini
        genai.configure(api_key=os.getenv('GOOGLE_API_KEY'))
        model = genai.GenerativeModel('gemini-2.5-flash')
        
        # ç”ŸæˆéŸ¿æ‡‰
        response = await asyncio.get_event_loop().run_in_executor(
            None, lambda: model.generate_content(prompt)
        )
        
        return response.text
    
    
    def _generate_mock_response(self, prompt: str) -> str:
        """ç”Ÿæˆæ¨¡æ“¬éŸ¿æ‡‰ï¼ˆç”¨æ–¼æ¸¬è©¦ï¼‰"""
        if "å…§å‘å•é¡Œ" in prompt:
            return json.dumps({
                "incoming_questions": [
                    "æ­¤æ¢æ–‡çš„ä¸»è¦å…§å®¹æ˜¯ä»€éº¼ï¼Ÿ",
                    "æ­¤æ¢æ–‡è¦å®šäº†å“ªäº›æ³•å¾‹è¦ä»¶ï¼Ÿ",
                    "æ ¹æ“šæ­¤æ¢æ–‡ï¼Œç›¸é—œçš„å®šç¾©æ˜¯ä»€éº¼ï¼Ÿ",
                    "æ­¤æ¢æ–‡çš„é©ç”¨ç¯„åœæ˜¯ä»€éº¼ï¼Ÿ",
                    "æ­¤æ¢æ–‡è¦å®šäº†ä»€éº¼æ³•å¾‹å¾Œæœï¼Ÿ"
                ]
            }, ensure_ascii=False)
        
        elif "å¤–å‘å•é¡Œ" in prompt:
            return json.dumps({
                "outgoing_questions": [
                    "é•åæ­¤æ¢æ–‡æœƒæœ‰ä»€éº¼æ³•å¾‹å¾Œæœï¼Ÿ",
                    "å¦‚ä½•ç”³è«‹æ­¤æ¢æ–‡è¦å®šçš„æ¬Šåˆ©ï¼Ÿ",
                    "æ­¤æ¢æ–‡èˆ‡å…¶ä»–æ³•æ¢æœ‰ä»€éº¼é—œè¯ï¼Ÿ",
                    "åœ¨ä»€éº¼æƒ…æ³ä¸‹é©ç”¨æ­¤æ¢æ–‡ï¼Ÿ",
                    "æ­¤æ¢æ–‡çš„å¯¦å‹™æ“ä½œç¨‹åºæ˜¯ä»€éº¼ï¼Ÿ"
                ]
            }, ensure_ascii=False)
        
        elif "Completely Irrelevant" in prompt or "Indirectly relevant" in prompt:
            return "Indirectly relevant"
        
        else:
            return "ç›¸é—œä¸”å¿…è¦"

class EmbeddingClientAdapter:
    """Embeddingå®¢æˆ¶ç«¯é©é…å™¨"""
    
    # ğŸš€ é¡è®Šé‡ï¼šå–®ä¾‹æ¨¡å‹ï¼Œæ‰€æœ‰å¯¦ä¾‹å…±äº«ï¼ˆé¿å…é‡è¤‡åŠ è¼‰ï¼‰
    _bge_model = None
    _bge_model_lock = None
    
    def __init__(self, embedding_client=None):
        self.embedding_client = embedding_client
        self.use_gemini = False
        self.use_bge = False
        self.use_sentence_transformers = False
        
        # åˆå§‹åŒ–é–ï¼ˆç·šç¨‹å®‰å…¨ï¼‰
        if EmbeddingClientAdapter._bge_model_lock is None:
            import threading
            EmbeddingClientAdapter._bge_model_lock = threading.Lock()
        
        # å˜—è©¦æª¢æ¸¬å¯ç”¨çš„Embeddingæœå‹™
        self._detect_available_embedding()
    
    def _detect_available_embedding(self):
        """æª¢æ¸¬å¯ç”¨çš„Embeddingæœå‹™"""
        try:
            # æª¢æŸ¥Gemini Embedding
            import google.generativeai as genai
            import os
            if os.getenv('GOOGLE_API_KEY'):
                self.use_gemini = True
                print("âœ… æª¢æ¸¬åˆ°Gemini Embedding APIå¯ç”¨")
        except ImportError:
            pass
        
        try:
            # æª¢æŸ¥BGE-M3
            import sentence_transformers
            self.use_bge = True
            self.use_sentence_transformers = True
            print("âœ… æª¢æ¸¬åˆ°Sentence Transformerså¯ç”¨")
        except ImportError:
            pass
    
    def encode(self, texts: List[str]) -> np.ndarray:
        """åŒæ­¥ç·¨ç¢¼æ–‡æœ¬ï¼ˆå„ªå…ˆä½¿ç”¨æœ¬åœ°BGE-M3æ¨¡å‹ï¼‰"""
        if self.use_bge:  # ğŸš€ å„ªå…ˆä½¿ç”¨BGE-M3æœ¬åœ°æ¨¡å‹ï¼ˆå¿«30å€ + ç„¡APIé™åˆ¶ï¼‰
            return self._encode_with_bge(texts)
        elif self.use_gemini:
            return self._encode_with_gemini(texts)
        else:
            return self._encode_mock(texts)
    
    async def encode_async(self, texts: List[str]) -> np.ndarray:
        """ç•°æ­¥ç·¨ç¢¼æ–‡æœ¬ï¼ˆå„ªå…ˆä½¿ç”¨æœ¬åœ°BGE-M3æ¨¡å‹ï¼‰"""
        if self.use_bge:  # ğŸš€ å„ªå…ˆä½¿ç”¨BGE-M3æœ¬åœ°æ¨¡å‹ï¼ˆå¿«30å€ + ç„¡APIé™åˆ¶ï¼‰
            return self._encode_with_bge(texts)
        elif self.use_gemini:
            return await self._encode_with_gemini_async(texts)
        else:
            return self._encode_mock(texts)
    
    async def _encode_with_gemini_async(
        self, 
        texts: List[str], 
        batch_size: int = 100,        # ğŸš€ å¢å¤§æ‰¹é‡å¤§å°ï¼ˆå®˜æ–¹æ”¯æŒæ‰¹é‡è°ƒç”¨ï¼‰
        delay_seconds: float = 1.0,   # ğŸš€ å‡å°‘å»¶è¿Ÿï¼ˆçœŸæ­£æ‰¹é‡è°ƒç”¨åé€Ÿç‡é™åˆ¶å®½æ¾ï¼‰
        output_dimensionality: int = 256  # ğŸš€ é™ä½ç»´åº¦ï¼ˆæ€§èƒ½ä»…é™2%ï¼Œé€Ÿåº¦å¿«3å€ï¼‰
    ) -> np.ndarray:
        """ä½¿ç”¨Geminiç•°æ­¥ç·¨ç¢¼ï¼ˆçœŸæ­£çš„æ‰¹é‡èª¿ç”¨ + é™ç¶­å„ªåŒ–ï¼‰
        
        æ ¹æ“šå®˜æ–¹æ–‡æª” https://ai.google.dev/gemini-api/docs/embeddings?hl=zh-tw å„ªåŒ–ï¼š
        1. ä½¿ç”¨æ‰¹é‡èª¿ç”¨APIï¼ˆä¸€æ¬¡å‚³éå¤šå€‹æ–‡æœ¬ï¼Œè€Œéé€å€‹èª¿ç”¨ï¼‰
        2. é™ä½ç¶­åº¦è‡³256ï¼ˆæ€§èƒ½åƒ…é™2%ï¼š67.55â†’66.19ï¼Œé€Ÿåº¦å¿«3å€ï¼‰
        3. å¤§å¹…æ¸›å°‘APIèª¿ç”¨æ¬¡æ•¸ï¼ˆ1842å€‹æ–‡æœ¬ â†’ 19æ¬¡APIèª¿ç”¨ï¼‰
        
        Args:
            texts: æ–‡æœ¬åˆ—è¡¨
            batch_size: æ‰¹é‡å¤§å°ï¼ˆé»˜èª100ï¼Œå®˜æ–¹æ”¯æŒæ‰¹é‡èª¿ç”¨ï¼‰
            delay_seconds: æ¯æ‰¹ä¹‹é–“çš„å»¶é²ï¼ˆç§’ï¼Œæ‰¹é‡èª¿ç”¨å¾Œå¯æ¸›å°‘å»¶é²ï¼‰
            output_dimensionality: è¼¸å‡ºç¶­åº¦ï¼ˆ256/768/1536/3072ï¼Œæ¨è–¦256ï¼‰
        """
        import google.generativeai as genai
        import os
        
        # é…ç½®Gemini
        genai.configure(api_key=os.getenv('GOOGLE_API_KEY'))
        
        all_embeddings = []
        total_texts = len(texts)
        
        print(f"ğŸ“Š ä½¿ç”¨æ‰¹é‡èª¿ç”¨æ¨¡å¼ï¼š{total_texts}å€‹æ–‡æœ¬ â†’ {(total_texts + batch_size - 1) // batch_size}æ¬¡APIèª¿ç”¨")
        print(f"ğŸ“Š ç¶­åº¦è¨­ç½®ï¼š{output_dimensionality}ï¼ˆå®˜æ–¹æ¨è–¦ï¼š256æ€§èƒ½åƒ…é™2%ä½†å¿«3å€ï¼‰")
        
        # æ‰¹é‡è™•ç†
        for batch_start in range(0, total_texts, batch_size):
            batch_end = min(batch_start + batch_size, total_texts)
            batch_texts = texts[batch_start:batch_end]
            
            max_retries = 3
            for attempt in range(max_retries):
                try:
                    # ğŸš€ å®˜æ–¹æ¨è–¦ï¼šæ‰¹é‡èª¿ç”¨ï¼ˆä¸€æ¬¡å‚³éæ•´å€‹åˆ—è¡¨ï¼‰
                    result = await asyncio.get_event_loop().run_in_executor(
                        None, 
                        lambda: genai.embed_content(
                            model="models/embedding-001",
                            content=batch_texts,  # ğŸš€ æ‰¹é‡ï¼šå‚³éåˆ—è¡¨è€Œéå–®å€‹æ–‡æœ¬
                            task_type="retrieval_document",
                            output_dimensionality=output_dimensionality  # ğŸš€ é™ç¶­å„ªåŒ–
                        )
                    )
                    
                    # æå–embeddingsï¼ˆæ‰¹é‡çµæœï¼‰
                    if isinstance(result, dict) and 'embedding' in result:
                        # å–®å€‹æ–‡æœ¬çš„çµæœ
                        all_embeddings.extend([result['embedding']])
                    elif hasattr(result, 'embeddings'):
                        # æ‰¹é‡æ–‡æœ¬çš„çµæœï¼ˆå®˜æ–¹APIè¿”å›æ ¼å¼ï¼‰
                        all_embeddings.extend([emb.values for emb in result.embeddings])
                    else:
                        # å˜—è©¦å…¶ä»–æ ¼å¼
                        all_embeddings.extend(result if isinstance(result, list) else [result])
                    
                    # æˆåŠŸå‰‡è·³å‡ºé‡è©¦å¾ªç’°
                    progress = batch_end / total_texts * 100
                    print(f"âœ… æ‰¹æ¬¡ {batch_start//batch_size + 1}/{(total_texts + batch_size - 1)//batch_size}: "
                          f"{batch_end}/{total_texts} ({progress:.1f}%) å®Œæˆ")
                    break
                    
                except Exception as e:
                    if attempt < max_retries - 1:
                        print(f"âš ï¸ Geminiæ‰¹é‡Embeddingå¤±æ•— (å˜—è©¦ {attempt + 1}/{max_retries}): {e}")
                        await asyncio.sleep(2)  # ç­‰å¾…2ç§’å¾Œé‡è©¦
                    else:
                        print(f"âŒ Geminiæ‰¹é‡Embeddingå¤±æ•—ï¼ˆå·²é‡è©¦{max_retries}æ¬¡ï¼‰: {e}")
                        # ä½¿ç”¨éš¨æ©Ÿå‘é‡ä½œç‚ºfallback
                        for _ in batch_texts:
                            all_embeddings.append(np.random.randn(output_dimensionality).astype(np.float32))
            
            # æ‰¹æ¬¡ä¹‹é–“å»¶é²ï¼Œé¿å…è§¸ç™¼é€Ÿç‡é™åˆ¶ï¼ˆæ‰¹é‡èª¿ç”¨å¾Œå¯å¤§å¹…æ¸›å°‘ï¼‰
            if batch_end < total_texts:
                await asyncio.sleep(delay_seconds)
        
        return np.array(all_embeddings)
    
    def _encode_with_gemini(self, texts: List[str]) -> np.ndarray:
        """ä½¿ç”¨GeminiåŒæ­¥ç·¨ç¢¼"""
        import google.generativeai as genai
        import os
        
        # é…ç½®Gemini
        genai.configure(api_key=os.getenv('GOOGLE_API_KEY'))
        
        embeddings = []
        for text in texts:
            try:
                result = genai.embed_content(
                    model="models/embedding-001",
                    content=text,
                    task_type="retrieval_document"
                )
                embeddings.append(result['embedding'])
            except Exception as e:
                print(f"âŒ Gemini Embeddingå¤±æ•—: {e}")
                # ä½¿ç”¨éš¨æ©Ÿå‘é‡ä½œç‚ºfallback
                embeddings.append(np.random.randn(768).astype(np.float32))
        
        return np.array(embeddings)
    
    def _encode_with_bge(self, texts: List[str], batch_size: int = 32, show_progress: bool = False) -> np.ndarray:
        """ä½¿ç”¨BGE-M3ç·¨ç¢¼ï¼ˆå–®ä¾‹æ¨¡å‹ + æ‰¹é‡è™•ç†å„ªåŒ–ï¼‰
        
        Args:
            texts: æ–‡æœ¬åˆ—è¡¨
            batch_size: æ‰¹é‡å¤§å°ï¼ˆé»˜èª32ï¼Œæ¸›å°‘å…§å­˜å ç”¨ï¼‰
            show_progress: æ˜¯å¦é¡¯ç¤ºé€²åº¦æ¢
        """
        try:
            from sentence_transformers import SentenceTransformer
            import gc
            
            # ğŸš€ ä½¿ç”¨å–®ä¾‹æ¨¡å‹ï¼ˆç·šç¨‹å®‰å…¨ï¼‰
            with EmbeddingClientAdapter._bge_model_lock:
                if EmbeddingClientAdapter._bge_model is None:
                    print("ğŸ“¥ é¦–æ¬¡åŠ è¼‰ BGE-M3 æ¨¡å‹ï¼ˆ~2.3 GBï¼‰...")
                    EmbeddingClientAdapter._bge_model = SentenceTransformer('BAAI/bge-m3')
                    print("âœ… BGE-M3 æ¨¡å‹åŠ è¼‰å®Œæˆ")
            
            # ä½¿ç”¨å…±äº«æ¨¡å‹é€²è¡Œç·¨ç¢¼
            embeddings = EmbeddingClientAdapter._bge_model.encode(
                texts,
                batch_size=batch_size,  # ğŸ”§ æ‰¹é‡å¤§å°å„ªåŒ–ï¼ˆé™ä½å…§å­˜å³°å€¼ï¼‰
                show_progress_bar=show_progress,
                convert_to_numpy=True,
                normalize_embeddings=False  # æ ¹æ“šéœ€æ±‚æ±ºå®šæ˜¯å¦æ­¸ä¸€åŒ–
            )
            
            # ğŸ§¹ å¼·åˆ¶åƒåœ¾å›æ”¶ï¼Œé‡‹æ”¾è‡¨æ™‚å…§å­˜
            gc.collect()
            
            return embeddings
            
        except Exception as e:
            print(f"âŒ BGE-M3 Embeddingå¤±æ•—: {e}")
            import traceback
            traceback.print_exc()
            # ä½¿ç”¨éš¨æ©Ÿå‘é‡ä½œç‚ºfallback
            return np.random.randn(len(texts), 1024).astype(np.float32)
    
    def _encode_mock(self, texts: List[str]) -> np.ndarray:
        """ç”Ÿæˆæ¨¡æ“¬embeddingï¼ˆç”¨æ–¼æ¸¬è©¦ï¼‰"""
        # ç”Ÿæˆéš¨æ©Ÿå‘é‡ä½œç‚ºæ¨¡æ“¬embedding
        return np.random.randn(len(texts), 768).astype(np.float32)
    
    @classmethod
    def unload_bge_model(cls):
        """æ‰‹å‹•é‡‹æ”¾ BGE-M3 æ¨¡å‹å…§å­˜ï¼ˆå¦‚æœå…§å­˜ä¸è¶³æ™‚èª¿ç”¨ï¼‰"""
        if cls._bge_model is not None:
            print("ğŸ§¹ æ­£åœ¨é‡‹æ”¾ BGE-M3 æ¨¡å‹å…§å­˜...")
            with cls._bge_model_lock:
                cls._bge_model = None
            import gc
            gc.collect()
            print("âœ… BGE-M3 æ¨¡å‹å·²é‡‹æ”¾")
        else:
            print("âš ï¸ BGE-M3 æ¨¡å‹æœªåŠ è¼‰ï¼Œç„¡éœ€é‡‹æ”¾")
    
    @classmethod
    def get_model_memory_info(cls):
        """ç²å–æ¨¡å‹å…§å­˜ä¿¡æ¯"""
        import psutil
        import os
        
        process = psutil.Process(os.getpid())
        memory_info = process.memory_info()
        
        return {
            "bge_model_loaded": cls._bge_model is not None,
            "process_memory_mb": memory_info.rss / 1024 / 1024,
            "process_memory_percent": process.memory_percent()
        }

class HopRAGClientManager:
    """HopRAGå®¢æˆ¶ç«¯ç®¡ç†å™¨"""
    
    def __init__(self):
        self.llm_client = LLMClientAdapter()
        self.embedding_client = EmbeddingClientAdapter()
        
    def get_llm_client(self) -> LLMClientAdapter:
        """ç²å–LLMå®¢æˆ¶ç«¯"""
        return self.llm_client
    
    def get_embedding_client(self) -> EmbeddingClientAdapter:
        """ç²å–Embeddingå®¢æˆ¶ç«¯"""
        return self.embedding_client
    
    def get_client_status(self) -> Dict[str, Any]:
        """ç²å–å®¢æˆ¶ç«¯ç‹€æ…‹"""
        return {
            "llm_status": {
                "gemini_available": self.llm_client.use_gemini,
                "model": "gemini-2.5-flash"
            },
            "embedding_status": {
                "gemini_available": self.embedding_client.use_gemini,
                "bge_available": self.embedding_client.use_bge,
                "sentence_transformers_available": self.embedding_client.use_sentence_transformers
            }
        }
