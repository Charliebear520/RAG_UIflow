"""
æ³•å¾‹æ¦‚å¿µåœ–æª¢ç´¢
æ§‹å»ºæ³•å¾‹æ¦‚å¿µåœ–ï¼ŒåŸºæ–¼æ¦‚å¿µé—œä¿‚é€²è¡Œæª¢ç´¢
"""

import re
import networkx as nx
from typing import List, Dict, Any, Set, Tuple, Optional
from dataclasses import dataclass
from collections import defaultdict, Counter
import numpy as np
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity


@dataclass
class LegalConcept:
    """æ³•å¾‹æ¦‚å¿µæ•¸æ“šçµæ§‹"""
    concept_id: str
    concept_name: str
    concept_type: str  # 'right', 'obligation', 'exception', 'condition', 'definition'
    content: str
    related_articles: List[str]
    importance_score: float
    semantic_embedding: Optional[np.ndarray] = None


@dataclass
class ConceptRelation:
    """æ¦‚å¿µé—œä¿‚æ•¸æ“šçµæ§‹"""
    source_concept: str
    target_concept: str
    relation_type: str  # 'implies', 'conflicts', 'includes', 'excludes', 'requires'
    confidence: float
    evidence: str


class LegalConceptGraph:
    """æ³•å¾‹æ¦‚å¿µåœ–"""
    
    def __init__(self):
        self.graph = nx.DiGraph()
        self.concepts: Dict[str, LegalConcept] = {}
        self.relations: List[ConceptRelation] = []
        
        # æ³•å¾‹æ¦‚å¿µæ¨¡æ¿
        self.concept_templates = {
            'copyright_rights': {
                'patterns': [
                    r'è‘—ä½œäººå°ˆæœ‰.*?æ¬Šåˆ©',
                    r'è‘—ä½œæ¬Šäºº.*?æ¬Šåˆ©',
                    r'å°ˆæœ‰.*?æ¬Šåˆ©'
                ],
                'type': 'right',
                'importance': 1.0
            },
            'trademark_rights': {
                'patterns': [
                    r'å•†æ¨™æ¬Šäºº.*?æ¬Šåˆ©',
                    r'å•†æ¨™.*?å°ˆç”¨æ¬Š',
                    r'è¨»å†Šå•†æ¨™.*?æ¬Šåˆ©'
                ],
                'type': 'right',
                'importance': 1.0
            },
            'exceptions': {
                'patterns': [
                    r'ä½†.*?ä¸åœ¨æ­¤é™',
                    r'é™¤å¤–.*?è¦å®š',
                    r'ä¸é©ç”¨.*?æƒ…å½¢'
                ],
                'type': 'exception',
                'importance': 0.8
            },
            'conditions': {
                'patterns': [
                    r'æœ‰ä¸‹åˆ—æƒ…å½¢ä¹‹ä¸€',
                    r'å¦‚.*?æ™‚',
                    r'æ–¼.*?æƒ…å½¢'
                ],
                'type': 'condition',
                'importance': 0.7
            },
            'obligations': {
                'patterns': [
                    r'æ‡‰.*?ç”³è«‹',
                    r'å¿…é ˆ.*?è¾¦ç†',
                    r'ä¸å¾—.*?è¡Œç‚º'
                ],
                'type': 'obligation',
                'importance': 0.9
            }
        }
        
        # é—œä¿‚è­˜åˆ¥æ¨¡å¼
        self.relation_patterns = {
            'implies': [
                r'å› æ­¤.*?æ‡‰',
                r'æ‰€ä»¥.*?å¿…é ˆ',
                r'æ•….*?å¾—'
            ],
            'conflicts': [
                r'ä½†.*?ä¸å¾—',
                r'ä¸åœ¨æ­¤é™',
                r'é™¤å¤–'
            ],
            'includes': [
                r'åŒ…æ‹¬.*?æƒ…å½¢',
                r'åŒ…å«.*?é …ç›®',
                r'æ¶µè“‹.*?ç¯„åœ'
            ],
            'excludes': [
                r'ä¸åŒ…æ‹¬.*?æƒ…å½¢',
                r'æŽ’é™¤.*?é …ç›®',
                r'ä¸æ¶µè“‹.*?ç¯„åœ'
            ],
            'requires': [
                r'æ‡‰.*?ç”³è«‹',
                r'å¿…é ˆ.*?å…·å‚™',
                r'éœ€è¦.*?æ¢ä»¶'
            ]
        }
    
    def build_graph(self, documents: List[Dict[str, Any]]) -> None:
        """æ§‹å»ºæ³•å¾‹æ¦‚å¿µåœ–"""
        print("ðŸ”¨ é–‹å§‹æ§‹å»ºæ³•å¾‹æ¦‚å¿µåœ–...")
        
        # 1. æå–æ¦‚å¿µ
        concepts = self._extract_concepts_from_documents(documents)
        print(f"ðŸ” æå–åˆ° {len(concepts)} å€‹æ³•å¾‹æ¦‚å¿µ")
        
        # 2. å»ºç«‹æ¦‚å¿µé—œä¿‚
        relations = self._establish_concept_relations(documents, concepts)
        print(f"ðŸ” å»ºç«‹ {len(relations)} å€‹æ¦‚å¿µé—œä¿‚")
        
        # 3. æ§‹å»ºåœ–çµæ§‹
        self._build_graph_structure(concepts, relations)
        print(f"ðŸ” æ§‹å»ºå®Œæˆï¼Œåœ–åŒ…å« {self.graph.number_of_nodes()} å€‹ç¯€é»žï¼Œ{self.graph.number_of_edges()} æ¢é‚Š")
        
        # 4. è¨ˆç®—æ¦‚å¿µåµŒå…¥
        self._compute_concept_embeddings()
        print("ðŸ” å®Œæˆæ¦‚å¿µåµŒå…¥è¨ˆç®—")
    
    def _extract_concepts_from_documents(self, documents: List[Dict[str, Any]]) -> List[LegalConcept]:
        """å¾žæ–‡æª”ä¸­æå–æ³•å¾‹æ¦‚å¿µ"""
        concepts = []
        concept_counter = Counter()
        
        for doc in documents:
            content = doc.get('content', '')
            
            for concept_name, concept_config in self.concept_templates.items():
                for pattern in concept_config['patterns']:
                    matches = re.finditer(pattern, content, re.MULTILINE | re.DOTALL)
                    
                    for match in matches:
                        concept_id = f"{concept_name}_{len(concepts)}"
                        
                        # æå–ç›¸é—œæ³•æ¢
                        related_articles = self._extract_related_articles(content, match.start(), match.end())
                        
                        concept = LegalConcept(
                            concept_id=concept_id,
                            concept_name=concept_name,
                            concept_type=concept_config['type'],
                            content=match.group().strip(),
                            related_articles=related_articles,
                            importance_score=concept_config['importance']
                        )
                        
                        concepts.append(concept)
                        concept_counter[concept_name] += 1
        
        # åŽ»é‡ä¸¦åˆä½µç›¸ä¼¼æ¦‚å¿µ
        unique_concepts = self._deduplicate_concepts(concepts)
        
        # æ›´æ–°æ¦‚å¿µå­—å…¸
        for concept in unique_concepts:
            self.concepts[concept.concept_id] = concept
        
        return unique_concepts
    
    def _extract_related_articles(self, content: str, start_pos: int, end_pos: int) -> List[str]:
        """æå–ç›¸é—œæ³•æ¢"""
        # åœ¨æ¦‚å¿µå‰å¾Œä¸€å®šç¯„åœå…§æŸ¥æ‰¾æ³•æ¢å¼•ç”¨
        search_start = max(0, start_pos - 200)
        search_end = min(len(content), end_pos + 200)
        search_text = content[search_start:search_end]
        
        article_pattern = r'ç¬¬\s*(\d+)\s*æ¢'
        articles = re.findall(article_pattern, search_text)
        return [f"ç¬¬{article}æ¢" for article in articles]
    
    def _deduplicate_concepts(self, concepts: List[LegalConcept]) -> List[LegalConcept]:
        """åŽ»é‡ä¸¦åˆä½µç›¸ä¼¼æ¦‚å¿µ"""
        unique_concepts = []
        seen_contents = set()
        
        for concept in concepts:
            content_key = concept.content.lower().strip()
            if content_key not in seen_contents:
                seen_contents.add(content_key)
                unique_concepts.append(concept)
            else:
                # åˆä½µåˆ°å·²å­˜åœ¨çš„æ¦‚å¿µ
                for existing in unique_concepts:
                    if existing.content.lower().strip() == content_key:
                        existing.related_articles.extend(concept.related_articles)
                        existing.related_articles = list(set(existing.related_articles))
                        break
        
        return unique_concepts
    
    def _establish_concept_relations(self, documents: List[Dict[str, Any]], 
                                   concepts: List[LegalConcept]) -> List[ConceptRelation]:
        """å»ºç«‹æ¦‚å¿µé—œä¿‚"""
        relations = []
        
        for doc in documents:
            content = doc.get('content', '')
            
            for relation_type, patterns in self.relation_patterns.items():
                for pattern in patterns:
                    matches = re.finditer(pattern, content, re.MULTILINE | re.DOTALL)
                    
                    for match in matches:
                        # åœ¨åŒ¹é…çš„ä¸Šä¸‹æ–‡é™„è¿‘æŸ¥æ‰¾ç›¸é—œæ¦‚å¿µ
                        context_start = max(0, match.start() - 300)
                        context_end = min(len(content), match.end() + 300)
                        context = content[context_start:context_end]
                        
                        # æ‰¾åˆ°ä¸Šä¸‹æ–‡ä¸­çš„æ¦‚å¿µ
                        context_concepts = self._find_concepts_in_context(context, concepts)
                        
                        if len(context_concepts) >= 2:
                            # å»ºç«‹é—œä¿‚
                            for i in range(len(context_concepts) - 1):
                                relation = ConceptRelation(
                                    source_concept=context_concepts[i].concept_id,
                                    target_concept=context_concepts[i + 1].concept_id,
                                    relation_type=relation_type,
                                    confidence=self._calculate_relation_confidence(match.group(), context),
                                    evidence=match.group()
                                )
                                relations.append(relation)
        
        # åŽ»é‡ä¸¦åˆä½µç›¸ä¼¼é—œä¿‚
        unique_relations = self._deduplicate_relations(relations)
        
        self.relations = unique_relations
        return unique_relations
    
    def _find_concepts_in_context(self, context: str, concepts: List[LegalConcept]) -> List[LegalConcept]:
        """åœ¨ä¸Šä¸‹æ–‡ä¸­æŸ¥æ‰¾æ¦‚å¿µ"""
        found_concepts = []
        
        for concept in concepts:
            if concept.content in context:
                found_concepts.append(concept)
        
        return found_concepts
    
    def _calculate_relation_confidence(self, evidence: str, context: str) -> float:
        """è¨ˆç®—é—œä¿‚ç½®ä¿¡åº¦"""
        # åŸºæ–¼è­‰æ“šçš„é•·åº¦å’Œä¸Šä¸‹æ–‡ç›¸é—œæ€§
        evidence_length = len(evidence)
        context_relevance = len(set(context.split()) & set(evidence.split())) / len(set(context.split()))
        
        confidence = min(1.0, evidence_length / 50 + context_relevance * 0.5)
        return confidence
    
    def _deduplicate_relations(self, relations: List[ConceptRelation]) -> List[ConceptRelation]:
        """åŽ»é‡ä¸¦åˆä½µç›¸ä¼¼é—œä¿‚"""
        unique_relations = []
        seen_relations = set()
        
        for relation in relations:
            relation_key = (relation.source_concept, relation.target_concept, relation.relation_type)
            if relation_key not in seen_relations:
                seen_relations.add(relation_key)
                unique_relations.append(relation)
            else:
                # åˆä½µåˆ°å·²å­˜åœ¨çš„é—œä¿‚
                for existing in unique_relations:
                    if (existing.source_concept == relation.source_concept and
                        existing.target_concept == relation.target_concept and
                        existing.relation_type == relation.relation_type):
                        # å–è¼ƒé«˜çš„ç½®ä¿¡åº¦
                        existing.confidence = max(existing.confidence, relation.confidence)
                        break
        
        return unique_relations
    
    def _build_graph_structure(self, concepts: List[LegalConcept], 
                             relations: List[ConceptRelation]) -> None:
        """æ§‹å»ºåœ–çµæ§‹"""
        # æ·»åŠ ç¯€é»ž
        for concept in concepts:
            self.graph.add_node(
                concept.concept_id,
                concept_name=concept.concept_name,
                concept_type=concept.concept_type,
                content=concept.content,
                related_articles=concept.related_articles,
                importance_score=concept.importance_score
            )
        
        # æ·»åŠ é‚Š
        for relation in relations:
            if (relation.source_concept in self.graph.nodes and 
                relation.target_concept in self.graph.nodes):
                self.graph.add_edge(
                    relation.source_concept,
                    relation.target_concept,
                    relation_type=relation.relation_type,
                    confidence=relation.confidence,
                    evidence=relation.evidence
                )
    
    def _compute_concept_embeddings(self) -> None:
        """è¨ˆç®—æ¦‚å¿µåµŒå…¥"""
        # æº–å‚™æ¦‚å¿µæ–‡æœ¬
        concept_texts = []
        concept_ids = []
        
        for concept_id, concept in self.concepts.items():
            # çµ„åˆæ¦‚å¿µå…§å®¹å’Œç›¸é—œæ³•æ¢
            combined_text = f"{concept.content} {' '.join(concept.related_articles)}"
            concept_texts.append(combined_text)
            concept_ids.append(concept_id)
        
        # ä½¿ç”¨TF-IDFè¨ˆç®—åµŒå…¥
        vectorizer = TfidfVectorizer(max_features=1000, stop_words=None)
        embeddings = vectorizer.fit_transform(concept_texts).toarray()
        
        # æ›´æ–°æ¦‚å¿µåµŒå…¥
        for i, concept_id in enumerate(concept_ids):
            self.concepts[concept_id].semantic_embedding = embeddings[i]


class LegalConceptGraphRetrieval:
    """æ³•å¾‹æ¦‚å¿µåœ–æª¢ç´¢"""
    
    def __init__(self, concept_graph: LegalConceptGraph):
        self.concept_graph = concept_graph
        self.graph = concept_graph.graph
        self.concepts = concept_graph.concepts
    
    def retrieve(self, query: str, k: int = 5) -> List[Dict[str, Any]]:
        """åŸºæ–¼æ¦‚å¿µåœ–çš„æª¢ç´¢"""
        print(f"ðŸ” é–‹å§‹æ¦‚å¿µåœ–æª¢ç´¢ï¼ŒæŸ¥è©¢: '{query}'")
        
        # 1. æŸ¥è©¢æ¦‚å¿µåŒ–
        query_concepts = self._conceptualize_query(query)
        print(f"ðŸ” æŸ¥è©¢æ¦‚å¿µåŒ–çµæžœ: {[c.concept_name for c in query_concepts]}")
        
        # 2. æ¦‚å¿µåœ–æŽ¨ç†
        reasoning_paths = self._graph_reasoning(query_concepts)
        print(f"ðŸ” æŽ¨ç†è·¯å¾‘æ•¸é‡: {len(reasoning_paths)}")
        
        # 3. åŸºæ–¼æŽ¨ç†è·¯å¾‘æª¢ç´¢
        results = self._retrieve_by_reasoning_paths(reasoning_paths, k)
        print(f"ðŸ” æª¢ç´¢çµæžœæ•¸é‡: {len(results)}")
        
        return results
    
    def _conceptualize_query(self, query: str) -> List[LegalConcept]:
        """æŸ¥è©¢æ¦‚å¿µåŒ–"""
        query_concepts = []
        
        # ç›´æŽ¥åŒ¹é…æ¦‚å¿µå…§å®¹
        for concept_id, concept in self.concepts.items():
            if self._is_concept_relevant(query, concept):
                query_concepts.append(concept)
        
        # åŸºæ–¼æ¦‚å¿µåç¨±åŒ¹é…
        query_lower = query.lower()
        for concept_id, concept in self.concepts.items():
            if concept.concept_name in query_lower or any(
                keyword in query_lower for keyword in concept.concept_name.split('_')
            ):
                if concept not in query_concepts:
                    query_concepts.append(concept)
        
        return query_concepts
    
    def _is_concept_relevant(self, query: str, concept: LegalConcept) -> bool:
        """åˆ¤æ–·æ¦‚å¿µæ˜¯å¦èˆ‡æŸ¥è©¢ç›¸é—œ"""
        query_lower = query.lower()
        concept_content_lower = concept.content.lower()
        
        # ç›´æŽ¥å­—ç¬¦ä¸²åŒ¹é…
        if any(word in concept_content_lower for word in query_lower.split()):
            return True
        
        # æ³•å¾‹é—œéµè©žåŒ¹é…
        legal_keywords = ['è‘—ä½œæ¬Š', 'æŽˆæ¬Š', 'ç¿»è­¯', 'å‡ºç‰ˆ', 'æ¬Šåˆ©', 'æ³•å¾‹', 'æ¢æ–‡', 'æ³•è¦']
        query_keywords = [kw for kw in legal_keywords if kw in query_lower]
        concept_keywords = [kw for kw in legal_keywords if kw in concept_content_lower]
        
        if query_keywords and concept_keywords:
            overlap = set(query_keywords) & set(concept_keywords)
            if overlap:
                return True
        
        # è©žå½™é‡ç–Šåº¦è¨ˆç®—ï¼ˆæ”¾å¯¬é–¾å€¼ï¼‰
        query_words = set(query_lower.split())
        concept_words = set(concept_content_lower.split())
        overlap = len(query_words & concept_words)
        relevance_score = overlap / max(len(query_words), 1)
        
        return relevance_score > 0.1  # é™ä½Žé–¾å€¼
    
    def _graph_reasoning(self, query_concepts: List[LegalConcept]) -> List[List[str]]:
        """æ¦‚å¿µåœ–æŽ¨ç†"""
        reasoning_paths = []
        
        for concept in query_concepts:
            # å¾žæŸ¥è©¢æ¦‚å¿µé–‹å§‹çš„æŽ¨ç†è·¯å¾‘
            paths = self._find_reasoning_paths(concept.concept_id, max_depth=3)
            reasoning_paths.extend(paths)
        
        # åŽ»é‡ä¸¦æŽ’åº
        unique_paths = []
        seen_paths = set()
        
        for path in reasoning_paths:
            path_key = tuple(path)
            if path_key not in seen_paths:
                seen_paths.add(path_key)
                unique_paths.append(path)
        
        # æŒ‰è·¯å¾‘é•·åº¦å’Œé‡è¦æ€§æŽ’åº
        unique_paths.sort(key=lambda x: (len(x), self._calculate_path_importance(x)), reverse=True)
        
        return unique_paths[:10]  # è¿”å›žå‰10å€‹æœ€ç›¸é—œçš„è·¯å¾‘
    
    def _find_reasoning_paths(self, start_concept: str, max_depth: int = 3) -> List[List[str]]:
        """æ‰¾åˆ°æŽ¨ç†è·¯å¾‘"""
        paths = []
        
        def dfs(current_concept: str, current_path: List[str], depth: int):
            if depth > max_depth:
                return
            
            current_path.append(current_concept)
            
            if len(current_path) > 1:  # è‡³å°‘åŒ…å«å…©å€‹æ¦‚å¿µ
                paths.append(current_path.copy())
            
            # éæ­·é„°å±…ç¯€é»ž
            for neighbor in self.graph.neighbors(current_concept):
                if neighbor not in current_path:  # é¿å…å¾ªç’°
                    dfs(neighbor, current_path, depth + 1)
            
            current_path.pop()
        
        dfs(start_concept, [], 0)
        return paths
    
    def _calculate_path_importance(self, path: List[str]) -> float:
        """è¨ˆç®—è·¯å¾‘é‡è¦æ€§"""
        total_importance = 0.0
        
        for concept_id in path:
            if concept_id in self.concepts:
                total_importance += self.concepts[concept_id].importance_score
        
        # è€ƒæ…®è·¯å¾‘é•·åº¦
        length_penalty = 1.0 / len(path) if len(path) > 0 else 0.0
        
        return total_importance * length_penalty
    
    def _retrieve_by_reasoning_paths(self, reasoning_paths: List[List[str]], 
                                   k: int) -> List[Dict[str, Any]]:
        """åŸºæ–¼æŽ¨ç†è·¯å¾‘æª¢ç´¢"""
        results = []
        
        for path in reasoning_paths:
            if len(results) >= k:
                break
            
            # æ”¶é›†è·¯å¾‘ä¸­æ‰€æœ‰æ¦‚å¿µçš„ç›¸é—œå…§å®¹
            path_results = self._collect_path_results(path)
            
            for result in path_results:
                if len(results) >= k:
                    break
                
                # è¨ˆç®—æŽ¨ç†åˆ†æ•¸
                reasoning_score = self._calculate_reasoning_score(path, result)
                result['reasoning_score'] = reasoning_score
                result['reasoning_path'] = path
                
                results.append(result)
        
        # æŒ‰æŽ¨ç†åˆ†æ•¸æŽ’åº
        results.sort(key=lambda x: x['reasoning_score'], reverse=True)
        
        return results[:k]
    
    def _collect_path_results(self, path: List[str]) -> List[Dict[str, Any]]:
        """æ”¶é›†è·¯å¾‘çµæžœ"""
        results = []
        
        for concept_id in path:
            if concept_id in self.concepts:
                concept = self.concepts[concept_id]
                
                result = {
                    'content': concept.content,
                    'concept_id': concept_id,
                    'concept_name': concept.concept_name,
                    'concept_type': concept.concept_type,
                    'related_articles': concept.related_articles,
                    'importance_score': concept.importance_score,
                    'metadata': {
                        'strategy': 'concept_graph_retrieval',
                        'concept_based': True
                    }
                }
                
                results.append(result)
        
        return results
    
    def _calculate_reasoning_score(self, path: List[str], result: Dict[str, Any]) -> float:
        """è¨ˆç®—æŽ¨ç†åˆ†æ•¸"""
        # åŸºç¤Žåˆ†æ•¸ï¼šæ¦‚å¿µé‡è¦æ€§
        base_score = result.get('importance_score', 0.0)
        
        # è·¯å¾‘åˆ†æ•¸ï¼šè·¯å¾‘çš„é€£è²«æ€§å’Œé•·åº¦
        path_score = self._calculate_path_coherence(path)
        
        # çµ„åˆåˆ†æ•¸
        reasoning_score = base_score * 0.6 + path_score * 0.4
        
        return reasoning_score
    
    def _calculate_path_coherence(self, path: List[str]) -> float:
        """è¨ˆç®—è·¯å¾‘é€£è²«æ€§"""
        if len(path) < 2:
            return 0.0
        
        coherence_score = 0.0
        
        for i in range(len(path) - 1):
            source = path[i]
            target = path[i + 1]
            
            if self.graph.has_edge(source, target):
                edge_data = self.graph[source][target]
                confidence = edge_data.get('confidence', 0.5)
                coherence_score += confidence
        
        # å¹³å‡é€£è²«æ€§
        avg_coherence = coherence_score / max(len(path) - 1, 1)
        
        return avg_coherence
