"""
å¢å¼·ç‰ˆä¸»ç¨‹åº - æ•´åˆæ–°çš„æ³•å¾‹èªç¾©æª¢ç´¢åŠŸèƒ½
"""

from .main import *  # å°å…¥åŸæœ‰åŠŸèƒ½
from .legal_semantic_chunking import LegalSemanticIntegrityChunking, MultiLevelSemanticChunking
from .legal_concept_graph import LegalConceptGraph, LegalConceptGraphRetrieval
from .adaptive_legal_rag import AdaptiveLegalRAG, QueryAnalyzer
from typing import List, Dict, Any
import json


# åˆå§‹åŒ–å¢å¼·ç‰ˆçµ„ä»¶
legal_semantic_chunker = LegalSemanticIntegrityChunking()
multi_level_chunker = MultiLevelSemanticChunking()
concept_graph = LegalConceptGraph()
concept_graph_retrieval = None
adaptive_rag = AdaptiveLegalRAG()


@app.post("/api/legal-semantic-chunk")
def legal_semantic_chunk(req: ChunkConfig):
    """æ³•å¾‹èªç¾©å®Œæ•´æ€§åˆ†å¡Š"""
    try:
        doc = store.get_doc(req.doc_id)
        if not doc:
            return JSONResponse(status_code=404, content={"error": f"æ–‡æª” {req.doc_id} ä¸å­˜åœ¨"})
        
        print(f"ğŸ” é–‹å§‹æ³•å¾‹èªç¾©å®Œæ•´æ€§åˆ†å¡Šï¼Œæ–‡æª”: {doc.filename}")
        
        # ä½¿ç”¨æ³•å¾‹èªç¾©å®Œæ•´æ€§åˆ†å¡Š
        chunks_with_span = legal_semantic_chunker.chunk(
            doc.text,
            max_chunk_size=req.chunk_size,
            overlap_ratio=req.overlap_ratio,
            preserve_concepts=True
        )
        
        # æå–ç´”æ–‡æœ¬chunks
        chunks = [chunk["content"] for chunk in chunks_with_span]
        
        # æ›´æ–°æ–‡æª”è¨˜éŒ„
        doc.chunks = chunks
        doc.chunk_size = req.chunk_size
        doc.overlap = int(req.chunk_size * req.overlap_ratio)
        doc.structured_chunks = chunks_with_span
        doc.chunking_strategy = "legal_semantic_integrity"
        store.add_doc(doc)
        
        store.reset_embeddings()
        
        # è¨ˆç®—çµ±è¨ˆä¿¡æ¯
        chunk_lengths = [len(chunk) for chunk in chunks] if chunks else []
        avg_chunk_length = sum(chunk_lengths) / len(chunk_lengths) if chunk_lengths else 0
        min_length = min(chunk_lengths) if chunk_lengths else 0
        max_length = max(chunk_lengths) if chunk_lengths else 0
        
        if chunk_lengths:
            variance = sum((length - avg_chunk_length) ** 2 for length in chunk_lengths) / len(chunk_lengths)
        else:
            variance = 0
        
        # è¨ˆç®—æ¦‚å¿µå®Œæ•´æ€§çµ±è¨ˆ
        concept_stats = _calculate_concept_statistics(chunks_with_span)
        
        return {
            "doc_id": req.doc_id,
            "chunk_count": len(chunks),
            "avg_chunk_length": avg_chunk_length,
            "min_chunk_length": min_length,
            "max_chunk_length": max_length,
            "length_variance": variance,
            "strategy": "legal_semantic_integrity",
            "config": req.dict(),
            "chunks_with_span": chunks_with_span,
            "concept_statistics": concept_stats
        }
        
    except Exception as e:
        print(f"âŒ æ³•å¾‹èªç¾©åˆ†å¡ŠéŒ¯èª¤: {e}")
        return JSONResponse(status_code=500, content={"error": f"åˆ†å¡ŠéŒ¯èª¤: {str(e)}"})


@app.post("/api/multi-level-semantic-chunk")
def multi_level_semantic_chunk(req: ChunkConfig):
    """å¤šå±¤æ¬¡èªç¾©åˆ†å¡Š"""
    try:
        doc = store.get_doc(req.doc_id)
        if not doc:
            return JSONResponse(status_code=404, content={"error": f"æ–‡æª” {req.doc_id} ä¸å­˜åœ¨"})
        
        print(f"ğŸ” é–‹å§‹å¤šå±¤æ¬¡èªç¾©åˆ†å¡Šï¼Œæ–‡æª”: {doc.filename}")
        
        # ä½¿ç”¨å¤šå±¤æ¬¡èªç¾©åˆ†å¡Š
        multi_level_chunks = multi_level_chunker.chunk(
            doc.text,
            max_chunk_size=req.chunk_size,
            overlap_ratio=req.overlap_ratio
        )
        
        # ä¿å­˜å¤šå±¤æ¬¡åˆ†å¡Šçµæœ
        doc.multi_level_chunks = multi_level_chunks
        doc.chunking_strategy = "multi_level_semantic"
        store.add_doc(doc)
        
        store.reset_embeddings()
        
        # è¨ˆç®—å„å±¤æ¬¡çµ±è¨ˆ
        level_statistics = {}
        for level_name, level_chunks in multi_level_chunks.items():
            chunk_lengths = [len(chunk["content"]) for chunk in level_chunks]
            level_statistics[level_name] = {
                "chunk_count": len(level_chunks),
                "avg_length": sum(chunk_lengths) / len(chunk_lengths) if chunk_lengths else 0,
                "min_length": min(chunk_lengths) if chunk_lengths else 0,
                "max_length": max(chunk_lengths) if chunk_lengths else 0
            }
        
        return {
            "doc_id": req.doc_id,
            "strategy": "multi_level_semantic",
            "config": req.dict(),
            "multi_level_chunks": multi_level_chunks,
            "level_statistics": level_statistics
        }
        
    except Exception as e:
        print(f"âŒ å¤šå±¤æ¬¡èªç¾©åˆ†å¡ŠéŒ¯èª¤: {e}")
        return JSONResponse(status_code=500, content={"error": f"åˆ†å¡ŠéŒ¯èª¤: {str(e)}"})


@app.post("/api/build-concept-graph")
def build_concept_graph():
    """æ§‹å»ºæ³•å¾‹æ¦‚å¿µåœ–"""
    try:
        print("ğŸ”¨ é–‹å§‹æ§‹å»ºæ³•å¾‹æ¦‚å¿µåœ–...")
        
        # ç²å–æ‰€æœ‰æ–‡æª”
        docs = store.list_docs()
        if not docs:
            return JSONResponse(status_code=400, content={"error": "æ²’æœ‰æ–‡æª”å¯ç”¨"})
        
        # æº–å‚™æ–‡æª”æ•¸æ“š
        documents = []
        for doc in docs:
            if doc.chunks:
                for i, chunk in enumerate(doc.chunks):
                    documents.append({
                        'content': chunk,
                        'doc_id': doc.id,
                        'chunk_index': i,
                        'filename': doc.filename
                    })
        
        # æ§‹å»ºæ¦‚å¿µåœ–
        concept_graph.build_graph(documents)
        
        # åˆå§‹åŒ–æ¦‚å¿µåœ–æª¢ç´¢
        global concept_graph_retrieval
        concept_graph_retrieval = LegalConceptGraphRetrieval(concept_graph)
        
        # è¨»å†Šåˆ°è‡ªé©æ‡‰RAG
        adaptive_rag.register_strategy('concept_graph', concept_graph_retrieval)
        
        # ç²å–æ¦‚å¿µåœ–çµ±è¨ˆ
        graph_stats = {
            'node_count': concept_graph.graph.number_of_nodes(),
            'edge_count': concept_graph.graph.number_of_edges(),
            'concept_count': len(concept_graph.concepts),
            'relation_count': len(concept_graph.relations)
        }
        
        print(f"âœ… æ¦‚å¿µåœ–æ§‹å»ºå®Œæˆ: {graph_stats}")
        
        return {
            "status": "success",
            "message": "æ¦‚å¿µåœ–æ§‹å»ºå®Œæˆ",
            "statistics": graph_stats
        }
        
    except Exception as e:
        print(f"âŒ æ¦‚å¿µåœ–æ§‹å»ºéŒ¯èª¤: {e}")
        return JSONResponse(status_code=500, content={"error": f"æ§‹å»ºéŒ¯èª¤: {str(e)}"})


@app.post("/api/concept-graph-retrieve")
def concept_graph_retrieve(req: RetrieveRequest):
    """æ¦‚å¿µåœ–æª¢ç´¢"""
    if not concept_graph_retrieval:
        return JSONResponse(status_code=400, content={"error": "æ¦‚å¿µåœ–æœªæ§‹å»ºï¼Œè«‹å…ˆèª¿ç”¨ /api/build-concept-graph"})
    
    try:
        print(f"ğŸ” é–‹å§‹æ¦‚å¿µåœ–æª¢ç´¢ï¼ŒæŸ¥è©¢: '{req.query}'")
        
        # åŸ·è¡Œæ¦‚å¿µåœ–æª¢ç´¢
        results = concept_graph_retrieval.retrieve(req.query, req.k)
        
        # è¨ˆç®—æª¢ç´¢æŒ‡æ¨™
        metrics = calculate_retrieval_metrics(req.query, results, req.k)
        
        # æ·»åŠ æ¦‚å¿µåœ–ç‰¹å®šä¿¡æ¯
        metrics["concept_graph_analysis"] = {
            "reasoning_paths_used": len(set(r.get('reasoning_path', []) for r in results)),
            "concept_matches": len([r for r in results if r.get('concept_based', False)]),
            "avg_reasoning_score": sum(r.get('reasoning_score', 0) for r in results) / len(results) if results else 0
        }
        
        metrics["note"] = f"æ¦‚å¿µåœ–æª¢ç´¢: ä½¿ç”¨{metrics['concept_graph_analysis']['reasoning_paths_used']}æ¢æ¨ç†è·¯å¾‘"
        
        return {
            "results": results,
            "metrics": metrics,
            "embedding_provider": "concept_graph",
            "embedding_model": "legal_concept_reasoning"
        }
        
    except Exception as e:
        print(f"âŒ æ¦‚å¿µåœ–æª¢ç´¢éŒ¯èª¤: {e}")
        return JSONResponse(status_code=500, content={"error": f"æª¢ç´¢éŒ¯èª¤: {str(e)}"})


@app.post("/api/adaptive-retrieve")
def adaptive_retrieve(req: RetrieveRequest):
    """è‡ªé©æ‡‰æª¢ç´¢"""
    try:
        print(f"ğŸš€ é–‹å§‹è‡ªé©æ‡‰æª¢ç´¢ï¼ŒæŸ¥è©¢: '{req.query}'")
        
        # ç¢ºä¿æª¢ç´¢ç­–ç•¥å·²è¨»å†Š
        if not adaptive_rag.retrieval_strategies:
            _register_default_strategies()
        
        # åŸ·è¡Œè‡ªé©æ‡‰æª¢ç´¢
        results = adaptive_rag.retrieve(req.query, req.k)
        
        # è¨ˆç®—æª¢ç´¢æŒ‡æ¨™
        metrics = calculate_retrieval_metrics(req.query, results, req.k)
        
        # æ·»åŠ è‡ªé©æ‡‰æª¢ç´¢ç‰¹å®šä¿¡æ¯
        if results:
            first_result = results[0]
            contributing_strategies = first_result.get('contributing_strategies', [])
            strategy_count = first_result.get('strategy_count', 0)
            
            metrics["adaptive_analysis"] = {
                "strategies_used": list(set(contributing_strategies)),
                "strategy_count": strategy_count,
                "fusion_performed": first_result.get('metadata', {}).get('adaptive_fusion', False),
                "avg_fused_score": sum(r.get('fused_score', 0) for r in results) / len(results)
            }
            
            metrics["note"] = f"è‡ªé©æ‡‰æª¢ç´¢: èåˆ{strategy_count}å€‹ç­–ç•¥"
        
        return {
            "results": results,
            "metrics": metrics,
            "embedding_provider": "adaptive_rag",
            "embedding_model": "multi_strategy_fusion"
        }
        
    except Exception as e:
        print(f"âŒ è‡ªé©æ‡‰æª¢ç´¢éŒ¯èª¤: {e}")
        return JSONResponse(status_code=500, content={"error": f"æª¢ç´¢éŒ¯èª¤: {str(e)}"})


@app.get("/api/strategy-performance")
def get_strategy_performance():
    """ç²å–ç­–ç•¥æ€§èƒ½çµ±è¨ˆ"""
    try:
        performance = adaptive_rag.performance_monitor.get_strategy_performance()
        
        return {
            "strategy_performance": performance,
            "total_retrievals": len(adaptive_rag.performance_monitor.retrieval_history)
        }
        
    except Exception as e:
        print(f"âŒ ç²å–ç­–ç•¥æ€§èƒ½éŒ¯èª¤: {e}")
        return JSONResponse(status_code=500, content={"error": f"ç²å–æ€§èƒ½éŒ¯èª¤: {str(e)}"})


def _calculate_concept_statistics(chunks_with_span: List[Dict[str, Any]]) -> Dict[str, Any]:
    """è¨ˆç®—æ¦‚å¿µçµ±è¨ˆä¿¡æ¯"""
    stats = {
        "total_chunks": len(chunks_with_span),
        "concept_chunks": 0,
        "definition_chunks": 0,
        "exception_chunks": 0,
        "condition_chunks": 0,
        "avg_importance_score": 0.0,
        "concept_density": 0.0
    }
    
    total_importance = 0.0
    total_concept_count = 0
    
    for chunk in chunks_with_span:
        metadata = chunk.get("metadata", {})
        semantic_features = metadata.get("semantic_features", {})
        
        concept_count = semantic_features.get("concept_count", 0)
        importance_score = semantic_features.get("importance_score", 0.0)
        
        if concept_count > 0:
            stats["concept_chunks"] += 1
            total_importance += importance_score
            total_concept_count += concept_count
        
        if semantic_features.get("has_definition", False):
            stats["definition_chunks"] += 1
        
        if semantic_features.get("has_exception", False):
            stats["exception_chunks"] += 1
        
        if semantic_features.get("has_condition", False):
            stats["condition_chunks"] += 1
    
    if stats["concept_chunks"] > 0:
        stats["avg_importance_score"] = total_importance / stats["concept_chunks"]
    
    if len(chunks_with_span) > 0:
        stats["concept_density"] = total_concept_count / len(chunks_with_span)
    
    return stats


def _register_default_strategies():
    """è¨»å†Šé»˜èªæª¢ç´¢ç­–ç•¥"""
    # è¨»å†Šå‘é‡æª¢ç´¢
    adaptive_rag.register_strategy('vector_search', {
        'retrieve': lambda query, **kwargs: retrieve_original(query, kwargs.get('k', 5))
    })
    
    # è¨»å†ŠHybridRAG
    adaptive_rag.register_strategy('hybrid_rag', {
        'retrieve': lambda query, **kwargs: hybrid_retrieve_original(query, kwargs.get('k', 5))
    })
    
    # è¨»å†Šå¤šå±¤æ¬¡æª¢ç´¢
    adaptive_rag.register_strategy('hierarchical', {
        'retrieve': lambda query, **kwargs: hierarchical_retrieve_original(query, kwargs.get('k', 5))
    })


def retrieve_original(query: str, k: int):
    """åŸå§‹å‘é‡æª¢ç´¢"""
    # é€™è£¡èª¿ç”¨åŸæœ‰çš„æª¢ç´¢é‚è¼¯
    pass


def hybrid_retrieve_original(query: str, k: int):
    """åŸå§‹HybridRAGæª¢ç´¢"""
    # é€™è£¡èª¿ç”¨åŸæœ‰çš„HybridRAGé‚è¼¯
    pass


def hierarchical_retrieve_original(query: str, k: int):
    """åŸå§‹å¤šå±¤æ¬¡æª¢ç´¢"""
    # é€™è£¡èª¿ç”¨åŸæœ‰çš„å¤šå±¤æ¬¡æª¢ç´¢é‚è¼¯
    pass
