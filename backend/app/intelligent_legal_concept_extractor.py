"""
æ™ºèƒ½æ³•å¾‹æ¦‚å¿µæå–å™¨ - è‡ªå‹•å¾æ³•å¾‹æ–‡æª”ä¸­æå–æ¦‚å¿µå’Œé—œä¿‚
"""

import re
import json
from typing import Dict, List, Any, Set, Tuple
from dataclasses import dataclass, asdict
from collections import defaultdict, Counter
import jieba
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np


@dataclass
class LegalConceptPattern:
    """æ³•å¾‹æ¦‚å¿µæ¨¡å¼"""
    pattern_type: str  # æ¦‚å¿µé¡å‹ï¼šå®šç¾©ã€æ¬Šåˆ©ã€ç¾©å‹™ã€ä¾‹å¤–ç­‰
    pattern_regex: str  # æ­£å‰‡è¡¨é”å¼æ¨¡å¼
    concept_category: str  # æ¦‚å¿µåˆ†é¡ï¼šè‘—ä½œæ¬Šã€å•†æ¨™æ¬Šç­‰
    importance_weight: float  # é‡è¦æ€§æ¬Šé‡


@dataclass
class ExtractedConcept:
    """æå–çš„æ³•å¾‹æ¦‚å¿µ"""
    concept_id: str
    concept_name: str
    concept_type: str  # å®šç¾©ã€æ¬Šåˆ©ã€ç¾©å‹™ã€ä¾‹å¤–ã€æ¢ä»¶ç­‰
    source_text: str
    context: str
    related_articles: List[str]
    legal_domain: str
    confidence: float
    synonyms: List[str]
    related_concepts: List[str]


@dataclass
class ConceptRelation:
    """æ¦‚å¿µé—œä¿‚"""
    source_concept: str
    target_concept: str
    relation_type: str  # åŒ…å«ã€é©ç”¨æ–¼ã€å‰æã€ä¾‹å¤–ç­‰
    confidence: float
    evidence: str


class IntelligentLegalConceptExtractor:
    """æ™ºèƒ½æ³•å¾‹æ¦‚å¿µæå–å™¨"""
    
    def __init__(self):
        self.concept_patterns = self._initialize_concept_patterns()
        self.legal_domains = self._initialize_legal_domains()
        self.extracted_concepts = {}
        self.concept_relations = []
        self.article_concepts = defaultdict(list)  # æ¢æ–‡ -> æ¦‚å¿µæ˜ å°„
        
    def _initialize_concept_patterns(self) -> List[LegalConceptPattern]:
        """åˆå§‹åŒ–æ³•å¾‹æ¦‚å¿µæ¨¡å¼"""
        return [
            # æ¬Šåˆ©å®šç¾©æ¨¡å¼
            LegalConceptPattern(
                pattern_type="æ¬Šåˆ©å®šç¾©",
                pattern_regex=r"([^ã€‚ï¼Œï¼›ï¼š]+)(?:å°ˆæœ‰|äº«æœ‰|å…·æœ‰)([^ã€‚ï¼Œï¼›ï¼š]*)æ¬Šåˆ©",
                concept_category="æ¬Šåˆ©",
                importance_weight=0.9
            ),
            LegalConceptPattern(
                pattern_type="æ¬Šåˆ©å®šç¾©2",
                pattern_regex=r"([^ã€‚ï¼Œï¼›ï¼š]+)æ¬Š[åˆ©]?(?:æ˜¯æŒ‡|ä¿‚æŒ‡|ç‚º)([^ã€‚ï¼Œï¼›ï¼š]*)",
                concept_category="æ¬Šåˆ©",
                importance_weight=0.9
            ),
            
            # ç¾©å‹™å®šç¾©æ¨¡å¼
            LegalConceptPattern(
                pattern_type="ç¾©å‹™å®šç¾©",
                pattern_regex=r"([^ã€‚ï¼Œï¼›ï¼š]+)(?:æ‡‰|å¿…é ˆ|å¾—|å¯ä»¥)([^ã€‚ï¼Œï¼›ï¼š]*)",
                concept_category="ç¾©å‹™",
                importance_weight=0.8
            ),
            
            # ä¾‹å¤–æ¢ä»¶æ¨¡å¼
            LegalConceptPattern(
                pattern_type="ä¾‹å¤–æ¢ä»¶",
                pattern_regex=r"(?:ä½†|æƒŸ|ä¾‹å¤–|é™¤å¤–)([^ã€‚ï¼Œï¼›ï¼š]*)",
                concept_category="ä¾‹å¤–",
                importance_weight=0.7
            ),
            
            # æ³•å¾‹å¾Œæœæ¨¡å¼
            LegalConceptPattern(
                pattern_type="æ³•å¾‹å¾Œæœ",
                pattern_regex=r"(?:é•å|ä¾µå®³|ä¾µçŠ¯)([^ã€‚ï¼Œï¼›ï¼š]*)(?:è€…|æ™‚)([^ã€‚ï¼Œï¼›ï¼š]*)",
                concept_category="å¾Œæœ",
                importance_weight=0.8
            ),
            
            # é©ç”¨æ¢ä»¶æ¨¡å¼
            LegalConceptPattern(
                pattern_type="é©ç”¨æ¢ä»¶",
                pattern_regex=r"(?:æ–¼|åœ¨)([^ã€‚ï¼Œï¼›ï¼š]*)(?:æƒ…å½¢|æƒ…æ³|æ¢ä»¶)([^ã€‚ï¼Œï¼›ï¼š]*)",
                concept_category="æ¢ä»¶",
                importance_weight=0.6
            )
        ]
    
    def _initialize_legal_domains(self) -> Dict[str, List[str]]:
        """åˆå§‹åŒ–æ³•å¾‹é ˜åŸŸé—œéµè©"""
        return {
            "è‘—ä½œæ¬Š": [
                "è‘—ä½œ", "è‘—ä½œæ¬Š", "ç‰ˆæ¬Š", "é‡è£½", "æ”¹ä½œ", "æ•£å¸ƒ", "å…¬é–‹å‚³è¼¸", 
                "å…¬é–‹æ¼”å‡º", "å…¬é–‹å±•ç¤º", "å‡ºç§Ÿ", "è¡ç”Ÿè‘—ä½œ", "ç·¨è¼¯è‘—ä½œ"
            ],
            "å•†æ¨™æ¬Š": [
                "å•†æ¨™", "å•†æ¨™æ¬Š", "è¨»å†Š", "ä½¿ç”¨", "è¿‘ä¼¼", "æ··æ·†", "è‘—åå•†æ¨™"
            ],
            "å°ˆåˆ©æ¬Š": [
                "å°ˆåˆ©", "ç™¼æ˜", "æ–°å‹", "æ–°å¼æ¨£", "æŠ€è¡“", "å‰µæ–°", "å¯¦æ–½"
            ]
        }
    
    def extract_concepts_from_documents(self, documents: List[Dict[str, Any]]) -> Dict[str, Any]:
        """å¾æ–‡æª”ä¸­æå–æ³•å¾‹æ¦‚å¿µ"""
        print("ğŸ” é–‹å§‹æ™ºèƒ½æ³•å¾‹æ¦‚å¿µæå–...")
        
        all_concepts = []
        all_relations = []
        
        for doc in documents:
            print(f"ğŸ“„ è™•ç†æ–‡æª”: {doc.get('filename', 'Unknown')}")
            
            # æŒ‰æ¢æ–‡åˆ†çµ„è™•ç†
            if 'structured_chunks' in doc and doc['structured_chunks']:
                for chunk in doc['structured_chunks']:
                    concepts = self._extract_concepts_from_chunk(chunk, doc)
                    all_concepts.extend(concepts)
                    
                    # æå–æ¦‚å¿µé—œä¿‚
                    relations = self._extract_concept_relations(chunk, concepts)
                    all_relations.extend(relations)
        
        # æ¦‚å¿µå»é‡å’Œåˆä½µ
        merged_concepts = self._merge_duplicate_concepts(all_concepts)
        
        # å»ºç«‹æ¦‚å¿µé—œä¿‚ç¶²çµ¡
        concept_network = self._build_concept_network(merged_concepts, all_relations)
        
        # å»ºç«‹æ¢æ–‡-æ¦‚å¿µæ˜ å°„
        article_concept_mapping = self._build_article_concept_mapping(merged_concepts)
        
        # ç”Ÿæˆæ¦‚å¿µæ¨ç†è¦å‰‡
        reasoning_rules = self._generate_reasoning_rules(merged_concepts, all_relations)
        
        result = {
            "extracted_concepts": merged_concepts,
            "concept_relations": all_relations,
            "concept_network": concept_network,
            "article_concept_mapping": article_concept_mapping,
            "reasoning_rules": reasoning_rules,
            "statistics": {
                "total_concepts": len(merged_concepts),
                "total_relations": len(all_relations),
                "concept_types": self._get_concept_type_statistics(merged_concepts),
                "legal_domains": self._get_domain_statistics(merged_concepts)
            }
        }
        
        print(f"âœ… æ¦‚å¿µæå–å®Œæˆ: {len(merged_concepts)}å€‹æ¦‚å¿µ, {len(all_relations)}å€‹é—œä¿‚")
        return result
    
    def _extract_concepts_from_chunk(self, chunk: Dict[str, Any], doc: Dict[str, Any]) -> List[ExtractedConcept]:
        """å¾åˆ†å¡Šä¸­æå–æ¦‚å¿µ"""
        concepts = []
        content = chunk.get('content', '')
        metadata = chunk.get('metadata', {})
        
        # æå–æ¢æ–‡ä¿¡æ¯
        article_info = self._extract_article_info(content, metadata)
        
        # æ‡‰ç”¨æ¦‚å¿µæ¨¡å¼
        for pattern in self.concept_patterns:
            matches = re.finditer(pattern.pattern_regex, content, re.MULTILINE)
            
            for match in matches:
                concept = self._create_concept_from_match(
                    match, pattern, content, article_info, doc
                )
                if concept:
                    concepts.append(concept)
        
        # æå–æ³•å¾‹é—œéµè©
        legal_keywords = self._extract_legal_keywords(content)
        for keyword, domain in legal_keywords:
            concept = ExtractedConcept(
                concept_id=f"keyword_{keyword}_{len(concepts)}",
                concept_name=keyword,
                concept_type="é—œéµè©",
                source_text=content,
                context=self._get_context_around_keyword(content, keyword),
                related_articles=article_info.get('articles', []),
                legal_domain=domain,
                confidence=0.6,
                synonyms=[],
                related_concepts=[]
            )
            concepts.append(concept)
        
        return concepts
    
    def _extract_concept_relations(self, chunk: Dict[str, Any], concepts: List[ExtractedConcept]) -> List[ConceptRelation]:
        """æå–æ¦‚å¿µé—œä¿‚"""
        relations = []
        content = chunk.get('content', '')
        
        # æ¬Šåˆ©-ç¾©å‹™é—œä¿‚
        rights_obligations = self._extract_rights_obligations_relations(content, concepts)
        relations.extend(rights_obligations)
        
        # åŒ…å«é—œä¿‚
        inclusion_relations = self._extract_inclusion_relations(content, concepts)
        relations.extend(inclusion_relations)
        
        # æ¢ä»¶é—œä¿‚
        condition_relations = self._extract_condition_relations(content, concepts)
        relations.extend(condition_relations)
        
        return relations
    
    def _create_concept_from_match(self, match, pattern: LegalConceptPattern, 
                                 content: str, article_info: Dict, doc: Dict) -> ExtractedConcept:
        """å¾åŒ¹é…çµæœå‰µå»ºæ¦‚å¿µ"""
        try:
            # æå–æ¦‚å¿µåç¨±
            concept_name = match.group(1).strip() if match.groups() else match.group(0).strip()
            
            # æ¸…ç†æ¦‚å¿µåç¨±
            concept_name = self._clean_concept_name(concept_name)
            
            if not concept_name or len(concept_name) < 2:
                return None
            
            # ç”Ÿæˆæ¦‚å¿µID
            concept_id = f"{pattern.concept_category}_{concept_name}_{hash(concept_name) % 10000}"
            
            # ç²å–ä¸Šä¸‹æ–‡
            context = self._get_context_around_match(content, match)
            
            # è­˜åˆ¥æ³•å¾‹é ˜åŸŸ
            legal_domain = self._identify_legal_domain(content)
            
            # æå–åŒç¾©è©
            synonyms = self._extract_synonyms(concept_name, content)
            
            concept = ExtractedConcept(
                concept_id=concept_id,
                concept_name=concept_name,
                concept_type=pattern.pattern_type,
                source_text=match.group(0),
                context=context,
                related_articles=article_info.get('articles', []),
                legal_domain=legal_domain,
                confidence=pattern.importance_weight,
                synonyms=synonyms,
                related_concepts=[]
            )
            
            return concept
            
        except Exception as e:
            print(f"âš ï¸ å‰µå»ºæ¦‚å¿µå¤±æ•—: {e}")
            return None
    
    def _extract_article_info(self, content: str, metadata: Dict[str, Any]) -> Dict[str, Any]:
        """æå–æ¢æ–‡ä¿¡æ¯"""
        articles = []
        
        # å¾metadataæå–
        if metadata.get('article'):
            articles.append(f"ç¬¬{metadata['article']}æ¢")
        
        # å¾å…§å®¹æå–
        article_matches = re.findall(r'ç¬¬(\d+(?:-\d+)?)æ¢', content)
        articles.extend([f"ç¬¬{art}æ¢" for art in article_matches])
        
        return {
            "articles": list(set(articles)),
            "chapter": metadata.get('chapter', ''),
            "section": metadata.get('section', '')
        }
    
    def _clean_concept_name(self, name: str) -> str:
        """æ¸…ç†æ¦‚å¿µåç¨±"""
        # ç§»é™¤å¤šé¤˜çš„æ¨™é»ç¬¦è™Ÿå’Œç©ºæ ¼
        name = re.sub(r'[ï¼Œã€‚ï¼›ï¼šï¼ï¼Ÿ\s]+', '', name)
        
        # ç§»é™¤å¸¸è¦‹çš„ç„¡æ„ç¾©è©
        meaningless_words = ['çš„', 'åœ¨', 'æ–¼', 'ç‚º', 'æ˜¯', 'æœ‰', 'åŠ', 'èˆ‡', 'æˆ–', 'ä½†', 'æƒŸ']
        for word in meaningless_words:
            name = name.replace(word, '')
        
        return name.strip()
    
    def _get_context_around_match(self, content: str, match) -> str:
        """ç²å–åŒ¹é…é …å‘¨åœçš„ä¸Šä¸‹æ–‡"""
        start = max(0, match.start() - 50)
        end = min(len(content), match.end() + 50)
        return content[start:end]
    
    def _get_context_around_keyword(self, content: str, keyword: str) -> str:
        """ç²å–é—œéµè©å‘¨åœçš„ä¸Šä¸‹æ–‡"""
        idx = content.find(keyword)
        if idx == -1:
            return content[:100]
        
        start = max(0, idx - 50)
        end = min(len(content), idx + len(keyword) + 50)
        return content[start:end]
    
    def _identify_legal_domain(self, content: str) -> str:
        """è­˜åˆ¥æ³•å¾‹é ˜åŸŸ"""
        content_lower = content.lower()
        
        for domain, keywords in self.legal_domains.items():
            for keyword in keywords:
                if keyword in content_lower:
                    return domain
        
        return "å…¶ä»–"
    
    def _extract_synonyms(self, concept_name: str, content: str) -> List[str]:
        """æå–åŒç¾©è©"""
        synonyms = []
        
        # å¸¸è¦‹çš„åŒç¾©è©æ¨¡å¼
        synonym_patterns = [
            r'([^ï¼Œã€‚ï¼›ï¼š]*)(?:äº¦ç¨±|åˆç¨±|åˆ¥ç¨±|ä¿—ç¨±)([^ï¼Œã€‚ï¼›ï¼š]*)',
            r'([^ï¼Œã€‚ï¼›ï¼š]*)(?:åŒ…æ‹¬|å«|æ¶µè“‹)([^ï¼Œã€‚ï¼›ï¼š]*)',
        ]
        
        for pattern in synonym_patterns:
            matches = re.finditer(pattern, content)
            for match in matches:
                if concept_name in match.group(0):
                    # æå–å¯èƒ½çš„åŒç¾©è©
                    potential_synonyms = [group.strip() for group in match.groups() if group.strip()]
                    synonyms.extend(potential_synonyms)
        
        return list(set(synonyms))
    
    def _extract_legal_keywords(self, content: str) -> List[Tuple[str, str]]:
        """æå–æ³•å¾‹é—œéµè©"""
        keywords = []
        content_lower = content.lower()
        
        for domain, domain_keywords in self.legal_domains.items():
            for keyword in domain_keywords:
                if keyword in content_lower:
                    keywords.append((keyword, domain))
        
        return keywords
    
    def _extract_rights_obligations_relations(self, content: str, concepts: List[ExtractedConcept]) -> List[ConceptRelation]:
        """æå–æ¬Šåˆ©-ç¾©å‹™é—œä¿‚"""
        relations = []
        
        # æ¬Šåˆ©-ç¾©å‹™é—œä¿‚æ¨¡å¼
        patterns = [
            r'([^ï¼Œã€‚ï¼›ï¼š]+)å°ˆæœ‰([^ï¼Œã€‚ï¼›ï¼š]+)æ¬Šåˆ©',
            r'([^ï¼Œã€‚ï¼›ï¼š]+)äº«æœ‰([^ï¼Œã€‚ï¼›ï¼š]+)æ¬Šåˆ©',
            r'([^ï¼Œã€‚ï¼›ï¼š]+)æ‡‰([^ï¼Œã€‚ï¼›ï¼š]+)',
            r'([^ï¼Œã€‚ï¼›ï¼š]+)å¿…é ˆ([^ï¼Œã€‚ï¼›ï¼š]+)'
        ]
        
        for pattern in patterns:
            matches = re.finditer(pattern, content)
            for match in matches:
                if len(match.groups()) >= 2:
                    source = match.group(1).strip()
                    target = match.group(2).strip()
                    
                    if source and target:
                        relation = ConceptRelation(
                            source_concept=source,
                            target_concept=target,
                            relation_type="æ¬Šåˆ©ç¾©å‹™",
                            confidence=0.8,
                            evidence=match.group(0)
                        )
                        relations.append(relation)
        
        return relations
    
    def _extract_inclusion_relations(self, content: str, concepts: List[ExtractedConcept]) -> List[ConceptRelation]:
        """æå–åŒ…å«é—œä¿‚"""
        relations = []
        
        # åŒ…å«é—œä¿‚æ¨¡å¼
        patterns = [
            r'([^ï¼Œã€‚ï¼›ï¼š]+)åŒ…æ‹¬([^ï¼Œã€‚ï¼›ï¼š]+)',
            r'([^ï¼Œã€‚ï¼›ï¼š]+)å«([^ï¼Œã€‚ï¼›ï¼š]+)',
            r'([^ï¼Œã€‚ï¼›ï¼š]+)æ¶µè“‹([^ï¼Œã€‚ï¼›ï¼š]+)'
        ]
        
        for pattern in patterns:
            matches = re.finditer(pattern, content)
            for match in matches:
                if len(match.groups()) >= 2:
                    source = match.group(1).strip()
                    target = match.group(2).strip()
                    
                    if source and target:
                        relation = ConceptRelation(
                            source_concept=source,
                            target_concept=target,
                            relation_type="åŒ…å«",
                            confidence=0.7,
                            evidence=match.group(0)
                        )
                        relations.append(relation)
        
        return relations
    
    def _extract_condition_relations(self, content: str, concepts: List[ExtractedConcept]) -> List[ConceptRelation]:
        """æå–æ¢ä»¶é—œä¿‚"""
        relations = []
        
        # æ¢ä»¶é—œä¿‚æ¨¡å¼
        patterns = [
            r'([^ï¼Œã€‚ï¼›ï¼š]+)æ™‚([^ï¼Œã€‚ï¼›ï¼š]+)',
            r'([^ï¼Œã€‚ï¼›ï¼š]+)æƒ…å½¢([^ï¼Œã€‚ï¼›ï¼š]+)',
            r'([^ï¼Œã€‚ï¼›ï¼š]+)æ¢ä»¶([^ï¼Œã€‚ï¼›ï¼š]+)'
        ]
        
        for pattern in patterns:
            matches = re.finditer(pattern, content)
            for match in matches:
                if len(match.groups()) >= 2:
                    source = match.group(1).strip()
                    target = match.group(2).strip()
                    
                    if source and target:
                        relation = ConceptRelation(
                            source_concept=source,
                            target_concept=target,
                            relation_type="æ¢ä»¶",
                            confidence=0.6,
                            evidence=match.group(0)
                        )
                        relations.append(relation)
        
        return relations
    
    def _merge_duplicate_concepts(self, concepts: List[ExtractedConcept]) -> List[ExtractedConcept]:
        """åˆä½µé‡è¤‡æ¦‚å¿µ"""
        concept_groups = defaultdict(list)
        
        # æŒ‰æ¦‚å¿µåç¨±åˆ†çµ„
        for concept in concepts:
            key = concept.concept_name.lower().strip()
            concept_groups[key].append(concept)
        
        merged_concepts = []
        
        for group in concept_groups.values():
            if len(group) == 1:
                merged_concepts.append(group[0])
            else:
                # åˆä½µå¤šå€‹ç›¸åŒæ¦‚å¿µ
                merged = self._merge_concept_group(group)
                merged_concepts.append(merged)
        
        return merged_concepts
    
    def _merge_concept_group(self, concepts: List[ExtractedConcept]) -> ExtractedConcept:
        """åˆä½µæ¦‚å¿µçµ„"""
        # é¸æ“‡ç½®ä¿¡åº¦æœ€é«˜çš„ä½œç‚ºä¸»æ¦‚å¿µ
        main_concept = max(concepts, key=lambda x: x.confidence)
        
        # åˆä½µå…¶ä»–å±¬æ€§
        all_synonyms = []
        all_articles = []
        all_contexts = []
        
        for concept in concepts:
            all_synonyms.extend(concept.synonyms)
            all_articles.extend(concept.related_articles)
            all_contexts.append(concept.context)
        
        main_concept.synonyms = list(set(all_synonyms))
        main_concept.related_articles = list(set(all_articles))
        main_concept.context = " | ".join(all_contexts[:3])  # ä¿ç•™å‰3å€‹ä¸Šä¸‹æ–‡
        
        return main_concept
    
    def _build_concept_network(self, concepts: List[ExtractedConcept], 
                             relations: List[ConceptRelation]) -> Dict[str, Any]:
        """å»ºç«‹æ¦‚å¿µç¶²çµ¡"""
        network = {
            "nodes": [],
            "edges": [],
            "statistics": {}
        }
        
        # æ·»åŠ ç¯€é»
        for concept in concepts:
            network["nodes"].append({
                "id": concept.concept_id,
                "name": concept.concept_name,
                "type": concept.concept_type,
                "domain": concept.legal_domain,
                "confidence": concept.confidence,
                "article_count": len(concept.related_articles)
            })
        
        # æ·»åŠ é‚Š
        for relation in relations:
            network["edges"].append({
                "source": relation.source_concept,
                "target": relation.target_concept,
                "type": relation.relation_type,
                "confidence": relation.confidence,
                "evidence": relation.evidence
            })
        
        # çµ±è¨ˆä¿¡æ¯
        network["statistics"] = {
            "total_nodes": len(network["nodes"]),
            "total_edges": len(network["edges"]),
            "avg_confidence": np.mean([node["confidence"] for node in network["nodes"]]),
            "domain_distribution": self._get_domain_distribution(network["nodes"])
        }
        
        return network
    
    def _build_article_concept_mapping(self, concepts: List[ExtractedConcept]) -> Dict[str, List[str]]:
        """å»ºç«‹æ¢æ–‡-æ¦‚å¿µæ˜ å°„"""
        mapping = defaultdict(list)
        
        for concept in concepts:
            for article in concept.related_articles:
                mapping[article].append({
                    "concept_name": concept.concept_name,
                    "concept_type": concept.concept_type,
                    "confidence": concept.confidence,
                    "context": concept.context[:100]
                })
        
        return dict(mapping)
    
    def _generate_reasoning_rules(self, concepts: List[ExtractedConcept], 
                                relations: List[ConceptRelation]) -> List[Dict[str, Any]]:
        """ç”Ÿæˆæ¨ç†è¦å‰‡"""
        rules = []
        
        # åŸºæ–¼æ¦‚å¿µé—œä¿‚ç”Ÿæˆæ¨ç†è¦å‰‡
        for relation in relations:
            rule = {
                "rule_id": f"rule_{len(rules)}",
                "condition": relation.source_concept,
                "conclusion": relation.target_concept,
                "relation_type": relation.relation_type,
                "confidence": relation.confidence,
                "evidence": relation.evidence
            }
            rules.append(rule)
        
        # åŸºæ–¼æ¦‚å¿µé¡å‹ç”Ÿæˆæ¨ç†è¦å‰‡
        concept_types = defaultdict(list)
        for concept in concepts:
            concept_types[concept.concept_type].append(concept)
        
        for concept_type, type_concepts in concept_types.items():
            if len(type_concepts) > 1:
                rule = {
                    "rule_id": f"type_rule_{concept_type}",
                    "condition": f"æŸ¥è©¢åŒ…å«{concept_type}ç›¸é—œè©å½™",
                    "conclusion": f"ç›¸é—œæ–¼{concept_type}æ¦‚å¿µ",
                    "relation_type": "é¡å‹æ¨ç†",
                    "confidence": 0.7,
                    "evidence": f"åŸºæ–¼{len(type_concepts)}å€‹{concept_type}æ¦‚å¿µ"
                }
                rules.append(rule)
        
        return rules
    
    def _get_concept_type_statistics(self, concepts: List[ExtractedConcept]) -> Dict[str, int]:
        """ç²å–æ¦‚å¿µé¡å‹çµ±è¨ˆ"""
        type_counts = defaultdict(int)
        for concept in concepts:
            type_counts[concept.concept_type] += 1
        return dict(type_counts)
    
    def _get_domain_statistics(self, concepts: List[ExtractedConcept]) -> Dict[str, int]:
        """ç²å–é ˜åŸŸçµ±è¨ˆ"""
        domain_counts = defaultdict(int)
        for concept in concepts:
            domain_counts[concept.legal_domain] += 1
        return dict(domain_counts)
    
    def _get_domain_distribution(self, nodes: List[Dict[str, Any]]) -> Dict[str, int]:
        """ç²å–é ˜åŸŸåˆ†å¸ƒ"""
        domain_counts = defaultdict(int)
        for node in nodes:
            domain_counts[node["domain"]] += 1
        return dict(domain_counts)


# å…¨å±€æå–å™¨å¯¦ä¾‹
intelligent_extractor = IntelligentLegalConceptExtractor()
