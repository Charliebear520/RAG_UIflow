"""
æ³•å¾‹èªç¾©å®Œæ•´æ€§åˆ†å¡Šç­–ç•¥
åŸºæ–¼æ³•å¾‹æ¦‚å¿µçš„å®Œæ•´æ€§å’Œèªç¾©é‚Šç•Œé€²è¡Œåˆ†å¡Š
"""

import re
from typing import List, Dict, Any, Tuple, Optional
from dataclasses import dataclass
from abc import ABC, abstractmethod


@dataclass
class LegalConcept:
    """æ³•å¾‹æ¦‚å¿µæ•¸æ“šçµæ§‹"""
    concept_type: str  # 'definition', 'right', 'exception', 'condition', 'procedure'
    content: str
    start_pos: int
    end_pos: int
    importance_score: float
    related_articles: List[str]
    semantic_boundaries: List[int]


class LegalSemanticIntegrityChunking:
    """æ³•å¾‹èªç¾©å®Œæ•´æ€§åˆ†å¡Šç­–ç•¥"""
    
    def __init__(self):
        # æ³•å¾‹æ¦‚å¿µå®Œæ•´æ€§è¦å‰‡
        self.legal_concept_patterns = {
            'definition': [
                r'æœ¬æ³•æ‰€ç¨±.*?æ˜¯æŒ‡.*?(?=ç¬¬|$|æœ¬æ³•|å‰é …|å¾Œé …)',
                r'.*?æŒ‡.*?è€….*?(?=ç¬¬|$|æœ¬æ³•|å‰é …|å¾Œé …)',
                r'.*?ç‚º.*?è€….*?(?=ç¬¬|$|æœ¬æ³•|å‰é …|å¾Œé …)'
            ],
            'right_definition': [
                r'è‘—ä½œäººå°ˆæœ‰.*?æ¬Šåˆ©.*?(?=ç¬¬|$|æœ¬æ³•|å‰é …|å¾Œé …)',
                r'å•†æ¨™æ¬Šäºº.*?æ¬Šåˆ©.*?(?=ç¬¬|$|æœ¬æ³•|å‰é …|å¾Œé …)',
                r'.*?å°ˆæœ‰.*?æ¬Šåˆ©.*?(?=ç¬¬|$|æœ¬æ³•|å‰é …|å¾Œé …)'
            ],
            'exception_clause': [
                r'ä½†.*?(?=ç¬¬|$|æœ¬æ³•|å‰é …|å¾Œé …)',
                r'ä¸åœ¨æ­¤é™.*?(?=ç¬¬|$|æœ¬æ³•|å‰é …|å¾Œé …)',
                r'é™¤å¤–.*?(?=ç¬¬|$|æœ¬æ³•|å‰é …|å¾Œé …)'
            ],
            'conditional_clause': [
                r'æœ‰ä¸‹åˆ—æƒ…å½¢ä¹‹ä¸€.*?(?=ç¬¬|$|æœ¬æ³•|å‰é …|å¾Œé …)',
                r'å¦‚.*?æ™‚.*?(?=ç¬¬|$|æœ¬æ³•|å‰é …|å¾Œé …)',
                r'æ–¼.*?æƒ…å½¢.*?(?=ç¬¬|$|æœ¬æ³•|å‰é …|å¾Œé …)'
            ],
            'procedural_clause': [
                r'æ‡‰.*?ç”³è«‹.*?(?=ç¬¬|$|æœ¬æ³•|å‰é …|å¾Œé …)',
                r'å¾—.*?è¾¦ç†.*?(?=ç¬¬|$|æœ¬æ³•|å‰é …|å¾Œé …)',
                r'ä¾.*?ç¨‹åº.*?(?=ç¬¬|$|æœ¬æ³•|å‰é …|å¾Œé …)'
            ]
        }
        
        # èªç¾©é‚Šç•Œæ¨™è¨˜
        self.semantic_boundaries = {
            'concept_shift': ['æœ¬æ³•', 'å‰é …', 'å¾Œé …', 'ä½†', 'å› æ­¤', 'æ‰€ä»¥'],
            'logical_connector': ['å› æ­¤', 'æ‰€ä»¥', 'ä½†æ˜¯', 'ç„¶è€Œ', 'å¦å¤–', 'æ­¤å¤–'],
            'legal_structure': ['ç¬¬.*æ¢', 'ç¬¬.*ç« ', 'ç¬¬.*ç¯€', 'ç¬¬.*é …'],
            'scope_marker': ['åœ¨.*ç¯„åœå…§', 'é™¤.*å¤–', 'ä¸é©ç”¨æ–¼']
        }
        
        # æ³•å¾‹æ¦‚å¿µé‡è¦æ€§æ¬Šé‡
        self.concept_importance_weights = {
            'definition': 1.0,      # å®šç¾©æœ€é‡è¦
            'right_definition': 0.9, # æ¬Šåˆ©å®šç¾©
            'exception_clause': 0.8, # ä¾‹å¤–æ¢æ¬¾
            'conditional_clause': 0.7, # æ¢ä»¶æ¢æ¬¾
            'procedural_clause': 0.6   # ç¨‹åºæ¢æ¬¾
        }
    
    def chunk(self, text: str, max_chunk_size: int = 1000, 
              overlap_ratio: float = 0.1, preserve_concepts: bool = True,
              **kwargs) -> List[Dict[str, Any]]:
        """
        åŸºæ–¼æ³•å¾‹èªç¾©å®Œæ•´æ€§çš„åˆ†å¡Š
        
        Args:
            text: è¼¸å…¥æ–‡æœ¬
            max_chunk_size: æœ€å¤§åˆ†å¡Šå¤§å°
            overlap_ratio: é‡ç–Šæ¯”ä¾‹
            preserve_concepts: æ˜¯å¦ä¿æŒæ¦‚å¿µå®Œæ•´æ€§
            
        Returns:
            List[Dict]: åŒ…å«contentã€metadataã€conceptsçš„chunkåˆ—è¡¨
        """
        
        # 1. è­˜åˆ¥æ³•å¾‹æ¦‚å¿µé‚Šç•Œ
        legal_concepts = self._identify_legal_concepts(text)
        print(f"ğŸ” è­˜åˆ¥åˆ° {len(legal_concepts)} å€‹æ³•å¾‹æ¦‚å¿µ")
        
        # 2. è¨ˆç®—èªç¾©é‚Šç•Œ
        semantic_boundaries = self._calculate_semantic_boundaries(text, legal_concepts)
        print(f"ğŸ” è¨ˆç®—å‡º {len(semantic_boundaries)} å€‹èªç¾©é‚Šç•Œ")
        
        # 3. åŸºæ–¼èªç¾©é€£è²«æ€§åˆ†å¡Š
        semantic_chunks = self._create_semantic_chunks(
            text, semantic_boundaries, max_chunk_size, overlap_ratio
        )
        print(f"ğŸ” å‰µå»º {len(semantic_chunks)} å€‹èªç¾©åˆ†å¡Š")
        
        # 4. ç¢ºä¿æ¦‚å¿µå®Œæ•´æ€§
        if preserve_concepts:
            integrity_chunks = self._ensure_concept_integrity(
                semantic_chunks, legal_concepts
            )
            print(f"ğŸ” ç¢ºä¿æ¦‚å¿µå®Œæ•´æ€§å¾Œå¾—åˆ° {len(integrity_chunks)} å€‹åˆ†å¡Š")
            return integrity_chunks
        
        return semantic_chunks
    
    def _identify_legal_concepts(self, text: str) -> List[LegalConcept]:
        """è­˜åˆ¥æ³•å¾‹æ¦‚å¿µ"""
        concepts = []
        
        for concept_type, patterns in self.legal_concept_patterns.items():
            for pattern in patterns:
                matches = re.finditer(pattern, text, re.MULTILINE | re.DOTALL)
                
                for match in matches:
                    concept = LegalConcept(
                        concept_type=concept_type,
                        content=match.group().strip(),
                        start_pos=match.start(),
                        end_pos=match.end(),
                        importance_score=self.concept_importance_weights.get(concept_type, 0.5),
                        related_articles=self._extract_related_articles(match.group()),
                        semantic_boundaries=self._find_semantic_boundaries(match.group())
                    )
                    concepts.append(concept)
        
        # æŒ‰é‡è¦æ€§æ’åº
        concepts.sort(key=lambda x: x.importance_score, reverse=True)
        return concepts
    
    def _extract_related_articles(self, content: str) -> List[str]:
        """æå–ç›¸é—œæ³•æ¢"""
        article_pattern = r'ç¬¬\s*(\d+)\s*æ¢'
        articles = re.findall(article_pattern, content)
        return [f"ç¬¬{article}æ¢" for article in articles]
    
    def _find_semantic_boundaries(self, content: str) -> List[int]:
        """æ‰¾åˆ°èªç¾©é‚Šç•Œ"""
        boundaries = []
        
        for boundary_type, markers in self.semantic_boundaries.items():
            for marker in markers:
                if isinstance(marker, str):
                    # å­—ç¬¦ä¸²æ¨™è¨˜
                    pos = content.find(marker)
                    if pos != -1:
                        boundaries.append(pos)
                else:
                    # æ­£å‰‡è¡¨é”å¼æ¨™è¨˜
                    pattern = re.compile(marker)
                    for match in pattern.finditer(content):
                        boundaries.append(match.start())
        
        return sorted(set(boundaries))
    
    def _calculate_semantic_boundaries(self, text: str, 
                                     concepts: List[LegalConcept]) -> List[int]:
        """è¨ˆç®—èªç¾©é‚Šç•Œ"""
        boundaries = set()
        
        # æ·»åŠ æ¦‚å¿µé‚Šç•Œ
        for concept in concepts:
            boundaries.add(concept.start_pos)
            boundaries.add(concept.end_pos)
            
            # æ·»åŠ æ¦‚å¿µå…§çš„èªç¾©é‚Šç•Œ
            for boundary in concept.semantic_boundaries:
                boundaries.add(concept.start_pos + boundary)
        
        # æ·»åŠ çµæ§‹é‚Šç•Œï¼ˆæ³•æ¢ã€ç« ç¯€ï¼‰
        structure_pattern = r'ç¬¬\s*\d+\s*[æ¢ç« ç¯€é …]'
        for match in re.finditer(structure_pattern, text):
            boundaries.add(match.start())
        
        # æ·»åŠ å¥å­é‚Šç•Œ
        sentence_pattern = r'[ã€‚ï¼ï¼Ÿ]'
        for match in re.finditer(sentence_pattern, text):
            boundaries.add(match.end())
        
        return sorted(list(boundaries))
    
    def _create_semantic_chunks(self, text: str, boundaries: List[int],
                               max_chunk_size: int, overlap_ratio: float) -> List[Dict[str, Any]]:
        """å‰µå»ºèªç¾©åˆ†å¡Š"""
        chunks = []
        overlap_size = int(max_chunk_size * overlap_ratio)
        
        start = 0
        chunk_index = 0
        
        while start < len(text):
            # æ‰¾åˆ°æœ€é©åˆçš„çµæŸä½ç½®
            end = self._find_optimal_end_position(
                text, start, max_chunk_size, boundaries
            )
            
            if end <= start:
                # å¦‚æœæ‰¾ä¸åˆ°åˆé©çš„çµæŸä½ç½®ï¼Œå¼·åˆ¶çµæŸ
                end = min(start + max_chunk_size, len(text))
            
            chunk_content = text[start:end]
            
            # åˆ†æchunkçš„èªç¾©ç‰¹å¾µ
            semantic_features = self._analyze_chunk_semantics(chunk_content)
            
            chunk = {
                "content": chunk_content,
                "span": {"start": start, "end": end},
                "metadata": {
                    "strategy": "legal_semantic_integrity",
                    "chunk_index": chunk_index,
                    "length": len(chunk_content),
                    "semantic_features": semantic_features,
                    "concept_density": semantic_features.get("concept_count", 0) / max(len(chunk_content), 1),
                    "importance_score": semantic_features.get("importance_score", 0.0)
                }
            }
            
            chunks.append(chunk)
            chunk_index += 1
            
            # è¨ˆç®—ä¸‹ä¸€å€‹chunkçš„é–‹å§‹ä½ç½®ï¼ˆè€ƒæ…®é‡ç–Šï¼‰
            start = max(start + max_chunk_size - overlap_size, end - overlap_size)
            
            if start >= len(text):
                break
        
        return chunks
    
    def _find_optimal_end_position(self, text: str, start: int, 
                                  max_chunk_size: int, boundaries: List[int]) -> int:
        """æ‰¾åˆ°æœ€å„ªçš„çµæŸä½ç½®"""
        ideal_end = start + max_chunk_size
        
        # æ‰¾åˆ°æœ€æ¥è¿‘ç†æƒ³çµæŸä½ç½®çš„èªç¾©é‚Šç•Œ
        optimal_boundary = None
        min_distance = float('inf')
        
        for boundary in boundaries:
            if start < boundary <= ideal_end:
                distance = abs(boundary - ideal_end)
                if distance < min_distance:
                    min_distance = distance
                    optimal_boundary = boundary
        
        if optimal_boundary:
            return optimal_boundary
        
        # å¦‚æœæ²’æœ‰æ‰¾åˆ°åˆé©çš„é‚Šç•Œï¼Œè¿”å›ç†æƒ³çµæŸä½ç½®
        return min(ideal_end, len(text))
    
    def _analyze_chunk_semantics(self, chunk_content: str) -> Dict[str, Any]:
        """åˆ†æchunkçš„èªç¾©ç‰¹å¾µ"""
        features = {
            "concept_count": 0,
            "importance_score": 0.0,
            "concept_types": [],
            "has_definition": False,
            "has_exception": False,
            "has_condition": False,
            "article_references": []
        }
        
        # æª¢æŸ¥å„ç¨®æ¦‚å¿µé¡å‹
        for concept_type, patterns in self.legal_concept_patterns.items():
            for pattern in patterns:
                if re.search(pattern, chunk_content, re.MULTILINE | re.DOTALL):
                    features["concept_count"] += 1
                    features["concept_types"].append(concept_type)
                    features["importance_score"] += self.concept_importance_weights.get(concept_type, 0.5)
                    
                    if concept_type == "definition":
                        features["has_definition"] = True
                    elif concept_type == "exception_clause":
                        features["has_exception"] = True
                    elif concept_type == "conditional_clause":
                        features["has_condition"] = True
        
        # æå–æ³•æ¢å¼•ç”¨
        article_pattern = r'ç¬¬\s*(\d+)\s*æ¢'
        articles = re.findall(article_pattern, chunk_content)
        features["article_references"] = [f"ç¬¬{article}æ¢" for article in articles]
        
        return features
    
    def _ensure_concept_integrity(self, chunks: List[Dict[str, Any]], 
                                 concepts: List[LegalConcept]) -> List[Dict[str, Any]]:
        """ç¢ºä¿æ¦‚å¿µå®Œæ•´æ€§"""
        integrity_chunks = []
        
        for chunk in chunks:
            chunk_start = chunk["span"]["start"]
            chunk_end = chunk["span"]["end"]
            
            # æª¢æŸ¥æ˜¯å¦æœ‰æ¦‚å¿µè¢«åˆ†å‰²
            incomplete_concepts = []
            for concept in concepts:
                concept_start = concept.start_pos
                concept_end = concept.end_pos
                
                # å¦‚æœæ¦‚å¿µè·¨è¶Šchunké‚Šç•Œ
                if (concept_start < chunk_end and concept_end > chunk_start and
                    not (chunk_start <= concept_start and concept_end <= chunk_end)):
                    incomplete_concepts.append(concept)
            
            # å¦‚æœæœ‰ä¸å®Œæ•´çš„æ¦‚å¿µï¼Œèª¿æ•´chunké‚Šç•Œ
            if incomplete_concepts:
                adjusted_chunk = self._adjust_chunk_for_concepts(
                    chunk, incomplete_concepts
                )
                integrity_chunks.append(adjusted_chunk)
            else:
                integrity_chunks.append(chunk)
        
        return integrity_chunks
    
    def _adjust_chunk_for_concepts(self, chunk: Dict[str, Any], 
                                  incomplete_concepts: List[LegalConcept]) -> Dict[str, Any]:
        """èª¿æ•´chunkä»¥åŒ…å«å®Œæ•´æ¦‚å¿µ"""
        original_start = chunk["span"]["start"]
        original_end = chunk["span"]["end"]
        
        # æ‰¾åˆ°éœ€è¦åŒ…å«çš„æ‰€æœ‰æ¦‚å¿µçš„é‚Šç•Œ
        min_start = original_start
        max_end = original_end
        
        for concept in incomplete_concepts:
            min_start = min(min_start, concept.start_pos)
            max_end = max(max_end, concept.end_pos)
        
        # æ›´æ–°chunkå…§å®¹å’Œå…ƒæ•¸æ“š
        adjusted_content = chunk["content"]  # é€™è£¡éœ€è¦å¾åŸå§‹æ–‡æœ¬é‡æ–°æå–
        
        chunk["span"]["start"] = min_start
        chunk["span"]["end"] = max_end
        chunk["metadata"]["adjusted_for_concepts"] = True
        chunk["metadata"]["incomplete_concept_count"] = len(incomplete_concepts)
        
        return chunk


class MultiLevelSemanticChunking:
    """å¤šå±¤æ¬¡èªç¾©åˆ†å¡Šç­–ç•¥ - å°æ‡‰è«–æ–‡ä¸­çš„å…­å€‹ç²’åº¦ç´šåˆ¥"""
    
    def __init__(self):
        # è«–æ–‡ä¸­çš„å…­å€‹å±¤æ¬¡é…ç½®
        self.semantic_levels = {
            'document': {
                'granularity': 'document',
                'size_range': (2000, 10000),
                'target_queries': ['æ•´éƒ¨', 'å…¨æ–‡', 'æ•´å€‹', 'å…¨éƒ¨'],
                'priority_concepts': ['complete_document'],
                'description': 'æ–‡ä»¶å±¤ç´š (Document Level) - æ•´å€‹æ³•å¾‹æ–‡æª”'
            },
            'document_component': {
                'granularity': 'document_component',
                'size_range': (1000, 3000),
                'target_queries': ['ç« ', 'éƒ¨åˆ†', 'ç·¨', 'ç¯‡'],
                'priority_concepts': ['chapter', 'part', 'section'],
                'description': 'æ–‡ä»¶çµ„æˆéƒ¨åˆ†å±¤ç´š (Document Component Level) - æ–‡æª”çš„ä¸»è¦çµ„æˆéƒ¨åˆ†'
            },
            'basic_unit_hierarchy': {
                'granularity': 'basic_unit_hierarchy',
                'size_range': (500, 1500),
                'target_queries': ['ç¯€', 'æ¨™é¡Œ', 'ç« ç¯€'],
                'priority_concepts': ['section', 'title', 'subsection'],
                'description': 'åŸºæœ¬å–®ä½å±¤æ¬¡çµæ§‹å±¤ç´š (Basic Unit Hierarchy Level) - æ›¸ç±ã€æ¨™é¡Œã€ç« ç¯€'
            },
            'basic_unit': {
                'granularity': 'basic_unit',
                'size_range': (200, 800),
                'target_queries': ['ç¬¬.*æ¢', 'æ¢æ–‡', 'æ³•æ¢'],
                'priority_concepts': ['article', 'clause', 'provision'],
                'description': 'åŸºæœ¬å–®ä½å±¤ç´š (Basic Unit Level) - æ–‡ç« /æ¢æ–‡ (article)'
            },
            'basic_unit_component': {
                'granularity': 'basic_unit_component',
                'size_range': (100, 400),
                'target_queries': ['æ®µè½', 'ä¸»æ–‡', 'å…§å®¹', 'å®šç¾©'],
                'priority_concepts': ['definition', 'paragraph', 'content'],
                'description': 'åŸºæœ¬å–®ä½çµ„æˆéƒ¨åˆ†å±¤ç´š (Basic Unit Component Level) - å¼·åˆ¶æ€§ä¸»æ–‡æˆ–æ®µè½'
            },
            'enumeration': {
                'granularity': 'enumeration',
                'size_range': (50, 200),
                'target_queries': ['é …', 'ç›®', 'æ¬¾', 'å­é …'],
                'priority_concepts': ['item', 'subitem', 'enumeration'],
                'description': 'åˆ—èˆ‰å±¤ç´š (Enumeration Level) - é …ç›®ã€å­é …'
            }
        }
    
    def chunk(self, text: str, **kwargs) -> Dict[str, List[Dict[str, Any]]]:
        """å‰µå»ºå¤šå±¤æ¬¡èªç¾©åˆ†å¡Š"""
        multi_level_chunks = {}
        
        # ä½¿ç”¨æ³•å¾‹èªç¾©å®Œæ•´æ€§åˆ†å¡Šä½œç‚ºåŸºç¤
        base_chunker = LegalSemanticIntegrityChunking()
        
        for level_name, level_config in self.semantic_levels.items():
            # æ ¹æ“šå±¤æ¬¡é…ç½®èª¿æ•´åƒæ•¸
            level_kwargs = kwargs.copy()
            level_kwargs['max_chunk_size'] = level_config['size_range'][1]
            level_kwargs['min_chunk_size'] = level_config['size_range'][0]
            
            # å‰µå»ºè©²å±¤æ¬¡çš„åˆ†å¡Š
            level_chunks = base_chunker.chunk(text, **level_kwargs)
            
            # æ ¹æ“šå±¤æ¬¡ç‰¹æ€§éæ¿¾å’Œèª¿æ•´åˆ†å¡Š
            filtered_chunks = self._filter_chunks_for_level(
                level_chunks, level_config
            )
            
            multi_level_chunks[level_name] = filtered_chunks
        
        return multi_level_chunks
    
    def _filter_chunks_for_level(self, chunks: List[Dict[str, Any]], 
                                level_config: Dict[str, Any]) -> List[Dict[str, Any]]:
        """æ ¹æ“šå±¤æ¬¡ç‰¹æ€§éæ¿¾åˆ†å¡Š"""
        filtered_chunks = []
        
        for chunk in chunks:
            semantic_features = chunk["metadata"].get("semantic_features", {})
            concept_types = semantic_features.get("concept_types", [])
            
            # æª¢æŸ¥æ˜¯å¦åŒ…å«è©²å±¤æ¬¡å„ªå…ˆçš„æ¦‚å¿µé¡å‹
            has_priority_concept = any(
                concept_type in concept_types 
                for concept_type in level_config['priority_concepts']
            )
            
            # æª¢æŸ¥chunkå¤§å°æ˜¯å¦åœ¨ç¯„åœå…§
            chunk_size = len(chunk["content"])
            size_in_range = (
                level_config['size_range'][0] <= chunk_size <= level_config['size_range'][1]
            )
            
            if has_priority_concept or size_in_range:
                # æ·»åŠ å±¤æ¬¡ç‰¹å®šçš„å…ƒæ•¸æ“š
                chunk["metadata"]["semantic_level"] = level_config['granularity']
                chunk["metadata"]["target_queries"] = level_config['target_queries']
                filtered_chunks.append(chunk)
        
        return filtered_chunks
