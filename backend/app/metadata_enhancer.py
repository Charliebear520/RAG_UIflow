"""
Metadataå¢å¼·æ¨¡çµ„
"""

import re
import hashlib
import jieba
import jieba.analyse
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass
from collections import Counter
import json


@dataclass
class LegalConcept:
    """æ³•å¾‹æ¦‚å¿µ"""
    concept_name: str
    concept_type: str
    legal_domain: str
    importance_score: float
    synonyms: List[str]
    confidence: float


@dataclass
class LegalRelation:
    """æ³•å¾‹é—œä¿‚"""
    relation_type: str
    subject: str
    object: str
    relation: str
    confidence: float


class MetadataEnhancer:
    """Metadataå¢å¼·å™¨ - å°ˆæ³¨æ–¼ã€Œæ¢ã€å±¤ç´šçš„metadataç”Ÿæˆå’Œå‘ä¸‹ç¹¼æ‰¿"""
    
    def __init__(self):
        self.legal_domains = self._initialize_legal_domains()
        self.legal_concept_patterns = self._initialize_concept_patterns()
        self.article_type_patterns = self._initialize_article_type_patterns()
        self.legal_synonyms = self._initialize_legal_synonyms()
        
        # ç·©å­˜æ©Ÿåˆ¶
        self.concept_cache = {}
        self.metadata_cache = {}
        
        # æ¢å±¤ç´šmetadataå­˜å„²ï¼ˆç”¨æ–¼å‘ä¸‹ç¹¼æ‰¿ï¼‰
        self.article_metadata_map = {}  # {article_id: enhanced_metadata}
        self.inheritance_hierarchy = {}  # {child_chunk_id: parent_article_id}
    
    def _initialize_legal_domains(self) -> Dict[str, List[str]]:
        """åˆå§‹åŒ–æ³•å¾‹é ˜åŸŸé—œéµè©"""
        return {
            "è‘—ä½œæ¬Šæ³•": ["è‘—ä½œæ¬Š", "è‘—ä½œ", "é‡è£½", "æ”¹ä½œ", "æ•£å¸ƒ", "å…¬é–‹å‚³è¼¸", "å…¬é–‹æ’­é€", "å…¬é–‹æ¼”å‡º", "å…¬é–‹å±•ç¤º", "å‡ºç§Ÿ", "åˆç†ä½¿ç”¨"],
            "å•†æ¨™æ³•": ["å•†æ¨™", "å•†æ¨™æ¬Š", "è¨»å†Š", "ä½¿ç”¨", "ä¾µæ¬Š", "æ··æ·†", "è­‰æ˜æ¨™ç« ", "åœ˜é«”æ¨™ç« ", "åœ˜é«”å•†æ¨™"],
            "å°ˆåˆ©æ³•": ["å°ˆåˆ©", "ç™¼æ˜", "æ–°å‹", "è¨­è¨ˆ", "ç”³è«‹", "å¯©æŸ¥", "å°ˆåˆ©æ¬Š", "å¯¦æ–½", "æˆæ¬Š"],
            "æ™ºæ…§è²¡ç”¢æ¬Šæ³•": ["æ™ºæ…§è²¡ç”¢æ¬Š", "æ™ºæ…§è²¡ç”¢", "IP", "çŸ¥è­˜ç”¢æ¬Š"],
            "æ°‘æ³•": ["å¥‘ç´„", "å‚µå‹™", "å‚µæ¬Š", "ç‰©æ¬Š", "æ‰€æœ‰æ¬Š", "å æœ‰", "ä¾µæ¬Šè¡Œç‚º", "æå®³è³ å„Ÿ"],
            "åˆ‘æ³•": ["çŠ¯ç½ª", "åˆ‘ç½°", "æœ‰æœŸå¾’åˆ‘", "ç½°é‡‘", "æ²’æ”¶", "ç·©åˆ‘"],
            "è¡Œæ”¿æ³•": ["è¡Œæ”¿è™•åˆ†", "è¡Œæ”¿æ•‘æ¿Ÿ", "è¨´é¡˜", "è¡Œæ”¿è¨´è¨Ÿ", "åœ‹å®¶è³ å„Ÿ"]
        }
    
    def _initialize_concept_patterns(self) -> Dict[str, Dict[str, Any]]:
        """åˆå§‹åŒ–æ³•å¾‹æ¦‚å¿µæ¨¡å¼"""
        return {
            "æ¬Šåˆ©å®šç¾©": {
                "patterns": [
                    r"([^ã€‚ï¼Œï¼›ï¼š]+)(?:å°ˆæœ‰|äº«æœ‰|å…·æœ‰)([^ã€‚ï¼Œï¼›ï¼š]*)æ¬Šåˆ©",
                    r"([^ã€‚ï¼Œï¼›ï¼š]+)æ¬Š[åˆ©]?(?:æ˜¯æŒ‡|ä¿‚æŒ‡|ç‚º)([^ã€‚ï¼Œï¼›ï¼š]*)",
                    r"([^ã€‚ï¼Œï¼›ï¼š]+)(?:å°ˆæœ‰|äº«æœ‰)([^ã€‚ï¼Œï¼›ï¼š]*)æ¬Š"
                ],
                "importance_weight": 0.9
            },
            "ç¾©å‹™è¦å®š": {
                "patterns": [
                    r"([^ã€‚ï¼Œï¼›ï¼š]+)(?:æ‡‰|å¿…é ˆ|å¾—|å¯ä»¥)([^ã€‚ï¼Œï¼›ï¼š]*)",
                    r"([^ã€‚ï¼Œï¼›ï¼š]+)(?:ä¸å¾—|ä¸å¯|ç¦æ­¢)([^ã€‚ï¼Œï¼›ï¼š]*)"
                ],
                "importance_weight": 0.8
            },
            "ä¾‹å¤–æ¢ä»¶": {
                "patterns": [
                    r"(?:ä½†|æƒŸ|ä¾‹å¤–|é™¤å¤–)([^ã€‚ï¼Œï¼›ï¼š]*)",
                    r"(?:ä¸åœ¨æ­¤é™|ä¸é©ç”¨)([^ã€‚ï¼Œï¼›ï¼š]*)"
                ],
                "importance_weight": 0.7
            },
            "æ³•å¾‹å¾Œæœ": {
                "patterns": [
                    r"(?:é•å|ä¾µå®³|ä¾µçŠ¯)([^ã€‚ï¼Œï¼›ï¼š]*)(?:è€…|æ™‚)([^ã€‚ï¼Œï¼›ï¼š]*)",
                    r"(?:è™•|ç§‘)([^ã€‚ï¼Œï¼›ï¼š]*)(?:ç½°|åˆ‘)"
                ],
                "importance_weight": 0.8
            }
        }
    
    def _initialize_article_type_patterns(self) -> Dict[str, List[str]]:
        """åˆå§‹åŒ–æ¢æ–‡é¡å‹æ¨¡å¼"""
        return {
            "æ¬Šåˆ©å®šç¾©": ["å°ˆæœ‰", "äº«æœ‰", "å…·æœ‰", "æ¬Šåˆ©", "æ˜¯æŒ‡", "ä¿‚æŒ‡", "ç‚º"],
            "ç¾©å‹™è¦å®š": ["æ‡‰", "å¿…é ˆ", "å¾—", "å¯ä»¥", "ä¸å¾—", "ä¸å¯", "ç¦æ­¢"],
            "ä¾‹å¤–æ¢ä»¶": ["ä½†", "æƒŸ", "ä¾‹å¤–", "é™¤å¤–", "ä¸åœ¨æ­¤é™", "ä¸é©ç”¨"],
            "æ³•å¾‹å¾Œæœ": ["é•å", "ä¾µå®³", "ä¾µçŠ¯", "è™•", "ç§‘", "ç½°", "åˆ‘", "è²¬ä»»"],
            "ç«‹æ³•ç›®çš„": ["ç‚º", "ç‚ºä¿éšœ", "ç‚ºç¶­è­·", "ç‚ºä¿ƒé€²", "ç‚ºä¿è­·", "åˆ¶å®šæœ¬æ³•"],
            "é©ç”¨æ¢ä»¶": ["é©ç”¨æ–¼", "é©ç”¨", "æ–¼", "åœ¨", "ç•¶", "å¦‚"],
            "ç¨‹åºè¦å®š": ["ç”³è«‹", "å¯©æŸ¥", "æ ¸å‡†", "ç™»è¨˜", "è¨»å†Š", "å…¬å‘Š"],
            "å®šç¾©æ¢æ–‡": ["ç¨±", "æŒ‡", "è¬‚", "ä¿‚æŒ‡", "æ˜¯æŒ‡", "ç‚º", "åŒ…æ‹¬"]
        }
    
    def _initialize_legal_synonyms(self) -> Dict[str, List[str]]:
        """åˆå§‹åŒ–æ³•å¾‹åŒç¾©è©"""
        return {
            "å…¬é–‹å‚³è¼¸": ["å…¬é–‹å‚³è¼¸", "æ•¸ä½å‚³è¼¸", "ç¶²è·¯å‚³è¼¸", "ç·šä¸Šå‚³è¼¸", "ä¸Šç·šæä¾›", "public transmission"],
            "å…¬é–‹æ’­é€": ["å…¬é–‹æ’­é€", "å»£æ’­", "æ’­é€", "broadcast"],
            "é‡è£½": ["é‡è£½", "è¤‡è£½", "æ‹·è²", "è¤‡æœ¬è£½ä½œ", "reproduction"],
            "æ•£å¸ƒ": ["æ•£å¸ƒ", "ç™¼è¡Œ", "æµé€š", "distribution"],
            "æ”¹ä½œ": ["æ”¹ä½œ", "æ”¹ç·¨", "ç¿»æ¡ˆ", "è¡ç”Ÿå‰µä½œ", "derivative"],
            "å¼•ç”¨": ["å¼•ç”¨", "ç¯€éŒ„", "æ‘˜éŒ„", "å¼•ç”¨ä»–äººè‘—ä½œ"],
            "åˆç†ä½¿ç”¨": ["åˆç†ä½¿ç”¨", "å…¬å¹³ä½¿ç”¨", "fair use"],
            "å•†æ¨™æ¬Š": ["å•†æ¨™æ¬Š", "å•†æ¨™å°ˆç”¨æ¬Š", "å•†æ¨™ä½¿ç”¨æ¬Š"],
            "è‘—ä½œæ¬Š": ["è‘—ä½œæ¬Š", "ç‰ˆæ¬Š", "copyright"],
            "å°ˆåˆ©æ¬Š": ["å°ˆåˆ©æ¬Š", "å°ˆåˆ©", "patent"],
            "ä¾µå®³": ["ä¾µå®³", "ä¾µçŠ¯", "é•å", "æå®³", "é•æ³•", "ä¸æ³•"],
            "è™•ç½°": ["è™•ç½°", "åˆ¶è£", "æ‡²ç½°", "penalty"],
            "æ¬Šåˆ©": ["æ¬Šåˆ©", "æ¬Šç›Š", "right", "entitlement"],
            "ç¾©å‹™": ["ç¾©å‹™", "è²¬ä»»", "duty", "obligation"]
        }
    
    def enhance_metadata_batch(self, chunks: List[Dict[str, Any]]) -> Dict[str, Dict[str, Any]]:
        """æ‰¹é‡å¢å¼·metadata - å°ˆæ³¨æ–¼ã€Œæ¢ã€å±¤ç´šä¸¦å¯¦ç¾å‘ä¸‹ç¹¼æ‰¿"""
        enhanced_metadata = {}
        
        # å»é‡è™•ç†
        unique_chunks = self._deduplicate_chunks(chunks)
        print(f"ğŸ“Š åŸå§‹chunks: {len(chunks)}, å»é‡å¾Œ: {len(unique_chunks)}")
        
        # ç¬¬ä¸€æ­¥ï¼šè­˜åˆ¥ä¸åŒå±¤ç´šçš„chunks
        article_chunks = []
        chapter_section_chunks = []
        other_chunks = []
        
        for chunk in unique_chunks:
            if self._is_article_level_chunk(chunk):
                article_chunks.append(chunk)
            elif self._is_chapter_section_level_chunk(chunk):
                chapter_section_chunks.append(chunk)
            else:
                other_chunks.append(chunk)
        
        print(f"ğŸ“‹ è­˜åˆ¥å‡ºã€Œæ¢ã€å±¤ç´šchunks: {len(article_chunks)}å€‹")
        print(f"ğŸ“‹ è­˜åˆ¥å‡ºã€Œç« ã€ç¯€ã€å±¤ç´šchunks: {len(chapter_section_chunks)}å€‹")
        print(f"ğŸ“‹ å…¶ä»–å±¤ç´šchunks: {len(other_chunks)}å€‹")
        
        # ç¬¬äºŒæ­¥ï¼šç‚ºã€Œæ¢ã€å±¤ç´šç”Ÿæˆmetadata
        article_metadata_results = {}
        for i, chunk in enumerate(article_chunks):
            chunk_id = chunk.get("chunk_id", f"article_chunk_{i}")
            content = chunk.get("content", "")
            original_metadata = chunk.get("metadata", {})
            
            # æª¢æŸ¥ç·©å­˜
            content_hash = hashlib.md5(content.encode()).hexdigest()
            if content_hash in self.metadata_cache:
                enhanced = self.metadata_cache[content_hash]
            else:
                # å¢å¼·metadataï¼ˆå°ˆæ³¨æ–¼æ¢å±¤ç´šï¼‰
                enhanced = self._enhance_article_level_chunk(content, original_metadata)
                # ç·©å­˜çµæœ
                self.metadata_cache[content_hash] = enhanced
            
            article_metadata_results[chunk_id] = enhanced
            
            # å­˜å„²åˆ°æ¢å±¤ç´šmetadataæ˜ å°„
            article_id = self._extract_article_id(chunk)
            if article_id:
                self.article_metadata_map[article_id] = enhanced
            
            if (i + 1) % 10 == 0:
                print(f"ğŸ“ˆ å·²è™•ç†ã€Œæ¢ã€å±¤ç´š {i + 1}/{len(article_chunks)} å€‹chunks")
        
        # ç¬¬ä¸‰æ­¥ï¼šç‚ºã€Œç« ã€ç¯€ã€å±¤ç´šé€²è¡Œä¸­ç­‰å¼·åº¦metadataå¢å¼·
        chapter_section_metadata_results = {}
        for i, chunk in enumerate(chapter_section_chunks):
            chunk_id = chunk.get("chunk_id", f"chapter_section_chunk_{i}")
            content = chunk.get("content", "")
            original_metadata = chunk.get("metadata", {})
            
            # æª¢æŸ¥ç·©å­˜
            content_hash = hashlib.md5(content.encode()).hexdigest()
            if content_hash in self.metadata_cache:
                enhanced = self.metadata_cache[content_hash]
            else:
                # é€²è¡Œä¸­ç­‰å¼·åº¦metadataå¢å¼·
                enhanced = self._enhance_chapter_section_chunk(content, original_metadata)
                # ç·©å­˜çµæœ
                self.metadata_cache[content_hash] = enhanced
            
            chapter_section_metadata_results[chunk_id] = enhanced
            
            if (i + 1) % 20 == 0:
                print(f"ğŸ“ˆ å·²è™•ç†ã€Œç« ã€ç¯€ã€å±¤ç´š {i + 1}/{len(chapter_section_chunks)} å€‹chunks")
        
        # ç¬¬å››æ­¥ï¼šç‚ºå…¶ä»–å±¤ç´šchunkså¯¦ç¾å‘ä¸‹ç¹¼æ‰¿
        inherited_metadata_results = {}
        for i, chunk in enumerate(other_chunks):
            chunk_id = chunk.get("chunk_id", f"other_chunk_{i}")
            content = chunk.get("content", "")
            original_metadata = chunk.get("metadata", {})
            
            # æŸ¥æ‰¾çˆ¶ç´šã€Œæ¢ã€çš„metadata
            parent_article_id = self._find_parent_article_id(chunk)
            inherited_metadata = None
            
            if parent_article_id and parent_article_id in self.article_metadata_map:
                inherited_metadata = self.article_metadata_map[parent_article_id].copy()
                # æ¨™è¨˜é€™æ˜¯ç¹¼æ‰¿çš„metadata
                inherited_metadata["inherited_from"] = parent_article_id
                inherited_metadata["inheritance_type"] = "downward_inheritance"
                
                # å»ºç«‹ç¹¼æ‰¿é—œä¿‚æ˜ å°„
                self.inheritance_hierarchy[chunk_id] = parent_article_id
                
                print(f"ğŸ”„ {chunk_id} ç¹¼æ‰¿äº† {parent_article_id} çš„metadata")
            
            # å¦‚æœæ²’æœ‰æ‰¾åˆ°ç¹¼æ‰¿çš„metadataï¼Œå‰‡é€²è¡Œè¼•é‡ç´šå¢å¼·
            if not inherited_metadata:
                inherited_metadata = self._enhance_lightweight_chunk(content, original_metadata)
            
            inherited_metadata_results[chunk_id] = inherited_metadata
            
            if (i + 1) % 100 == 0:
                print(f"ğŸ“ˆ å·²è™•ç†å…¶ä»–å±¤ç´š {i + 1}/{len(other_chunks)} å€‹chunks")
        
        # ç¬¬äº”æ­¥ï¼šåˆä½µçµæœ
        enhanced_metadata.update(article_metadata_results)
        enhanced_metadata.update(chapter_section_metadata_results)
        enhanced_metadata.update(inherited_metadata_results)
        
        # æ˜ å°„å›åŸå§‹chunks
        final_results = self._map_enhanced_to_original(chunks, enhanced_metadata)
        
        print(f"âœ… å®Œæˆmetadataå¢å¼·ï¼šæ¢å±¤ç´š({len(article_metadata_results)}) + ç« ç¯€å±¤ç´š({len(chapter_section_metadata_results)}) + ç¹¼æ‰¿å±¤ç´š({len(inherited_metadata_results)})")
        
        return final_results
    
    def _deduplicate_chunks(self, chunks: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """å»é‡ï¼šç›¸åŒå…§å®¹åªä¿ç•™ä¸€å€‹"""
        content_hash = {}
        unique_chunks = []
        
        for chunk in chunks:
            content = chunk.get("content", "")
            content_md5 = hashlib.md5(content.encode()).hexdigest()
            
            if content_md5 not in content_hash:
                content_hash[content_md5] = chunk
                unique_chunks.append(chunk)
        
        return unique_chunks
    
    def _map_enhanced_to_original(self, original_chunks: List[Dict[str, Any]], 
                                 enhanced_metadata: Dict[str, Dict[str, Any]]) -> Dict[str, Dict[str, Any]]:
        """å°‡å¢å¼·metadataæ˜ å°„å›åŸå§‹chunks"""
        final_results = {}
        
        for chunk in original_chunks:
            chunk_id = chunk.get("chunk_id", "")
            content = chunk.get("content", "")
            
            # æ‰¾åˆ°å°æ‡‰çš„å¢å¼·metadata
            content_hash = hashlib.md5(content.encode()).hexdigest()
            for enhanced_id, enhanced_data in enhanced_metadata.items():
                if enhanced_id in content_hash or content in enhanced_data.get("content", ""):
                    final_results[chunk_id] = enhanced_data
                    break
        
        return final_results
    
    def _enhance_single_chunk(self, content: str, original_metadata: Dict[str, Any]) -> Dict[str, Any]:
        """å¢å¼·å–®å€‹chunkçš„metadata"""
        # æ¸…ç†åŸå§‹metadata
        cleaned_metadata = self._clean_metadata(original_metadata)
        
        # æå–å„ç¨®å¢å¼·ä¿¡æ¯
        enhanced = {
            **cleaned_metadata,
            "legal_concepts": self._extract_legal_concepts(content),
            "semantic_keywords": self._extract_semantic_keywords(content),
            "article_type": self._classify_article_type(content),
            "legal_domain": self._classify_legal_domain(content),
            "legal_relations": self._extract_legal_relations(content),
            "query_intent_tags": self._extract_query_intent_tags(content),
            "semantic_similarity": self._precompute_semantic_similarity(content)
        }
        
        return enhanced
    
    def _clean_metadata(self, metadata: Dict[str, Any]) -> Dict[str, Any]:
        """æ¸…ç†metadataï¼Œç§»é™¤ä¸å¿…è¦çš„å­—æ®µ"""
        # è¦ç§»é™¤çš„å­—æ®µ
        remove_fields = ["spans", "page_range", "chunk_index", "length", "chunk_by", "strategy"]
        
        # ä¿ç•™çš„å­—æ®µ
        keep_fields = [
            "id", "category", "article_label", "article_number", "article_suffix",
            "law_name", "chapter", "section", "article", "item", "level"
        ]
        
        cleaned = {}
        for key, value in metadata.items():
            if key in keep_fields and key not in remove_fields:
                cleaned[key] = value
        
        return cleaned
    
    def _extract_legal_concepts(self, content: str) -> List[Dict[str, Any]]:
        """æå–æ³•å¾‹æ¦‚å¿µ"""
        concepts = []
        
        # ä½¿ç”¨æ¨¡å¼åŒ¹é…æå–æ¦‚å¿µ
        for concept_type, config in self.legal_concept_patterns.items():
            for pattern in config["patterns"]:
                matches = re.finditer(pattern, content, re.MULTILINE)
                for match in matches:
                    concept_name = match.group(1).strip()
                    if concept_name:
                        # è­˜åˆ¥æ³•å¾‹é ˜åŸŸ
                        legal_domain = self._identify_legal_domain_from_content(content)
                        
                        # æå–åŒç¾©è©
                        synonyms = self._extract_synonyms_for_concept(concept_name, content)
                        
                        concept = {
                            "concept_name": concept_name,
                            "concept_type": concept_type,
                            "legal_domain": legal_domain,
                            "importance_score": config["importance_weight"],
                            "synonyms": synonyms,
                            "confidence": 0.8
                        }
                        concepts.append(concept)
        
        # ä½¿ç”¨é—œéµè©æå–è£œå……æ¦‚å¿µ
        keyword_concepts = self._extract_keyword_concepts(content)
        concepts.extend(keyword_concepts)
        
        # å»é‡
        concepts = self._deduplicate_concepts(concepts)
        
        return concepts
    
    def _extract_keyword_concepts(self, content: str) -> List[Dict[str, Any]]:
        """ä½¿ç”¨é—œéµè©æå–è£œå……æ³•å¾‹æ¦‚å¿µ"""
        concepts = []
        
        # ä½¿ç”¨jiebaæå–é—œéµè©
        if jieba:
            keywords = jieba.analyse.extract_tags(content, topK=10, withWeight=True)
        else:
            # ç°¡å–®çš„è©é »çµ±è¨ˆ
            words = re.findall(r'[\u4e00-\u9fff]+', content)
            word_freq = Counter(words)
            keywords = [(word, freq/len(words)) for word, freq in word_freq.most_common(10)]
        
        for keyword, weight in keywords:
            if weight > 0.1:  # åªä¿ç•™é‡è¦é—œéµè©
                legal_domain = self._identify_legal_domain_from_keyword(keyword)
                if legal_domain != "å…¶ä»–":
                    concept = {
                        "concept_name": keyword,
                        "concept_type": "é—œéµè©",
                        "legal_domain": legal_domain,
                        "importance_score": weight,
                        "synonyms": self.legal_synonyms.get(keyword, []),
                        "confidence": 0.6
                    }
                    concepts.append(concept)
        
        return concepts
    
    def _extract_semantic_keywords(self, content: str) -> Dict[str, Any]:
        """æå–èªç¾©é—œéµè©"""
        # ä½¿ç”¨jiebaåˆ†è©å’ŒTF-IDF
        if jieba:
            words = jieba.analyse.extract_tags(content, topK=20, withWeight=True)
        else:
            words = []
        
        # åˆ†é¡é—œéµè©
        legal_terms = ["æ¬Šåˆ©", "ç¾©å‹™", "ç¦æ­¢", "è™•ç½°", "è¦å®š", "é©ç”¨", "é•å", "ä¾µå®³", "è‘—ä½œæ¬Š", "å•†æ¨™", "å°ˆåˆ©"]
        domain_terms = ["è‘—ä½œæ¬Š", "å•†æ¨™", "å°ˆåˆ©", "æ™ºæ…§è²¡ç”¢æ¬Š"]
        action_terms = ["ä¿éšœ", "ç¶­è­·", "ä¿ƒé€²", "åˆ¶å®š", "ä¿è­·", "é™åˆ¶"]
        
        primary_keywords = []
        secondary_keywords = []
        domain_keywords = []
        action_keywords = []
        keyword_weights = {}
        
        for word, weight in words:
            keyword_weights[word] = weight
            if word in legal_terms:
                primary_keywords.append(word)
            elif word in domain_terms:
                domain_keywords.append(word)
            elif word in action_terms:
                action_keywords.append(word)
            else:
                secondary_keywords.append(word)
        
        return {
            "primary_keywords": primary_keywords,
            "secondary_keywords": secondary_keywords,
            "domain_keywords": domain_keywords,
            "action_keywords": action_keywords,
            "keyword_weights": keyword_weights
        }
    
    def _classify_article_type(self, content: str) -> Dict[str, Any]:
        """åˆ†é¡æ¢æ–‡é¡å‹"""
        detected_type = "ä¸€èˆ¬è¦å®š"
        confidence = 0.5
        
        for article_type, patterns in self.article_type_patterns.items():
            match_count = sum(1 for pattern in patterns if pattern in content)
            if match_count > 0:
                type_confidence = min(0.9, match_count * 0.2 + 0.3)
                if type_confidence > confidence:
                    detected_type = article_type
                    confidence = type_confidence
        
        return {
            "article_type": detected_type,
            "article_purpose": f"å®šç¾©{detected_type}çš„ç›¸é—œè¦å®š",
            "legal_function": "æ¬Šåˆ©ä¿è­·" if "æ¬Šåˆ©" in detected_type else "ç¾©å‹™è¦ç¯„",
            "scope": "ä¸€èˆ¬é©ç”¨",
            "severity": "é‡è¦" if detected_type in ["æ¬Šåˆ©å®šç¾©", "æ³•å¾‹å¾Œæœ"] else "ä¸€èˆ¬",
            "confidence": confidence
        }
    
    def _classify_legal_domain(self, content: str) -> Dict[str, Any]:
        """åˆ†é¡æ³•å¾‹é ˜åŸŸ"""
        domain_scores = {}
        
        for domain, keywords in self.legal_domains.items():
            score = sum(1 for keyword in keywords if keyword in content)
            domain_scores[domain] = score
        
        if domain_scores:
            best_domain = max(domain_scores.items(), key=lambda x: x[1])
            max_score = best_domain[1]
            confidence = min(1.0, max_score / 5.0)
            
            return {
                "legal_domain": best_domain[0],
                "sub_domain": best_domain[0],
                "domain_keywords": [kw for kw in self.legal_domains[best_domain[0]] if kw in content],
                "related_domains": [d for d, s in domain_scores.items() if d != best_domain[0] and s > 0],
                "domain_confidence": confidence
            }
        
        return {
            "legal_domain": "å…¶ä»–",
            "sub_domain": "å…¶ä»–",
            "domain_keywords": [],
            "related_domains": [],
            "domain_confidence": 0.1
        }
    
    def _extract_legal_relations(self, content: str) -> List[Dict[str, Any]]:
        """æå–æ³•å¾‹é—œä¿‚"""
        relations = []
        
        # æ¬Šåˆ©ç¾©å‹™é—œä¿‚
        rights_pattern = r"([^ã€‚ï¼Œï¼›ï¼š]+)(?:äº«æœ‰|å…·æœ‰|å°ˆæœ‰)([^ã€‚ï¼Œï¼›ï¼š]*)æ¬Šåˆ©"
        obligations_pattern = r"([^ã€‚ï¼Œï¼›ï¼š]+)(?:æ‡‰|å¿…é ˆ)([^ã€‚ï¼Œï¼›ï¼š]*)"
        
        for pattern in [rights_pattern, obligations_pattern]:
            matches = re.finditer(pattern, content)
            for match in matches:
                subject = match.group(1).strip()
                relation = "äº«æœ‰" if "äº«æœ‰" in pattern else "æ‡‰"
                object = match.group(2).strip()
                
                if subject and object:
                    relation_obj = {
                        "relation_type": "æ¬Šåˆ©ç¾©å‹™",
                        "subject": subject,
                        "object": object,
                        "relation": relation,
                        "confidence": 0.7
                    }
                    relations.append(relation_obj)
        
        return relations
    
    def _extract_query_intent_tags(self, content: str) -> List[str]:
        """æå–æŸ¥è©¢æ„åœ–æ¨™ç±¤"""
        intent_tags = []
        
        intent_patterns = {
            "æ¬Šåˆ©æŸ¥è©¢": ["ä»€éº¼æ˜¯", "å®šç¾©", "æ¬Šåˆ©", "ä»€éº¼æ¬Š"],
            "ç¾©å‹™æŸ¥è©¢": ["å¿…é ˆ", "æ‡‰", "ä¸å¾—", "ç¦æ­¢", "ç¾©å‹™"],
            "ä¾‹å¤–æŸ¥è©¢": ["ä¾‹å¤–", "é™¤å¤–", "ä½†", "æƒŸ", "ä¸é©ç”¨"],
            "å¾ŒæœæŸ¥è©¢": ["è™•ç½°", "é•å", "å¾Œæœ", "è²¬ä»»", "è³ å„Ÿ"],
            "é©ç”¨æŸ¥è©¢": ["é©ç”¨", "æ¢ä»¶", "æƒ…æ³", "ä½•æ™‚"]
        }
        
        for intent, patterns in intent_patterns.items():
            if any(pattern in content for pattern in patterns):
                intent_tags.append(intent)
        
        return intent_tags
    
    def _precompute_semantic_similarity(self, content: str) -> Dict[str, Any]:
        """é è¨ˆç®—èªç¾©ç›¸ä¼¼åº¦"""
        # æå–å¸¸è¦‹æŸ¥è©¢æ¨¡å¼
        common_queries = []
        
        # åŸºæ–¼å…§å®¹ç”Ÿæˆå¯èƒ½çš„æŸ¥è©¢
        if "æ¬Šåˆ©" in content:
            common_queries.extend(["ä»€éº¼æ˜¯æ¬Šåˆ©", "æ¬Šåˆ©çš„å®šç¾©", "æ¬Šåˆ©ç¯„åœ"])
        if "ç¾©å‹™" in content:
            common_queries.extend(["ä»€éº¼æ˜¯ç¾©å‹™", "ç¾©å‹™çš„è¦å®š", "ç¾©å‹™å…§å®¹"])
        if "è™•ç½°" in content or "é•å" in content:
            common_queries.extend(["é•åå¾Œæœ", "è™•ç½°è¦å®š", "æ³•å¾‹è²¬ä»»"])
        
        return {
            "common_queries": common_queries[:5],  # é™åˆ¶æ•¸é‡
            "similar_articles": [],  # éœ€è¦å¾ŒçºŒè¨ˆç®—
            "semantic_cluster": "è‡ªå‹•èšé¡çµæœ"
        }
    
    def _identify_legal_domain_from_content(self, content: str) -> str:
        """å¾å…§å®¹è­˜åˆ¥æ³•å¾‹é ˜åŸŸ"""
        for domain, keywords in self.legal_domains.items():
            for keyword in keywords:
                if keyword in content:
                    return domain
        return "å…¶ä»–"
    
    def _identify_legal_domain_from_keyword(self, keyword: str) -> str:
        """å¾é—œéµè©è­˜åˆ¥æ³•å¾‹é ˜åŸŸ"""
        for domain, keywords in self.legal_domains.items():
            if keyword in keywords:
                return domain
        return "å…¶ä»–"
    
    def _extract_synonyms_for_concept(self, concept_name: str, content: str) -> List[str]:
        """ç‚ºæ¦‚å¿µæå–åŒç¾©è©"""
        synonyms = []
        
        # å¾åŒç¾©è©å­—å…¸ç²å–
        if concept_name in self.legal_synonyms:
            synonyms.extend(self.legal_synonyms[concept_name])
        
        # å¾å…§å®¹ä¸­æå–åŒç¾©è©æ¨¡å¼
        synonym_patterns = [
            r'([^ï¼Œã€‚ï¼›ï¼š]*)(?:äº¦ç¨±|åˆç¨±|åˆ¥ç¨±|ä¿—ç¨±)([^ï¼Œã€‚ï¼›ï¼š]*)',
            r'([^ï¼Œã€‚ï¼›ï¼š]*)(?:åŒ…æ‹¬|å«|æ¶µè“‹)([^ï¼Œã€‚ï¼›ï¼š]*)',
        ]
        
        for pattern in synonym_patterns:
            matches = re.finditer(pattern, content)
            for match in matches:
                if concept_name in match.group(0):
                    potential_synonyms = [group.strip() for group in match.groups() if group.strip()]
                    synonyms.extend(potential_synonyms)
        
        return list(set(synonyms))
    
    def _deduplicate_concepts(self, concepts: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """å»é‡æ³•å¾‹æ¦‚å¿µ"""
        unique_concepts = []
        seen_names = set()
        
        for concept in concepts:
            concept_name = concept["concept_name"]
            if concept_name not in seen_names:
                seen_names.add(concept_name)
                unique_concepts.append(concept)
        
        return unique_concepts
    
    def _is_article_level_chunk(self, chunk: Dict[str, Any]) -> bool:
        """åˆ¤æ–·æ˜¯å¦ç‚ºã€Œæ¢ã€å±¤ç´šçš„chunk"""
        content = chunk.get("content", "")
        metadata = chunk.get("metadata", {})
        
        # æ–¹æ³•1ï¼šæª¢æŸ¥metadataä¸­çš„å±¤ç´šä¿¡æ¯
        if metadata:
            level = metadata.get("level", "")
            chunk_by = metadata.get("chunk_by", "")
            if level == "basic_unit" or chunk_by == "article":
                return True
        
        # æ–¹æ³•2ï¼šæª¢æŸ¥å…§å®¹ä¸­æ˜¯å¦åŒ…å«æ¢è™Ÿæ¨¡å¼
        article_patterns = [
            r"ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+æ¢",
            r"ç¬¬\d+æ¢",
            r"æ¢æ–‡",
            r"è¦å®š"
        ]
        
        for pattern in article_patterns:
            if re.search(pattern, content):
                return True
        
        # æ–¹æ³•3ï¼šæª¢æŸ¥chunk_idæ˜¯å¦åŒ…å«æ¢å±¤ç´šæ¨™è­˜
        chunk_id = chunk.get("chunk_id", "")
        if "article" in chunk_id.lower() or "æ¢" in chunk_id:
            return True
        
        return False
    
    def _is_chapter_section_level_chunk(self, chunk: Dict[str, Any]) -> bool:
        """åˆ¤æ–·æ˜¯å¦ç‚ºã€Œç« ã€ç¯€ã€å±¤ç´šçš„chunk"""
        content = chunk.get("content", "")
        metadata = chunk.get("metadata", {})
        
        # æ–¹æ³•1ï¼šæª¢æŸ¥metadataä¸­çš„å±¤ç´šä¿¡æ¯
        if metadata:
            level = metadata.get("level", "")
            chunk_by = metadata.get("chunk_by", "")
            if level in ["document_component", "basic_unit_hierarchy"] or chunk_by in ["chapter", "section"]:
                return True
        
        # æ–¹æ³•2ï¼šæª¢æŸ¥å…§å®¹ä¸­æ˜¯å¦åŒ…å«ç« ç¯€æ¨¡å¼
        chapter_section_patterns = [
            r"ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+ç« ",
            r"ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å\d]+ç¯€",
            r"ç« \s*[ï¼š:]",
            r"ç¯€\s*[ï¼š:]",
            r"ç¸½å‰‡|åˆ†å‰‡|é™„å‰‡"
        ]
        
        for pattern in chapter_section_patterns:
            if re.search(pattern, content):
                return True
        
        # æ–¹æ³•3ï¼šæª¢æŸ¥chunk_idæ˜¯å¦åŒ…å«ç« ç¯€å±¤ç´šæ¨™è­˜
        chunk_id = chunk.get("chunk_id", "")
        if any(keyword in chunk_id.lower() for keyword in ["chapter", "section", "ç« ", "ç¯€"]):
            return True
        
        return False
    
    def _extract_article_id(self, chunk: Dict[str, Any]) -> Optional[str]:
        """å¾chunkä¸­æå–æ¢ID"""
        metadata = chunk.get("metadata", {})
        content = chunk.get("content", "")
        
        # å¾metadataæå–
        if metadata:
            law_name = metadata.get("category", "")
            article_number = metadata.get("article_number")
            if law_name and article_number:
                return f"{law_name}_ç¬¬{article_number}æ¢"
        
        # å¾å…§å®¹æå–
        match = re.search(r"ç¬¬(\d+)æ¢", content)
        if match:
            article_num = match.group(1)
            # å˜—è©¦å¾å…§å®¹ä¸­æå–æ³•å
            law_match = re.search(r"([^ã€‚ï¼Œï¼›ï¼š]+æ³•)", content)
            if law_match:
                law_name = law_match.group(1)
                return f"{law_name}_ç¬¬{article_num}æ¢"
        
        return None
    
    def _find_parent_article_id(self, chunk: Dict[str, Any]) -> Optional[str]:
        """ç‚ºéæ¢å±¤ç´šçš„chunkæŸ¥æ‰¾çˆ¶ç´šæ¢ID"""
        metadata = chunk.get("metadata", {})
        content = chunk.get("content", "")
        
        # å¾metadataæŸ¥æ‰¾
        if metadata:
            law_name = metadata.get("category", "")
            article_number = metadata.get("article_number")
            if law_name and article_number:
                return f"{law_name}_ç¬¬{article_number}æ¢"
        
        # å¾å…§å®¹ä¸­æŸ¥æ‰¾æ¢è™Ÿ
        # æŸ¥æ‰¾æœ€è¿‘çš„æ¢è™Ÿï¼ˆå‘ä¸Šæœç´¢ï¼‰
        lines = content.split('\n')
        for line in lines:
            match = re.search(r"ç¬¬(\d+)æ¢", line)
            if match:
                article_num = match.group(1)
                # å˜—è©¦å¾ä¸Šä¸‹æ–‡æå–æ³•å
                law_match = re.search(r"([^ã€‚ï¼Œï¼›ï¼š]+æ³•)", content)
                if law_match:
                    law_name = law_match.group(1)
                    return f"{law_name}_ç¬¬{article_num}æ¢"
        
        return None
    
    def _enhance_article_level_chunk(self, content: str, original_metadata: Dict[str, Any]) -> Dict[str, Any]:
        """ç‚ºã€Œæ¢ã€å±¤ç´šchunké€²è¡Œå®Œæ•´metadataå¢å¼·"""
        # æ¸…ç†åŸå§‹metadata
        cleaned_metadata = self._clean_metadata(original_metadata)
        
        # æå–å„ç¨®å¢å¼·ä¿¡æ¯
        enhanced = {
            **cleaned_metadata,
            "legal_concepts": self._extract_legal_concepts(content),
            "semantic_keywords": self._extract_semantic_keywords(content),
            "article_type": self._classify_article_type(content),
            "legal_domain": self._classify_legal_domain(content),
            "legal_relations": self._extract_legal_relations(content),
            "query_intent_tags": self._extract_query_intent_tags(content),
            "semantic_similarity": self._precompute_semantic_similarity(content),
            "enhancement_level": "full",  # æ¨™è¨˜ç‚ºå®Œæ•´å¢å¼·
            "is_article_level": True
        }
        
        return enhanced
    
    def _enhance_chapter_section_chunk(self, content: str, original_metadata: Dict[str, Any]) -> Dict[str, Any]:
        """ç‚ºã€Œç« ã€ç¯€ã€å±¤ç´šchunké€²è¡Œä¸­ç­‰å¼·åº¦metadataå¢å¼·"""
        # æ¸…ç†åŸå§‹metadata
        cleaned_metadata = self._clean_metadata(original_metadata)
        
        # é€²è¡Œä¸­ç­‰å¼·åº¦çš„metadataå¢å¼·
        enhanced = {
            **cleaned_metadata,
            "semantic_keywords": self._extract_chapter_section_keywords(content),
            "legal_domain": self._classify_legal_domain(content),
            "chapter_section_type": self._classify_chapter_section_type(content),
            "legal_concepts": self._extract_chapter_section_concepts(content),
            "scope_keywords": self._extract_scope_keywords(content),
            "enhancement_level": "medium",  # æ¨™è¨˜ç‚ºä¸­ç­‰å¼·åº¦å¢å¼·
            "is_article_level": False,
            "is_chapter_section_level": True
        }
        
        return enhanced
    
    def _enhance_lightweight_chunk(self, content: str, original_metadata: Dict[str, Any]) -> Dict[str, Any]:
        """ç‚ºéæ¢å±¤ç´šchunké€²è¡Œè¼•é‡ç´šmetadataå¢å¼·"""
        # æ¸…ç†åŸå§‹metadata
        cleaned_metadata = self._clean_metadata(original_metadata)
        
        # åªé€²è¡ŒåŸºæœ¬çš„é—œéµè©æå–
        enhanced = {
            **cleaned_metadata,
            "semantic_keywords": self._extract_basic_keywords(content),
            "legal_domain": self._classify_basic_legal_domain(content),
            "enhancement_level": "lightweight",  # æ¨™è¨˜ç‚ºè¼•é‡ç´šå¢å¼·
            "is_article_level": False,
            "is_chapter_section_level": False
        }
        
        return enhanced
    
    def _extract_basic_keywords(self, content: str) -> Dict[str, Any]:
        """æå–åŸºæœ¬é—œéµè©ï¼ˆè¼•é‡ç´šç‰ˆæœ¬ï¼‰"""
        # ä½¿ç”¨jiebaåˆ†è©
        if jieba:
            words = jieba.analyse.extract_tags(content, topK=10, withWeight=True)
        else:
            words = []
        
        keyword_weights = {word: weight for word, weight in words}
        
        return {
            "primary_keywords": [word for word, weight in words if weight > 0.1],
            "keyword_weights": keyword_weights
        }
    
    def _extract_chapter_section_keywords(self, content: str) -> Dict[str, Any]:
        """æå–ç« ã€ç¯€å±¤ç´šçš„é—œéµè©"""
        # ä½¿ç”¨jiebaåˆ†è©
        if jieba:
            words = jieba.analyse.extract_tags(content, topK=15, withWeight=True)
        else:
            words = []
        
        # åˆ†é¡é—œéµè©
        structural_keywords = ["ç« ", "ç¯€", "ç¸½å‰‡", "åˆ†å‰‡", "é™„å‰‡", "è¦å®š", "åŸå‰‡"]
        legal_terms = ["æ¬Šåˆ©", "ç¾©å‹™", "è²¬ä»»", "è™•ç½°", "ç¨‹åº", "é©ç”¨"]
        scope_keywords = ["ç¯„åœ", "å®šç¾©", "åŸå‰‡", "ä¸€èˆ¬", "ç‰¹æ®Š", "ä¾‹å¤–"]
        
        primary_keywords = []
        structural_keywords_list = []
        legal_terms_list = []
        keyword_weights = {}
        
        for word, weight in words:
            keyword_weights[word] = weight
            
            if word in structural_keywords:
                structural_keywords_list.append(word)
            elif word in legal_terms:
                legal_terms_list.append(word)
            elif weight > 0.1:
                primary_keywords.append(word)
        
        return {
            "primary_keywords": primary_keywords,
            "structural_keywords": structural_keywords_list,
            "legal_terms": legal_terms_list,
            "keyword_weights": keyword_weights
        }
    
    def _classify_chapter_section_type(self, content: str) -> Dict[str, Any]:
        """åˆ†é¡ç« ã€ç¯€é¡å‹"""
        chapter_type = "ä¸€èˆ¬ç« ç¯€"
        confidence = 0.5
        
        # æª¢æŸ¥ç‰¹æ®Šç« ç¯€é¡å‹
        if "ç¸½å‰‡" in content or "ä¸€èˆ¬" in content:
            chapter_type = "ç¸½å‰‡æ€§ç« ç¯€"
            confidence = 0.9
        elif "åˆ†å‰‡" in content or "ç‰¹åˆ¥" in content:
            chapter_type = "åˆ†å‰‡æ€§ç« ç¯€"
            confidence = 0.9
        elif "é™„å‰‡" in content or "é™„" in content:
            chapter_type = "é™„å‰‡æ€§ç« ç¯€"
            confidence = 0.9
        elif "ç½°å‰‡" in content or "è™•ç½°" in content:
            chapter_type = "ç½°å‰‡æ€§ç« ç¯€"
            confidence = 0.8
        elif "ç¨‹åº" in content or "æ‰‹çºŒ" in content:
            chapter_type = "ç¨‹åºæ€§ç« ç¯€"
            confidence = 0.8
        
        return {
            "chapter_section_type": chapter_type,
            "type_description": f"å®šç¾©{chapter_type}çš„ç›¸é—œè¦å®š",
            "confidence": confidence
        }
    
    def _extract_chapter_section_concepts(self, content: str) -> List[Dict[str, Any]]:
        """æå–ç« ã€ç¯€å±¤ç´šçš„æ³•å¾‹æ¦‚å¿µï¼ˆç°¡åŒ–ç‰ˆï¼‰"""
        concepts = []
        
        # æå–çµæ§‹æ€§æ¦‚å¿µ
        structural_concepts = {
            "ç¸½å‰‡": {"importance": 0.8, "type": "çµæ§‹æ€§æ¦‚å¿µ"},
            "åˆ†å‰‡": {"importance": 0.8, "type": "çµæ§‹æ€§æ¦‚å¿µ"},
            "é™„å‰‡": {"importance": 0.7, "type": "çµæ§‹æ€§æ¦‚å¿µ"},
            "ç½°å‰‡": {"importance": 0.9, "type": "çµæ§‹æ€§æ¦‚å¿µ"},
            "ç¨‹åº": {"importance": 0.7, "type": "ç¨‹åºæ€§æ¦‚å¿µ"}
        }
        
        for concept, info in structural_concepts.items():
            if concept in content:
                concepts.append({
                    "concept_name": concept,
                    "concept_type": info["type"],
                    "legal_domain": "ç¨‹åºæ³•",
                    "importance_score": info["importance"],
                    "synonyms": [],
                    "confidence": 0.8
                })
        
        return concepts
    
    def _extract_scope_keywords(self, content: str) -> Dict[str, Any]:
        """æå–ç¯„åœé—œéµè©"""
        scope_patterns = {
            "é©ç”¨ç¯„åœ": ["é©ç”¨", "ç¯„åœ", "é©ç”¨æ–¼"],
            "å®šç¾©ç¯„åœ": ["å®šç¾©", "æŒ‡", "è¬‚"],
            "ä¾‹å¤–ç¯„åœ": ["ä¾‹å¤–", "é™¤å¤–", "ä¸é©ç”¨"],
            "ç¨‹åºç¯„åœ": ["ç¨‹åº", "æ‰‹çºŒ", "æ–¹å¼"]
        }
        
        detected_scopes = []
        for scope_type, keywords in scope_patterns.items():
            if any(keyword in content for keyword in keywords):
                detected_scopes.append(scope_type)
        
        return {
            "scope_types": detected_scopes,
            "scope_description": "å®šç¾©é©ç”¨ç¯„åœå’Œé™åˆ¶æ¢ä»¶"
        }
    
    def _classify_basic_legal_domain(self, content: str) -> Dict[str, Any]:
        """åŸºæœ¬æ³•å¾‹é ˜åŸŸåˆ†é¡ï¼ˆè¼•é‡ç´šç‰ˆæœ¬ï¼‰"""
        for domain, keywords in self.legal_domains.items():
            if any(keyword in content for keyword in keywords):
                return {
                    "legal_domain": domain,
                    "confidence": 0.7
                }
        
        return {
            "legal_domain": "å…¶ä»–",
            "confidence": 0.1
        }
    
    def get_enhancement_stats(self) -> Dict[str, Any]:
        """ç²å–å¢å¼·çµ±è¨ˆä¿¡æ¯"""
        return {
            "cache_size": len(self.metadata_cache),
            "legal_domains": len(self.legal_domains),
            "concept_patterns": len(self.legal_concept_patterns),
            "article_type_patterns": len(self.article_type_patterns),
            "legal_synonyms": len(self.legal_synonyms),
            "article_metadata_count": len(self.article_metadata_map),
            "inheritance_relations": len(self.inheritance_hierarchy)
        }
    
    def get_article_metadata_map(self) -> Dict[str, Any]:
        """ç²å–æ¢å±¤ç´šmetadataæ˜ å°„"""
        return self.article_metadata_map.copy()
    
    def get_inheritance_hierarchy(self) -> Dict[str, str]:
        """ç²å–ç¹¼æ‰¿é—œä¿‚æ˜ å°„"""
        return self.inheritance_hierarchy.copy()
