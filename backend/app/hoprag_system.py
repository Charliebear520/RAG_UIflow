"""
HopRAGç³»çµ± - å¾Œè™•ç†å¢å¼·æ¨¡å¼
åŸºæ–¼è«–æ–‡HopRAG: Multi-Hop Reasoning for Logic-Aware Retrieval-Augmented Generation
"""

import json
import numpy as np
import networkx as nx
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass, asdict
from enum import Enum
import asyncio
import time
from datetime import datetime

@dataclass
class PseudoQuery:
    """å½æŸ¥è©¢æ•¸æ“šçµæ§‹"""
    query_id: str
    content: str
    query_type: str  # "incoming" æˆ– "outgoing"
    embedding: Optional[np.ndarray] = None
    similarity_threshold: float = 0.7

@dataclass
class LegalNode:
    """æ³•å¾‹ç¯€é»æ•¸æ“šçµæ§‹"""
    node_id: str
    node_type: str  # "basic_unit" æˆ– "basic_unit_component" (åŸ "article" æˆ– "item")
    content: str
    contextualized_text: str
    law_name: str
    article_number: str
    item_number: Optional[str] = None
    parent_article_id: Optional[str] = None
    metadata: Dict[str, Any] = None
    
    # å½æŸ¥è©¢
    incoming_questions: List[str] = None
    outgoing_questions: List[str] = None
    pseudo_queries: Dict[str, List[PseudoQuery]] = None
    
    # åœ–çµæ§‹ç›¸é—œ
    outgoing_edges: List[str] = None
    incoming_edges: List[str] = None

    def __post_init__(self):
        if self.metadata is None:
            self.metadata = {}
        if self.incoming_questions is None:
            self.incoming_questions = []
        if self.outgoing_questions is None:
            self.outgoing_questions = []
        if self.pseudo_queries is None:
            self.pseudo_queries = {"incoming": [], "outgoing": []}
        if self.outgoing_edges is None:
            self.outgoing_edges = []
        if self.incoming_edges is None:
            self.incoming_edges = []

class PseudoQueryGenerator:
    """å½æŸ¥è©¢ç”Ÿæˆå™¨"""
    
    def __init__(self, llm_client):
        self.llm_client = llm_client
        
    async def generate_pseudo_queries_for_node(self, node: LegalNode) -> LegalNode:
        """ç‚ºå–®å€‹ç¯€é»ç”Ÿæˆå…§å‘å’Œå¤–å‘å•é¡Œ"""
        print(f"ğŸ” ç‚ºç¯€é» {node.node_id} ç”Ÿæˆå½æŸ¥è©¢...")
        
        # ç”Ÿæˆå…§å‘å•é¡Œ
        incoming_questions = await self._generate_incoming_questions(node)
        
        # ç”Ÿæˆå¤–å‘å•é¡Œ
        outgoing_questions = await self._generate_outgoing_questions(node)
        
        # æ›´æ–°ç¯€é»
        node.incoming_questions = incoming_questions
        node.outgoing_questions = outgoing_questions
        
        # å‰µå»ºPseudoQueryå°è±¡
        node.pseudo_queries = {
            "incoming": [
                PseudoQuery(
                    query_id=f"{node.node_id}_in_{i}",
                    content=question,
                    query_type="incoming"
                ) for i, question in enumerate(incoming_questions)
            ],
            "outgoing": [
                PseudoQuery(
                    query_id=f"{node.node_id}_out_{i}",
                    content=question,
                    query_type="outgoing"
                ) for i, question in enumerate(outgoing_questions)
            ]
        }
        
        print(f"âœ… ç¯€é» {node.node_id} å½æŸ¥è©¢ç”Ÿæˆå®Œæˆï¼š{len(incoming_questions)}å€‹å…§å‘ï¼Œ{len(outgoing_questions)}å€‹å¤–å‘")
        return node
    
    async def _generate_incoming_questions(self, node: LegalNode) -> List[str]:
        """ç”Ÿæˆå…§å‘å•é¡Œ - å¯ä»¥ç›´æ¥å¾è©²æ–‡æœ¬ä¸­æ‰¾åˆ°ç­”æ¡ˆçš„å•é¡Œ"""
        
        prompt = f"""
æ‚¨æ˜¯ä¸€ä½æ³•å¾‹å°ˆå®¶ï¼Œéœ€è¦ç‚ºä»¥ä¸‹æ³•å¾‹æ¢æ–‡ç”Ÿæˆã€Œå…§å‘å•é¡Œã€ã€‚

å…§å‘å•é¡Œå®šç¾©ï¼šå¯ä»¥ç›´æ¥å¾é€™æ®µæ–‡æœ¬ä¸­æ‰¾åˆ°å®Œæ•´ç­”æ¡ˆçš„å•é¡Œã€‚

æ³•å¾‹æ¢æ–‡å…§å®¹ï¼š
{node.contextualized_text}

è«‹ç”Ÿæˆ3-5å€‹å…§å‘å•é¡Œï¼Œè¦æ±‚ï¼š
1. å•é¡Œæ‡‰è©²ç›´æ¥å¾ä¸Šè¿°æ¢æ–‡ä¸­æ‰¾åˆ°ç­”æ¡ˆ
2. å•é¡Œæ‡‰è©²æ¶µè“‹æ¢æ–‡çš„ä¸»è¦æ³•å¾‹æ¦‚å¿µå’Œè¦å®š
3. å•é¡Œæ‡‰è©²æ¸…æ™°ã€å…·é«”ï¼Œä¾¿æ–¼ç†è§£
4. å•é¡Œæ‡‰è©²ç”¨ç¹é«”ä¸­æ–‡è¡¨é”
5. å•é¡Œæ‡‰è©²å…·æœ‰æ³•å¾‹å¯¦å‹™æ„ç¾©

è«‹ä»¥JSONæ ¼å¼è¿”å›ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
{{
    "incoming_questions": [
        "å•é¡Œ1",
        "å•é¡Œ2", 
        "å•é¡Œ3",
        "å•é¡Œ4",
        "å•é¡Œ5"
    ]
}}

è«‹ç¢ºä¿JSONæ ¼å¼æ­£ç¢ºï¼Œä¸è¦åŒ…å«ä»»ä½•å…¶ä»–æ–‡å­—ã€‚
"""
        
        try:
            response = await self.llm_client.generate_async(prompt)
            
            # è§£æJSONéŸ¿æ‡‰
            if response.strip().startswith('{'):
                result = json.loads(response.strip())
                questions = result.get('incoming_questions', [])
                
                # é©—è­‰å•é¡Œè³ªé‡
                validated_questions = self._validate_questions(questions, "incoming")
                return validated_questions[:5]  # é™åˆ¶æœ€å¤š5å€‹å•é¡Œ
            else:
                # å¦‚æœéŸ¿æ‡‰ä¸æ˜¯JSONæ ¼å¼ï¼Œå˜—è©¦æå–å•é¡Œ
                questions = self._extract_questions_from_text(response)
                return questions[:5]
                
        except Exception as e:
            print(f"âŒ ç”Ÿæˆå…§å‘å•é¡Œå¤±æ•—: {e}")
            # è¿”å›é»˜èªå•é¡Œ
            return self._generate_default_incoming_questions(node)
    
    async def _generate_outgoing_questions(self, node: LegalNode) -> List[str]:
        """ç”Ÿæˆå¤–å‘å•é¡Œ - ç”±è©²æ–‡æœ¬å¼•ç™¼ä½†éœ€è¦åƒè€ƒå…¶ä»–æ³•æ¢æ‰èƒ½å®Œæ•´å›ç­”çš„å•é¡Œ"""
        
        prompt = f"""
æ‚¨æ˜¯ä¸€ä½æ³•å¾‹å°ˆå®¶ï¼Œéœ€è¦ç‚ºä»¥ä¸‹æ³•å¾‹æ¢æ–‡ç”Ÿæˆã€Œå¤–å‘å•é¡Œã€ã€‚

å¤–å‘å•é¡Œå®šç¾©ï¼šç”±é€™æ®µæ–‡æœ¬å¼•ç™¼ï¼Œä½†éœ€è¦åƒè€ƒå…¶ä»–æ³•æ¢æ‰èƒ½å®Œæ•´å›ç­”çš„å•é¡Œã€‚

æ³•å¾‹æ¢æ–‡å…§å®¹ï¼š
{node.contextualized_text}

è«‹ç”Ÿæˆ3-5å€‹å¤–å‘å•é¡Œï¼Œè¦æ±‚ï¼š
1. å•é¡Œæ‡‰è©²ç”±ä¸Šè¿°æ¢æ–‡å¼•ç™¼ï¼Œä½†ç­”æ¡ˆéœ€è¦åƒè€ƒå…¶ä»–ç›¸é—œæ³•æ¢
2. å•é¡Œæ‡‰è©²å…·æœ‰é‚è¼¯é—œè¯æ€§ï¼Œèƒ½å¤ å¼•å°åˆ°å…¶ä»–ç›¸é—œæ³•å¾‹æ¢æ–‡
3. å•é¡Œæ‡‰è©²æ¶µè“‹æ¢æ–‡çš„å»¶ä¼¸æ³•å¾‹æ¦‚å¿µå’Œå¯¦å‹™æ‡‰ç”¨
4. å•é¡Œæ‡‰è©²ç”¨ç¹é«”ä¸­æ–‡è¡¨é”
5. å•é¡Œæ‡‰è©²å…·æœ‰æ³•å¾‹å¯¦å‹™æ„ç¾©

è«‹ä»¥JSONæ ¼å¼è¿”å›ï¼Œæ ¼å¼å¦‚ä¸‹ï¼š
{{
    "outgoing_questions": [
        "å•é¡Œ1",
        "å•é¡Œ2",
        "å•é¡Œ3", 
        "å•é¡Œ4",
        "å•é¡Œ5"
    ]
}}

è«‹ç¢ºä¿JSONæ ¼å¼æ­£ç¢ºï¼Œä¸è¦åŒ…å«ä»»ä½•å…¶ä»–æ–‡å­—ã€‚
"""
        
        try:
            response = await self.llm_client.generate_async(prompt)
            
            # è§£æJSONéŸ¿æ‡‰
            if response.strip().startswith('{'):
                result = json.loads(response.strip())
                questions = result.get('outgoing_questions', [])
                
                # é©—è­‰å•é¡Œè³ªé‡
                validated_questions = self._validate_questions(questions, "outgoing")
                return validated_questions[:5]  # é™åˆ¶æœ€å¤š5å€‹å•é¡Œ
            else:
                # å¦‚æœéŸ¿æ‡‰ä¸æ˜¯JSONæ ¼å¼ï¼Œå˜—è©¦æå–å•é¡Œ
                questions = self._extract_questions_from_text(response)
                return questions[:5]
                
        except Exception as e:
            print(f"âŒ ç”Ÿæˆå¤–å‘å•é¡Œå¤±æ•—: {e}")
            # è¿”å›é»˜èªå•é¡Œ
            return self._generate_default_outgoing_questions(node)
    
    def _validate_questions(self, questions: List[str], question_type: str) -> List[str]:
        """é©—è­‰å•é¡Œè³ªé‡"""
        validated = []
        
        for question in questions:
            if isinstance(question, str) and len(question.strip()) > 10:
                # åŸºæœ¬é©—è­‰ï¼šå•é¡Œé•·åº¦ã€åŒ…å«å•è™Ÿç­‰
                question = question.strip()
                if question.endswith('ï¼Ÿ') or question.endswith('?'):
                    validated.append(question)
                else:
                    # å¦‚æœæ²’æœ‰å•è™Ÿï¼Œæ·»åŠ ä¸€å€‹
                    validated.append(question + 'ï¼Ÿ')
        
        return validated
    
    def _extract_questions_from_text(self, text: str) -> List[str]:
        """å¾æ–‡æœ¬ä¸­æå–å•é¡Œ"""
        questions = []
        lines = text.split('\n')
        
        for line in lines:
            line = line.strip()
            if ('ï¼Ÿ' in line or '?' in line) and len(line) > 10:
                # æ¸…ç†è¡Œå…§å®¹
                line = line.replace('- ', '').replace('* ', '').replace('1. ', '').replace('2. ', '').replace('3. ', '')
                questions.append(line)
        
        return questions[:5]
    
    def _generate_default_incoming_questions(self, node: LegalNode) -> List[str]:
        """ç”Ÿæˆé»˜èªå…§å‘å•é¡Œ"""
        article_num = node.article_number
        
        return [
            f"{article_num}çš„ä¸»è¦å…§å®¹æ˜¯ä»€éº¼ï¼Ÿ",
            f"{article_num}è¦å®šäº†å“ªäº›æ³•å¾‹è¦ä»¶ï¼Ÿ",
            f"æ ¹æ“š{article_num}ï¼Œç›¸é—œçš„å®šç¾©æ˜¯ä»€éº¼ï¼Ÿ",
            f"{article_num}çš„é©ç”¨ç¯„åœæ˜¯ä»€éº¼ï¼Ÿ",
            f"{article_num}è¦å®šäº†ä»€éº¼æ³•å¾‹å¾Œæœï¼Ÿ"
        ]
    
    def _generate_default_outgoing_questions(self, node: LegalNode) -> List[str]:
        """ç”Ÿæˆé»˜èªå¤–å‘å•é¡Œ"""
        article_num = node.article_number
        
        return [
            f"é•å{article_num}æœƒæœ‰ä»€éº¼æ³•å¾‹å¾Œæœï¼Ÿ",
            f"å¦‚ä½•ç”³è«‹{article_num}è¦å®šçš„æ¬Šåˆ©ï¼Ÿ",
            f"{article_num}èˆ‡å…¶ä»–æ³•æ¢æœ‰ä»€éº¼é—œè¯ï¼Ÿ",
            f"åœ¨ä»€éº¼æƒ…æ³ä¸‹é©ç”¨{article_num}ï¼Ÿ",
            f"{article_num}çš„å¯¦å‹™æ“ä½œç¨‹åºæ˜¯ä»€éº¼ï¼Ÿ"
        ]

class HopRAGGraphDatabase:
    """HopRAGåœ–æ•¸æ“šåº« - åŸºæ–¼NetworkX"""
    
    def __init__(self):
        self.graph = nx.DiGraph()
        self.nodes: Dict[str, LegalNode] = {}
        self.embedding_model = None
        self.similarity_threshold = 0.7
        self.max_edges_per_node = 10
        
    def add_node(self, node: LegalNode):
        """æ·»åŠ ç¯€é»åˆ°åœ–æ•¸æ“šåº«"""
        self.nodes[node.node_id] = node
        
        # æ·»åŠ åˆ°NetworkXåœ–
        node_attrs = {
            'node_type': node.node_type,
            'content': node.content,
            'contextualized_text': node.contextualized_text,
            'law_name': node.law_name,
            'article_number': node.article_number,
            'item_number': node.item_number,
            'parent_article_id': node.parent_article_id,
            'metadata': node.metadata,
            'incoming_questions': node.incoming_questions,
            'outgoing_questions': node.outgoing_questions
        }
        
        self.graph.add_node(node.node_id, **node_attrs)
        
    def set_embedding_model(self, embedding_model):
        """è¨­ç½®embeddingæ¨¡å‹"""
        self.embedding_model = embedding_model
        
    async def build_graph_edges(self):
        """æ§‹å»ºåœ–é‚Š - é‚ŠåŒ¹é…ç®—æ³•"""
        print("ğŸ”— é–‹å§‹æ§‹å»ºHopRAGåœ–é‚Š...")
        
        # Step 1: ç‚ºæ‰€æœ‰å½æŸ¥è©¢ç”Ÿæˆembedding
        await self._generate_pseudo_query_embeddings()
        
        # Step 2: åŸ·è¡Œé‚ŠåŒ¹é…ç®—æ³•
        await self._perform_edge_matching()
        
        # Step 3: çµ±è¨ˆå’Œé©—è­‰
        self._validate_graph_structure()
        
        print(f"âœ… åœ–é‚Šæ§‹å»ºå®Œæˆï¼ç¯€é»æ•¸: {self.graph.number_of_nodes()}, é‚Šæ•¸: {self.graph.number_of_edges()}")
    
    async def _generate_pseudo_query_embeddings(self):
        """ç‚ºæ‰€æœ‰å½æŸ¥è©¢ç”Ÿæˆembedding"""
        print("ğŸ“Š ç”Ÿæˆå½æŸ¥è©¢embeddingå‘é‡...")
        
        all_queries = []
        query_mapping = {}
        
        # æ”¶é›†æ‰€æœ‰å½æŸ¥è©¢
        for node_id, node in self.nodes.items():
            for pseudo_query in node.pseudo_queries.get("outgoing", []):
                all_queries.append(pseudo_query.content)
                query_mapping[pseudo_query.content] = (node_id, "outgoing", pseudo_query)
                
            for pseudo_query in node.pseudo_queries.get("incoming", []):
                all_queries.append(pseudo_query.content)
                query_mapping[pseudo_query.content] = (node_id, "incoming", pseudo_query)
        
        if not all_queries:
            print("âš ï¸ æ²’æœ‰å½æŸ¥è©¢éœ€è¦ç”Ÿæˆembedding")
            return
        
        # æ‰¹é‡ç”Ÿæˆembedding
        try:
            if hasattr(self.embedding_model, 'encode'):
                embeddings = self.embedding_model.encode(all_queries)
            else:
                # å¦‚æœæ˜¯ç•°æ­¥æ–¹æ³•
                embeddings = await self.embedding_model.encode_async(all_queries)
            
            # å°‡embeddingåˆ†é…å›å½æŸ¥è©¢
            for i, query_content in enumerate(all_queries):
                node_id, query_type, pseudo_query = query_mapping[query_content]
                pseudo_query.embedding = embeddings[i]
                
        except Exception as e:
            print(f"âŒ ç”Ÿæˆå½æŸ¥è©¢embeddingå¤±æ•—: {e}")
    
    async def _perform_edge_matching(self):
        """åŸ·è¡Œé‚ŠåŒ¹é…ç®—æ³•"""
        print("ğŸ”— åŸ·è¡Œé‚ŠåŒ¹é…ç®—æ³•...")
        
        node_ids = list(self.nodes.keys())
        edge_count = 0
        
        for i, node_a in enumerate(node_ids):
            outgoing_queries_a = self.nodes[node_a].pseudo_queries.get("outgoing", [])
            
            for j, node_b in enumerate(node_ids):
                if i == j:
                    continue
                    
                incoming_queries_b = self.nodes[node_b].pseudo_queries.get("incoming", [])
                
                # è¨ˆç®—æœ€ä½³ç›¸ä¼¼åº¦
                best_similarity = 0
                best_outgoing_query = None
                best_incoming_query = None
                
                for out_query in outgoing_queries_a:
                    for in_query in incoming_queries_b:
                        if out_query.embedding is not None and in_query.embedding is not None:
                            similarity = self._calculate_similarity(out_query, in_query)
                            
                            if similarity > best_similarity:
                                best_similarity = similarity
                                best_outgoing_query = out_query
                                best_incoming_query = in_query
                
                # å¦‚æœç›¸ä¼¼åº¦è¶…éé–¾å€¼ï¼Œå»ºç«‹é‚Š
                if best_similarity >= self.similarity_threshold:
                    edge_attr = {
                        'pseudo_query': f"{best_outgoing_query.content} -> {best_incoming_query.content}",
                        'similarity_score': best_similarity,
                        'outgoing_query_id': best_outgoing_query.query_id,
                        'incoming_query_id': best_incoming_query.query_id,
                        'edge_type': self._determine_edge_type(node_a, node_b)
                    }
                    
                    self.graph.add_edge(node_a, node_b, **edge_attr)
                    edge_count += 1
                    
                    # é™åˆ¶æ¯å€‹ç¯€é»çš„æœ€å¤§å‡ºé‚Šæ•¸é‡
                    out_degree = self.graph.out_degree(node_a)
                    if out_degree >= self.max_edges_per_node:
                        break
            
            if i % 10 == 0:
                print(f"  è™•ç†é€²åº¦: {i+1}/{len(node_ids)} ç¯€é»")
        
        print(f"  é‚ŠåŒ¹é…å®Œæˆï¼Œå…±å»ºç«‹ {edge_count} æ¢é‚Š")
    
    def _calculate_similarity(self, query_a: PseudoQuery, query_b: PseudoQuery) -> float:
        """è¨ˆç®—å…©å€‹å½æŸ¥è©¢çš„ç›¸ä¼¼åº¦"""
        if query_a.embedding is None or query_b.embedding is None:
            return 0.0
        
        # é¤˜å¼¦ç›¸ä¼¼åº¦
        similarity = np.dot(query_a.embedding, query_b.embedding) / (
            np.linalg.norm(query_a.embedding) * np.linalg.norm(query_b.embedding)
        )
        
        return float(similarity)
    
    def _determine_edge_type(self, from_node: str, to_node: str) -> str:
        """ç¢ºå®šé‚Šçš„é¡å‹ - ä½¿ç”¨æ–°çš„å±¤ç´šå‘½å"""
        from_type = self.nodes[from_node].node_type
        to_type = self.nodes[to_node].node_type
        
        if from_type == 'basic_unit' and to_type == 'basic_unit':
            return 'basic_unit_to_basic_unit'
        elif from_type == 'basic_unit' and to_type == 'basic_unit_component':
            return 'basic_unit_to_component'
        elif from_type == 'basic_unit_component' and to_type == 'basic_unit':
            return 'component_to_basic_unit'
        else:
            return 'component_to_component'
    
    def _validate_graph_structure(self):
        """é©—è­‰åœ–çµæ§‹"""
        print("ğŸ“Š åœ–çµæ§‹çµ±è¨ˆ:")
        print(f"  ç¯€é»ç¸½æ•¸: {self.graph.number_of_nodes()}")
        print(f"  é‚Šç¸½æ•¸: {self.graph.number_of_edges()}")
        
        # çµ±è¨ˆç¯€é»é¡å‹
        basic_unit_count = sum(1 for node in self.nodes.values() if node.node_type == 'basic_unit')
        component_count = sum(1 for node in self.nodes.values() if node.node_type == 'basic_unit_component')
        print(f"  åŸºæœ¬å–®å…ƒç¯€é» (basic_unit): {basic_unit_count}")
        print(f"  åŸºæœ¬å–®å…ƒçµ„ä»¶ç¯€é» (basic_unit_component): {component_count}")
        
        # çµ±è¨ˆé‚Šé¡å‹
        edge_types = {}
        for from_node, to_node, data in self.graph.edges(data=True):
            edge_type = data.get('edge_type', 'unknown')
            edge_types[edge_type] = edge_types.get(edge_type, 0) + 1
        
        print("  é‚Šé¡å‹åˆ†å¸ƒ:")
        for edge_type, count in edge_types.items():
            print(f"    {edge_type}: {count}")
    
    def get_neighbors_with_edges(self, node_id: str) -> List[Dict[str, Any]]:
        """ç²å–ç¯€é»çš„é„°å±…åŠå…¶é‚Šä¿¡æ¯"""
        neighbors = []
        
        if node_id in self.graph:
            for neighbor_id in self.graph.successors(node_id):
                edge_data = self.graph[node_id][neighbor_id]
                neighbor_data = self.graph.nodes[neighbor_id]
                
                neighbors.append({
                    'node_id': neighbor_id,
                    'node_type': neighbor_data['node_type'],
                    'content': neighbor_data['content'],
                    'contextualized_text': neighbor_data['contextualized_text'],
                    'pseudo_query': edge_data.get('pseudo_query', ''),
                    'similarity_score': edge_data.get('similarity_score', 0.0),
                    'edge_type': edge_data.get('edge_type', '')
                })
        
        return neighbors
    
    def get_node_count(self) -> int:
        """ç²å–ç¯€é»æ•¸é‡"""
        return self.graph.number_of_nodes()
    
    def get_edge_count(self) -> int:
        """ç²å–é‚Šæ•¸é‡"""
        return self.graph.number_of_edges()
    
    def is_graph_built(self) -> bool:
        """æª¢æŸ¥åœ–æ˜¯å¦å·²æ§‹å»º"""
        return self.graph.number_of_nodes() > 0 and self.graph.number_of_edges() > 0

class HopRAGTraversalEngine:
    """HopRAGåœ–éæ­·æª¢ç´¢å¼•æ“"""
    
    def __init__(self, graph_db: HopRAGGraphDatabase, llm_client):
        self.graph_db = graph_db
        self.llm_client = llm_client
        self.max_hops = 4
        self.top_k_per_hop = 20
        
    async def traverse_with_reasoning(self, query: str, initial_nodes: List[str], max_hops: int = None) -> Dict[int, List[str]]:
        """åŸºæ–¼æ¨ç†çš„åœ–éæ­·"""
        if max_hops is None:
            max_hops = self.max_hops
            
        print(f"ğŸ” é–‹å§‹HopRAGæª¢ç´¢ï¼ŒæŸ¥è©¢: '{query}'")
        
        hop_results = {0: initial_nodes}
        current_nodes = initial_nodes
        
        for hop in range(1, max_hops + 1):
            print(f"  ç¬¬ {hop} è·³æª¢ç´¢...")
            next_nodes = []
            
            for node_id in current_nodes:
                # ç²å–é„°å±…ç¯€é»
                neighbors = self.graph_db.get_neighbors_with_edges(node_id)
                
                # ä½¿ç”¨LLMæ¨ç†åˆ¤æ–·ç›¸é—œæ€§
                relevant_neighbors = await self._filter_by_llm_reasoning(
                    query, node_id, neighbors
                )
                
                next_nodes.extend(relevant_neighbors)
            
            # å»é‡å’Œé™åˆ¶æ•¸é‡
            next_nodes = list(set(next_nodes))[:self.top_k_per_hop]
            hop_results[hop] = next_nodes
            current_nodes = next_nodes
            
            if not current_nodes:
                break
        
        return hop_results
    
    async def _filter_by_llm_reasoning(self, query: str, current_node: str, neighbors: List[Dict[str, Any]]) -> List[str]:
        """ä½¿ç”¨LLMæ¨ç†éæ¿¾é„°å±…ç¯€é»"""
        relevant_neighbors = []
        
        for neighbor in neighbors:
            # æ§‹å»ºæ¨ç†æç¤º
            reasoning_prompt = self._build_reasoning_prompt(
                query, current_node, neighbor
            )
            
            try:
                # èª¿ç”¨LLMé€²è¡Œæ¨ç†
                decision = await self.llm_client.generate_async(reasoning_prompt)
                
                # è§£ææ±ºç­–
                if self._is_relevant_decision(decision):
                    relevant_neighbors.append(neighbor['node_id'])
                    
            except Exception as e:
                print(f"âŒ LLMæ¨ç†å¤±æ•—: {e}")
                # å¦‚æœLLMå¤±æ•—ï¼ŒåŸºæ–¼ç›¸ä¼¼åº¦åˆ†æ•¸åˆ¤æ–·
                if neighbor.get('similarity_score', 0) > 0.8:
                    relevant_neighbors.append(neighbor['node_id'])
        
        return relevant_neighbors
    
    def _build_reasoning_prompt(self, query: str, current_node: str, neighbor: Dict[str, Any]) -> str:
        """æ§‹å»ºLLMæ¨ç†æç¤º"""
        current_content = self.graph_db.nodes[current_node].contextualized_text
        neighbor_content = neighbor['contextualized_text']
        pseudo_query = neighbor['pseudo_query']
        
        prompt = f"""
æ‚¨æ˜¯ä¸€å€‹æ³•å¾‹å•ç­”æ©Ÿå™¨äººã€‚æˆ‘éœ€è¦è©•ä¼°ä¸€å€‹è¼”åŠ©å•é¡Œæ˜¯å¦èˆ‡ä¸»å•é¡Œç›¸é—œã€‚

ä¸»å•é¡Œ: {query}

ç•¶å‰ç¯€é»å…§å®¹: {current_content}

è¼”åŠ©å•é¡Œ: {pseudo_query}

ç›®æ¨™ç¯€é»å…§å®¹: {neighbor_content}

è«‹è©•ä¼°è¼”åŠ©å•é¡Œèˆ‡ä¸»å•é¡Œçš„ç›¸é—œæ€§ï¼Œåªèƒ½è¿”å›ä»¥ä¸‹ä¸‰ç¨®çµæœä¹‹ä¸€ï¼š
1. "Completely Irrelevant" - å®Œå…¨ç„¡é—œ
2. "Indirectly relevant" - é–“æ¥ç›¸é—œ  
3. "Relevant and Necessary" - ç›¸é—œä¸”å¿…è¦

æ±ºç­–:"""
        
        return prompt
    
    def _is_relevant_decision(self, decision: str) -> bool:
        """åˆ¤æ–·LLMæ±ºç­–æ˜¯å¦è¡¨ç¤ºç›¸é—œ"""
        decision_lower = decision.lower().strip()
        return ("relevant and necessary" in decision_lower or 
                "indirectly relevant" in decision_lower)

class HopRAGSystem:
    """å®Œæ•´çš„HopRAGç³»çµ±"""
    
    def __init__(self, llm_client, embedding_model):
        self.llm_client = llm_client
        self.embedding_model = embedding_model
        
        # åˆå§‹åŒ–çµ„ä»¶
        self.pseudo_query_generator = PseudoQueryGenerator(llm_client)
        self.graph_db = HopRAGGraphDatabase()
        self.graph_db.set_embedding_model(embedding_model)
        self.traversal_engine = HopRAGTraversalEngine(self.graph_db, llm_client)
        
        # ç‹€æ…‹
        self.is_graph_built = False
        
    async def build_graph_from_multi_level_chunks(self, multi_level_chunks: Dict[str, Dict[str, List[Dict]]]):
        """å¾å¤šå±¤æ¬¡chunksæ§‹å»ºHopRAGåœ–"""
        print("ğŸ—ï¸ é–‹å§‹æ§‹å»ºHopRAGåœ–è­œ...")
        
        # Step 1: å‰µå»ºç¯€é»
        await self._create_nodes_from_chunks(multi_level_chunks)
        
        # Step 2: ç”Ÿæˆå½æŸ¥è©¢
        await self._generate_pseudo_queries()
        
        # Step 3: æ§‹å»ºåœ–é‚Š
        await self.graph_db.build_graph_edges()
        
        self.is_graph_built = True
        print("âœ… HopRAGåœ–è­œæ§‹å»ºå®Œæˆï¼")
    
    async def _create_nodes_from_chunks(self, multi_level_chunks: Dict[str, Dict[str, List[Dict]]]):
        """å¾chunkså‰µå»ºç¯€é»"""
        print("ğŸ“ å‰µå»ºç¯€é»...")
        print(f"ğŸ“Š å¤šå±¤æ¬¡åˆ†å¡Šæ•¸æ“šçµæ§‹: {list(multi_level_chunks.keys())}")
        
        for doc_id, levels in multi_level_chunks.items():
            print(f"ğŸ“„ è™•ç†æ–‡æª”: {doc_id}, å±¤æ¬¡: {list(levels.keys())}")
            # è™•ç†æ¢ç´šç¯€é» (basic_unitå±¤æ¬¡)
            if 'basic_unit' in levels:
                print(f"ğŸ“„ è™•ç† {len(levels['basic_unit'])} å€‹basic_unit chunks")
                for chunk_idx, chunk in enumerate(levels['basic_unit']):
                    try:
                        # ç¢ºä¿chunkçµæ§‹å®Œæ•´
                        if not isinstance(chunk, dict):
                            print(f"âŒ Chunk {chunk_idx} ä¸æ˜¯å­—å…¸é¡å‹: {type(chunk)}")
                            continue
                        
                        # ç¢ºä¿metadataå­˜åœ¨
                        if 'metadata' not in chunk:
                            chunk['metadata'] = {}
                        
                        # ç¢ºä¿contentå­˜åœ¨
                        if 'content' not in chunk:
                            print(f"âŒ Chunk {chunk_idx} æ²’æœ‰contentæ¬„ä½")
                            continue
                        
                        # ç”Ÿæˆnode_idï¼Œå¦‚æœmetadataä¸­æ²’æœ‰idæ¬„ä½
                        node_id = chunk['metadata'].get('id', f"{doc_id}_basic_unit_{chunk_idx}")
                    
                        basic_unit_node = LegalNode(
                            node_id=node_id,
                            node_type='basic_unit',
                            content=chunk['content'],
                            contextualized_text=chunk['content'],  # ä½¿ç”¨contentä½œç‚ºcontextualized_text
                            law_name=chunk['metadata'].get('law_name', ''),
                            article_number=chunk['metadata'].get('article_label', ''),
                            metadata=chunk['metadata']
                        )
                        self.graph_db.add_node(basic_unit_node)
                        
                    except Exception as e:
                        print(f"âŒ å‰µå»ºbasic_unitç¯€é»å¤±æ•— (chunk {chunk_idx}): {e}")
                        continue
            
            # è™•ç†é …ç´šç¯€é» (basic_unit_componentå±¤æ¬¡)
            if 'basic_unit_component' in levels:
                print(f"ğŸ“„ è™•ç† {len(levels['basic_unit_component'])} å€‹basic_unit_component chunks")
                for chunk_idx, chunk in enumerate(levels['basic_unit_component']):
                    try:
                        # ç¢ºä¿chunkçµæ§‹å®Œæ•´
                        if not isinstance(chunk, dict):
                            print(f"âŒ Component Chunk {chunk_idx} ä¸æ˜¯å­—å…¸é¡å‹: {type(chunk)}")
                            continue
                        
                        # ç¢ºä¿metadataå­˜åœ¨
                        if 'metadata' not in chunk:
                            chunk['metadata'] = {}
                        
                        # ç¢ºä¿contentå­˜åœ¨
                        if 'content' not in chunk:
                            print(f"âŒ Component Chunk {chunk_idx} æ²’æœ‰contentæ¬„ä½")
                            continue
                        
                        # ç”Ÿæˆnode_idï¼Œå¦‚æœmetadataä¸­æ²’æœ‰idæ¬„ä½
                        node_id = chunk['metadata'].get('id', f"{doc_id}_basic_unit_component_{chunk_idx}")
                        
                        component_node = LegalNode(
                            node_id=node_id,
                            node_type='basic_unit_component',
                            content=chunk['content'],
                            contextualized_text=chunk['content'],
                            law_name=chunk['metadata'].get('law_name', ''),
                            article_number=chunk['metadata'].get('article_label', ''),
                            item_number=chunk['metadata'].get('item_label', ''),
                            parent_article_id=chunk['metadata'].get('parent_article_id'),
                            metadata=chunk['metadata']
                        )
                        self.graph_db.add_node(component_node)
                        
                    except Exception as e:
                        print(f"âŒ å‰µå»ºbasic_unit_componentç¯€é»å¤±æ•— (chunk {chunk_idx}): {e}")
                        continue
        
        print(f"âœ… ç¯€é»å‰µå»ºå®Œæˆï¼Œå…± {self.graph_db.get_node_count()} å€‹ç¯€é»")
    
    async def _generate_pseudo_queries(self):
        """ç‚ºæ‰€æœ‰ç¯€é»ç”Ÿæˆå½æŸ¥è©¢"""
        print("ğŸ¤– ç”Ÿæˆå½æŸ¥è©¢...")
        
        nodes = list(self.graph_db.nodes.values())
        total_nodes = len(nodes)
        
        for i, node in enumerate(nodes):
            try:
                await self.pseudo_query_generator.generate_pseudo_queries_for_node(node)
                
                if (i + 1) % 10 == 0:
                    print(f"  é€²åº¦: {i+1}/{total_nodes} ç¯€é»")
                    
            except Exception as e:
                print(f"âŒ ç¯€é» {node.node_id} å½æŸ¥è©¢ç”Ÿæˆå¤±æ•—: {e}")
                continue
        
        print("âœ… å½æŸ¥è©¢ç”Ÿæˆå®Œæˆï¼")
    
    async def enhanced_retrieve(self, query: str, base_results: List[Dict], k: int = 5) -> List[Dict[str, Any]]:
        """HopRAGå¢å¼·æª¢ç´¢"""
        if not self.is_graph_built:
            print("âš ï¸ HopRAGåœ–è­œæœªæ§‹å»ºï¼Œè¿”å›åŸºç¤çµæœ")
            return base_results[:k]
        
        # å¾åŸºç¤çµæœæå–ç¯€é»ID
        base_node_ids = []
        for result in base_results:
            # å˜—è©¦å¾çµæœä¸­æå–node_id
            node_id = result.get('node_id') or result.get('id') or result.get('chunk_id')
            if node_id and node_id in self.graph_db.nodes:
                base_node_ids.append(node_id)
        
        if not base_node_ids:
            print("âš ï¸ åŸºç¤çµæœä¸­æ²’æœ‰æ‰¾åˆ°æœ‰æ•ˆçš„ç¯€é»ID")
            return base_results[:k]
        
        # HopRAGå¤šè·³æª¢ç´¢
        hop_results = await self.traversal_engine.traverse_with_reasoning(
            query=query,
            initial_nodes=base_node_ids,
            max_hops=4
        )
        
        # åˆä½µçµæœ
        enhanced_results = self._merge_results(base_results, hop_results, query)
        
        return enhanced_results[:k]
    
    def _merge_results(self, base_results: List[Dict], hop_results: Dict[int, List[str]], query: str) -> List[Dict[str, Any]]:
        """åˆä½µåŸºç¤çµæœå’ŒHopRAGçµæœ"""
        enhanced_results = []
        
        # æ·»åŠ åŸºç¤çµæœ
        for result in base_results:
            enhanced_result = result.copy()
            enhanced_result['hop_level'] = 0
            enhanced_result['hop_source'] = 'base_retrieval'
            enhanced_results.append(enhanced_result)
        
        # æ·»åŠ HopRAGçµæœ
        for hop_level, node_ids in hop_results.items():
            if hop_level == 0:  # è·³éåŸºç¤çµæœ
                continue
                
            for node_id in node_ids:
                if node_id in self.graph_db.nodes:
                    node = self.graph_db.nodes[node_id]
                    
                    enhanced_result = {
                        'node_id': node_id,
                        'content': node.content,
                        'contextualized_text': node.contextualized_text,
                        'law_name': node.law_name,
                        'article_number': node.article_number,
                        'item_number': node.item_number,
                        'node_type': node.node_type,
                        'hop_level': hop_level,
                        'hop_source': 'hoprag_traversal',
                        'metadata': node.metadata
                    }
                    
                    enhanced_results.append(enhanced_result)
        
        # å»é‡ï¼ˆåŸºæ–¼node_idï¼‰
        seen_nodes = set()
        unique_results = []
        for result in enhanced_results:
            node_id = result.get('node_id')
            if node_id and node_id not in seen_nodes:
                seen_nodes.add(node_id)
                unique_results.append(result)
        
        return unique_results
    
    def get_graph_statistics(self) -> Dict[str, Any]:
        """ç²å–åœ–çµ±è¨ˆä¿¡æ¯"""
        if not self.is_graph_built:
            return {"error": "åœ–è­œæœªæ§‹å»º"}
        
        return {
            "total_nodes": self.graph_db.get_node_count(),
            "total_edges": self.graph_db.get_edge_count(),
            "basic_unit_nodes": sum(1 for node in self.graph_db.nodes.values() if node.node_type == 'basic_unit'),
            "basic_unit_component_nodes": sum(1 for node in self.graph_db.nodes.values() if node.node_type == 'basic_unit_component'),
            "graph_built": self.is_graph_built,
            "similarity_threshold": self.graph_db.similarity_threshold,
            "max_edges_per_node": self.graph_db.max_edges_per_node
        }
