# 評測修復完成說明

## 問題確認

您反映的問題完全正確：**即使沒有先生成問題，直接點擊開始評測仍然能跑出評估數值**。

經過檢查發現，問題在於前端調用的是 `/api/evaluate/fixed-size` 端點（在 main.py 中），而不是我們之前修復的 `/evaluate` 端點（在 routes.py 中）。

## 根本原因

1. **多個評測端點**：系統中存在兩個評測端點

   - `/api/evaluate/fixed-size` (main.py) - 前端實際調用的端點
   - `/evaluate` (routes.py) - 我們之前修復的端點

2. **預設問題**：main.py 中的評測使用硬編碼的預設問題，而不是基於文檔生成的問題

3. **邏輯不一致**：兩個端點有不同的問題來源邏輯

## 修復內容

### 1. 更新 main.py 中的 DocRecord 類

```python
@dataclass
class DocRecord:
    # ... 其他字段
    generated_questions: Optional[List[str]] = None  # 新增：存儲生成的問題
```

### 2. 修復 main.py 中的評測端點

```python
@app.post("/api/evaluate/fixed-size")
def start_fixed_size_evaluation(req: FixedSizeEvaluationRequest, background_tasks: BackgroundTasks):
    # 檢查是否已有生成的問題
    if not hasattr(doc, 'generated_questions') or not doc.generated_questions:
        return JSONResponse(
            status_code=400,
            content={"error": "請先使用「生成問題」功能為文檔生成測試問題，然後再進行評測"}
        )

    # 使用文檔中存儲的問題而不是預設問題
    req.test_queries = doc.generated_questions
```

### 3. 修復 main.py 中的問題生成端點

```python
@app.post("/api/generate-questions")
def generate_questions(req: GenerateQuestionsRequest):
    # 將生成的問題存儲到文檔記錄中
    question_texts = [q.question for q in questions]
    doc.generated_questions = question_texts
    store.docs[req.doc_id] = doc  # 更新文檔記錄
```

## 修復驗證

### 測試結果

✅ DocRecord 模型正確支持 generated_questions 字段
✅ 檢查邏輯正確識別是否有生成的問題
✅ 問題生成後正確存儲到文檔記錄中

### 預期行為

現在當用戶沒有先生成問題就直接點擊開始評測時，系統會返回錯誤：

```json
{
  "error": "請先使用「生成問題」功能為文檔生成測試問題，然後再進行評測"
}
```

## 正確的評測流程

```
1. 上傳PDF文檔
   ↓
2. 選擇分塊策略並配置參數
   ↓
3. 生成問題（必須步驟）
   ↓
4. 開始評測（基於生成的問題）
```

## 技術細節

### 問題存儲機制

- 問題生成後，問題文本存儲在 `doc.generated_questions` 中
- 評測時使用文檔特定的問題，而不是預設問題
- 確保問題與文檔內容高度相關

### 錯誤處理

- 評測前強制檢查是否有生成的問題
- 提供清晰的錯誤提示指導用戶正確操作
- 確保評測流程的邏輯完整性

## 總結

✅ **問題已完全修復**：現在評測必須基於已生成的問題
✅ **邏輯一致性**：所有評測端點都使用相同的檢查邏輯
✅ **用戶體驗**：提供清晰的錯誤提示和操作指導
✅ **結果準確性**：評測基於文檔特定的問題，結果更可信

感謝您的細心測試和反饋！這個修復讓整個評測系統變得更加嚴謹和可靠。
