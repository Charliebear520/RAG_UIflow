#!/usr/bin/env python3
"""
æ¸¬è©¦æ–°å¢çš„æ³•å¾‹RAGæ”¹é€²åŠŸèƒ½
"""

import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), 'backend'))

def test_legal_semantic_chunking():
    """æ¸¬è©¦æ³•å¾‹èªç¾©å®Œæ•´æ€§åˆ†å¡Š"""
    print("ğŸ” æ¸¬è©¦æ³•å¾‹èªç¾©å®Œæ•´æ€§åˆ†å¡Š...")
    
    try:
        from app.legal_semantic_chunking import LegalSemanticIntegrityChunking
        
        # å‰µå»ºæ¸¬è©¦æ–‡æœ¬
        test_text = """
        ç¬¬28æ¢ è‘—ä½œäººå°ˆæœ‰å°‡å…¶è‘—ä½œæ”¹ä½œæˆè¡ç”Ÿè‘—ä½œæˆ–ç·¨è¼¯æˆç·¨è¼¯è‘—ä½œä¹‹æ¬Šåˆ©ã€‚ä½†è¡¨æ¼”ä¸é©ç”¨ä¹‹ã€‚
        å‰é …è¦å®šï¼Œæ–¼è‘—ä½œäººå°‡å…¶è‘—ä½œä¹‹è‘—ä½œè²¡ç”¢æ¬Šè®“èˆ‡ä»–äººæˆ–æˆæ¬Šä»–äººåˆ©ç”¨å¾Œï¼Œäº¦é©ç”¨ä¹‹ã€‚
        """
        
        # åˆå§‹åŒ–åˆ†å¡Šå™¨
        chunker = LegalSemanticIntegrityChunking()
        
        # åŸ·è¡Œåˆ†å¡Š
        chunks = chunker.chunk(test_text, max_chunk_size=500, preserve_concepts=True)
        
        print(f"âœ… æ³•å¾‹èªç¾©åˆ†å¡ŠæˆåŠŸï¼")
        print(f"   - åˆ†å¡Šæ•¸é‡: {len(chunks)}")
        for i, chunk in enumerate(chunks):
            print(f"   - åˆ†å¡Š {i+1}: {chunk['content'][:100]}...")
            print(f"     æ¦‚å¿µå¯†åº¦: {chunk['metadata']['concept_density']:.3f}")
            print(f"     é‡è¦æ€§åˆ†æ•¸: {chunk['metadata']['importance_score']:.3f}")
        
        return True
        
    except Exception as e:
        print(f"âŒ æ³•å¾‹èªç¾©åˆ†å¡Šæ¸¬è©¦å¤±æ•—: {e}")
        return False

def test_concept_graph():
    """æ¸¬è©¦æ¦‚å¿µåœ–æ§‹å»º"""
    print("\nğŸ” æ¸¬è©¦æ³•å¾‹æ¦‚å¿µåœ–æ§‹å»º...")
    
    try:
        from app.legal_concept_graph import LegalConceptGraph
        
        # å‰µå»ºæ¸¬è©¦æ–‡æª”
        test_documents = [
            {
                'content': 'è‘—ä½œäººå°ˆæœ‰é‡è£½æ¬Šï¼Œå³æŒ‡ä»¥å°åˆ·ã€è¤‡å°ã€éŒ„éŸ³ã€éŒ„å½±ã€æ”å½±ã€ç­†éŒ„æˆ–å…¶ä»–æ–¹æ³•ç›´æ¥ã€é–“æ¥ã€æ°¸ä¹…æˆ–æš«æ™‚ä¹‹é‡è¤‡è£½ä½œã€‚',
                'doc_id': 'test_doc_1',
                'chunk_index': 0,
                'filename': 'copyright_test.txt'
            },
            {
                'content': 'å•†æ¨™æ¬Šäººæ–¼è¨»å†ŠæŒ‡å®šä¹‹å•†å“æˆ–æœå‹™ï¼Œå–å¾—å•†æ¨™æ¬Šã€‚ä½†å•†æ¨™æ¬Šäººä¸å¾—ç¦æ­¢ä»–äººä»¥ç¬¦åˆå•†æ¥­äº¤æ˜“ç¿’æ…£ä¹‹èª å¯¦ä¿¡ç”¨æ–¹æ³•ï¼Œä½¿ç”¨è‡ªå·±ä¹‹å§“åæˆ–åç¨±ã€‚',
                'doc_id': 'test_doc_2', 
                'chunk_index': 0,
                'filename': 'trademark_test.txt'
            }
        ]
        
        # åˆå§‹åŒ–æ¦‚å¿µåœ–
        concept_graph = LegalConceptGraph()
        
        # æ§‹å»ºæ¦‚å¿µåœ–
        concept_graph.build_graph(test_documents)
        
        print(f"âœ… æ¦‚å¿µåœ–æ§‹å»ºæˆåŠŸï¼")
        print(f"   - ç¯€é»æ•¸é‡: {concept_graph.graph.number_of_nodes()}")
        print(f"   - é‚Šæ•¸é‡: {concept_graph.graph.number_of_edges()}")
        print(f"   - æ¦‚å¿µæ•¸é‡: {len(concept_graph.concepts)}")
        print(f"   - é—œä¿‚æ•¸é‡: {len(concept_graph.relations)}")
        
        # é¡¯ç¤ºéƒ¨åˆ†æ¦‚å¿µ
        print("   - æ¦‚å¿µç¤ºä¾‹:")
        for i, (concept_id, concept) in enumerate(list(concept_graph.concepts.items())[:3]):
            print(f"     {i+1}. {concept.concept_name} ({concept.concept_type})")
        
        return True
        
    except Exception as e:
        print(f"âŒ æ¦‚å¿µåœ–æ§‹å»ºæ¸¬è©¦å¤±æ•—: {e}")
        return False

def test_adaptive_rag():
    """æ¸¬è©¦è‡ªé©æ‡‰æª¢ç´¢"""
    print("\nğŸ” æ¸¬è©¦è‡ªé©æ‡‰æª¢ç´¢...")
    
    try:
        from app.adaptive_legal_rag import QueryAnalyzer, QueryType
        
        # åˆå§‹åŒ–æŸ¥è©¢åˆ†æå™¨
        analyzer = QueryAnalyzer()
        
        # æ¸¬è©¦ä¸åŒé¡å‹çš„æŸ¥è©¢
        test_queries = [
            "ä»€éº¼æ˜¯è‘—ä½œæ¬Šï¼Ÿ",
            "è‘—ä½œæ¬Šæ³•ç¬¬28æ¢è¦å®šä»€éº¼ï¼Ÿ",
            "å¦‚ä½•ç”³è«‹å•†æ¨™è¨»å†Šï¼Ÿ",
            "è‘—ä½œæ¬Šèˆ‡å•†æ¨™æ¬Šçš„å·®åˆ¥æ˜¯ä»€éº¼ï¼Ÿ"
        ]
        
        print(f"âœ… è‡ªé©æ‡‰æª¢ç´¢æ¸¬è©¦æˆåŠŸï¼")
        for query in test_queries:
            analysis = analyzer.analyze_query(query)
            print(f"   - æŸ¥è©¢: '{query}'")
            print(f"     é¡å‹: {analysis.query_type.value}")
            print(f"     ç½®ä¿¡åº¦: {analysis.confidence:.3f}")
            print(f"     è¤‡é›œåº¦: {analysis.complexity_score:.3f}")
            print(f"     æ¨è–¦ç­–ç•¥: {analysis.recommended_strategies}")
        
        return True
        
    except Exception as e:
        print(f"âŒ è‡ªé©æ‡‰æª¢ç´¢æ¸¬è©¦å¤±æ•—: {e}")
        return False

def test_enhanced_api():
    """æ¸¬è©¦å¢å¼·ç‰ˆAPIç«¯é»"""
    print("\nğŸ” æ¸¬è©¦å¢å¼·ç‰ˆAPIç«¯é»...")
    
    try:
        # æª¢æŸ¥APIç«¯é»å®šç¾©
        with open('backend/app/enhanced_main.py', 'r', encoding='utf-8') as f:
            content = f.read()
            
        api_endpoints = [
            '/api/legal-semantic-chunk',
            '/api/multi-level-semantic-chunk', 
            '/api/build-concept-graph',
            '/api/concept-graph-retrieve',
            '/api/adaptive-retrieve'
        ]
        
        print(f"âœ… APIç«¯é»æª¢æŸ¥æˆåŠŸï¼")
        for endpoint in api_endpoints:
            if endpoint in content:
                print(f"   âœ“ {endpoint}")
            else:
                print(f"   âœ— {endpoint} (ç¼ºå¤±)")
        
        return True
        
    except Exception as e:
        print(f"âŒ APIç«¯é»æ¸¬è©¦å¤±æ•—: {e}")
        return False

def main():
    """ä¸»æ¸¬è©¦å‡½æ•¸"""
    print("ğŸš€ é–‹å§‹æ¸¬è©¦æ³•å¾‹RAGç³»çµ±æ”¹é€²åŠŸèƒ½...\n")
    
    test_results = []
    
    # åŸ·è¡Œå„é …æ¸¬è©¦
    test_results.append(("æ³•å¾‹èªç¾©åˆ†å¡Š", test_legal_semantic_chunking()))
    test_results.append(("æ¦‚å¿µåœ–æ§‹å»º", test_concept_graph()))
    test_results.append(("è‡ªé©æ‡‰æª¢ç´¢", test_adaptive_rag()))
    test_results.append(("APIç«¯é»", test_enhanced_api()))
    
    # é¡¯ç¤ºæ¸¬è©¦çµæœ
    print("\n" + "="*50)
    print("ğŸ“Š æ¸¬è©¦çµæœç¸½çµ")
    print("="*50)
    
    passed = 0
    total = len(test_results)
    
    for test_name, result in test_results:
        status = "âœ… é€šé" if result else "âŒ å¤±æ•—"
        print(f"{test_name}: {status}")
        if result:
            passed += 1
    
    print(f"\nç¸½é«”çµæœ: {passed}/{total} é …æ¸¬è©¦é€šé")
    
    if passed == total:
        print("ğŸ‰ æ‰€æœ‰æ”¹é€²åŠŸèƒ½æ¸¬è©¦é€šéï¼")
    else:
        print("âš ï¸  éƒ¨åˆ†æ¸¬è©¦å¤±æ•—ï¼Œè«‹æª¢æŸ¥ä¾è³´é …å’Œé…ç½®")

if __name__ == "__main__":
    main()
